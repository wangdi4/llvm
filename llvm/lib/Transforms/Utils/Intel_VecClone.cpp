//=---- Intel_VecClone.cpp - Vector function to loop transform -*- C++ -*----=//
//
// Copyright (C) 2015-2019 Intel Corporation. All rights reserved.
//
// The information and source code contained herein is the exclusive property
// of Intel Corporation and may not be disclosed, examined or reproduced in
// whole or in part without explicit written authorization from the company.
//
// Main author:
// ------------
// Matt Masten (C) 2017 [matt.masten@intel.com]
//
// Major revisions:
// ----------------
// May 2015, initial development -- Matt Masten
//
// ===--------------------------------------------------------------------=== //
///
/// \file
/// This pass inserts the body of a vector function inside a vector length
/// trip count scalar loop for functions that are declared SIMD. The pass
/// follows the Intel vector ABI requirements for name mangling encodings.
///
/// Conceptually, this pass performs the following transformation:
///
/// Before Translation:
///
/// main.cpp
///
/// #pragma omp declare simd uniform(a) linear(k)
///
/// extern float dowork(float *a, int k);
///
/// float a[4096];
/// int main() {
///   int k;
/// #pragma clang loop vectorize(enable)
///   for (k = 0; k < 4096; k++) {
///     a[k] = k * 0.5;
///     a[k] = dowork(a, k);
///   }
/// }
///
/// dowork.cpp
///
/// #pragma omp declare simd uniform(a) linear(k) #0
/// float dowork(float *a, int k) {
///   a[k] = sinf(a[k]) + 9.8f;
/// }
///
/// attributes #0 = { nounwind uwtable "_ZGVbM4ul_", "ZGVbN4ul_", ... }
///
/// After Translation:
///
/// dowork.cpp
///
/// // Non-masked variant
///
/// <VL x float> "_ZGVbN4ul_dowork(float *a, int k) {
///   alloc <VL x float> vec_ret;
///   for (int i = k; i < k + VL, i++) {
///     a[i] = sinf(a[i]) + 9.8f;
///   }
///   load vec_ret, a[k:VL]
///   return vec_ret;
/// }
///
/// // Masked variant
///
/// <VL x float> "_ZGVbM4ul_dowork(float *a, int k, <VL x int> mask) {
///   alloc <VL x float> vec_ret;
///   for (int i = k; i < k + VL, i++) {
///     if (mask[i] != 0)
///       a[i] = sinf(a[i]) + 9.8f;
///   }
///   load vec_ret, a[k:VL]
///   return vec_ret;
/// }
///
///
///
// ===--------------------------------------------------------------------=== //

// This pass is flexible enough to deal with two forms of LLVM IR, namely when
// Mem2Reg has run and when Mem2Reg has not run. Each form requires different
// handling of parameters.
//
// When Mem2Reg has run:
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
//   %add = add nsw i32 %x, %i
//   ret i32 %add
// }
//
// When Mem2Reg has not run:
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
// %i.addr = alloca i32, align 4
// %x.addr = alloca i32, align 4
// store i32 %i, i32* %i.addr, align 4
// store i32 %x, i32* %x.addr, align 4
// %0 = load i32, i32* %x.addr, align 4
// %1 = load i32, i32* %i.addr, align 4
// %add = add nsw i32 %0, %1
//  ret i32 %add
// }
//
// When Mem2Reg has not been run (i.e., parameters have not been registerized),
// we end up with an alloca, store to alloca, and load from alloca sequence.
// When parameters have already been registerized, users of the parameter use
// the parameter directly and not through a load of a new SSA temp. For either
// case, this pass will expand the vector parameters/return to vector types,
// alloca new space for them on the stack, and do an initial store to the
// alloca. Linear and uniform parameters will be used directly, instead of
// through a load instruction.
//
// The function "insertDirectiveIntrinsics" creates a WRN region around the
// for-loop. VPlan works on this region. The beginning of the WRN region is
// marked with @llvm.directive.region.entry() and the end with
// @llvm.directive.region.end(). In the above example, a new basic-block is
// created before loop's header:
//
// simd.begin.region:                                ; preds = %entry
// %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(),
// "QUAL.OMP.UNIFORM"(float* %a), "QUAL.OMP.LINEAR"(i32 %k, i32 1),
// "QUAL.OMP.SIMDLEN"(i32 4) ]
//     br label %simd.loop
//
// and a new basic-block is emitted after loop's latch:
//
// simd.loop.exit:                                   ; preds = %simd.loop
// %indvar = add nuw i32 %index, 1
// %vl.cond = icmp ult i32 %indvar, 4
// br i1 %vl.cond, label %simd.loop, label %simd.end.region
//
// The pass must run at all optimization levels because it is possible that
// a loop calling the vector function is vectorized, but the vector function
// itself is not vectorized. For example, above main.cpp may be compiled at
// -O2, but dowork.cpp may be compiled at -O0. Therefore, it is required that
// the attribute list for the vector function specify all variants that must
// be generated by this pass so as to avoid any linking problems. This pass
// also serves to canonicalize the input IR to the loop vectorizer.

#include "llvm/Transforms/Utils/Intel_VecClone.h"
#include "llvm/Analysis/Directives.h"
#include "llvm/Analysis/GlobalsModRef.h"
#include "llvm/Analysis/Intel_Andersens.h"
#include "llvm/Analysis/Intel_VectorVariant.h"
#include "llvm/Analysis/Passes.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/IR/BasicBlock.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/InitializePasses.h"
#include "llvm/PassRegistry.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include "llvm/Transforms/Utils/GeneralUtils.h"
#include "llvm/Transforms/Utils/IntrinsicUtils.h"
#include <map>
#include <set>
#include <string>

#include "llvm/IR/Verifier.h"

#define SV_NAME "vec-clone"
#define DEBUG_TYPE "VecClone"

using namespace llvm;

// Support for future opaque pointers. Will become the only option in future.
static cl::opt<bool>
    EmitTypedOMP("vec-clone-typed-omp", cl::init(false), cl::Hidden,
                 cl::desc("Emit 'TYPED' version of OMP clauses."));

VecClone::VecClone() : ModulePass(ID) {
  initializeVecClonePass(*PassRegistry::getPassRegistry());
}

bool VecClone::runOnModule(Module &M) { return Impl.runImpl(M, getLimiter()); }

#if INTEL_CUSTOMIZATION
// The following two functions are virtual and they are overloaded when
// VecClone is called by language-specific optimizations. Their default
// implementation is empty.
void VecCloneImpl::handleLanguageSpecifics(Function &F, PHINode *Phi,
                                       Function *Clone,
                                       BasicBlock *EntryBlock,
                                       const VectorVariant &Variant) {}

void VecCloneImpl::languageSpecificInitializations(Module &M) {}
#endif // INTEL_CUSTOMIZATION

Function *VecCloneImpl::CloneFunction(Function &F, VectorVariant &V,
                                      ValueToValueMapTy &VMap) {

  LLVM_DEBUG(dbgs() << "Cloning Function: " << F.getName() << "\n");
  LLVM_DEBUG(F.dump());

  FunctionType* OrigFunctionType = F.getFunctionType();
  Type *CharacteristicType = calcCharacteristicType(F, V);

  std::vector<VectorKind> ParmKinds = V.getParameters();
  SmallVector<Type*, 4> ParmTypes;
  FunctionType::param_iterator ParmIt = OrigFunctionType->param_begin();
  FunctionType::param_iterator ParmEnd = OrigFunctionType->param_end();
  std::vector<VectorKind>::iterator VKIt = ParmKinds.begin();
  for (; ParmIt != ParmEnd; ++ParmIt, ++VKIt) {
    if (VKIt->isVector()) {
      unsigned VF = V.getVlen();
      if (auto *FVT = dyn_cast<FixedVectorType>(*ParmIt))
        VF *= FVT->getNumElements();
      ParmTypes.push_back(FixedVectorType::get((*ParmIt)->getScalarType(), VF));
    } else {
      ParmTypes.push_back(*ParmIt);
    }
  }

  if (V.isMasked()) {
    Type *MaskScalarTy = (Usei1MaskForSimdFunctions) ?
      Type::getInt1Ty(F.getContext()) : CharacteristicType;
    Type *MaskVecTy = FixedVectorType::get(MaskScalarTy, V.getVlen());
    ParmTypes.push_back(MaskVecTy);
  }

  Function* Clone = getOrInsertVectorVariantFunction(&F, V.getVlen(), ParmTypes,
                                                     &V, V.isMasked());

  // Remove vector variant attributes from the original function. They are
  // not needed for the cloned function and it prevents any attempts at
  // trying to clone the function again in case the pass is called more than
  // once.
  AttrBuilder AB;
  for (auto Attr : getVectorVariantAttributes(F)) {
    AB.addAttribute(Attr);
  }

  F.removeFnAttrs(AB);

  // Copy all the attributes from the scalar function to its vector version
  // except for the vector variant attributes.
  Clone->copyAttributesFrom(&F);

  // Remove incompatible argument attributes (applied to the scalar argument,
  // does not apply to its vector counterpart).
  Function::arg_iterator ArgIt = Clone->arg_begin();
  Function::arg_iterator ArgEnd = Clone->arg_end();
  // TODO (Dave Kreitzer): Once we pull down the changes that add the
  //   Function::removeParamAttrs method, we should use it in lieu of
  //   Function::removeAttributes. We just need to change the Idx
  //   initialization here to start at 0.
  for (uint64_t Idx = 1; ArgIt != ArgEnd; ++ArgIt, ++Idx) {
    Type* ArgType = (*ArgIt).getType();
    AB = AttributeFuncs::typeIncompatible(ArgType);
    Clone->removeFnAttrs(AB);
  }

  ArgIt = F.arg_begin();
  ArgEnd = F.arg_end();
  Function::arg_iterator NewArgIt = Clone->arg_begin();
  for (; ArgIt != ArgEnd; ++ArgIt, ++NewArgIt) {
    NewArgIt->setName(ArgIt->getName());
    VMap[&*ArgIt] = &*NewArgIt;
  }

  if (V.isMasked()) {
    Argument &MaskArg = *NewArgIt;
    MaskArg.setName("mask");
  }

  SmallVector<ReturnInst*, 8> Returns;
  CloneFunctionInto(Clone, &F, VMap, CloneFunctionChangeType::LocalChangesOnly,
                    Returns);
  // For some reason, this causes DCE to remove calls to these functions.
  // Disable for now.
  //Clone->setCallingConv(CallingConv::X86_RegCall);

  LLVM_DEBUG(dbgs() << "After Cloning and Parameter/Return Expansion\n");
  LLVM_DEBUG(Clone->dump());

  return Clone;
}

// Checks whether the store of the entry block should be kept in the entry block
// or it should be moved inside the simd loop.
static bool shouldStoreStayInEntryBB(Function *Clone,
                                     std::vector<VectorKind> &ParmKinds,
                                     Instruction *Store) {
  Value *StoreData = Store->getOperand(0);
  Value *StoreAddr = Store->getOperand(1);
  if (!isa<AllocaInst>(StoreAddr))
    return false;

  return llvm::any_of(
      Clone->args(), [StoreData, ParmKinds](const Argument &Arg) {
        VectorKind ArgKind = ParmKinds[Arg.getArgNo()];
        return &Arg == StoreData && (ArgKind.isVector() || ArgKind.isLinear());
      });
}

BasicBlock *VecCloneImpl::splitEntryIntoLoop(Function *Clone, VectorVariant &V,
                                             BasicBlock *EntryBlock) {

  // EntryInsts contains all instructions that need to stay in the entry basic
  // block. These instructions include allocas and stores involving vector and
  // linear parameters to alloca. Linear parameter stores to alloca are kept in
  // the entry block because there will be a load from this alloca in the loop
  // for which we will apply the stride. Instructions involving uniform
  // parameter stores to alloca should be sunk into the loop to maintain
  // uniform behavior. All instructions involving private variables are also
  // sunk into the loop.

  SmallVector<Instruction *, 4> EntryInsts;
  for (auto BBIt = EntryBlock->begin(), BBEnd = EntryBlock->end();
       BBIt != BBEnd; ++BBIt) {
    if (isa<AllocaInst>(BBIt) ||
        (isa<StoreInst>(&*BBIt) &&
         shouldStoreStayInEntryBB(Clone, V.getParameters(), &*BBIt))) {
      // If this is a store of a vector parameter, keep it in the entry block
      // because it will be modified with the vector alloca reference. Since
      // the parameter has already been expanded, this becomes a vector store
      // (i.e., packing instruction) that we do not want to appear in the
      // scalar loop. It is correct to leave linear parameter stores in the
      // entry or move them to the scalar loop, but leaving them in the entry
      // block prevents an additional store inside the loop. If the store does
      // not have a local alloca in the entry block, then is is moved inside the
      // loop. Uniform parameter stores must be moved to the loop body to behave
      // as uniform. Consider the following:
      //
      // __declspec(vector(uniform(x)))
      // int foo(int a, int x) {
      //   x++;
      //   return (a + x);
      // }
      //
      // Assume x = 1 for the call to foo. This implies x = 2 for the vector
      // add. e.g., a[0:VL-1] + <2, 2, 2, 2>. If the initial store of x to the
      // stack is done in the entry block outside of the loop, then x will be
      // incremented by one each time within the loop because the increment of
      // x will reside in the loop. Therefore, if the store of x is sunk into
      // the loop, the initial value of 1 will always be stored to a temp
      // before the increment, resulting in the value of 2 always being
      // computed in the scalar loop.
      EntryInsts.push_back(&*BBIt);

      // Add alloca to SIMD loop private
      if (auto AllocaVal = dyn_cast<AllocaInst>(BBIt))
        PrivateAllocas.insert(AllocaVal);
    }
  }

  BasicBlock *LoopBlock =
      EntryBlock->splitBasicBlock(EntryBlock->begin(), "simd.loop");

  for (auto *Inst : EntryInsts) {
    Inst->removeFromParent();
    Inst->insertBefore(EntryBlock->getTerminator());
  }

  LLVM_DEBUG(dbgs() << "After Entry Block Split\n");
  LLVM_DEBUG(Clone->dump());

  return LoopBlock;
}

BasicBlock *VecCloneImpl::splitLoopIntoReturn(Function *Clone,
                                              BasicBlock *LoopBlock) {
  assert(count_if(*Clone,
                  [](const BasicBlock &BB) {
                    return isa<ReturnInst>(BB.getTerminator());
                  }) <= 1 &&
         "Unsupported CFG for VecClone!");

  auto RetBlockIt = find_if(*Clone, [](const BasicBlock &BB) {
    return isa<ReturnInst>(BB.getTerminator());
                                    });
  if (RetBlockIt == Clone->end())
    return nullptr;

  BasicBlock &RetBlock = *RetBlockIt;
  auto *Return = cast<ReturnInst>(RetBlock.getTerminator());

  Instruction *SplitPt = Return;

  if (&RetBlock == LoopBlock) {
    // If the return is from a preceeding load from alloca, make sure the load
    // is also put in the return block. This is the old scalar load that will
    // end up getting replaced with the vector return and will get cleaned up
    // later.

    // TODO: Why don't we do it for functions that consist from multiple BB?
    // That's probably due to historical reasons when we didn't split the return
    // block for them, which has been fixed since.

    // Make sure this is not a void function before getting the return
    // operand.
    if (!Clone->getReturnType()->isVoidTy()) {
      auto *RetLoad = dyn_cast<LoadInst>(Return->getOperand(0));
      if (RetLoad && isa<AllocaInst>(RetLoad->getPointerOperand()))
        SplitPt = RetLoad;
    }
  }

  return RetBlock.splitBasicBlock(SplitPt, "return");
}

PHINode *VecCloneImpl::createPhiAndBackedgeForLoop(
    Function *Clone, BasicBlock *EntryBlock, BasicBlock *LoopBlock,
    BasicBlock *LoopExitBlock, BasicBlock *ReturnBlock, int VectorLength) {
  // Create the phi node for the top of the loop block and add the back
  // edge to the loop from the loop exit.

  PHINode *Phi = PHINode::Create(Type::getInt32Ty(Clone->getContext()), 2,
                                 "index", &*LoopBlock->getFirstInsertionPt());

  Constant *Inc = ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 1);
  Constant *IndInit = ConstantInt::get(Type::getInt32Ty(Clone->getContext()),
                                       0);

  Instruction *Induction = BinaryOperator::CreateNUWAdd(Phi, Inc, "indvar",
                                                        LoopExitBlock);

  Constant *VL = ConstantInt::get(Type::getInt32Ty(Clone->getContext()),
                                  VectorLength);

  Instruction *VLCmp = new ICmpInst(*LoopExitBlock, CmpInst::ICMP_ULT,
                                    Induction, VL, "vl.cond");

  BranchInst::Create(LoopBlock, ReturnBlock, VLCmp, LoopExitBlock);

  Phi->addIncoming(IndInit, EntryBlock);
  Phi->addIncoming(Induction, LoopExitBlock);

  LLVM_DEBUG(dbgs() << "After Loop Insertion\n");
  LLVM_DEBUG(Clone->dump());

  return Phi;
}

Instruction *VecCloneImpl::expandVectorParameters(
    Function *Clone, Function &OrigFn, VectorVariant &V, BasicBlock *EntryBlock,
    std::vector<ParmRef> &VectorParmMap, ValueToValueMapTy &VMap,
    AllocaInst *&LastAlloca) {
  // For vector parameters, expand the existing alloca to a vector. Then,
  // bitcast the vector and store this instruction in a map. The map is later
  // used to insert the new instructions and to replace the old scalar memory
  // references. If there are no parameters, then the function simply does not
  // perform any expansion since we iterate over the function's arg list.

  Instruction *Mask = nullptr;

  ArrayRef<VectorKind> ParmKinds = V.getParameters();

  Argument *LastArg = &*(Clone->arg_end() - 1);

  for (auto ArgIt : enumerate(Clone->args())) {
    Argument *Arg = &ArgIt.value();

    // If the original parameter isn't vector, we should not widen it.
    if (!ParmKinds[ArgIt.index()].isVector())
      continue;

    // This function is run after the arguments have been already widened!
    VectorType *VecType = cast<VectorType>(Arg->getType());

    // Some args other than the mask may not have users, but have not been
    // removed as dead. In those cases, just go on to the next argument.
    // There's no need to expand non-mask parameters with no users.
    bool MaskArg = V.isMasked() && Arg == LastArg;

    if (!MaskArg && Arg->getNumUses() == 0)
      continue;

    // Create a new vector alloca and bitcast to a pointer to the element
    // type. The following is an example of what the cast should look like:
    //
    // %veccast = bitcast <2 x i32>* %vec_a.addr to i32*
    //
    // geps using the bitcast will appear in a scalar form instead of
    // casting to an array or using vector. For example,
    //
    // %vecgep1 = getelementptr i32, i32* %veccast, i32 %index
    //
    // instead of:
    //
    // getelementptr inbounds [4 x i32], [4 x i32]* %a, i32 0, i64 1
    //
    // We do this to put the geps in a more scalar form.

    const DataLayout &DL = Clone->getParent()->getDataLayout();

    AllocaInst *VecAlloca =
        new AllocaInst(VecType, DL.getAllocaAddrSpace(), nullptr,
                       DL.getPrefTypeAlign(VecType), "vec." + Arg->getName());
    if (LastAlloca)
      VecAlloca->insertAfter(LastAlloca);
    else
      VecAlloca->insertBefore(&EntryBlock->front());
    LastAlloca = VecAlloca;

    Type *ArgTy = nullptr;
    for (const auto &Pair : VMap)
      if (Pair.second == Arg)
        ArgTy = Pair.first->getType();

    // If the argument is a mask, it does not exist in VMap.
    if (!ArgTy)
      ArgTy = VecType->getElementType();

    PointerType *ElemTypePtr =
        PointerType::get(ArgTy, VecAlloca->getType()->getAddressSpace());

    BitCastInst *VecParmCast = nullptr;
    if (MaskArg) {
      Mask = new BitCastInst(VecAlloca, ElemTypePtr, "mask.cast");
      // Mask points to the bitcast of the alloca instruction to element type
      // pointer. Insert the bitcast after all of the other bitcasts for vector
      // parameters.
      Mask->insertBefore(EntryBlock->getTerminator());
    } else {
      VecParmCast = new BitCastInst(VecAlloca, ElemTypePtr,
                                    "vec." + Arg->getName() + ".cast");
      VecParmCast->insertBefore(EntryBlock->getTerminator());
    }

    StoreInst *StoreUser = nullptr;
    AllocaInst *Alloca = nullptr;

    if (Mask)
      return Mask;

    Value *VectorParm = nullptr;

    for (User *U : Arg->users()) {
      StoreUser = dyn_cast<StoreInst>(U);

      if (StoreUser) {
        // For non-mask parameters, find the initial store of the parameter
        // to an alloca instruction. Map this alloca to the vector bitcast
        // created above so that we can update the old scalar references.
        Alloca = dyn_cast<AllocaInst>(StoreUser->getPointerOperand());
        VectorParm = Alloca;
        break;
      }
    }

    if (!Alloca) {
      // Since Mem2Reg has run, there is no existing scalar store for
      // the parameter, but we must still pack (store) the expanded vector
      // parameter to a new vector alloca. This store is created here and
      // put in a container for later insertion. We cannot insert it here
      // since this will be a new user of the parameter and we are still
      // iterating over the original users of the parameter. This will
      // invalidate the iterator. We also map the parameter directly to the
      // vector bitcast so that we can later update any users of the
      // parameter.
      Value *ArgValue = cast<Value>(Arg);
      StoreInst *Store = new StoreInst(ArgValue, VecAlloca, false /*volatile*/,
                                       DL.getABITypeAlign(ArgValue->getType()));

      // Insert any necessary vector parameter stores here. This is needed for
      // when there were no existing scalar stores that we can update to
      // vector stores for the parameter. This is needed when Mem2Reg has
      // registerized parameters. The stores are inserted after the allocas in
      // the entry block.
      Store->insertBefore(EntryBlock->getTerminator());
      VectorParm = ArgValue;
    }

    // Mapping not needed for the mask parameter because there will
    // be no users of it to replace. This parameter will only be used to
    // introduce if conditions on each mask bit.

    // FIXME: handling of byval/byref parameters.
    VectorParmMap.emplace_back(VectorParm, VecParmCast,
                               OrigFn.getArg(Arg->getArgNo())->getType());
  }

  return nullptr;
}

Instruction *VecCloneImpl::createExpandedReturn(Function *Clone,
                                                BasicBlock *EntryBlock,
                                                Type *OrigFuncReturnType,
                                                AllocaInst *&LastAlloca) {
  // Expand the return temp to a vector.

  VectorType *AllocaType = cast<VectorType>(Clone->getReturnType());

  const DataLayout &DL = Clone->getParent()->getDataLayout();
  AllocaInst *VecAlloca =
      new AllocaInst(AllocaType, DL.getAllocaAddrSpace(), nullptr,
                     DL.getPrefTypeAlign(AllocaType), "vec.retval");
  if (LastAlloca)
    VecAlloca->insertAfter(LastAlloca);
  else
    VecAlloca->insertBefore(&EntryBlock->front());
  LastAlloca = VecAlloca;

  auto *ElemTypePtr =
      PointerType::get(OrigFuncReturnType,
                       VecAlloca->getType()->getAddressSpace());

  BitCastInst *VecCast = new BitCastInst(VecAlloca, ElemTypePtr, "ret.cast");
  VecCast->insertBefore(EntryBlock->getTerminator());

  return VecCast;
}

Instruction *VecCloneImpl::expandReturn(Function *Clone, Function &F,
                                        BasicBlock *EntryBlock,
                                        BasicBlock *LoopBlock,
                                        BasicBlock *ReturnBlock,
                                        std::vector<ParmRef> &VectorParmMap,
                                        AllocaInst *&LastAlloca) {
  // Determine how the return is currently handled, since this will determine
  // if a new vector alloca is required for it. For simple functions, an alloca
  // may not have been created for the return value. The function may just
  // simply return a value defined by some operation that now exists within the
  // loop. If an alloca was generated already, then the return block will load
  // from it and then return. Thus, we look for a return resulting from a load
  // of an alloca in the return block. If found, we have already expanded all
  // alloca instructions to vector types and the old scalar references have
  // already been replaced with them. In this case, we only need to pack the
  // results from the vector alloca into a temp and return the temp. If a vector
  // alloca was not generated for the return, we need to add one for it because
  // we have a scalar reference in the loop that needs to be replaced. After
  // creating the new vector alloca, replace the reference to it in the loop and
  // then pack the results into a temp and return it.
  //
  // Example 1: // alloca not generated in entry block
  //
  // loop:
  //   ... // some set of instructions
  //   %add1 = add nsw i32 %1, %2
  //   br label %loop.exit (loop exit contains br to return block)
  //
  // return:
  //   ret i32 %add1
  //
  // Example 1a: // return is from a load, load operand not an alloca
  //
  // loop:
  //   ... // some set of instructions
  //   %arrayidx = getelementptr inbounds [4096 x i32], [4096 x i32]* @a,
  //               i64 0, i64 %idxprom
  //   %0 = load i32, i32* %arrayidx, align 4
  //   ret i32 %0
  //
  // Example 2:
  //
  // loop:
  //  ... // some set of instructions
  //   %vecgep1 = getelementptr <2 x i32>* %vec_ret, i32 0, i32 %index
  //   store i32 %add2, i32* %vecgep1
  //   br label %loop.exit (loop exit contains br to return block)
  //
  // return:
  //   %7 = load i32, i32*, %retval // the original scalar alloca
  //   ret i32 %7
  //

  ReturnInst *FuncReturn = dyn_cast<ReturnInst>(ReturnBlock->getTerminator());
  assert(FuncReturn && "Expected ret instruction to terminate the return\
                        basic block");

  LoadInst *Load = dyn_cast<LoadInst>(FuncReturn->getOperand(0));
  AllocaInst *Alloca = nullptr;
  if (Load)
    Alloca = dyn_cast<AllocaInst>(Load->getOperand(0));

  // We need to generate a vector alloca for the return vector.
  // Two cases exist, here:
  //
  // 1) For simple functions, the return is a temp defined within the
  //    loop body and the temp is not loaded from an alloca, or the return is
  //    a constant. (obviously, also not loaded from an alloca)
  //
  // 2) The return temp traces back to an alloca.
  //
  // For both cases, generate a vector alloca so that we can later load from it
  // and return the vector temp from the function. The alloca is used to load
  // and store from so that the scalar loop contains load/store/gep
  // instructions.
  //
  // Additionally, for case 1 we must generate a gep and store after the
  // instruction that defines the original return temp, so that we can store
  // the result into the proper index of the return vector. For case 2, we must
  // go into the loop and replace the old scalar alloca reference with the one
  // just created as vector.
  //
  // Finally, the vector alloca might already exist. This happends when the
  // return value is also a function argument. In this case, the vector alloca
  // has already been created in expandVectorParameters().

  Instruction *VecReturn = NULL;

  if (isa<Argument>(FuncReturn->getReturnValue()))
    for (auto &Param : VectorParmMap)
      if (Param.VectorParm == FuncReturn->getReturnValue())
        return Param.VectorParmCast;

  Type *RetTy = F.getReturnType();
  // GEPs into vectors of i1 do not make sense, so promote it to i8,
  // similar to later CodeGen processing.
  if (RetTy->isIntegerTy(1))
    RetTy = Type::getInt8Ty(RetTy->getContext());

  if (!Alloca) {

    // Case 1

    VecReturn = createExpandedReturn(Clone, EntryBlock, RetTy, LastAlloca);
    Value *RetVal = FuncReturn->getReturnValue();
    Instruction *RetFromTemp = dyn_cast<Instruction>(RetVal);

    Instruction *InsertPt;
    Value *ValToStore;
    Instruction *Phi = &*LoopBlock->begin();

    if (RetFromTemp) {
      // If we're returning from an SSA temp, set the insert point to the
      // definition of the temp.
      InsertPt = RetFromTemp;
      ValToStore = RetFromTemp;
    } else {
      // If we're returning a constant, then set the insert point to the loop
      // phi. From here, a store to the vector using the constant is inserted.
      InsertPt = Phi;
      ValToStore = RetVal;
    }

    // Generate a gep from the bitcast of the vector alloca used for the return
    // vector.
    GetElementPtrInst *VecGep = GetElementPtrInst::Create(
        RetTy, VecReturn, Phi, VecReturn->getName() + ".gep");
    VecGep->insertAfter(InsertPt);

    // If a conflict with the promoted type, extend.
    if (RetTy->isIntegerTy()) {
      IntegerType *ValToStoreTy = cast<IntegerType>(ValToStore->getType());
      if (RetTy != ValToStoreTy) {
        assert(ValToStoreTy->getBitWidth() <
               cast<IntegerType>(RetTy)->getBitWidth() &&
               "Expect the type to be promoted.");
        ZExtInst *ZExt =
          new ZExtInst(ValToStore, RetTy, ValToStore->getName() + ".zext");
        ZExt->insertAfter(InsertPt);
        ValToStore = ZExt;
      }
    }

    // Store the constant or temp to the appropriate lane in the return vector.
    StoreInst *VecStore =
        new StoreInst(ValToStore, VecGep, false /*volatile*/,
                      Clone->getParent()->getDataLayout().getABITypeAlign(
                          ValToStore->getType()));
    VecStore->insertAfter(VecGep);

    return VecReturn;
  } else {

    // Case 2
    auto AllocaIt = llvm::find_if(VectorParmMap, [Alloca](const ParmRef &Entry) {
      return Entry.VectorParm == Alloca;
    });
    if (AllocaIt != VectorParmMap.end()) {
      // There's already a vector alloca created for the return, which is the
      // same one used for the parameter. E.g., we're returning the updated
      // parameter.
      return AllocaIt->VectorParmCast;
    } else {
      // A new return vector is needed because we do not load the return value
      // from an alloca.
      VecReturn = createExpandedReturn(Clone, EntryBlock, RetTy, LastAlloca);
      VectorParmMap.emplace_back(Alloca, VecReturn, RetTy);
      return VecReturn;
    }
  }
  return nullptr;
}

Instruction *VecCloneImpl::expandVectorParametersAndReturn(
    Function *Clone, Function &F, VectorVariant &V, Instruction *&Mask,
    BasicBlock *EntryBlock, BasicBlock *LoopBlock, BasicBlock *ReturnBlock,
    std::vector<ParmRef> &VectorParmMap, ValueToValueMapTy &VMap) {

  // The function arguments are processed from left to right. The corresponding
  // allocas should be emitted in a specific order: the alloca that corresponds
  // to the most left argument should be emitted at the top of the entry block.
  AllocaInst *LastAlloca = nullptr;

  // If there are no parameters, then this function will do nothing and this
  // is the expected behavior.
  Mask = expandVectorParameters(Clone, F, V, EntryBlock, VectorParmMap, VMap,
                                LastAlloca);

  // If the function returns void, then don't attempt to expand to vector.
  Instruction *ExpandedReturn = ReturnBlock->getTerminator();
  if (!Clone->getReturnType()->isVoidTy()) {
    ExpandedReturn = expandReturn(Clone, F, EntryBlock, LoopBlock, ReturnBlock,
                                  VectorParmMap, LastAlloca);
    assert(ExpandedReturn && "The return value has not been widened.");
  }

  // Insert the mask parameter store to alloca and bitcast if this is a masked
  // variant.
  if (Mask) {
    Value *MaskVector = Mask->getOperand(0);

    // MaskParm points to the function's mask parameter.
    Function::arg_iterator MaskParm = Clone->arg_end();
    MaskParm--;

    // Find the last parameter store in the function entry block and insert the
    // the store of the mask parameter after it. We do this just to make the
    // LLVM IR easier to read. If there are no parameters, just insert the store
    // before the terminator. For safety, if we cannot find a store, then insert
    // this store after the last alloca. At this point, there will at least be
    // an alloca for either a parameter or return. This code just ensures that
    // the EntryBlock instructions are grouped by alloca, followed by store,
    // followed by bitcast for readability reasons.

    StoreInst *MaskStore =
        new StoreInst(&*MaskParm, MaskVector, false /*volatile*/,
                      Clone->getParent()->getDataLayout().getABITypeAlign(
                          MaskParm->getType()));
    MaskStore->insertBefore(EntryBlock->getTerminator());
  }

  LLVM_DEBUG(dbgs() << "After Parameter/Return Expansion\n");
  LLVM_DEBUG(Clone->dump());

  return ExpandedReturn;
}

void VecCloneImpl::updateScalarMemRefsWithVector(
    Function *Clone, Function &F, BasicBlock *EntryBlock,
    BasicBlock *ReturnBlock, PHINode *Phi,
    ArrayRef<ParmRef> VectorParmMap) {
  // This function replaces the old scalar uses of a parameter with a reference
  // to the new vector one. A gep is inserted using the vector bitcast created
  // in the entry block and any uses of the parameter are replaced with this
  // gep. The only users that will not be updated are those in the entry block
  // that do the initial store to the vector alloca of the parameter.

  for (auto &VectorParmMapIt : VectorParmMap) {
    Value *Parm = VectorParmMapIt.VectorParm;
    Instruction *Cast = VectorParmMapIt.VectorParmCast;

    for (auto &U:  make_early_inc_range(Parm->uses())) {
      auto *User = cast<Instruction>(U.getUser());

      if (isa<StoreInst>(User) && User->getParent() == EntryBlock) {
        // The user is the parameter store to alloca in the entry block. Replace
        // the old scalar alloca with the new vector one.
        AllocaInst *VecAlloca = dyn_cast<AllocaInst>(Cast->getOperand(0));
        User->setOperand(1, VecAlloca);
        continue;
      }

      BitCastInst *BitCast = cast<BitCastInst>(Cast);
      GetElementPtrInst *VecGep = nullptr;
      if (!isa<PHINode>(User))
        VecGep =
            GetElementPtrInst::Create(VectorParmMapIt.OrigElemType, BitCast,
                                      Phi, BitCast->getName() + ".gep", User);

      if (isa<AllocaInst>(Parm) &&
          (isa<LoadInst>(User) || isa<StoreInst>(User))) {
        // We've transformed/repurposed original alloca, so the update is
        // simple.
        assert(VecGep && "Expect VecGep to be a non-null value.");
        User->setOperand(U.getOperandNo(), VecGep);
        continue;
      }

      // Otherwise, we need to load the value from the gep first before
      // using it. This effectively loads the particular element from
      // the vector parameter.
      if (PHINode *PHIUser = dyn_cast<PHINode>(User)) {
        BasicBlock *IncommingBB = PHIUser->getIncomingBlock(U.getOperandNo());
        VecGep = GetElementPtrInst::Create(
            VectorParmMapIt.OrigElemType, BitCast, Phi,
            BitCast->getName() + ".gep", IncommingBB->getTerminator());
      }
      assert(VecGep && "Expect VecGep to be a non-null value.");
      Type *LoadTy = VecGep->getResultElementType();
      LoadInst *ParmElemLoad = new LoadInst(
          LoadTy, VecGep, "vec." + Parm->getName() + ".elem",
          false /*volatile*/,
          Clone->getParent()->getDataLayout().getABITypeAlign(LoadTy));
      ParmElemLoad->insertAfter(VecGep);
      User->setOperand(U.getOperandNo(), ParmElemLoad);
    }
  }

  LLVM_DEBUG(dbgs() << "After Alloca Replacement\n");
  LLVM_DEBUG(Clone->dump());
}

Value *VecCloneImpl::generateStrideForParameter(Function *Clone, Argument *Arg,
                                                Instruction *ParmUser,
                                                int Stride, PHINode *Phi) {
  // For linear values, a mul + add/gep sequence is needed to generate the
  // correct value. i.e., val = linear_var + stride * loop_index;

  // Insert the stride related instructions after the user if the
  // instruction involves a redefinition of the parameter. For these
  // situations, we want to apply the stride to this SSA temp. For other
  // instructions, e.g., add, the instruction computing the stride must be
  // inserted before the user.
  IRBuilder<> Builder(isa<LoadInst>(ParmUser) ? ParmUser->getNextNode()
                                              : ParmUser);

  if (Arg->getType()->isPointerTy()) {
    auto *Mul = Builder.CreateMul(ConstantInt::get(Phi->getType(), Stride), Phi,
                                  "stride.mul");

    // Linear updates to pointer parameters involves an address calculation, so
    // use gep. To properly update linear pointers we only need to multiply the
    // loop index and stride since gep is indexed starting at 0 from the base
    // address passed to the vector function.
    auto *ParmPtrType = cast<PointerType>(Arg->getType());

    // The base address used for linear gep computations.
    Value *BaseAddr = nullptr;
    StringRef RefName;

    if (auto *ParmLoad = dyn_cast<LoadInst>(ParmUser)) {
      // We are loading from the alloca of the pointer parameter (no Mem2Reg)
      // i.e., loading a pointer to an SSA temp.
      BaseAddr = ParmUser;
      RefName = ParmLoad->getOperand(0)->getName();
    } else {
      // The user is using the pointer parameter directly.
      BaseAddr = Arg;
      RefName = BaseAddr->getName();
    }

    // Mul is always generated as i32 since it is calculated using the i32 loop
    // phi that is inserted by this pass. No cast on Mul is necessary because
    // gep can use a base address of one type with an index of another type.
    Value *LinearParmGep = Builder.CreateGEP(ParmPtrType->getElementType(),
                                             BaseAddr, Mul, RefName + ".gep");

    return LinearParmGep;
  }

  Value *PhiCast = Phi;
  // The instruction might involve redefinition of the parameter. For
  // example, a load from the parameter's associated alloca or a cast. In this
  // case, we emit the stride based on the type of ParmUser. In any other
  // case, we use the type of the arguments.
  Value *Val =
      isa<LoadInst>(ParmUser) ? cast<Value>(ParmUser) : cast<Value>(Arg);
  if (Val->getType() != Phi->getType()) {
    PhiCast = Builder.CreateCast(
        CastInst::getCastOpcode(Phi, false /* SrcIsSigned */, Val->getType(),
                                false /* DestIsSigned */),
        Phi, Val->getType(), "phi.cast");
  }

  Value *Mul =
      Builder.CreateMul(GeneralUtils::getConstantValue(
                            Val->getType(), Clone->getContext(), Stride),
                        PhiCast, "stride.mul");

  // The user of the parameter is an instruction that results in a
  // redefinition of it. e.g., a load from an alloca (no Mem2Reg) or a cast
  // instruction. In either case, the stride needs to be applied to this
  // temp. Otherwise, the user is an instruction that does not redefine the
  // temp, such as an add instruction. For these cases, the stride must be
  // computed before the user and the reference to the parameter must be
  // replaced with this instruction.
  assert(!Val->getType()->isFloatingPointTy() &&
         "The value should not be floating point!");
  auto Add = Builder.CreateAdd(Val, Mul, "stride.add");
  return Add;
}

void VecCloneImpl::updateLinearReferences(Function *Clone, Function &F,
                                          VectorVariant &V, PHINode *Phi) {
  // Add stride to parameters marked as linear. This is done by finding all
  // users of the scalar alloca associated with the parameter. The user should
  // be a load from this alloca to a temp. The stride is then added to this temp
  // and its uses are replaced with the new temp. Or, if Mem2Reg eliminates the
  // alloca/load, the parameter is used directly and this use is updated with
  // the stride.

  std::vector<VectorKind> ParmKinds = V.getParameters();

  for (Argument &Arg : Clone->args()) {
    VectorKind ParmKind = ParmKinds[Arg.getArgNo()];
    SmallDenseMap<Instruction*, int> LinearParmUsers;

    if (ParmKind.isLinear()) {

      int Stride = ParmKind.getStride();

      for (User *ArgUser : Arg.users()) {

        // Collect all uses of the parameter so that they can later be used to
        // apply the stride.
        if (StoreInst *ParmStore = dyn_cast<StoreInst>(ArgUser)) {

          // This code traces the store of the parameter to its associated
          // alloca. Then, we look for a load from that alloca to a temp. This
          // is the value we need to add the stride to. This is for when
          // Mem2Reg has not been run.
          AllocaInst *Alloca = dyn_cast<AllocaInst>(ParmStore->getPointerOperand());

          if (Alloca) {
            for (auto *AU : Alloca->users())
              if (LoadInst *ParmLoad = dyn_cast<LoadInst>(AU))
                // The parameter is being loaded from an alloca to a new SSA
                // temp. We must replace the users of this load with an
                // instruction that adds the result of this load with the
                // stride.
                LinearParmUsers[ParmLoad] = Stride;
          } else {
            // Mem2Reg has run, so the parameter is directly referenced in the
            // store instruction.
            LinearParmUsers[ParmStore] = Stride;
          }
        } else {
          // Mem2Reg has registerized the parameters, so users of it will use
          // it directly, and not through a load of the parameter.
          LinearParmUsers[cast<Instruction>(ArgUser)] = Stride;
        }
      }
    }

    for (auto UserIt : LinearParmUsers) {
      Instruction *User = UserIt.first;
      int Stride = UserIt.second;

      // For each user of parameter:

      // We must deal with two cases here, based on whether Mem2Reg has been
      // run.
      //
      // Example:
      //
      // __declspec(vector(linear(i:1),uniform(x),vectorlength(4)))
      // extern int foo(int i, int x) {
      //   return (x + i);
      // }
      //
      // 1) We are loading the parameter from an alloca and the SSA temp as a
      //    result of the load is what we need to add the stride to. Then, any
      //    users of that temp must be replaced. The only load instructions put
      //    in the collection above are guaranteed to be associated with the
      //    parameter's alloca. Thus, we only need to check to see if a load is
      //    in the map to know what to do.
      //
      // Before Linear Update:
      //
      // simd.loop:                     ; preds = %simd.loop.exit, %entry
      //   %index = phi i32 [ 0, %entry ], [ %indvar, %simd.loop.exit ]
      //   store i32 %x, i32* %x.addr, align 4
      //   %0 = load i32, i32* %x.addr, align 4
      //   %1 = load i32, i32* %i.addr, align 4 <--- %i
      //   %add = add nsw i32 %0, %1            <--- replace %1 with stride
      //   %ret.cast.gep = getelementptr i32, i32* %ret.cast, i32 %index
      //   store i32 %add, i32* %ret.cast.gep
      //   br label %simd.loop.exit
      //
      // After Linear Update:
      //
      // simd.loop:                     ; preds = %simd.loop.exit, %entry
      //   %index = phi i32 [ 0, %entry ], [ %indvar, %simd.loop.exit ]
      //   store i32 %x, i32* %x.addr, align 4
      //   %0 = load i32, i32* %x.addr, align 4
      //   %1 = load i32, i32* %i.addr, align 4
      //   %stride.mul = mul i32 1, %index
      //   %stride.add = add i32 %1, %stride.mul <--- stride
      //   %add = add nsw i32 %0, %stride.add    <--- new %i with stride
      //   %ret.cast.gep = getelementptr i32, i32* %ret.cast, i32 %index
      //   store i32 %add, i32* %ret.cast.gep
      //   br label %simd.loop.exit
      //
      // 2) The user uses the parameter directly, and so we must apply the
      //    stride directly to the parameter. Any users of the parameter must
      //    then be updated.
      //
      // Before Linear Update:
      //
      // simd.loop:                     ; preds = %simd.loop.exit, %entry
      //   %index = phi i32 [ 0, %entry ], [ %indvar, %simd.loop.exit ]
      //   %add = add nsw i32 %x, %i <-- direct usage of %i
      //   %ret.cast.gep = getelementptr i32, i32* %ret.cast, i32 %index
      //   store i32 %add, i32* %ret.cast.gep
      //   br label %simd.loop.exit
      //
      // After Linear Update:
      //
      // simd.loop:                     ; preds = %simd.loop.exit, %entry
      //   %index = phi i32 [ 0, %entry ], [ %indvar, %simd.loop.exit ]
      //   %stride.mul = mul i32 1, %index
      //   %stride.add = add i32 %i, %stride.mul <--- stride
      //   %add = add nsw i32 %x, %stride.add    <--- new %i with stride
      //   %ret.cast.gep = getelementptr i32, i32* %ret.cast, i32 %index
      //   store i32 %add, i32* %ret.cast.gep
      //   br label %simd.loop.exit

      // The stride calculation is inserted before the use. In some cases this
      // can lead to redundant instructions, but they will be optimized away
      // later. Inserting them this way makes the algorithm simpler.
      Value *StrideInst =
          generateStrideForParameter(Clone, &Arg, User, Stride, Phi);

      SmallVector<Instruction*, 4> InstsToUpdate;
      Value *ParmUser;

      if (isa<LoadInst>(User)) {
        // Case 1
        ParmUser = User;

        // Find the users of the redefinition of the parameter so that we
        // can apply the stride to those instructions.
        for (auto *StrideUser : ParmUser->users()) {
          if (StrideUser != StrideInst) {
            // We've already inserted the stride which is now also a user of
            // the parameter, so don't update that instruction. Otherwise,
            // we'll create a self reference. Hence, why we don't use
            // replaceAllUsesWith().
            InstsToUpdate.push_back(cast<Instruction>(StrideUser));
          }
        }
      } else {
        // Case 2
        ParmUser = &Arg;
        InstsToUpdate.push_back(User);
      }
 
      // Replace the old references to the parameter with the instruction
      // that applies the stride.
      for (unsigned J = 0; J < InstsToUpdate.size(); ++J) {
        unsigned NumOps = InstsToUpdate[J]->getNumOperands();
        for (unsigned K = 0; K < NumOps; ++K) {
          if (InstsToUpdate[J]->getOperand(K) == ParmUser) {
            InstsToUpdate[J]->setOperand(K, StrideInst);
          }

          // Replace the old references to the parameter with the instruction
          // that applies the stride.
          for (unsigned J = 0; J < InstsToUpdate.size(); ++J) {
            unsigned NumOps = InstsToUpdate[J]->getNumOperands();
            for (unsigned K = 0; K < NumOps; ++K) {
              if (InstsToUpdate[J]->getOperand(K) == ParmUser) {
                InstsToUpdate[J]->setOperand(K, StrideInst);
              }
            }
          }
        }
      }
    }
  }

  LLVM_DEBUG(dbgs() << "After Linear Updates\n");
  LLVM_DEBUG(Clone->dump());
}

void VecCloneImpl::updateReturnBlockInstructions(Function *Clone,
                                                 BasicBlock *ReturnBlock,
                                                 Instruction *ExpandedReturn) {
  // If the vector function returns void, then there is no need to do any
  // packing. The only instruction in the ReturnBlock is 'ret void', so
  // we can just leave this instruction and we're done.
  if (Clone->getReturnType()->isVoidTy())
    return;

  // Remove all instructions from the return block. These will be replaced
  // with the instructions necessary to return a vector temp. The verifier
  // will complain if we remove the definitions of users first, so remove
  // instructions from the bottom up.
  while (!ReturnBlock->empty())
    ReturnBlock->back().eraseFromParent();

  // Pack up the elements into a vector temp and return it. If the return
  // vector was bitcast to a pointer to the element type, we must bitcast to
  // vector before returning.
  // Operand 0 is the actual alloc reference in the bitcast.
  AllocaInst *Alloca = cast<AllocaInst>(ExpandedReturn->getOperand(0));
  PointerType *PtrVecType =
      PointerType::get(Clone->getReturnType(),
                       Alloca->getType()->getAddressSpace());
  BitCastInst *BitCast =
    new BitCastInst(ExpandedReturn, PtrVecType,
                    "vec." + ExpandedReturn->getName(),
                    ReturnBlock);

  // Return can't be void here due to early exit at the top of this function.
  // Return is expected to be a bitcast instruction because we always create
  // a vector alloca for the return value and cast that to a scalar pointer.
  // for use within the loop. I.e., this cast is used with the loop index to
  // reference a specific vector element. At the point of the function return,
  // the scalar cast is converted back to vector and we load from that to the
  // return vector.
  // TODO: this can actually be simplified further by just returning Alloca
  // from above. There doesn't seem to be a good reason to do this extra
  // casting. Leaving for now because this change is NFC.
  LoadInst *VecReturn =
      new LoadInst(Clone->getReturnType(), BitCast, "vec.ret", ReturnBlock);
  ReturnInst::Create(Clone->getContext(), VecReturn, ReturnBlock);

  LLVM_DEBUG(dbgs() << "After Return Block Update\n");
  LLVM_DEBUG(Clone->dump());
}

// Stores the parameter in the stack and loads it.
static void emitLoadStoreForParameter(AllocaInst *Alloca, Value *ArgValue,
                                      BasicBlock *LoopPreHeader) {
  // Emit the load in the simd.loop.preheader block.
  IRBuilder<> Builder(&*LoopPreHeader->begin());
  LoadInst *Load = Builder.CreateLoad(Alloca->getAllocatedType(), Alloca,
                                      "load." + ArgValue->getName());
  ArgValue->replaceAllUsesWith(Load);
  // After updating the uses of the function parameter with its stack variable,
  // we emit the store.
  Builder.SetInsertPoint(Alloca->getNextNode());
  Builder.CreateStore(ArgValue, Alloca);
}

// Creates the simd.begion.region block which marks the beginning of the WRN
// region. Given the function parameters, emits the correct directive in the
// simd.begion.region block. If the parameters are linear or uniform, a new
// basic block(simd.loop.preheader) is created between the simd.begin.region
// block and the simd.loop block (header). VPLoopEntity needs the addresses of
// the uniform/linear parameters. For this reason, we need to pass the address
// of the parameters to the directives instead of their values. In VecClone, we
// have the values, not the addresses. So, we create a stack variable for each
// uniform and linear parameter and store them in the stack (the store is
// emitted in the EntryBlock). Next, we load it and we update its uses (the load
// is emitted in simd.loop.preheader). This is similar to the code emitted by
// the front-end for simd loops.
CallInst *VecCloneImpl::insertBeginRegion(Module &M, Function *Clone,
                                          Function &F, VectorVariant &V,
                                          BasicBlock *EntryBlock) {
  std::vector<VectorKind> ParmKinds = V.getParameters();

  BasicBlock *LoopPreHeader = EntryBlock->splitBasicBlock(
      EntryBlock->getTerminator(), "simd.loop.preheader");

  IRBuilder<> Builder(&*EntryBlock->begin());

  SmallVector<llvm::OperandBundleDef, 4> OpndBundles;
  OpndBundles.emplace_back(
      std::string(IntrinsicUtils::getDirectiveString(DIR_OMP_SIMD)), None);

  auto AddClause = [&OpndBundles](OMP_CLAUSES Clause, auto &&... Ops) {
    std::vector<Value *> Arr = {{Ops...}};
    OpndBundles.emplace_back(
        std::string(IntrinsicUtils::getClauseString(Clause)), std::move(Arr));
  };

  auto AddTypedClause = [&OpndBundles, AddClause](OMP_CLAUSES Clause,
                                                  Value *Ptr, Type *Ty,
                                                  auto &&... Ops) {
    if (!EmitTypedOMP) {
      AddClause(Clause, Ptr, Ops...);
      return;
    }

    std::vector<Value *> Arr = {
        {Ptr, Constant::getNullValue(Ty),
         ConstantInt::get(Type::getInt32Ty(Ty->getContext()), 1), // #Elts
         Ops...}};
    OpndBundles.emplace_back(
        (IntrinsicUtils::getClauseString(Clause) + Twine(":TYPED")).str(),
        std::move(Arr));
  };

  // Insert vectorlength directive
  AddClause(QUAL_OMP_SIMDLEN, Builder.getInt32(V.getVlen()));

  // Add directives for linear and vector parameters. Vector parameters can be
  // marked as private.
  for (Argument &Arg : Clone->args()) {
    VectorKind ParmKind = ParmKinds[Arg.getArgNo()];

    // In O0, the parameters are also stored in the stack. mem2reg which would
    // have cleaned up the redundant memory operations does not run in O0. In
    // O0, all the parameters are marked as privates even if they are linear or
    // uniform. This does not have an impact on program's correctness. For
    // linear parameters, VecClone will emit a stride. For all the other
    // optimization levels, we do not have this problem.
    // TODO: CMPLRLLVM-9851: The linear and uniform parameters which are
    // currently marked as privates will be marked as linear and uniform
    // respectively.
    if (Arg.hasOneUse())
      if (auto *Store = dyn_cast<StoreInst>(Arg.user_back()))
        if (isa<AllocaInst>(Store->getPointerOperand()))
          continue;

    if (!ParmKind.isLinear() && !ParmKind.isUniform())
      continue;

  // No need to allocate another stack slot for an argument if there is already
  // a dedicated storage for it on the stack.
    Value *Memory = &Arg;
    Type *ValueType = Arg.getPointeeInMemoryValueType();
    // TODO:  Should it be !hasPointeeInMemoryValueAttr() instead?
    if (!Arg.hasByValAttr()) {
      ValueType = Arg.getType();
      Memory = Builder.CreateAlloca(Arg.getType(), nullptr,
                                    "alloca." + Arg.getName());
      emitLoadStoreForParameter(cast<AllocaInst>(Memory), &Arg, LoopPreHeader);
    }

    if (ParmKind.isLinear())
      AddTypedClause(QUAL_OMP_LINEAR, Memory, ValueType,
                     Builder.getInt32(ParmKind.getStride()));

    if (ParmKind.isUniform())
      AddTypedClause(QUAL_OMP_UNIFORM, Memory, ValueType);
  }

  // Add PrivateAllocas to privates.
  for (Value *AllocaVal : PrivateAllocas)
    AddTypedClause(QUAL_OMP_PRIVATE, AllocaVal,
                   cast<AllocaInst>(AllocaVal)->getAllocatedType());

  // Create simd.begin.region block which indicates the begining of the WRN
  // region.
  CallInst *SIMDBeginCall = CallInst::Create(
      Intrinsic::getDeclaration(&M, Intrinsic::directive_region_entry), None,
      OpndBundles, "entry.region");
  SIMDBeginCall->insertBefore(EntryBlock->getTerminator());
  EntryBlock->splitBasicBlock(SIMDBeginCall, "simd.begin.region");
  return SIMDBeginCall;
}

void VecCloneImpl::insertEndRegion(Module &M, Function *Clone,
                                   BasicBlock *LoopExitBlock,
                                   BasicBlock *ReturnBlock,
                                   CallInst *EntryDirCall) {
  BasicBlock *EndDirectiveBlock = BasicBlock::Create(
      Clone->getContext(), "simd.end.region", Clone, ReturnBlock);

  BranchInst *LoopExitBranch =
      dyn_cast<BranchInst>(LoopExitBlock->getTerminator());
  assert(LoopExitBranch && "Expecting br instruction for loop exit block");
  LoopExitBranch->setOperand(1, EndDirectiveBlock);

  BranchInst::Create(ReturnBlock, EndDirectiveBlock);

  CallInst *SIMDEndCall =
      IntrinsicUtils::createSimdDirectiveEnd(M, EntryDirCall);
  SIMDEndCall->insertBefore(EndDirectiveBlock->getTerminator());
}

void VecCloneImpl::insertDirectiveIntrinsics(Module &M, Function *Clone,
                                             Function &F, VectorVariant &V,
                                             BasicBlock *EntryBlock,
                                             BasicBlock *LoopExitBlock,
                                             BasicBlock *ReturnBlock) {
  CallInst *EntryDirCall = insertBeginRegion(M, Clone, F, V, EntryBlock);
  insertEndRegion(M, Clone, LoopExitBlock, ReturnBlock, EntryDirCall);
  LLVM_DEBUG(dbgs() << "After Directives Insertion\n");
  LLVM_DEBUG(Clone->dump());
}

bool VecCloneImpl::isSimpleFunction(Function *Func) {
  // For really simple functions, there is no need to go through the process
  // of inserting a loop.

  // Example:
  //
  // void foo(void) {
  //   return;
  // }
  //
  // No need to insert a loop for this case since it's basically a no-op. Just
  // clone the function and return. It's possible that we could have some code
  // inside of a vector function that modifies global memory. Let that case go
  // through.
  return isa<ReturnInst>(Func->front().front()) &&
         Func->getReturnType()->isVoidTy();
}

void VecCloneImpl::insertSplitForMaskedVariant(Function *Clone,
                                               BasicBlock *LoopBlock,
                                               BasicBlock *LoopExitBlock,
                                               Instruction *Mask,
                                               PHINode *Phi) {
  BasicBlock *LoopThenBlock =
      LoopBlock->splitBasicBlock(LoopBlock->getFirstNonPHI(),
                                 "simd.loop.then");

  BasicBlock *LoopElseBlock = BasicBlock::Create(Clone->getContext(),
                                                 "simd.loop.else",
                                                 Clone, LoopExitBlock);

  BranchInst::Create(LoopExitBlock, LoopElseBlock);

  auto *BitCast = cast<BitCastInst>(Mask);
  auto *Alloca = cast<AllocaInst>(BitCast->getOperand(0));

  Type *PointeeType =
      cast<VectorType>(Alloca->getAllocatedType())->getElementType();

  GetElementPtrInst *MaskGep =
      GetElementPtrInst::Create(PointeeType, Mask, Phi, "mask.gep",
                                LoopBlock->getTerminator());

  Type *LoadTy = MaskGep->getResultElementType();
  LoadInst *MaskLoad = new LoadInst(LoadTy, MaskGep, "mask.parm",
                                    LoopBlock->getTerminator());

  Type *CompareTy = MaskLoad->getType();
  Instruction *MaskCmp;
  Constant* Zero;

  // Generate the compare instruction to see if the mask bit is on. In ICC, we
  // use the movemask intrinsic which takes both float/int mask registers and
  // converts to an integer scalar value, one bit representing each element.
  if (CompareTy->isIntegerTy()) {
    Zero = GeneralUtils::getConstantValue(CompareTy, Clone->getContext(),
                                               0);
    MaskCmp = new ICmpInst(LoopBlock->getTerminator(), CmpInst::ICMP_NE,
                           MaskLoad, Zero, "mask.cond");
  } else if (CompareTy->isFloatingPointTy()) {
    Zero = GeneralUtils::getConstantValue(CompareTy, Clone->getContext(),
                                               0.0);
    MaskCmp = new FCmpInst(LoopBlock->getTerminator(), CmpInst::FCMP_UNE,
                           MaskLoad, Zero, "mask.cond");
  } else {
    llvm_unreachable("Unsupported mask compare");
  }

  Instruction *Term = LoopBlock->getTerminator();
  Term->eraseFromParent();
  BranchInst::Create(LoopThenBlock, LoopElseBlock, MaskCmp, LoopBlock);

  LLVM_DEBUG(dbgs() << "After Split Insertion For Masked Variant\n");
  LLVM_DEBUG(Clone->dump());
}

void VecCloneImpl::removeScalarAllocasForVectorParams(
    ArrayRef<ParmRef> VectorParmMap) {
  for (auto &VectorParmMapIt : VectorParmMap) {
    Value *Parm = VectorParmMapIt.VectorParm;
    if (AllocaInst *ScalarAlloca = dyn_cast<AllocaInst>(Parm)) {
      ScalarAlloca->eraseFromParent();

      // The scalar alloca needs to be removed from the allocas that are marked
      // as SIMD loop private.
      PrivateAllocas.remove(ScalarAlloca);
    }
  }
}

void VecCloneImpl::disableLoopUnrolling(BasicBlock *Latch) {
  // Set disable unroll metadata on the conditional branch of the loop latch
  // for the simd loop. The following is an example of what the loop latch
  // and Metadata will look like. The !llvm.loop marks the beginning of the
  // loop Metadata and is always placed on the terminator of the loop latch.
  // (i.e., simd.loop.exit in this case). According to LLVM documentation, to
  // properly set the loop Metadata, the 1st operand of !16 must be a self-
  // reference to avoid some type of Metadata merging conflicts that have
  // apparently arisen in the past. This is part of LLVM history that I do not
  // know. Also, according to LLVM documentation, any Metadata nodes referring
  // to themselves are marked as distinct. As such, all Metadata corresponding
  // to a loop belongs to that loop alone and no sharing of Metadata can be
  // done across different loops.
  //
  // simd.loop.exit:        ; preds = %simd.loop, %if.else, %if.then
  //  %indvar = add nuw i32 %index, 1
  //  %vl.cond = icmp ult i32 %indvar, 2
  //  br i1 %vl.cond, label %simd.loop, label %simd.end.region, !llvm.loop !16
  //
  // !16 = distinct !{!16, !17}
  // !17 = !{!"llvm.loop.unroll.disable"}

  SmallVector<Metadata *, 4> MDs;

  // Reserve first location for self reference to the LoopID metadata node.
  MDs.push_back(nullptr);

  // Add unroll(disable) metadata to disable future unrolling.
  LLVMContext &Context = Latch->getContext();
  SmallVector<Metadata *, 1> DisableOperands;
  DisableOperands.push_back(MDString::get(Context, "llvm.loop.unroll.disable"));
  MDNode *DisableNode = MDNode::get(Context, DisableOperands);
  MDs.push_back(DisableNode);

  MDNode *NewLoopID = MDNode::get(Context, MDs);
  // Set operand 0 to refer to the loop id itself.
  NewLoopID->replaceOperandWith(0, NewLoopID);
  Latch->getTerminator()->setMetadata("llvm.loop", NewLoopID);
}

PreservedAnalyses VecClonePass::run(Module &M, ModuleAnalysisManager &AM) {
  // NOTE: Update here if new analyses are needed before VecClone
  // (getAnalysisUsage from LegacyPM)
  if (!Impl.runImpl(M))
    return PreservedAnalyses::all();

  auto PA = PreservedAnalyses::none();
  PA.preserve<AndersensAA>();
  PA.preserve<GlobalsAA>();
  return PA;
}

void VecClone::getAnalysisUsage(AnalysisUsage &AU) const {
  // VecClone pass does not make any changes in the existing functions and
  // Andersens analysis is conservative on new functions. So we can consider it
  // as preserved.
  AU.addPreserved<AndersensAAWrapperPass>(); // INTEL
  AU.addPreserved<GlobalsAAWrapperPass>();
}

bool VecCloneImpl::runImpl(Module &M, LoopOptLimiter Limiter) {

  LLVM_DEBUG(dbgs() << "\nExecuting SIMD Function Cloning ...\n\n");

#if INTEL_CUSTOMIZATION
  // Language specific hook
  languageSpecificInitializations(M);
#endif // INTEL_CUSTOMIZATION

  MapVector<Function *, std::vector<StringRef>> FunctionsToVectorize;
  getFunctionsToVectorize(M, FunctionsToVectorize);

  // VectorParmMap contains the mapping of the parameter to the bitcast
  // instruction that casts the vector alloca for vector parameters to a scalar
  // pointer for use in the simd loop. When parameters are registerized, the
  // Value* in the map correponds directly to the function parameter. When
  // parameters are not registerized, then the Value* in the map is the original
  // scalar alloca before expansion. Later, users of the parameter, either
  // directly or through the alloca, are replaced with a gep using the bitcast
  // of the vector alloca for the parameter and the current loop induction
  // variable value.
  //
  // IMPORTANT NOTE: std::vector was used here because later we emit LLVM
  // instructions using the members of ParmRef, and these instructions should be
  // ordered consistently for easier testability.

  std::vector<ParmRef> VectorParmMap;

  for (auto VarIt : FunctionsToVectorize) {
    Function& F = *(VarIt.first);

    if (!doesLoopOptPipelineAllowToRun(Limiter, F))
      continue;

    std::vector<StringRef> Variants = VarIt.second;

    for (VectorVariant Variant : Variants) {
      // VecClone runs after OCLVecClone. Hence, VecClone will be triggered
      // again for the OpenCL kernels. To prevent this, we do not process
      // functions whose name include the current vector variant name. The
      // vector variant name is a combination of the scalar function name and
      // the Vector ABI encoding.
      if (Variant.getName() && M.getFunction(*Variant.getName()))
        continue;

      // Clone the original function.
      LLVM_DEBUG(dbgs() << "Before SIMD Function Cloning\n");
      LLVM_DEBUG(F.dump());
      ValueToValueMapTy VMap;
      Function *Clone = CloneFunction(F, Variant, VMap);
      BasicBlock *EntryBlock = &Clone->front();

      if (isSimpleFunction(Clone))
        continue;

      BasicBlock *LoopBlock = splitEntryIntoLoop(Clone, Variant, EntryBlock);
      BasicBlock *ReturnBlock = splitLoopIntoReturn(Clone, &Clone->back());
      if (!ReturnBlock) {
        // OpenCL, it's valid to have an infinite loop inside kernel with no
        // independent forward progress guarantee. As such, creating a VecClone
        // loop around the body is required. Handle such cases.
        // For OpenMP cases, it's probably UB in the incoming IR, so creation of
        // the loop is still valid.
        ReturnBlock =
            BasicBlock::Create(Clone->getContext(), "unreachable.ret", Clone);
        IRBuilder<> B(ReturnBlock);
        B.CreateUnreachable();
      }

      BasicBlock *LoopExitBlock = BasicBlock::Create(
          Clone->getContext(), "simd.loop.exit", Clone, ReturnBlock);
      ReturnBlock->replaceAllUsesWith(LoopExitBlock);

      PHINode *Phi = createPhiAndBackedgeForLoop(Clone, EntryBlock,
                                                 LoopBlock, LoopExitBlock,
                                                 ReturnBlock,
                                                 Variant.getVlen());

      // At this point, we've gathered some parameter information and have
      // restructured the function into an entry block, a set of blocks
      // forming the loop, a loop exit block, and a return block. Now,
      // we can go through and update instructions since we know what
      // is part of the loop.

      // Create a new vector alloca instruction for all vector parameters and
      // return. For parameters, replace the initial store to the old alloca
      // with the vector one. Users of the old alloca within the loop will be
      // replaced with a gep using this address along with the proper loop
      // index.

      Instruction *Mask = nullptr;
      Instruction *ExpandedReturn = expandVectorParametersAndReturn(
          Clone, F, Variant, Mask, EntryBlock, LoopBlock, ReturnBlock,
          VectorParmMap, VMap);
      updateScalarMemRefsWithVector(Clone, F, EntryBlock, ReturnBlock, Phi,
                                    VectorParmMap);

      // Update any linear variables with the appropriate stride. This function
      // will insert a mul/add sequence before the use of the parameter. For
      // linear pointer parameters, the stride calculation is just a mul
      // instruction using the loop induction var and the stride value on the
      // parameter. This mul instruction is then used as the index of the gep
      // that will be inserted before the next use of the parameter. The
      // function also updates the users of the parameter with the new
      // calculation involving the stride.
      updateLinearReferences(Clone, F, Variant, Phi);

      // Remove the old scalar instructions associated with the return and
      // replace with packing instructions.
      updateReturnBlockInstructions(Clone, ReturnBlock, ExpandedReturn);

      // Remove the old scalar allocas associated with vector parameters since
      // these have now been replaced with vector ones.
      removeScalarAllocasForVectorParams(VectorParmMap);

      VectorParmMap.clear();

      // If this is the masked vector variant, insert the mask condition and
      // if/else blocks.
      if (Variant.isMasked()) {
        insertSplitForMaskedVariant(Clone, LoopBlock, LoopExitBlock, Mask, Phi);
      }

#if INTEL_CUSTOMIZATION
      // Language specific hook.
      handleLanguageSpecifics(F, Phi, Clone, EntryBlock, Variant);
#endif // INTEL_CUSTOMIZATION

      // Insert the basic blocks that mark the beginning/end of the SIMD loop.
      insertDirectiveIntrinsics(M, Clone, F, Variant, EntryBlock,
                                LoopExitBlock, ReturnBlock);
      PrivateAllocas.clear();

      // Add may-have-openmp-directive attribute since we inserted directives.
      Clone->addFnAttr("may-have-openmp-directive", "true");

      LLVM_DEBUG(dbgs() << "After SIMD Function Cloning\n");
      LLVM_DEBUG(Clone->dump());

      // Disable unrolling from kicking in on the simd loop.
      disableLoopUnrolling(LoopExitBlock);
    } // End of function cloning for the variant
  } // End of function cloning for all variants

  //FIXME: return false if all functions were skipped or IR was not modified.
  return true; // LLVM IR has been modified
}

ModulePass *llvm::createVecClonePass() {
  return new llvm::VecClone();
}

char VecClone::ID = 0;

static const char lv_name[] = "VecClone";
INITIALIZE_PASS_BEGIN(VecClone, SV_NAME, lv_name,
                      false /* modifies CFG */, false /* transform pass */)
INITIALIZE_PASS_END(VecClone, SV_NAME, lv_name,
                    false /* modififies CFG */, false /* transform pass */)
