//=---- Intel_VecClone.cpp - Vector function to loop transform -*- C++ -*----=//
//
// Copyright (C) 2015-2023 Intel Corporation. All rights reserved.
//
// The information and source code contained herein is the exclusive property
// of Intel Corporation and may not be disclosed, examined or reproduced in
// whole or in part without explicit written authorization from the company.
//
// Main author:
// ------------
// Matt Masten (C) 2017 [matt.masten@intel.com]
//
// Major revisions:
// ----------------
// May 2015, initial development -- Matt Masten
//
// ===--------------------------------------------------------------------=== //
///
/// \file
/// This pass inserts the body of a vector function inside a vector length
/// trip count scalar loop for functions that are declared SIMD. The pass
/// follows the Intel vector ABI requirements for name mangling encodings.
///
/// Conceptually, this pass performs the following transformation:
///
/// Before Translation:
///
/// main.cpp
///
/// #pragma omp declare simd uniform(a) linear(k)
///
/// extern float dowork(float *a, int k);
///
/// float a[4096];
/// int main() {
///   int k;
/// #pragma clang loop vectorize(enable)
///   for (k = 0; k < 4096; k++) {
///     a[k] = k * 0.5;
///     a[k] = dowork(a, k);
///   }
/// }
///
/// dowork.cpp
///
/// #pragma omp declare simd uniform(a) linear(k) #0
/// float dowork(float *a, int k) {
///   a[k] = sinf(a[k]) + 9.8f;
/// }
///
/// attributes #0 = { nounwind uwtable "_ZGVbM4ul_", "ZGVbN4ul_", ... }
///
/// After Translation:
///
/// dowork.cpp
///
/// // Non-masked variant
///
/// <VL x float> "_ZGVbN4ul_dowork(float *a, int k) {
///   alloc <VL x float> vec_ret;
///   for (int i = k; i < k + VL, i++) {
///     a[i] = sinf(a[i]) + 9.8f;
///   }
///   load vec_ret, a[k:VL]
///   return vec_ret;
/// }
///
/// // Masked variant
///
/// <VL x float> "_ZGVbM4ul_dowork(float *a, int k, <VL x int> mask) {
///   alloc <VL x float> vec_ret;
///   for (int i = k; i < k + VL, i++) {
///     if (mask[i] != 0)
///       a[i] = sinf(a[i]) + 9.8f;
///   }
///   load vec_ret, a[k:VL]
///   return vec_ret;
/// }
///
///
///
// ===--------------------------------------------------------------------=== //

// This pass is flexible enough to deal with two forms of LLVM IR, namely when
// Mem2Reg has run and when Mem2Reg has not run.
//
// When Mem2Reg has run:
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
//   %add = add nsw i32 %x, %i
//   ret i32 %add
// }
//
// When Mem2Reg has not run:
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
// %i.addr = alloca i32, align 4
// %x.addr = alloca i32, align 4
// store i32 %i, i32* %i.addr, align 4
// store i32 %x, i32* %x.addr, align 4
// %0 = load i32, i32* %x.addr, align 4
// %1 = load i32, i32* %i.addr, align 4
// %add = add nsw i32 %0, %1
//  ret i32 %add
// }
//
// Both forms are handled the same by always generating new allocas for
// vector arguments and updating users via def-use chain. This keeps the
// implementation straightfoward for all inputs.
//
// The function "insertDirectiveIntrinsics" creates a WRN region around the
// for-loop. VPlan works on this region. The beginning of the WRN region is
// marked with @llvm.directive.region.entry() and the end with
// @llvm.directive.region.end(). In the above example, a new basic-block is
// created before loop's header:
//
// simd.begin.region:                                ; preds = %entry
// %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(),
// "QUAL.OMP.UNIFORM"(float* %a), "QUAL.OMP.LINEAR"(i32 %k, i32 1),
// "QUAL.OMP.SIMDLEN"(i32 4) ]
//     br label %simd.loop.header
//
// and a new basic-block is emitted after loop's latch:
//
// simd.loop.latch:                            ; preds = %simd.loop.header
// %indvar = add nuw nsw i32 %index, 1
// %vl.cond = icmp ult i32 %indvar, 4
// br i1 %vl.cond, label %simd.loop.header, label %simd.end.region
//
// The pass must run at all optimization levels because it is possible that
// a loop calling the vector function is vectorized, but the vector function
// itself is not vectorized. For example, above main.cpp may be compiled at
// -O2, but dowork.cpp may be compiled at -O0. Therefore, it is required that
// the attribute list for the vector function specify all variants that must
// be generated by this pass so as to avoid any linking problems. This pass
// also serves to canonicalize the input IR to the loop vectorizer.

#include "llvm/Transforms/Utils/Intel_VecClone.h"
#include "llvm/ADT/StringExtras.h"
#include "llvm/Analysis/Directives.h"
#include "llvm/Analysis/GlobalsModRef.h"
#include "llvm/Analysis/Intel_Andersens.h"
#include "llvm/Analysis/Passes.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/Analysis/Intel_OPAnalysisUtils.h"
#include "llvm/IR/BasicBlock.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/DIBuilder.h"
#include "llvm/IR/DebugInfo.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/InitializePasses.h"
#include "llvm/PassRegistry.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"
#include "llvm/TargetParser/X86TargetParser.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include "llvm/Transforms/Utils/GeneralUtils.h"
#include "llvm/Transforms/Utils/IntrinsicUtils.h"
#include <map>
#include <set>
#include <string>

#include "llvm/IR/Verifier.h"

#define SV_NAME "vec-clone"
#define DEBUG_TYPE "VecClone"

using namespace llvm;

extern bool Usei1MaskForSimdFunctions;

bool VFABILegalizationEnabled = true;
static cl::opt<bool, true> LegalizationEnabled(
    "vec-clone-legalize-enabled", cl::Hidden,
    cl::location(VFABILegalizationEnabled),
    cl::desc("Enable arguments and return value legalization for "
             "-vecabi=cmdtarget vector variants."));

// Support for future opaque pointers. Will become the only option in future.
static cl::opt<bool>
    EmitTypedOMP("vec-clone-typed-omp", cl::init(true), cl::Hidden,
                 cl::desc("Emit 'TYPED' version of OMP clauses."));

static constexpr const char *VectorDispatchAttrName = "vector-dispatch";

/// Get all functions marked for vectorization in module \p M, and populate
/// map FuncVars that maps each such function to corresponding list of variants.
static void getFunctionsToVectorize(
    llvm::Module &M, MapVector<Function *, std::vector<StringRef>> &FuncVars) {

  // FuncVars will contain a 1-many mapping between the original scalar
  // function and the vector variant encoding strings (represented as
  // attributes). The encodings correspond to functions that will be created by
  // the caller of this function as vector versions of the original function.
  // For example, if foo() is a function marked as a simd function, it will have
  // several vector variant encodings like: "_ZGVbM4_foo", "_ZGVbN4_foo",
  // "_ZGVcM8_foo", "_ZGVcN8_foo", "_ZGVdM8_foo", "_ZGVdN8_foo", "_ZGVeM16_foo",
  // "_ZGVeN16_foo". The caller of this function will then clone foo() and name
  // the clones using the above name manglings. The variant encodings correspond
  // to differences in masked/non-masked execution, vector length, and target
  // vector register size, etc. For more details on the vector function
  // encodings, please refer to 'vector-function-abi-variant' attribute
  // description at https://llvm.org/docs/LangRef.html#call-site-attributes

  for (auto It = M.begin(), End = M.end(); It != End; ++It) {
    Function &F = *It;
    if (F.hasFnAttribute(VectorUtils::VectorVariantsAttrName) &&
        !F.isDeclaration()) {
      Attribute Attr = F.getFnAttribute(VectorUtils::VectorVariantsAttrName);
      StringRef VariantsStr = Attr.getValueAsString();
      SmallVector<StringRef, 8> Variants;
      VariantsStr.split(Variants, ',');
      for (unsigned i = 0; i < Variants.size(); i++) {
        FuncVars[&F].push_back(Variants[i]);
      }
    }
  }
}
// Populate \p CPUDispatchMap with vector variants CPU dispatching data (if any)
// for function \p F.
static void getVariantsCPUDispatchData(
    const Function &F,
    SmallDenseMap<StringRef, SmallVector<StringRef>> &CPUDispatchMap) {
  if (!F.hasFnAttribute(VectorDispatchAttrName))
    return;

  // CPUDispatchMap will contain a one-to-many mapping between a vector variant
  // and a list of CPU targets that the variant have to be specialized for.
  // CPU dispatch data for vector variants are separated by semicolon.
  // CPU dispatch target list for a variant is coma separated and can have just
  // a single entry. For example:
  //  _ZGVYN8v__Z4funcPi:haswell;_ZGVZN16v__Z4funcPi:skylake_avx512,tigerlake

  Attribute Attr = F.getFnAttribute(VectorDispatchAttrName);
  StringRef VariantsStr = Attr.getValueAsString();
  SmallVector<StringRef, 8> Variants;
  VariantsStr.split(Variants, ';');
  assert(!Variants.empty() && "Empty CPU dispatch data.");
  // Now parse data for each vector variant.
  for (StringRef VariantData : Variants) {
    size_t ColonPos = VariantData.find_first_of(':');
    assert(ColonPos != StringRef::npos &&
           "CPU dispatch data for vector variants is broken.");
    StringRef VectorVariant = VariantData.substr(0, ColonPos);
    assert(VFInfo::isVectorVariant(VectorVariant) &&
           "CPU dispatch data expected to be for a vector variant.");
    StringRef DispatchList = VariantData.substr(ColonPos + 1);
    assert(!DispatchList.empty() && "Empty CPU dispatch list.");

    SmallVector<StringRef, 4> DispatchTargets;
    DispatchList.split(DispatchTargets, ',');
    SmallVector<StringRef> &DispatchVec = CPUDispatchMap[VectorVariant];
    for (StringRef TargetCPU : DispatchTargets)
      DispatchVec.push_back(TargetCPU);
  }
}

#ifndef NDEBUG
// Check that the name is valid for X86 target parser.
static bool isValidCPUName(StringRef Name, const Module *M) {
  if (Name.empty())
    return false;
  const Triple TT{M->getTargetTriple()};
  return X86::parseTuneCPU(Name, TT.getArch() != llvm::Triple::x86) !=
         llvm::X86::CK_None;
}
#endif // NDEBUG

// Resolve target CPU name into name which is consumable by X86 target parser.
static StringRef resolveCPUName(StringRef CPUName, const Module *M) {
  // Lookup through additional aliases, if any.
  // I.e. resolve "corei7" to "core_i7_sse4_2".
  auto AliasName = [=](StringRef Name) -> StringRef {
    return StringSwitch<StringRef>(Name)
#define CPU_SPECIFIC_ALIAS_ADDITIONAL(NEW_NAME, NAME) .Case(NEW_NAME, NAME)
#include "llvm/TargetParser/X86TargetParser.def"
        .Default(Name);
  };
  auto GetTuneName = [=](StringRef Name) -> StringRef {
    return StringSwitch<StringRef>(AliasName(Name))
#define CPU_SPECIFIC(NAME, TUNE_NAME, MANGLING, FEATURES) .Case(NAME, TUNE_NAME)
#define CPU_SPECIFIC_ALIAS(NEW_NAME, TUNE_NAME, NAME) .Case(NEW_NAME, TUNE_NAME)
#include "llvm/TargetParser/X86TargetParser.def"
        .Default("");
  };

  // An important note!
  // This is rather strange that there are two tables used to lookup a CPU
  // names. X86 target parser uses array of CPUs defined separately and it is
  // just a subset of those defined in X86TargetParser.def. But clang front-end
  // looks through the latter when resolving CPU name for dispatch-targets
  // attribute. That particularly creates a problem that we cannot apply that
  // name directly for "target-cpu"/"tune-cpu" attributes because
  // parseArchX86/parseTuneCPU do lookups at the smaller table. For example
  // "core_i7_sse4_2" is accepted for ompx_processor clause of simd declare
  // pragma while "corei7" or "nehalem" is not accepted. So in order to generate
  // valid attribute value we need to convert "core_i7_sse4_2" to another name,
  // the alias that the X86 target parser would accept. Using tune name for
  // that purpose does not sound like the right fit but it actually does exactly
  // what we want. Looking via an alias name before further lookup is just
  // a safety measure which technically is not required. So we will keep it
  // for a case if pragma implementation changed to allow more aliases.
  StringRef TargetCpu = GetTuneName(CPUName);
  LLVM_DEBUG(dbgs() << "Dispatch target CPU " << CPUName << " resolved into "
                    << TargetCpu << "\n");
  assert(isValidCPUName(TargetCpu, M) && "Unsupported CPU name");
  return TargetCpu;
}

// This routine does actually apply CPU-specific settings for a new clone
// according to "target-dispatch" attribute data of the scalar function.
static void applyTargetCPUData(
    Function *Clone,
    const SmallDenseMap<StringRef, SmallVector<StringRef>> &CPUDispatchMap) {
  if (CPUDispatchMap.empty())
    return;
  auto It = CPUDispatchMap.find(Clone->getName());
  // Should we consider this to be an error?
  if (It == CPUDispatchMap.end())
    return;
  ArrayRef<StringRef> TargetCpuList = It->second;
  if (TargetCpuList.size() == 1) {
    // Targeted for specific CPU
    StringRef TargetCpu =
        resolveCPUName(TargetCpuList.front(), Clone->getParent());
    LLVM_DEBUG(dbgs() << "Targeting " << Clone->getName() << " for "
                      << TargetCpu << "\n");
    SmallVector<StringRef, 64> TargetCPUFeatures;
    X86::getFeaturesForCPU(TargetCpu, TargetCPUFeatures);

    Clone->addFnAttr("target-features", "+" + join(TargetCPUFeatures, ",+"));

    Clone->removeFnAttr("target-cpu");
    Clone->addFnAttr("target-cpu", TargetCpu);

    Clone->removeFnAttr("tune-cpu");
    Clone->addFnAttr("tune-cpu", TargetCpu);

    return;
  }

  // We have multiple targets. So schedule the routine for auto-cpu dispatch.

  LLVM_DEBUG(dbgs() << "Auto-cpu dispatching " << Clone->getName() << "\n");
  LLVMContext &Ctx = Clone->getContext();

  SmallVector<Metadata *> TargetMDs;
  for (StringRef TargetCPU : TargetCpuList) {
    StringRef TargetCpu = resolveCPUName(TargetCPU, Clone->getParent());
    TargetMDs.push_back(MDString::get(Ctx, TargetCpu));
  }
  MDNode *AutoCPUMultiVersionMetadata = MDNode::get(Ctx, TargetMDs);
  Clone->addMetadata("llvm.vec.auto.cpu.dispatch", *AutoCPUMultiVersionMetadata);
}

VecClone::VecClone() : ModulePass(ID) {
  initializeVecClonePass(*PassRegistry::getPassRegistry());
}

bool VecClone::runOnModule(Module &M) {
  auto &OROP = getAnalysis<OptReportOptionsPass>();
  ORBuilder.setup(M.getContext(), OROP.getVerbosity());
  return Impl.runImpl(M, &ORBuilder, getLimiter());
}

bool VecCloneImpl::vlaAllocasExist(Function &F) {
  for (auto BBIt = F.front().begin(), BBEnd = F.front().end(); BBIt != BBEnd;
       ++BBIt) {
    if (auto *Alloca = dyn_cast<AllocaInst>(BBIt)) {
      if (Alloca->isArrayAllocation() &&
          !isa<ConstantInt>(Alloca->getArraySize()))
        return true;
    }
  }
  return false;
}

// The following two functions are virtual and they are overloaded when
// VecClone is called by language-specific optimizations. Their default
// implementation is empty.
void VecCloneImpl::handleLanguageSpecifics(Function &F, PHINode *Phi,
                                           Function *Clone,
                                           BasicBlock *EntryBlock,
                                           const VFInfo &Variant,
                                           const ValueToValueMapTy &VMap) {}

void VecCloneImpl::languageSpecificInitializations(Module &M) {}

Function *VecCloneImpl::Factory::run() {
  cloneFunction();
  // Return early if initial clone failed due to legalization limitation
  // or no need for further processing.
  if (!Clone || isSimpleFunction())
    return Clone;

  LoopHeader = splitEntryIntoLoop();

  LoopPreHeader = EntryBlock->splitBasicBlock(EntryBlock->getTerminator(),
                                              "simd.loop.preheader");

  ReturnBlock = splitLoopIntoReturn();
  if (!ReturnBlock) {
    // OpenCL, it's valid to have an infinite loop inside kernel with no
    // independent forward progress guarantee. As such, creating a VecClone
    // loop around the body is required. Handle such cases.
    // For OpenMP cases, it's probably UB in the incoming IR, so creation of
    // the loop is still valid.
    ReturnBlock =
        BasicBlock::Create(Clone->getContext(), "unreachable.ret", Clone);
    IRBuilder<> B(ReturnBlock);
    B.CreateUnreachable();
  }

  LoopLatch = BasicBlock::Create(Clone->getContext(), "simd.loop.latch", Clone,
                                 ReturnBlock);
  ReturnBlock->replaceAllUsesWith(LoopLatch);

  PHINode *Phi = createPhiAndBackedgeForLoop();

  // At this point, we've gathered some parameter information and have
  // restructured the function into an entry block, a set of blocks
  // forming the loop, a loop latch block, and a return block. Now,
  // we can go through and update instructions since we know what
  // is part of the loop.

  // Create a new vector alloca instruction for all vector arguments and
  // return. Store the vector argument to the alloca. Replace users with
  // gep/load using the loop index.

  Instruction *Mask = nullptr;
  Instruction *WidenedReturn = widenVectorArgumentsAndReturn(Mask, Phi);

  // Mark uniform memory for SIMD directives
  processUniformArgs();

  // Update any linear variables with the appropriate stride. This function
  // will insert a mul/add sequence before the use of the argument. For
  // linear pointer arguments, the stride calculation is just a mul
  // instruction using the loop induction var and the stride value on the
  // argument. This mul instruction is then used as the index of the gep
  // that will be inserted before the next use of the argument. The
  // function also updates the users of the argument with the new
  // calculation involving the stride. Also mark linear memory for SIMD
  // directives.
  processLinearArgs(Phi);

  // Remove the old scalar instructions associated with the return and
  // replace with packing instructions.
  updateReturnBlockInstructions(WidenedReturn);

  // If this is the masked vector variant, insert the mask condition and
  // if/else blocks.
  if (V.isMasked())
    insertSplitForMaskedVariant(Mask, Phi);
  // Language specific hook.
  Parent->handleLanguageSpecifics(F, Phi, Clone, EntryBlock, V, VMap);

  // Insert the basic blocks that mark the beginning/end of the SIMD loop.
  insertDirectiveIntrinsics();

  // Add may-have-openmp-directive attribute since we inserted directives.
  Clone->addFnAttr("may-have-openmp-directive", "true");

  // Disable unrolling from kicking in on the simd loop.
  disableLoopUnrolling();

  return Clone;
}

void VecCloneImpl::Factory::cloneFunction() {
  StringRef FName = F.getName();
  LLVM_DEBUG(dbgs() << "Cloning Function: " << FName << "\n");
  LLVM_DEBUG(F.dump());

  Type *MaskEltTy = nullptr;
  if (V.isMasked()) {
    // IGC requires device versions of Intel math functions to have
    // masks of i32 elements
    if (llvm::isSVMLDeviceScalarFunctionName(FName))
      MaskEltTy = IntegerType::getInt32Ty(F.getContext());
    else
      MaskEltTy = Usei1MaskForSimdFunctions
                      ? Type::getInt1Ty(F.getContext())
                      : llvm::calcCharacteristicType(F, V);
  }

  llvm::buildVectorVariantLogicalSignature(F, V, MaskEltTy, LogicalArgTypes,
                                           LogicalRetType);

  ArgChunks.assign(LogicalArgTypes.size(), 1);

  if (VFABILegalizationEnabled && VFInfo::isIntelVFABIMangling(V.VectorName)) {
    if (!VFABI::supportedVectorVariantLegalization(V, LogicalArgTypes,
                                                   LogicalRetType)) {
      LLVM_DEBUG(dbgs() << "Unable to legalize " << FName
                        << " for vector variant " << V.VectorName << "\n");
      return;
    }

    VFABI::calcVectorVariantParamChunks(ArgChunks, RetChunks, LogicalArgTypes,
                                        LogicalRetType, V,
                                        DL.getPointerSizeInBits() == 64);
  }

  Clone = getOrInsertVectorVariantFunction(
      F, V, LogicalArgTypes, LogicalRetType, ArgChunks, RetChunks);

  auto SetArgName = [](Function::arg_iterator &ArgIt, StringRef Name,
                       int NumParts) {
    bool Fragmented = NumParts != 1;
    int Part = 0;
    while (--NumParts >= 0) {
      if (Fragmented)
        ArgIt->setName(Name + "." + std::to_string(Part));
      else
        ArgIt->setName(Name);
      ++ArgIt;
      ++Part;
    }
  };

  Function::arg_iterator NewArgIt = Clone->arg_begin();
  for (Argument &Arg : F.args()) {
    VMap[&Arg] = &*NewArgIt;
    ReverseVMap[&*NewArgIt] = &Arg;
    int NumChunks = ArgChunks[Arg.getArgNo()];
    SetArgName(NewArgIt, Arg.getName(), NumChunks);
  }

  if (V.isMasked()) {
    int NumChunks = ArgChunks[F.arg_size()];
    SetArgName(NewArgIt, "mask", NumChunks);
  }

  SmallVector<ReturnInst *, 8> Returns;
  CloneFunctionInto(Clone, &F, VMap, CloneFunctionChangeType::LocalChangesOnly,
                    Returns);
  // Reinstate attributes as CloneFunctionInto takes it back from the scalar
  // function.
  updateVectorVariantAttributes(*Clone, F, V, LogicalArgTypes, ArgChunks);

  // Strip off vector variants and dispatch attributes.
  AttributeMask AM;
  AM.addAttribute(VectorUtils::VectorVariantsAttrName);
  if (F.hasFnAttribute(VectorDispatchAttrName))
    AM.addAttribute(VectorDispatchAttrName);
  Clone->removeFnAttrs(AM);

  /// Add the 'align' attribute to any params with specified alignment.
  NewArgIt = Clone->arg_begin();
  for (const auto &[I, P] : enumerate(V.getParameters())) {
    MaybeAlign ParamAlign = P.Alignment;
    int NumChunks = ArgChunks[I];
    if (!ParamAlign) {
      NewArgIt = std::next(NewArgIt, NumChunks);
      continue;
    }
    while (--NumChunks >= 0) {
      assert(NewArgIt->getType()->isPtrOrPtrVectorTy() &&
             "Can only align ptr or ptr-of-vec params");
      NewArgIt->addAttr(Attribute::get(
          Clone->getContext(), Attribute::Alignment, ParamAlign->value()));
      ++NewArgIt;
    }
  }

  EntryBlock = &Clone->front();

  LLVM_DEBUG(dbgs() << "After Cloning and Parameter/Return Expansion\n");
  LLVM_DEBUG(Clone->dump());
}

BasicBlock *VecCloneImpl::Factory::splitEntryIntoLoop() {
  SmallVector<Instruction *, 4> EntryInsts;
  for (auto BBIt = EntryBlock->begin(), BBEnd = EntryBlock->end();
       BBIt != BBEnd; ++BBIt) {
    if (auto Alloca = dyn_cast<AllocaInst>(BBIt)) {
      EntryInsts.push_back(Alloca);
      // Add alloca to SIMD loop private
      PrivateMemory.insert(Alloca);
    }
  }

  BasicBlock *LpHeader =
      EntryBlock->splitBasicBlock(EntryBlock->begin(), "simd.loop.header");

  for (auto *Inst : EntryInsts) {
    Inst->removeFromParent();
    Inst->insertBefore(EntryBlock->getTerminator());
  }

  LLVM_DEBUG(dbgs() << "After Entry Block Split\n");
  LLVM_DEBUG(Clone->dump());
  return LpHeader;
}

BasicBlock *VecCloneImpl::Factory::splitLoopIntoReturn() {
  assert(count_if(*Clone,
                  [](const BasicBlock &BB) {
                    return isa<ReturnInst>(BB.getTerminator());
                  }) <= 1 &&
         "Unsupported CFG for VecClone!");

  auto RetBlockIt = find_if(*Clone, [](const BasicBlock &BB) {
    return isa<ReturnInst>(BB.getTerminator());
  });
  if (RetBlockIt == Clone->end())
    return nullptr;

  BasicBlock &RetBlock = *RetBlockIt;
  auto *Return = cast<ReturnInst>(RetBlock.getTerminator());
  return RetBlock.splitBasicBlock(Return, "return");
}

PHINode *VecCloneImpl::Factory::createPhiAndBackedgeForLoop() {
  // Create the phi node for the top of the loop header and add the back
  // edge to the loop from the loop latch.
  int VectorLength = V.getVF();
  PHINode *Phi = PHINode::Create(Type::getInt32Ty(Clone->getContext()), 2,
                                 "index", &*LoopHeader->getFirstInsertionPt());

  Constant *Inc = ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 1);
  Constant *IndInit =
      ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 0);

  Instruction *Induction =
      BinaryOperator::CreateAdd(Phi, Inc, "indvar", LoopLatch);
  Induction->setHasNoUnsignedWrap(true);
  Induction->setHasNoSignedWrap(true);

  Constant *VL =
      ConstantInt::get(Type::getInt32Ty(Clone->getContext()), VectorLength);

  Instruction *VLCmp =
      new ICmpInst(*LoopLatch, CmpInst::ICMP_ULT, Induction, VL, "vl.cond");

  BranchInst::Create(LoopHeader, ReturnBlock, VLCmp, LoopLatch);

  Phi->addIncoming(IndInit, LoopPreHeader);
  Phi->addIncoming(Induction, LoopLatch);

  LLVM_DEBUG(dbgs() << "After Loop Insertion\n");
  LLVM_DEBUG(Clone->dump());

  return Phi;
}

static AssumeInst *insertAlignmentAssumption(IRBuilder<> &Builder,
                                             Value *AlignedVal, Align ArgAlign,
                                             const DataLayout &DL) {
  CallInst *Assume =
      Builder.CreateAlignmentAssumption(DL, AlignedVal, ArgAlign.value());
  Assume->setMetadata("intel.vecclone.align.assume",
                      MDNode::get(Builder.getContext(), {}));
  return cast<AssumeInst>(Assume);
}

void VecCloneImpl::Factory::updateVectorArgumentUses(
    Argument *Arg, Argument *OrigArg, Type *ElemType, Instruction *VecArg,
    MaybeAlign ArgAlign, PHINode *Phi) {
  // This code updates argument users with a gep/load of an element for a
  // specific lane using the loop index.
  for (auto &U : make_early_inc_range(Arg->uses())) {
    auto *User = cast<Instruction>(U.getUser());

    // Don't update any users in the entry block; e.g., the store of the
    // vector argument to the widened alloca.
    if (User->getParent() == EntryBlock)
      continue;

    // If arg is returned, make sure gep and load appear in the loop.
    Instruction *InsertPt =
        isa<ReturnInst>(User) ? LoopHeader->getFirstNonPHI() : User;

    GetElementPtrInst *VecGep = nullptr;
    if (!isa<PHINode>(User))
      VecGep = GetElementPtrInst::Create(ElemType, VecArg, Phi,
                                         VecArg->getName() + ".gep", InsertPt);

    // Otherwise, we need to load the value from the gep first before
    // using it. This effectively loads the particular element from
    // the vector argument.
    if (PHINode *PHIUser = dyn_cast<PHINode>(User)) {
      BasicBlock *IncommingBB = PHIUser->getIncomingBlock(U.getOperandNo());
      VecGep = GetElementPtrInst::Create(ElemType, VecArg, Phi,
                                         VecArg->getName() + ".gep",
                                         IncommingBB->getTerminator());
    }
    assert(VecGep && "Expect VecGep to be a non-null value.");

    Type *LoadTy = VecGep->getResultElementType();
    LoadInst *ArgElemLoad =
        new LoadInst(LoadTy, VecGep, "vec." + OrigArg->getName() + ".elem",
                     false /*volatile*/, DL.getABITypeAlign(LoadTy));
    ArgElemLoad->insertAfter(VecGep);

    if (ArgAlign) {
      // If the argument had specified alignment, insert an assumption on the
      // element load to propagate this to downstream uses.
      IRBuilder<> Builder(ArgElemLoad->getNextNode());
      insertAlignmentAssumption(Builder, ArgElemLoad, *ArgAlign, DL);
    }

    Value *ArgValue = ArgElemLoad;
    Type *OrigArgTy = OrigArg->getType();
    if (OrigArgTy->isIntOrIntVectorTy(1)) {
      // If the original arg type was `i1` or `<N x i1>`, we need truncate the
      // value back to its original type after loads.
      assert(ElemType->isIntOrIntVectorTy(8) &&
             "expected element type to be promoted to i8 from i1");
      TruncInst *TruncatedLoad = new TruncInst(
          ArgElemLoad, OrigArgTy, ArgElemLoad->getName() + ".trunc");
      TruncatedLoad->insertAfter(ArgElemLoad);
      ArgValue = TruncatedLoad;
    }

    // If the user happens to be a return instruction, then the operand
    // of the return is replaced with the load also. This doesn't matter
    // because this instruction is later replaced with the return vector
    // in updateReturnBlockInstructions().
    User->setOperand(U.getOperandNo(), ArgValue);
  }
}

Value *VecCloneImpl::Factory::generateUnpackIntMask(FixedVectorType *VecArgTy,
                                                    Value *Arg,
                                                    Instruction *InsertPt) {
  // Generate unpacking code for the integer mask. Note that we may use
  // only some least significant bits off the while integer value.
  // If we have mask type i32 and 8 mask elements per chunk we generate
  // something like this (assuming double here is element type of logical
  // mask):
  // %1 = trunc i32 %mask to i8
  // %2 = bitcast i8 %1 to <8 x i1>
  // %3 = sext <8 x i1> %i2 to <8 x i64>
  // %4 = bitcast <8 x i64> %i3 to <8 x double>

  Value *ArgChunk = Arg;
  unsigned NumBits = VecArgTy->getNumElements();
  auto *ArgTy = cast<IntegerType>(Arg->getType());
  // Note that type of the argument may have unused bits.
  // We need to know what would be "useful" type, i.e. one where all bits
  // are used.
  Type *EffectiveMaskIntTy = ArgTy;
  if (NumBits < ArgTy->getBitWidth()) {
    EffectiveMaskIntTy = Type::getIntNTy(ArgTy->getContext(), NumBits);
    ArgChunk = new TruncInst(ArgChunk, EffectiveMaskIntTy,
                             Arg->getName() + ".trunc", InsertPt);
  }
  ArgChunk = new BitCastInst(
      ArgChunk,
      FixedVectorType::get(Type::getInt1Ty(ArgTy->getContext()), NumBits),
      Arg->getName() + ".vec", InsertPt);

  // If logical argument is not already integer, we need to promote (sign
  // extend) vector of i1 elements to vector of integer elements of equal
  // size to logical mask element.
  // It would also mean that we need final value cast to the type of
  // logical argument.

  Type *SextEltTy = VecArgTy->getElementType();
  bool NeedValueCast = !SextEltTy->isIntegerTy();
  if (NeedValueCast)
    SextEltTy = Type::getIntNTy(ArgTy->getContext(),
                                SextEltTy->getPrimitiveSizeInBits());
  ArgChunk = new SExtInst(ArgChunk, FixedVectorType::get(SextEltTy, NumBits),
                          Arg->getName() + ".vec.sext", InsertPt);
  if (NeedValueCast)
    ArgChunk = new BitCastInst(
        ArgChunk, FixedVectorType::get(VecArgTy->getElementType(), NumBits),
        Arg->getName() + ".vec.cast", InsertPt);

  return ArgChunk;
}

Instruction *
VecCloneImpl::Factory::widenVectorArgumentsAndReturn(Instruction *&Mask,
                                                     PHINode *Phi) {

  // Generate store of argument into allocated area (VecAI).
  // If arguments passed by multiple chunks, then corresponding number of stores
  // generated to fill in the entire location with the logical argument value.
  // If the argument is mask and does require unpacking (UnpackIntMask is true),
  // then unpacking sequence is generated before storing a mask chunk.
  auto GenStore = [this](AllocaInst *VecAI, Function::arg_iterator &ArgIt,
                         int NumParts, bool UnpackIntMask) {
    Instruction *InsertPt = EntryBlock->getTerminator();
    auto *WideVecTy = cast<FixedVectorType>(VecAI->getAllocatedType());
    FixedVectorType *VecArgTy =
        UnpackIntMask ? nullptr : cast<FixedVectorType>(ArgIt->getType());
    if (!VecArgTy) {
      // If we have to unpack mask then type of mask (or its chunk) is integer.
      // So we will need to unpuck that mask and transform it into vector type
      // of what would be chunk type for non-packed mask.
      unsigned ChunkVF = WideVecTy->getNumElements() / NumParts;
      VecArgTy = FixedVectorType::get(WideVecTy->getElementType(), ChunkVF);
    }

    Instruction *Ptr = VecAI;
    // generate bitcast to legal argument type if required.
    if (VecArgTy != WideVecTy && !VecAI->getType()->isOpaquePointerTy())
      Ptr = new BitCastInst(
          VecAI,
          PointerType::get(VecArgTy, VecAI->getType()->getAddressSpace()),
          VecAI->getName() + ".subv.cast", InsertPt);

    Align Alignmt = DL.getABITypeAlign(VecArgTy);
    int Chunk = 0;
    while (--NumParts >= 0) {
      Value *ArgChunk = UnpackIntMask
                            ? generateUnpackIntMask(VecArgTy, ArgIt, InsertPt)
                            : ArgIt;

      Instruction *ChunkPtr = Ptr;
      if (VecArgTy != WideVecTy) {
        auto *GEP = GetElementPtrInst::Create(
            VecArgTy, Ptr,
            ConstantInt::get(Type::getInt32Ty(VecArgTy->getContext()), Chunk),
            VecAI->getName() + ".gep." + std::to_string(Chunk), InsertPt);
        GEP->setIsInBounds(true);
        ChunkPtr = GEP;
      }

      auto *SI = new StoreInst(ArgChunk, ChunkPtr, false /*volatile*/, Alignmt);
      SI->insertBefore(InsertPt);

      ++Chunk;
      ++ArgIt;
    }
  };

  auto GenBitCast = [this](AllocaInst *AI, Type *ElemType,
                           const Twine &&Name) -> Instruction * {
    if (AI->getType()->isOpaquePointerTy())
      return AI;
    Instruction *BC = new BitCastInst(
        AI, PointerType::get(ElemType, AI->getType()->getAddressSpace()), Name);
    BC->insertBefore(EntryBlock->getTerminator());
    return BC;
  };

  AllocaInst *LastAlloca = nullptr;
  auto GenAlloca = [&LastAlloca, this](Type *VecType, const Twine &&Name) {
    AllocaInst *AI = new AllocaInst(VecType, DL.getAllocaAddrSpace(), nullptr,
                                    DL.getPrefTypeAlign(VecType), Name);
    if (LastAlloca)
      AI->insertAfter(LastAlloca);
    else
      AI->insertBefore(&EntryBlock->front());
    LastAlloca = AI;
    return AI;
  };

  // Create a completely new VF-widened alloca for each vector argument. Then
  // store the argument. Arguments are processed from left to right. The
  // corresponding allocas should be emitted in a specific order: the alloca
  // that corresponds to the most left argument should be emitted at the top
  // of the entry block.
  // If there are no arguments, go stright to return.

  Function::arg_iterator ArgIt = Clone->arg_begin();
  Function::arg_iterator OrigArgIt = F.arg_begin();
  for (const auto &[I, Parm] : enumerate(V.getParameters())) {
    bool IsMaskArg = Parm.isMask();
    Argument *OrigArg = nullptr;
    if (!IsMaskArg) {
      OrigArg = OrigArgIt;
      ++OrigArgIt;
    }
    // Actual argument types can be legalized to fit vector ABI requirement
    // as multiple registers may be required to pass single logical argument.
    int NumChunks = ArgChunks[I];

    // If the original parameter isn't vector, we should not widen it.
    // Some args other than the mask may not have users, but have not been
    // removed as dead. In those cases, just go on to the next argument.
    // There's no need to widen non-mask arguments with no users.
    if ((!Parm.isVector() && !Parm.isLinearVal()) ||
        (!IsMaskArg && !ArgIt->hasNUsesOrMore(1))) {
      ArgIt = std::next(ArgIt, NumChunks);
      continue;
    }

    // This function is run after the arguments have been already widened!
    Type *ArgType = ArgIt->getType();
    Type *LogicalArgType = LogicalArgTypes[I];
    assert((ArgType == LogicalArgType || NumChunks > 1 ||
            (IsMaskArg && VFABI::hasPackedMask(V) && ArgType->isIntegerTy())) &&
           "Incorrect vector function signature?");
    (void)ArgType;

    // Create a new vector alloca and bitcast to a pointer to the element
    // type. The following is an example of what the cast should look like:
    //
    // %veccast = bitcast <2 x i32>* %vec_a.addr to i32*
    //
    // geps using the bitcast will appear in a scalar form instead of
    // casting to an array or using vector. For example,
    //
    // %vecgep1 = getelementptr i32, i32* %veccast, i32 %index
    //
    // instead of:
    //
    // getelementptr inbounds [4 x i32], [4 x i32]* %a, i32 0, i64 1
    //
    // We do this to put the geps in a scalar form that can be indexed
    // using the loop index.
    // Note that for alloca we need to use logical vector type.
    AllocaInst *VecAlloca = GenAlloca(
        LogicalArgType, IsMaskArg ? "vec.mask" : "vec." + OrigArg->getName());

    Type *ElemType = OrigArg ? OrigArg->getType() : nullptr;
    if (!ElemType) {
      // If the argument is a mask, the scalar function argument does not exist.
      // We need to get this element type from logical argument type as
      // actual mask argument may be packed into integer.
      ElemType = cast<FixedVectorType>(LogicalArgType)->getElementType();
    } else if (ElemType->isIntOrIntVectorTy(1)) {
      // If the original argument type was `i1`, then we promoted it to an
      // `i8`. Use `i8` as the element type instead, to avoid a GEP into a
      // `<VF x i1>` when updating the argument's uses.
      assert(cast<FixedVectorType>(ArgType)->getElementType()->isIntegerTy(8) &&
             "expected element type to be promoted to i8 from i1");
      ElemType = ElemType->getWithNewBitWidth(8);
    }

    Instruction *VecArg =
        GenBitCast(VecAlloca, ElemType, VecAlloca->getName() + ".cast");

    Argument *Arg = ArgIt;
    // Store the vector argument into the new VF-widened alloca.
    GenStore(VecAlloca, ArgIt, NumChunks, IsMaskArg && VFABI::hasPackedMask(V));
    if (IsMaskArg) {
      Mask = VecArg;
      continue;
    }
    // Don't RAUW linear val arguments because they need to be handled in
    // processLinearArgs().
    if (!Parm.isLinearVal())
      updateVectorArgumentUses(Arg, OrigArg, ElemType, VecArg, Parm.Alignment,
                               Phi);
  }

  // If the function returns void, then don't attempt to widen to vector.
  if (Clone->getReturnType()->isVoidTy())
    return ReturnBlock->getTerminator();

  auto *FuncReturn = cast<ReturnInst>(ReturnBlock->getTerminator());
  Value *ValToStore = FuncReturn->getOperand(0);
  Type *RetEltTy = F.getReturnType();
  // GEPs into vectors of i1 do not make sense, so promote it to i8,
  // similar to later CodeGen processing.
  if (RetEltTy->isIntegerTy(1))
    RetEltTy = Type::getInt8Ty(RetEltTy->getContext());

  auto *InsertPt = dyn_cast<Instruction>(ValToStore);
  if (InsertPt) {
    InsertPt = InsertPt->getNextNode();
    // If InsertPt is a PHINode, move it to the last PHINode in the BB.
    while (isa<PHINode>(InsertPt))
      InsertPt = cast<Instruction>(InsertPt)->getNextNode();
  } else {
    // Could be returning a constant, so insert gep/store at end of the
    // LoopHeader.
    InsertPt = LoopHeader->getFirstNonPHI();
  }

  IRBuilder<> Builder(InsertPt);

  // If a conflict with the promoted type, extend.
  // TODO: write test that returns i1 argument directly. When we reach
  // here, the cast<Integer> will fail because the argument has already
  // been widened to vector. For now, check for IntegerType before
  // promoting return values.
  if (RetEltTy->isIntegerTy())
    if (auto *ValToStoreTy = dyn_cast<IntegerType>(ValToStore->getType())) {
      if (RetEltTy != ValToStoreTy) {
        assert(ValToStoreTy->getBitWidth() <
                   cast<IntegerType>(RetEltTy)->getBitWidth() &&
               "Expect the type to be promoted.");
        Value *ZExt = Builder.CreateZExt(ValToStore, RetEltTy,
                                         ValToStore->getName() + ".zext");
        ValToStore = ZExt;
      }
    }
  // Expand the return temp to a vector. Also create the bitcast to
  // scalar element type ptr so that it can be used to reference
  // individual elements in the loop using loop index.
  auto *VecRetType = cast<VectorType>(LogicalRetType);
  AllocaInst *VecAlloca = GenAlloca(VecRetType, "vec.retval");
  Instruction *VecReturn = GenBitCast(VecAlloca, RetEltTy, "ret.cast");
  Value *VecGep = Builder.CreateGEP(RetEltTy, VecReturn, Phi,
                                    VecReturn->getName() + ".gep");
  Builder.CreateAlignedStore(ValToStore, VecGep, DL.getABITypeAlign(RetEltTy),
                             false);
  // Done with the return value. Note that at this point the return instruction
  // of the Clone still remains scalar. It will be replaced with widened one
  // later in updateReturnBlockInstructions.

  LLVM_DEBUG(dbgs() << "After Parameter/Return Expansion\n");
  LLVM_DEBUG(Clone->dump());
  return VecReturn;
}

Value *VecCloneImpl::Factory::generateStrideForArgument(
    Value *Arg, Instruction *ArgUser, Value *Stride, PHINode *Phi,
    const VFParameter &Parm) {
  // For linear values, a mul + add/gep sequence is needed to generate the
  // correct value. i.e., val = linear_var + stride * loop_index;

  // Insert the stride related instructions before the user.
  IRBuilder<> Builder(ArgUser);

  if (auto *ArgTy = dyn_cast<PointerType>(Arg->getType())) {
    // For pointer types the stride is specified in bytes!
    if (!ArgTy->isOpaque()) {
      // For pointer args with variable stride, cast the Stride to the same
      // type as the loop index Phi.
      Value *StrideCast = Stride;
      if (Stride->getType() != Phi->getType()) {
        StrideCast = Builder.CreateCast(
            CastInst::getCastOpcode(Stride, false /* SrcIsSigned */,
                                    Phi->getType(), false /* DestIsSigned */),
            Stride, Phi->getType(), "stride.cast");
      }
      // Try to make compute nicely looking without byte arithmetic. Purely for
      // aesthetic purposes.
      auto *Mul = Builder.CreateMul(StrideCast, Phi, "stride.mul");

      // Linear updates to pointer arguments involves an address calculation,
      // so use gep. To properly update linear pointers we only need to
      // multiply the loop index and stride since gep is indexed starting at 0
      // from the base address passed to the vector function.

      // Mul is always generated as i32 since it is calculated using the i32
      // loop phi that is inserted by this pass. No cast on Mul is necessary
      // because gep can use a base address of one type with an index of
      // another type.
      Value *LinearArgGep =
          Builder.CreateGEP(ArgTy->getNonOpaquePointerElementType(), Arg, Mul,
                            Arg->getName() + ".gep");

      return LinearArgGep;
    }
    Value *ByteStride = Stride;
    if (Parm.isVariableStride()) {
      // If stride is a variable for opaque pointer, then it is specified as
      // as a stride in number of elements. Since we generate an i8* gep, the
      // stride needs to be specified in bytes. E.g., if %c is the element
      // stride, then the stride in bytes is %c * pointee size. The pointee
      // size information is obtained via the
      // llvm.intel.directive.elementsize intrinsic.
      Value *EltSize = PointeeTypeSize.lookup(ReverseVMap[Arg]);
      assert(EltSize && "No llvm.intel.directive.elementsize intrinsic?");
      // TODO: We can change VecClone to generate i64 phis for the loop iv and
      // make the last conversion unnecessary. This will also remove a
      // potential trunc from the incoming IR to VPlan. EltSize is always
      // generated as i64.
      ByteStride = Builder.CreateSExt(ByteStride, EltSize->getType());
      ByteStride = Builder.CreateMul(ByteStride, EltSize);
      ByteStride = Builder.CreateSExtOrTrunc(ByteStride, Phi->getType());
    }
    // GEP index should be %c * pointee size * loopiv
    ByteStride = Builder.CreateMul(ByteStride, Phi, "stride.bytes");
    auto *Gep = Builder.CreateGEP(Builder.getInt8Ty(), Arg, ByteStride,
                                  Arg->getName() + ".gep");
    return Gep;
  }

  // If Stride is a constant, then it has already been typed appropriately in
  // processLinearArgs when creating the constant value. However, if Stride is
  // in a variable then it may need to be cast to the arg type. The decision to
  // cast here is done because IRBuilder is needed for correct insertion.
  Value *StrideCast = Stride;
  if (Stride->getType() != Arg->getType()) {
    StrideCast = Builder.CreateCast(
        CastInst::getCastOpcode(Stride, false /* SrcIsSigned */, Arg->getType(),
                                false /* DestIsSigned */),
        Stride, Arg->getType(), "stride.cast");
  }

  // Cast the loop index to match the stride type for the multiply part of the
  // stride calculation. Integer stride is calculated as:
  // arg + loop idx * stride.
  Value *PhiCast = Phi;
  if (StrideCast->getType() != Phi->getType()) {
    PhiCast =
        Builder.CreateCast(CastInst::getCastOpcode(Phi, false /* SrcIsSigned */,
                                                   StrideCast->getType(),
                                                   false /* DestIsSigned */),
                           Phi, StrideCast->getType(), "phi.cast");
  }

  Value *Mul = Builder.CreateMul(StrideCast, PhiCast, "stride.mul");

  // Floating point strides are not allowed.
  assert(!Arg->getType()->isFloatingPointTy() &&
         "The value should not be floating point!");
  // At this point, the loop index, stride, and arg types should match.
  auto Add = Builder.CreateAdd(Arg, Mul, "stride.add");
  return Add;
}

// Emit debug intrinsics to describe parameters in the clone.
static void emitDebugForParameter(Value *ArgValue, AllocaInst *Alloca,
                                  LoadInst *Load) {
  Function *Clone = cast<Argument>(ArgValue)->getParent();
  DISubprogram *CloneSP = Clone->getSubprogram();
  if (!CloneSP)
    return; // No debug information in the clone.

  // Debug intrinsics referencing the original argument have been rewritten
  // to point to the "Load" instruction by emitLoadStoreForParameter().
  SmallVector<DbgVariableIntrinsic *, 1> DVIs;
  findDbgUsers(DVIs, Load);
  if (DVIs.empty())
    return; // No debug information found for the parameter.

  // Ignore variables described by llvm.dbg.declare intrinsics.
  auto IsDbgDeclare = [](DbgVariableIntrinsic *I) -> bool {
    return isa<DbgDeclareInst>(I);
  };
  if (llvm::any_of(DVIs, IsDbgDeclare))
    return;

  Module *M = Clone->getParent();
  DICompileUnit *Unit = CloneSP->getUnit();
  DIBuilder DIB(*M, true, Unit);

  SmallPtrSet<DILocalVariable *, 1> VariableSet;
  for (DbgVariableIntrinsic *DVI : DVIs) {
    DILocalVariable *DV = DVI->getVariable();
    DIExpression *DE = DVI->getExpression();
    DILocation *DL = DVI->getDebugLoc().get();

    // Emit only one debug intrinsic per-variable per-parameter.
    if (DVI->getNumVariableLocationOps() == 1 && !VariableSet.contains(DV)) {
      DIB.insertDbgValueIntrinsic(ArgValue, DV, DE, DL, Alloca);
      VariableSet.insert(DV);
    }
  }
}

// Emits store and load of \p ArgValue and replaces all uses of \p ArgValue with
// the load.
static LoadInst *emitLoadStoreForParameter(AllocaInst *Alloca, Value *ArgValue,
                                           MaybeAlign ArgAlign,
                                           BasicBlock *LoopPreHeader) {
  // Emit the load in the simd.loop.preheader block.
  IRBuilder<> Builder(&*LoopPreHeader->begin());
  LoadInst *Load = Builder.CreateLoad(Alloca->getAllocatedType(), Alloca,
                                      "load." + ArgValue->getName());
  ArgValue->replaceAllUsesWith(Load);

  if (ArgAlign) {
    // If the argument is aligned, we need to insert an alignment assumption on
    // the loaded value, so this alignment gets propagated downstream.
    insertAlignmentAssumption(Builder, Load, *ArgAlign,
                              Load->getModule()->getDataLayout());
  }

  // After updating the uses of the function argument with its stack variable,
  // we emit the store.
  Builder.SetInsertPoint(Alloca->getNextNode());
  Builder.CreateStore(ArgValue, Alloca);

  // Emit debug information into the entry block for the parameters.
  emitDebugForParameter(ArgValue, Alloca, Load);

  return Load;
}

// Create an alloca for args where memory is not already on the stack.
// This is done regardless of whether there is an existing alloca for
// the argument. In those cases we still create a new alloca that will
// be marked appropriately (e.g., linear, uniform) and the old alloca
// can be marked as private because loads/stores using it will be in
// the loop. This approach helps simplify the implementation because
// we don't have to distinguish between opt levels.
static void getOrCreateArgMemory(Argument &Arg, BasicBlock *EntryBlock,
                                 BasicBlock *LoopPreHeader, MaybeAlign ArgAlign,
                                 Value *&ArgVal, Value *&ArgMemory) {
  ArgVal = &Arg;
  ArgMemory = &Arg;
  // TODO:  Should it be hasPointeeInMemoryValueAttr() instead?
  if (Arg.hasByValAttr())
    return;
  IRBuilder<> Builder(&*EntryBlock->begin());
  AllocaInst *ArgAlloca =
      Builder.CreateAlloca(Arg.getType(), nullptr, "alloca." + Arg.getName());
  ArgVal = emitLoadStoreForParameter(cast<AllocaInst>(ArgAlloca), ArgVal,
                                     ArgAlign, LoopPreHeader);
  ArgMemory = ArgAlloca;
}

void VecCloneImpl::Factory::processUniformArgs() {
  Function::arg_iterator ArgIt = Clone->arg_begin();
  for (const auto &[I, Parm] : enumerate(V.getParameters())) {
    int NumChunks = ArgChunks[I];
    if (!Parm.isUniform()) {
      ArgIt = std::next(ArgIt, NumChunks);
      continue;
    }
    assert(NumChunks == 1 && "Uniform argument passed in chunks?");
    Argument *Arg = ArgIt;
    Value *ArgVal;
    Value *ArgMemory;
    getOrCreateArgMemory(*Arg, EntryBlock, LoopPreHeader, Parm.Alignment,
                         ArgVal, ArgMemory);
    UniformMemory[Arg] = std::make_pair(ArgMemory, ArgVal);
    ++ArgIt;
  }
}

void VecCloneImpl::Factory::processLinearArgs(PHINode *Phi) {
  // Add stride to arguments marked as linear. These instructions are added
  // before the arg user and uses are updated accordingly.

  auto GetVariableStrideArg = [this](const VFParameter &P) {
    int OrigStrideArgPos = P.getStrideArgumentPosition();
    int CloneStrideArgPos = 0;
    while (--OrigStrideArgPos >= 0)
      CloneStrideArgPos += ArgChunks[OrigStrideArgPos];
    Argument *StrideArg = Clone->getArg(CloneStrideArgPos);
    return StrideArg;
  };

  Function::arg_iterator ArgIt = Clone->arg_begin();
  for (const auto &[I, Parm] : enumerate(V.getParameters())) {
    int NumChunks = ArgChunks[I];
    Argument *Arg = ArgIt;
    if (Parm.isLinear() || Parm.isLinearRef()) {
      Value *StrideVal = nullptr;
      Value *ArgVal = nullptr;
      Value *ArgMemory = nullptr;
      if (Parm.isConstantStrideLinear()) {
        int Stride = Parm.getStride();
        if (auto *PtrTy = dyn_cast<PointerType>(Arg->getType())) {
          // For pointer types with constant stride, the gep index is the same
          // type as the phi (loop index), which is i32.
          if (!PtrTy->isOpaque()) {
            unsigned PointeeEltSize =
                DL.getTypeAllocSize(PtrTy->getNonOpaquePointerElementType());
            assert(Stride % PointeeEltSize == 0 &&
                   "Stride is expected to be a multiple of element size!");
            Stride /= PointeeEltSize;
          }
          // For opaque pointers with constant stride, the pointee type
          // information is already "baked" into the variant encoding because
          // since the stride is specified in bytes and we will generate an i8*
          // gep in generateStrideForArgument(). Thus, no need to adjust the
          // stride in that case.
          StrideVal = ConstantInt::get(Phi->getType(), Stride);
        } else {
          assert(!Parm.isLinearRef() &&
                 "linear ref modifier should be pointer");
          assert(Arg->getType()->isIntegerTy() &&
                 "Expected integer type for arg");
          // For integer types with constant stride, the value of the stride
          // must be the same type as the arg.
          StrideVal = GeneralUtils::getConstantValue(
              Arg->getType(), Clone->getContext(), Stride);
        }
        getOrCreateArgMemory(*Arg, EntryBlock, LoopPreHeader, Parm.Alignment,
                             ArgVal, ArgMemory);
        LinearMemory[ArgMemory] = StrideVal;
      } else if (Parm.isVariableStride()) {
        // Get the stride value from the argument holding it.
        Argument *StrideArg = GetVariableStrideArg(Parm);
        StrideVal = UniformMemory[StrideArg].second;
        getOrCreateArgMemory(*Arg, EntryBlock, LoopPreHeader, Parm.Alignment,
                             ArgVal, ArgMemory);
        LinearMemory[ArgMemory] = UniformMemory[StrideArg].first;
      } else {
        llvm_unreachable("Unsupported linear modifier");
      }

      // Eventually, this for loop can be removed once we start relying on
      // just marking the allocas as linear and letting VPlan deal with
      // accounting for stride calculations rather than VecClone inserting
      // new instructions for the stride and updating users.
      for (auto &U : make_early_inc_range(ArgVal->uses())) {
        auto *User = cast<Instruction>(U.getUser());

        if (Parm.isAligned()) {
          // Skip the alignment assumption we added
          auto *Assume = dyn_cast<AssumeInst>(User);
          if (Assume && Assume->hasMetadata("intel.vecclone.align.assume"))
            continue;
        }

        Value *StrideInst =
            generateStrideForArgument(ArgVal, User, StrideVal, Phi, Parm);
        User->setOperand(U.getOperandNo(), StrideInst);
      }
    } else if (Parm.isLinearUVal() || Parm.isLinearVal()) {
      // The following example shows the before/after LLVM of the linear uval
      // modifier transformation. The basic idea is that the stride calculation
      // must be added to the value loaded from the reference (pointer) arg.
      // In order to do this for all optimization levels, we keep track of any
      // loads from either the arg directly or through aliases (store/load of
      // the arg). In this example, %0 is the value for which stride must be
      // applied. For the val reference modifier, the arg is passed via vector,
      // but all values can be recreated from the value loaded from the pointer
      // at lane 0 using the stride. After that is done, both uval/val follow
      // the same code path.
      //
      // Input LLVM for #pragma omp declare simd linear(val(k):2)
      //
      // simd.loop.preheader:
      //   br label %simd.loop.header
      //
      // simd.loop.header:
      //   %index = phi i32 [ 0, %simd.loop.preheader ],
      //                    [ %indvar, %simd.loop.latch ]
      //   %0 = load i64, i64* %x, align 8, !tbaa !4
      //   %add = add nsw i64 %0, 1
      //   %ret.cast.gep = getelementptr i64, i64* %ret.cast, i32 %index
      //   store i64 %add, i64* %ret.cast.gep, align 8
      //   br label %simd.loop.latch
      //
      // Output LLVM:
      //
      // entry:
      //   %alloca.x = alloca i64*, align 8
      //   store i64* %x, i64** %alloca.x, align 8
      //   br label %simd.loop.preheader
      //
      // simd.loop.preheader:
      //   %load.x = load i64*, i64** %alloca.x, align 8
      //   br label %simd.loop.header
      //
      // simd.loop.header:
      //   %index = phi i32 [ 0, %simd.loop.preheader ],
      //                    [ %indvar, %simd.loop.latch ]
      //   %0 = load i64, i64* %load.x, align 8, !tbaa !4
      //   %phi.cast = zext i32 %index to i64
      //   %stride.mul = mul i64 2, %phi.cast (stride on pragma is 2)
      //   %stride.add = add i64 %0, %stride.mul
      //   %add = add nsw i64 %stride.add, 1
      //   %ret.cast.gep = getelementptr i64, i64* %ret.cast, i32 %index
      //   store i64 %add, i64* %ret.cast.gep, align 8
      //   br label %simd.loop.latch

      // Note: for unoptimized incoming LLVM, there will be an additional
      // level of pointer dereferencing because the arg (a pointer) will be
      // stored/loaded to/from local memory. Thus, the first load will result
      // in a pointer, essentially an alias to the original arg. The second
      // load will be used to access the actual value, which is then used to
      // apply the stride.

      Value *ArgMemory = nullptr;
      Value *ScalarArg = nullptr;
      if (Parm.isLinearVal()) {
        IRBuilder<> Builder(&*EntryBlock->begin());
        Instruction *PreHeaderInsertPt = &*LoopPreHeader->begin();
        // Linear reference val arguments are passed as vector, so extract the
        // base ptr (elem 0) and create local memory for it. Then, replace all
        // users of Arg with the load from this memory. Since this is a special
        // case for linear arguments, we don't use getOrCreateArgMemory() for
        // two reasons.
        //
        // 1) We don't want to clobber the extract that was just created since
        //    it will use Arg.
        // 2) We can't use RAUW because the argument has been widened and RAUW
        //    will complain. See comment below.
        auto *BasePtrExtract = Builder.CreateExtractElement(
            Arg, (uint64_t)0, Arg->getName() + ".ext");
        auto *ArgElemType = cast<VectorType>(Arg->getType())->getElementType();
        ArgMemory = Builder.CreateAlloca(
            ArgElemType, nullptr, "alloca." + Arg->getName() + ".scalar");
        Builder.CreateStore(BasePtrExtract, ArgMemory);
        Builder.SetInsertPoint(PreHeaderInsertPt);
        ScalarArg =
            Builder.CreateLoad(cast<AllocaInst>(ArgMemory)->getAllocatedType(),
                               ArgMemory, "load." + ArgMemory->getName());
        for (auto &U : make_early_inc_range(Arg->uses())) {
          auto *User = cast<Instruction>(U.getUser());
          // Replace uses of Arg with the extracted base ptr. Be careful not to
          // replace the newly created extract instruction. We only want to
          // replace the uses in the loop.
          if (User->getParent() != EntryBlock) {
            // Note: we can't update users with RAUW because the argument has
            // become vector and RAUW will complain if the value replaced has a
            // different type than the new value. i.e., once the argument is
            // widened in the function signature, it's type is reflected in all
            // uses.
            User->setOperand(U.getOperandNo(), ScalarArg);
          }
        }
      } else {
        Value *ArgVal = nullptr;
        getOrCreateArgMemory(*Arg, EntryBlock, LoopPreHeader, Parm.Alignment,
                             ArgVal, ArgMemory);
        ScalarArg = ArgVal; // the load from the new arg memory

        // The linear(uval()) parameter is a uniform scalar pointer. It points
        // to a linear value, actually an initial scalar value. The vector value
        // for it is calculated by formula: *p + step * {0,1, 2, ...VF-1}. The
        // vector store to this pointer should not be executed. We need to
        // re-direct that store to a fake memory. The openmp standard explicitly
        // says that "the program must not depend on the value of the list item
        // upon return from the procedure" and "the program must not depend on
        // the storage of the argument in the procedure".
        assert(Arg->getType()->isPointerTy() &&
               "Pointer VecClone processLinearArgs Arg type is expected.");
        // We can't use PointeeTypeSize here due to the stores can be of
        // different types, we need a real type of the store. If there is no
        // type detected we don't create anything.
        // TODO: check the case when the values of different types are stored
        // using that pointer
        Type *ArgPtrElemType = inferPtrElementType(*ArgVal);
        if (ArgPtrElemType) {
          IRBuilder<> Builder(EntryBlock->getTerminator());
          const unsigned VF = V.getVF();
          auto *ArgPtrElemVectorType =
              VectorType::get(ArgPtrElemType, VF, false);
          // This generates fake vector memory for storing updated value
          AllocaInst *ArgFakeAlloca = Builder.CreateAlloca(
              ArgPtrElemVectorType, ArgVal->getType()->getPointerAddressSpace(),
              nullptr, "alloca.fake." + Arg->getName());
          Value *ScalarLoad = Builder.CreateLoad(
              ArgPtrElemType,
              Builder.CreateLoad(
                  cast<AllocaInst>(ArgMemory)->getAllocatedType(), ArgMemory,
                  "load." + Arg->getName()),
              "load.elem." + Arg->getName());
          Value *VectorValue = Builder.CreateVectorSplat(VF, ScalarLoad);
          Builder.CreateStore(VectorValue, ArgFakeAlloca);

          Builder.SetInsertPoint(LoopHeader->getFirstNonPHI());
          Value *VecGep = Builder.CreateGEP(
              ArgPtrElemType,
              Builder.CreatePointerCast(ArgFakeAlloca,
                                        ArgPtrElemType->getPointerTo()),
              Phi, ArgFakeAlloca->getName() + ".gep");
          // Replace all direct uses to fake vector memory
          ArgVal->replaceAllUsesWith(VecGep);
          // Update ScalarArg to point fake vector memory so that we will add
          // stride for it
          ScalarArg = VecGep;
        }
      }

      // For both uval/val modifiers, ScalarArg is now the pointer argument
      // that is used for finding the loaded values for which stride is applied.

      SmallVector<Value *, 4> ArgLocalMem;
      SmallVector<LoadInst *, 4> ArgValLoads;
      // Determine whether the argument is stored through local memory. If
      // so, then record the local memory used for the arg. Later, we'll
      // find the associated loads from this memory to find aliases for the
      // arg. If a value is loaded directly from the arg, then record that
      // this is a direct load from the arg.
      for (auto *U : ScalarArg->users()) {
        auto *User = cast<Instruction>(U);
        // Unoptimized case where the incoming LLVM has a store of the argument
        // (pointer) is made to local memory. Note that at this point we have
        // created a new alloca/store/load for the arg and that load has now
        // replaced the original parameter. E.g.
        // Incoming LLVM: store i64* %x, i64** %x.addr, align 8
        // As of this point: store i64* %load.x, i64** %x.addr, align 8
        // %x.addr will be recorded in ArgLocalMem and was the original alloca
        // for the arg. However, by always handling args through memory (even
        // for optimized incoming LLVM) this allows VecClone to work the same
        // across all opt levels because RAUW can simply be done on any
        // existing memory and all LLVM can be handled basically the same.
        // However, here we have the additional requirement of finding the load
        // that will yield the value on which the stride needs to be applied. We
        // have two cases for that:
        // 1) There is an additional store/load through memory to get the
        //    arg pointer to load the value from.
        // 2) The arg pointer is loaded from directly.
        if (auto *StoreUser = dyn_cast<StoreInst>(User)) {
          // Case 1
          // Don't include any possible store in the entry block because for
          // Val ref modifiers, there will be an extract of elem 0 from the
          // original arg and stored to local memory. We're only interested
          // in the stores that are made within the loop.
          if (StoreUser->getParent() != EntryBlock)
            ArgLocalMem.push_back(StoreUser->getPointerOperand());
        }
        // Case 2 - Value is loaded directly from the argument.
        // E.g., %0 = load i64, i64* %load.x, align 8
        if (auto *ArgValLoad = dyn_cast<LoadInst>(User))
          ArgValLoads.push_back(ArgValLoad);
      }

      // Find the aliases for the pointer args and the loads from those
      // aliases. Aliases in this context refers to the loaded pointer from
      // local memory. E.g., if %load.x is the arg pointer
      // store i64* %load.x, i64** %x.addr, align 8
      // %0 = load i64*, i64** %x.addr, align 8
      // %1 = load i64, i64* %0
      // %0 is the alias for the arg pointer because it goes through %x.addr
      // %1 is the value loaded from the arg and stride must be applied here
      for (auto *ArgMem : ArgLocalMem) {
        for (auto *ArgMemU : ArgMem->users()) {
          auto *ArgMemUser = cast<Instruction>(ArgMemU);
          if (isa<LoadInst>(ArgMemUser)) {
            for (auto *ArgLoadU : ArgMemUser->users()) {
              auto *ArgLoadUser = cast<Instruction>(ArgLoadU);
              if (auto *ArgLoad = dyn_cast<LoadInst>(ArgLoadUser))
                ArgValLoads.push_back(ArgLoad);
            }
          }
        }
      }

      // At this point ArgValLoads contains all the Values for which stride
      // needs to be applied.

      // Get the constant or variable stride value
      Value *StrideVal = nullptr;
      if (Parm.isConstantStrideLinear()) {
        int Stride = Parm.getStride();
        StrideVal = GeneralUtils::getConstantValue(Phi->getType(),
                                                   Clone->getContext(), Stride);
        LinearMemory[ArgMemory] = StrideVal;
      } else if (Parm.isVariableStride()) {
        // Get the stride value from the argument holding it.
        Argument *StrideArg = GetVariableStrideArg(Parm);
        StrideVal = UniformMemory[StrideArg].second;
        // TODO: temporarily just set stride to constant 0 because there is an
        // importing bug in VPlan that needs to be fixed for both HIR and LLVM.
        // Since VecClone adds the instructions for stride calculation in this
        // function, VPlan doesn't need to have a correct value set to generate
        // correct code. However, once the linear processing moves to VPlan,
        // this will need to be fixed. All that needs to be done here is to
        // remove TempStrideVal and replace with the value coming from
        // UniformMemory.
        Value *TempStrideVal = GeneralUtils::getConstantValue(
            Phi->getType(), Clone->getContext(), 0);
        // Uncomment this line of code and remove the line after it once
        // importing bugs are fixed.
        // LinearMemory[ArgMemory] = UniformMemory[StrideArg].first;
        LinearMemory[ArgMemory] = TempStrideVal;
      } else {
        llvm_unreachable("Unsupported linear modifier");
      }
      // Generate stride instruction and update users of the load.
      for (auto *ArgValLoad : ArgValLoads) {
        for (auto &U : make_early_inc_range(ArgValLoad->uses())) {
          auto *User = cast<Instruction>(U.getUser());
          Value *StrideInst =
              generateStrideForArgument(ArgValLoad, User, StrideVal, Phi, Parm);
          User->setOperand(U.getOperandNo(), StrideInst);
        }
      }
    }
    ArgIt = std::next(ArgIt, NumChunks);
  }

  LLVM_DEBUG(dbgs() << "After Linear Updates\n");
  LLVM_DEBUG(Clone->dump());
}

void VecCloneImpl::Factory::updateReturnBlockInstructions(
    Instruction *WidenedReturn) {
  // If the vector function returns void, then there is no need to do any
  // packing. The only instruction in the ReturnBlock is 'ret void', so
  // we can just leave this instruction and we're done.
  if (Clone->getReturnType()->isVoidTy())
    return;

  // Remove all instructions from the return block. These will be replaced
  // with the instructions necessary to return a vector temp. The verifier
  // will complain if we remove the definitions of users first, so remove
  // instructions from the bottom up.
  while (!ReturnBlock->empty())
    ReturnBlock->back().eraseFromParent();

  auto GetRetAllocaInst = [](Instruction *WidenedReturn) {
    Value *V = WidenedReturn;
    if (!WidenedReturn->getType()->isOpaquePointerTy()) {
      assert(isa<BitCastInst>(WidenedReturn) && "Expected cast instruction");
      V = WidenedReturn->getOperand(0);
    }
    return cast<AllocaInst>(V);
  };

  // Pack up the elements into a vector temp and return it.
  // Return can't be void here due to early exit at the top of this function.
  // With regular pointers WidenedReturn is expected to be a bitcast instruction
  // because we always create a vector alloca for the return value and cast that
  // to a scalar pointer for use within the loop. I.e., this cast is used with
  // the loop index to reference a specific vector element. With opaque pointers
  // WidenedReturn is already the return vector alloca instruction. At the point
  // of the function return, we load return value from the vector alloca.
  AllocaInst *RetAlloca = GetRetAllocaInst(WidenedReturn);

  int NumParts = RetChunks;
  if (NumParts == 1) {
    // No legalization required. Return just a vector value.
    auto *VecReturn =
        new LoadInst(Clone->getReturnType(), RetAlloca, "vec.ret", ReturnBlock);
    ReturnInst::Create(Clone->getContext(), VecReturn, ReturnBlock);
  } else {
    // Legalize vector return value by returning a struct by value with an
    // elements the required number of chunks.
    auto *RetTy = cast<StructType>(Clone->getReturnType());
    Type *ChunkTy = RetTy->getElementType(0);

    Value *Ptr = RetAlloca;
    if (!Ptr->getType()->isOpaquePointerTy())
      Ptr = new BitCastInst(
          Ptr,
          PointerType::get(ChunkTy, RetAlloca->getType()->getAddressSpace()),
          Ptr->getName() + ".subv.cast", ReturnBlock);

    int Chunk = 0;
    Value *VecReturn = PoisonValue::get(RetTy);
    while (--NumParts >= 0) {
      auto *ChunkGEP = GetElementPtrInst::Create(
          ChunkTy, Ptr,
          ConstantInt::get(Type::getInt32Ty(ChunkTy->getContext()), Chunk),
          RetAlloca->getName() + ".gep." + std::to_string(Chunk), ReturnBlock);
      ChunkGEP->setIsInBounds(true);

      auto *ChunkLd = new LoadInst(
          ChunkTy, ChunkGEP, "vec.ret." + std::to_string(Chunk), ReturnBlock);

      VecReturn = InsertValueInst::Create(
          VecReturn, ChunkLd, Chunk,
          RetAlloca->getName() + ".ins." + std::to_string(Chunk), ReturnBlock);
      ++Chunk;
    }

    ReturnInst::Create(Clone->getContext(), VecReturn, ReturnBlock);
  }

  LLVM_DEBUG(dbgs() << "After Return Block Update\n");
  LLVM_DEBUG(Clone->dump());
}

static Type *getMemoryType(Value *Memory) {
  if (AllocaInst *Alloca = dyn_cast<AllocaInst>(Memory))
    return Alloca->getAllocatedType();
  else if (Argument *Arg = dyn_cast<Argument>(Memory)) {
    // Represents byval args where there is already a stack slot
    // available for the arg.
    return Arg->getPointeeInMemoryValueType();
  } else
    llvm_unreachable("Arg memory should be on the stack");
}

// Creates the simd.begion.region block which marks the beginning of the WRN
// region. Given the function arguments, emits the correct directive in the
// simd.begion.region block. If the arguments are linear or uniform, a new
// basic block (simd.loop.preheader) is created between the simd.begin.region
// block and the simd.loop.header block. VPLoopEntity needs the addresses of
// the uniform/linear arguments. For this reason, we need to pass the address
// of the arguments to the directives instead of their values. In VecClone, we
// have the values, not the addresses. So, we create a stack variable for each
// uniform and linear argument and store them in the stack (the store is
// emitted in the EntryBlock). Next, we load it and we update its uses (the load
// is emitted in simd.loop.preheader). This is similar to the code emitted by
// the front-end for simd loops.
CallInst *VecCloneImpl::Factory::insertBeginRegion() {
  IRBuilder<> Builder(&*EntryBlock->begin());

  SmallVector<llvm::OperandBundleDef, 4> OpndBundles;
  OpndBundles.emplace_back(
      std::string(IntrinsicUtils::getDirectiveString(DIR_OMP_SIMD)),
      std::nullopt);

  auto Clause = [](OMP_CLAUSES ClauseId, auto &&...Mods) -> std::string {
    std::initializer_list<StringRef> Modifiers = {Mods...};
    std::string Result = IntrinsicUtils::getClauseString(ClauseId).str();
    if (Modifiers.size() == 0)
      return Result;
    raw_string_ostream SS(Result);
    SS << ":";
    ListSeparator LS;
    for (StringRef Mod : Modifiers)
      SS << LS << Mod;
    return Result;
  };

  auto AddTypedClause = [&OpndBundles, Clause](OMP_CLAUSES ClauseId, Value *Ptr,
                                               Type *Ty, auto &&...Ops) {
    if (!EmitTypedOMP) {
      OpndBundles.push_back(OperandBundleDef{Clause(ClauseId), {Ptr, Ops...}});
      return;
    }

    std::string ClauseString = IntrinsicUtils::getClauseString(ClauseId).str();
    std::string ClauseStringUpdates = "TYPED";
    if (Ptr->getType()->isOpaquePointerTy() &&
        ClauseString == "QUAL.OMP.LINEAR") {
      ClauseStringUpdates += ".PTR_TO_PTR";
      Ty = llvm::Type::getInt8Ty(Ty->getContext());
    }
    OpndBundles.push_back(OperandBundleDef{
        Clause(ClauseId, ClauseStringUpdates),
        {Ptr, Constant::getNullValue(Ty),
         ConstantInt::get(Type::getInt32Ty(Ty->getContext()), 1), // #Elts
         Ops...}});
  };

  // Insert vectorlength directive
  OpndBundles.emplace_back(Clause(QUAL_OMP_SIMDLEN),
                           Builder.getInt32(V.getVF()));

  // Mark linear memory for the SIMD directives
  for (const auto &LinearMem : LinearMemory) {
    Type *LinearTy = getMemoryType(LinearMem.first);
    AddTypedClause(QUAL_OMP_LINEAR, LinearMem.first, LinearTy,
                   LinearMem.second);
  }

  // Mark uniform memory for the SIMD directives
  for (const auto &UniformMem : UniformMemory) {
    // The alloca for the arg.
    Value *ArgMemory = UniformMem.second.first;
    Type *UniformTy = getMemoryType(ArgMemory);
    AddTypedClause(QUAL_OMP_UNIFORM, ArgMemory, UniformTy);
  }

  // Mark private memory for the SIMD directives
  for (Value *PrivateMem : PrivateMemory) {
    assert(isa<AllocaInst>(PrivateMem) &&
           "private memory is expected to be an alloca instruction");
    AddTypedClause(QUAL_OMP_PRIVATE, PrivateMem,
                   cast<AllocaInst>(PrivateMem)->getAllocatedType());
  }

  // Create simd.begin.region block which indicates the begining of the WRN
  // region.
  CallInst *SIMDBeginCall = CallInst::Create(
      Intrinsic::getDeclaration(&M, Intrinsic::directive_region_entry),
      std::nullopt, OpndBundles, "entry.region");
  SIMDBeginCall->insertBefore(EntryBlock->getTerminator());
  EntryBlock->splitBasicBlock(SIMDBeginCall, "simd.begin.region");
  return SIMDBeginCall;
}

void VecCloneImpl::Factory::insertEndRegion(CallInst *EntryDirCall) {
  BasicBlock *EndDirectiveBlock = BasicBlock::Create(
      Clone->getContext(), "simd.end.region", Clone, ReturnBlock);

  BranchInst *LoopLatchBranch =
      dyn_cast<BranchInst>(LoopLatch->getTerminator());
  assert(LoopLatchBranch && "Expecting br instruction for loop latch block");
  LoopLatchBranch->setOperand(1, EndDirectiveBlock);

  BranchInst::Create(ReturnBlock, EndDirectiveBlock);

  CallInst *SIMDEndCall =
      IntrinsicUtils::createSimdDirectiveEnd(M, EntryDirCall);
  SIMDEndCall->insertBefore(EndDirectiveBlock->getTerminator());
}

void VecCloneImpl::Factory::insertDirectiveIntrinsics() {
  CallInst *EntryDirCall = insertBeginRegion();
  insertEndRegion(EntryDirCall);
  LLVM_DEBUG(dbgs() << "After Directives Insertion\n");
  LLVM_DEBUG(Clone->dump());
}

bool VecCloneImpl::Factory::isSimpleFunction() {
  // For really simple functions, there is no need to go through the process
  // of inserting a loop.

  // Example:
  //
  // void foo(void) {
  //   return;
  // }
  //
  // No need to insert a loop for this case since it's basically a no-op. Just
  // clone the function and return. It's possible that we could have some code
  // inside of a vector function that modifies global memory. Let that case go
  // through.
  return isa<ReturnInst>(EntryBlock->front()) &&
         Clone->getReturnType()->isVoidTy();
}

void VecCloneImpl::Factory::insertSplitForMaskedVariant(Instruction *Mask,
                                                        PHINode *Phi) {
  BasicBlock *LoopThenBlock = LoopHeader->splitBasicBlock(
      LoopHeader->getFirstNonPHI(), "simd.loop.then");

  BasicBlock *LoopElseBlock = BasicBlock::Create(
      Clone->getContext(), "simd.loop.else", Clone, LoopLatch);

  BranchInst::Create(LoopLatch, LoopElseBlock);

  AllocaInst *Alloca = nullptr;
  if (isa<BitCastInst>(Mask)) {
    auto *BitCast = cast<BitCastInst>(Mask);
    Alloca = cast<AllocaInst>(BitCast->getOperand(0));
  } else {
    assert(isa<AllocaInst>(Mask) && "Expected alloca inst");
    Alloca = cast<AllocaInst>(Mask);
  }

  Type *PointeeType =
      cast<VectorType>(Alloca->getAllocatedType())->getElementType();

  GetElementPtrInst *MaskGep = GetElementPtrInst::Create(
      PointeeType, Mask, Phi, "mask.gep", LoopHeader->getTerminator());

  Type *LoadTy = MaskGep->getResultElementType();
  LoadInst *MaskLoad =
      new LoadInst(LoadTy, MaskGep, "mask.parm", LoopHeader->getTerminator());

  Type *CompareTy = MaskLoad->getType();
  Instruction *MaskCmp;
  Constant *Zero;

  // Generate the compare instruction to see if the mask bit is on. In ICC, we
  // use the movemask intrinsic which takes both float/int mask registers and
  // converts to an integer scalar value, one bit representing each element.
  if (CompareTy->isIntegerTy()) {
    Zero = GeneralUtils::getConstantValue(CompareTy, Clone->getContext(), 0);
    MaskCmp = new ICmpInst(LoopHeader->getTerminator(), CmpInst::ICMP_NE,
                           MaskLoad, Zero, "mask.cond");
  } else if (CompareTy->isFloatingPointTy()) {
    Zero = GeneralUtils::getConstantValue(CompareTy, Clone->getContext(), 0.0);
    MaskCmp = new FCmpInst(LoopHeader->getTerminator(), CmpInst::FCMP_UNE,
                           MaskLoad, Zero, "mask.cond");
  } else {
    llvm_unreachable("Unsupported mask compare");
  }

  Instruction *Term = LoopHeader->getTerminator();
  Term->eraseFromParent();
  BranchInst::Create(LoopThenBlock, LoopElseBlock, MaskCmp, LoopHeader);

  LLVM_DEBUG(dbgs() << "After Split Insertion For Masked Variant\n");
  LLVM_DEBUG(Clone->dump());
}

void VecCloneImpl::Factory::disableLoopUnrolling() {
  // Set disable unroll metadata on the conditional branch of the loop latch
  // for the simd loop. The following is an example of what the loop latch
  // and Metadata will look like. The !llvm.loop marks the beginning of the
  // loop Metadata and is always placed on the terminator of the loop latch.
  // (i.e., simd.loop.latch in this case). According to LLVM documentation, to
  // properly set the loop Metadata, the 1st operand of !16 must be a self-
  // reference to avoid some type of Metadata merging conflicts that have
  // apparently arisen in the past. This is part of LLVM history that I do not
  // know. Also, according to LLVM documentation, any Metadata nodes referring
  // to themselves are marked as distinct. As such, all Metadata corresponding
  // to a loop belongs to that loop alone and no sharing of Metadata can be
  // done across different loops.
  //
  // simd.loop.latch:        ; preds = %simd.loop.header, %if.else, %if.then
  //  %indvar = add nuw nsw i32 %index, 1
  //  %vl.cond = icmp ult i32 %indvar, 2
  //  br i1 %vl.cond, label %simd.loop.header, label %simd.end.region, !llvm.loop !16
  //
  // !16 = distinct !{!16, !17}
  // !17 = !{!"llvm.loop.unroll.disable"}

  SmallVector<Metadata *, 4> MDs;

  // Reserve first location for self reference to the LoopID metadata node.
  MDs.push_back(nullptr);

  // Add unroll(disable) metadata to disable future unrolling.
  LLVMContext &Context = LoopLatch->getContext();
  SmallVector<Metadata *, 1> DisableOperands;
  DisableOperands.push_back(MDString::get(Context, "llvm.loop.unroll.disable"));
  MDNode *DisableNode = MDNode::get(Context, DisableOperands);
  MDs.push_back(DisableNode);

  MDNode *NewLoopID = MDNode::get(Context, MDs);
  // Set operand 0 to refer to the loop id itself.
  NewLoopID->replaceOperandWith(0, NewLoopID);
  LoopLatch->getTerminator()->setMetadata("llvm.loop", NewLoopID);
}

PreservedAnalyses VecClonePass::run(Module &M, ModuleAnalysisManager &AM) {
  // NOTE: Update here if new analyses are needed before VecClone
  // (getAnalysisUsage from LegacyPM)

  auto &OROA = AM.getResult<OptReportOptionsAnalysis>(M);
  ORBuilder.setup(M.getContext(), OROA.getVerbosity());

  if (!Impl.runImpl(M, &ORBuilder))
    return PreservedAnalyses::all();

  auto PA = PreservedAnalyses::none();
  PA.preserve<AndersensAA>();
  PA.preserve<GlobalsAA>();
  return PA;
}

void VecClone::getAnalysisUsage(AnalysisUsage &AU) const {
  // VecClone pass does not make any changes in the existing functions.
  // So we can consider it as preserved.
  AU.addPreserved<GlobalsAAWrapperPass>();
  AU.addRequired<OptReportOptionsPass>();
}

bool VecCloneImpl::runImpl(Module &M, OptReportBuilder *ORBuilder,
                           LoopOptLimiter Limiter) {

  LLVM_DEBUG(dbgs() << "\nExecuting SIMD Function Cloning ...\n\n");

  // Language specific hook
  languageSpecificInitializations(M);

  MapVector<Function *, std::vector<StringRef>> FunctionsToVectorize;
  getFunctionsToVectorize(M, FunctionsToVectorize);

  for (const auto &VarIt : FunctionsToVectorize) {
    Function &F = *(VarIt.first);

    if (!doesLoopOptPipelineAllowToRun(Limiter, F))
      continue;

    if (vlaAllocasExist(F)) {
      LLVM_DEBUG(dbgs() << "Bail out due to presence of array alloca(s)\n");
      F.removeFnAttr(VectorUtils::VectorVariantsAttrName);
      if (F.hasFnAttribute(VectorDispatchAttrName))
        F.removeFnAttr(VectorDispatchAttrName);
      if (ORBuilder)
        (*ORBuilder)(F).addRemark(OptReportVerbosity::Medium,
                                  OptRemarkID::VecCloneVLAPresence);
      continue;
    }

    // Get pointee type information for linear ptr args and cache for later
    // use when generating a gep for them in processLinearArgs(). Then, remove
    // these intrinsics from the original function so they don't cause a
    // crash downstream. Plus, removing them here in the original function
    // before cloning will ensure they don't make it into the vector versions
    // of the function.
    SmallVector<IntrinsicInst *, 2> IntrinsicsToRemove;
    /// The map of linear pointer args to pointee type size.
    DenseMap<Value *, Value *> PointeeTypeSize;
    for (auto &Inst : instructions(F)) {
      Instruction *I = &Inst;
      if (IntrinsicInst *II = dyn_cast<IntrinsicInst>(I)) {
        if (II->getIntrinsicID() == Intrinsic::intel_directive_elementsize) {
          Value *Arg = II->getOperand(0);
          Value *ElemSize = II->getOperand(1);
          PointeeTypeSize[Arg] = ElemSize;
          IntrinsicsToRemove.push_back(II);
        }
      }
    }

    for (auto II : IntrinsicsToRemove)
      II->eraseFromParent();

    auto Variants = map_range(VarIt.second, [](StringRef Name) {
      return VFABI::demangleForVFABI(Name);
    });

    SmallDenseMap<StringRef, SmallVector<StringRef>> CPUDispatchMap;
    getVariantsCPUDispatchData(F, CPUDispatchMap);

    for (const VFInfo &Variant : Variants) {
      // VecClone runs after OCLVecClone. Hence, VecClone will be triggered
      // again for the OpenCL kernels. To prevent this, we do not process
      // functions whose name include the current vector variant name. The
      // vector variant name is a combination of the scalar function name and
      // the Vector ABI encoding.
      if (M.getFunction(Variant.VectorName))
        continue;

      // Clone the original function.
      LLVM_DEBUG(dbgs() << "Before SIMD Function Cloning\n");
      LLVM_DEBUG(F.dump());

      assert(F.getFunctionType()->getNumParams() ==
                 (Variant.isMasked() ? Variant.getParameters().size() - 1
                                     : Variant.getParameters().size()) &&
             "Number of arguments of a variant and the original do not match.");

      Factory VecCloneFactory(this, M, M.getDataLayout(), F, Variant,
                              PointeeTypeSize);
      Function *Clone = VecCloneFactory.run();

      if (!Clone) {
        F.removeFnAttr(VectorUtils::VectorVariantsAttrName);
        if (ORBuilder)
          (*ORBuilder)(F).addRemark(OptReportVerbosity::Medium,
                                    OptRemarkID::VecCloneVariantLegalization,
                                    Variant.VectorName);
        continue;
      }

      applyTargetCPUData(Clone, CPUDispatchMap);

      LLVM_DEBUG(dbgs() << "After SIMD Function Cloning\n");
      LLVM_DEBUG(Clone->dump());

    } // End of function cloning for the variant

    if (F.hasFnAttribute(VectorDispatchAttrName))
      F.removeFnAttr(VectorDispatchAttrName);
    // TODO: Remove "vector-variants" attribute as we are done with cloning.
  } // End of function cloning for all variants

  // FIXME: return false if all functions were skipped or IR was not modified.
  return true; // LLVM IR has been modified
}

ModulePass *llvm::createVecClonePass() {
  return new llvm::VecClone();
}

char VecClone::ID = 0;

static const char lv_name[] = "VecClone";
INITIALIZE_PASS_BEGIN(VecClone, SV_NAME, lv_name,
                      false /* modifies CFG */, false /* transform pass */)
INITIALIZE_PASS_DEPENDENCY(OptReportOptionsPass)
INITIALIZE_PASS_END(VecClone, SV_NAME, lv_name,
                    false /* modififies CFG */, false /* transform pass */)
