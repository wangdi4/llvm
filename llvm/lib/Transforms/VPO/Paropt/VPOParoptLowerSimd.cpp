#if INTEL_COLLAB
//===-- VPOParoptLowerSimd.cpp - Lower SIMD Kernel for GenX architecture --===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// VPOParoptLowerSimd.cpp implements the interface to lower VPlan vectorizer
/// generated code and kernel for GenX vector-back.
///
//===----------------------------------------------------------------------===//

#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/GenXIntrinsics/GenXIntrinsics.h"
#include "llvm/IR/GetElementPtrTypeIterator.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/Module.h"
#include "llvm/IR/Operator.h"
#include "llvm/Pass.h"

#include "llvm/IR/Function.h"
#include "llvm/IR/LegacyPassManager.h"
#include "llvm/IR/PassManager.h"
#include "llvm/InitializePasses.h"
#include "llvm/Transforms/Utils/InferAddressSpacesUtils.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptLowerSimd.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptTransform.h"

using namespace llvm;

#define DEBUG_TYPE "vpo-paropt-lower-simd"

#define SLM_BTI 254

class VPOParoptLowerSimd : public FunctionPass {
public:
  static char ID; // Pass identification, replacement for typeid
  VPOParoptLowerSimd() : FunctionPass(ID) {
    initializeVPOParoptLowerSimdPass(*PassRegistry::getPassRegistry());
  }

  // run VPOParoptLowerSImd pass on the specified module
  bool runOnFunction(Function &F) override {
    // Infer generic AS
    auto &TTI = getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    InferAddrSpaces(TTI, vpo::ADDRESS_SPACE_GENERIC, F);

    FunctionAnalysisManager FAM;
    auto PA = Impl.run(F, FAM);
    return !PA.areAllPreserved();
  }

  void getAnalysisUsage(AnalysisUsage &AU) const {
    AU.addRequired<TargetTransformInfoWrapperPass>();
  }

private:
  VPOParoptLowerSimdPass Impl;
};

char VPOParoptLowerSimd::ID = 0;
INITIALIZE_PASS_BEGIN(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                      "Lower SIMD code generated by VPlan vectorizer for GenX",
                      false, false)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                    "Lower SIMD code generated by VPlan vectorizer for GenX",
                    false, false)
// Public interface to the VPOParoptLowerSimdPass.
FunctionPass *llvm::createVPOParoptLowerSimdPass() {
  return new VPOParoptLowerSimd();
}

// The regexp for Simd intrinsics:
// /^_Z(\d+)__esimd_\w+/
static constexpr char SIMD_INTRIN_PREF0[] = "_Z";
static constexpr char SPIRV_INTRIN_PREF[] = "__spirv_";

static constexpr char GENX_KERNEL_METADATA[] = "genx.kernels";
static constexpr char GENX_SLM_OFFSET[] = "genx.slm.offset";

#define SYCL_GLOBAL_AS 1
#define SYCL_SLM_AS 3

// Newly created GenX intrinsic might have different return type than expected.
// This helper function creates cast operation from GenX intrinsic return type
// to currently expected. Returns pointer to created cast instruction if it
// was created, otherwise returns NewI.
static Instruction *addCastInstIfNeeded(Instruction *OldI, Instruction *NewI) {
  Type *NITy = NewI->getType();
  Type *OITy = OldI->getType();
  if (OITy != NITy) {
    auto CastOpcode = CastInst::getCastOpcode(NewI, false, OITy, false);
    NewI = CastInst::Create(CastOpcode, NewI, OITy,
                            NewI->getName() + ".cast.ty", OldI);
  }
  return NewI;
}

static int getIndexForSuffix(StringRef Suff) {
  return llvm::StringSwitch<int>(Suff)
      .Case("x", 0)
      .Case("y", 1)
      .Case("z", 2)
      .Default(-1);
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that returns vector of coordinates.
// Example:
//   %call = call spir_func i64 @_Z23__spirv_WorkgroupSize_xv()
//     =>
//   %call.esimd = tail call <3 x i32> @llvm.genx.local.size.v3i32()
//   %wgsize.x = extractelement <3 x i32> %call.esimd, i32 0
//   %wgsize.x.cast.ty = zext i32 %wgsize.x to i64
static Instruction *generateVectorGenXForSpirv(CallInst &CI, StringRef Suff,
                                               const std::string &IntrinName,
                                               StringRef ValueName) {
  std::string IntrName =
      std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + IntrinName;
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  LLVMContext &Ctx = CI.getModule()->getContext();
  Type *I32Ty = Type::getInt32Ty(Ctx);
  Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
      CI.getModule(), ID, {FixedVectorType::get(I32Ty, 3)});
  Instruction *IntrI =
      IntrinsicInst::Create(NewFDecl, {}, CI.getName() + ".esimd", &CI);
  int ExtractIndex = getIndexForSuffix(Suff);
  assert(ExtractIndex != -1 && "Extract index is invalid.");
  Twine ExtractName = ValueName + Suff;
  Instruction *ExtrI = ExtractElementInst::Create(
      IntrI, ConstantInt::get(I32Ty, ExtractIndex), ExtractName, &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, ExtrI);
  return CastI;
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that has exact mapping.
// Example:
//   %call = call spir_func i64 @_Z21__spirv_WorkgroupId_xv()
//     =>
//   %group.id.x = tail call i32 @llvm.genx.group.id.x()
//   %group.id.x.cast.ty = zext i32 %group.id.x to i64
static Instruction *generateGenXForSpirv(CallInst &CI, StringRef Suff,
                                         const std::string &IntrinName) {
  std::string IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                         IntrinName + Suff.str();
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI.getModule(), ID, {});
  Instruction *IntrI =
      IntrinsicInst::Create(NewFDecl, {}, IntrinName + Suff.str(), &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, IntrI);
  return CastI;
}

// This function translates SPIRV intrinsic into GenX intrinsic.
// TODO: Currently, we do not support mixing SYCL and Simd kernels.
// Later for Simd and SYCL kernels to coexist, we likely need to
// clone call graph that lead from Simd kernel to SPIRV intrinsic and
// translate SPIRV intrinsics to GenX intrinsics only in cloned subgraph.
static void
translateSpirvIntrinsic(CallInst *CI, StringRef SpirvIntrName,
                        SmallVector<Instruction *, 8> &SimdToErases) {
  auto translateSpirvIntr = [&SpirvIntrName, &SimdToErases,
                             CI](StringRef SpvIName, auto TranslateFunc) {
    if (SpirvIntrName.consume_front(SpvIName)) {
      Value *TranslatedV = TranslateFunc(*CI, SpirvIntrName.substr(1, 1));
      CI->replaceAllUsesWith(TranslatedV);
      SimdToErases.push_back(CI);
    }
  };

  translateSpirvIntr("WorkgroupSize", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
  });
  translateSpirvIntr("LocalInvocationId", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
  });
  translateSpirvIntr("WorkgroupId", [](CallInst &CI, StringRef Suff) {
    return generateGenXForSpirv(CI, Suff, "group.id.");
  });
  translateSpirvIntr("GlobalInvocationId", [](CallInst &CI, StringRef Suff) {
    // GlobalId = LocalId + WorkGroupSize * GroupId
    Instruction *LocalIdI =
        generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *GroupIdI = generateGenXForSpirv(CI, Suff, "group.id.");
    Instruction *MulI =
        BinaryOperator::CreateMul(WGSizeI, GroupIdI, "mul", &CI);
    return BinaryOperator::CreateAdd(LocalIdI, MulI, "add", &CI);
  });
  translateSpirvIntr("GlobalSize", [](CallInst &CI, StringRef Suff) {
    // GlobalSize = WorkGroupSize * NumWorkGroups
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *NumWGI = generateVectorGenXForSpirv(
        CI, Suff, "group.count.v3i32", "group_count.");
    return BinaryOperator::CreateMul(WGSizeI, NumWGI, "mul", &CI);
  });
  // TODO: Support GlobalOffset SPIRV intrinsics
  translateSpirvIntr("GlobalOffset", [](CallInst &CI, StringRef Suff) {
    return llvm::Constant::getNullValue(CI.getType());
  });
  translateSpirvIntr("NumWorkgroups", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "group.count.v3i32",
                                      "group_count.");
  });
}

static std::string getMDString(MDNode *N, unsigned I) {
  if (!N)
    return "";

  Metadata *Op = N->getOperand(I);
  if (!Op)
    return "";

  if (MDString *Str = dyn_cast<MDString>(Op)) {
    return Str->getString().str();
  }

  return "";
}

static Value *translateReduceOpIntrinsic(CallInst *CI) {
  assert(isa<llvm::IntrinsicInst>(CI));
  LLVMContext &CTX = CI->getContext();
  // TODO: introduce GenXIRBuilder helper
  auto GetReduceSeq = [&](Value *Src, unsigned Size,
                          unsigned ElemOffset) -> Value * {
    if (Size == 1) {
      if (Src->getType()->isVectorTy()) {
        Value *Index = ConstantInt::get(Type::getInt32Ty(CTX), ElemOffset);
        return ExtractElementInst::Create(Src, Index, ".reduce.single",
                                          CI /*InsertBefore*/);
      }
      assert(ElemOffset == 0);
      return Src;
    }

    Type *SrcType = Src->getType();
    Type *SrcElementType = cast<FixedVectorType>(SrcType)->getElementType();

    Type *Tys[] = {FixedVectorType::get(SrcElementType, Size), SrcType,
                   Type::getInt16Ty(CTX)};
    auto *Decl = GenXIntrinsic::getGenXDeclaration(
        CI->getModule(), GenXIntrinsic::genx_rdregioni, Tys);

    Value *Args[] = {
        Src,
        ConstantInt::get(Type::getInt32Ty(CTX), 0),    // VStride
        ConstantInt::get(Type::getInt32Ty(CTX), Size), // Width
        ConstantInt::get(Type::getInt32Ty(CTX), 1u),   // Stride
        ConstantInt::get(Type::getInt16Ty(CTX),
                         ElemOffset *
                             (SrcElementType->getPrimitiveSizeInBits() /
                              8)),                  // Offset in bytes
        ConstantInt::get(Type::getInt32Ty(CTX), 0), // Parent width (ignored)
    };

    return CallInst::Create(Decl, Args, Src->getName() + ".reduce.seq",
                            CI /*InsertBefore*/);
  };

  Instruction::BinaryOps ReduceOp;
  switch (CI->getIntrinsicID()) {
  default:
    llvm_unreachable("unimplemented intrinsic");
  case Intrinsic::vector_reduce_add:
    ReduceOp = Instruction::Add;
    break;
  case Intrinsic::vector_reduce_mul:
    ReduceOp = Instruction::Mul;
    break;
  case Intrinsic::vector_reduce_xor:
    ReduceOp = Instruction::Xor;
    break;
  case Intrinsic::vector_reduce_or:
    ReduceOp = Instruction::Or;
    break;
  }

  Value *OperandToReduce = CI->getOperand(0);
  unsigned N =
      cast<FixedVectorType>(OperandToReduce->getType())->getNumElements();
  // Split vector into two parts. First part's size  is alwayas a power of two
  // so we can simply apply reduction by spliiting it into parts. Reduction with
  // a second part is applicable it's size equals to the first part size
  unsigned N1, N2;
  if (llvm::isPowerOf2_32(N)) {
    N1 = N;
    N2 = 0;
  } else {
    N1 = 1u << llvm::Log2_32(N);
    N2 = N - N1;
  }

  Value *CurrentReduceRes = OperandToReduce;
  IRBuilder<> Builder(CI);
  for (unsigned i = N1 / 2; i >= 1; i /= 2) {
    auto *RSeq1 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq2 = GetReduceSeq(CurrentReduceRes, i, i);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq1, RSeq2);

    unsigned CurrentReduceSize;
    if (auto *VT = dyn_cast<FixedVectorType>(CurrentReduceRes->getType()))
      CurrentReduceSize = VT->getNumElements();
    else
      CurrentReduceSize = 1;
    // Continue reducing if second part has suitable size, fall back
    // to reducing first part otherwise
    if (CurrentReduceSize > N2)
      continue;

    // Reduction across parts
    auto *RSeq21 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq22 = GetReduceSeq(OperandToReduce, i, N - N2);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq21, RSeq22);
    N2 -= i;
  }
  assert(CurrentReduceRes->getType() == CI->getType());
  CI->replaceAllUsesWith(CurrentReduceRes);
  return CurrentReduceRes;
}

//
// Trace the GetElementPtrInst[s] back to its base, which should be a SLM
// global variable. Convert the GEPs into multiple instructions that compute
// the byte offset from the base represented by these GEP instructions.
//
// Returns the final offset value if the GEP was able to be expanded to
// multiple instructions.
//
static Value *getSLMOffset(Value *V, Instruction *InsPos) {
  SmallVector<GEPOperator *, 4> GEPs;
  Value *base = V->stripPointerCasts();
  // gep : the last gep of pointer address, null if no GEP at all.
  while (GEPOperator *gep = dyn_cast<GEPOperator>(base)) {
    GEPs.push_back(gep);
    base = gep->getPointerOperand()->stripPointerCasts();
  }
  auto GV = dyn_cast<GlobalVariable>(base);
  // \TODO: what if the base is a PHINode?
  if (!GV)
    return nullptr;

  Module *M = GV->getParent();
  const DataLayout *DL = &M->getDataLayout();
  Type *int32Ty = Type::getInt32Ty(M->getContext());
  // obtain the offset of the global variable
  unsigned Offset;
  auto Invalid = GV->getAttribute(GENX_SLM_OFFSET)
                     .getValueAsString()
                     .getAsInteger(0, Offset);
  assert(!Invalid);

  // initialize the PointerValue representing the offset
  Value *PointerValue = ConstantInt::get(int32Ty, Offset);
  const int nGEPs = GEPs.size();
  // GEPs is in the reverse order of execution! The last GEP is the first
  // one to execute.  For example:
  //    %37 = getelementptr inbounds float, float addrspace(1)* %signalw, i64
  //    16384 %38 = bitcast float addrspace(1)* %37 to[16 x[32 x[32 x float]]]
  //    addrspace(1)* %39 = getelementptr inbounds[16 x[32 x[32 x float]]], [16
  //    x[32 x[32 x float]]]
  //                        addrspace(1)* %38, i64 0, i64 % 34, i64 % 17, i64 %
  //                        18
  //    store float %36, float addrspace(1)* %39, align 4
  //
  //  GEPs = [%39, %37]  // GEPs[0] = %39, GEPs[1] = %37
  //
  for (int i = nGEPs; i > 0; --i) {
    GEPOperator *GEP = GEPs[i - 1];
    Value *PtrOp = GEP->getPointerOperand();
    PointerType *PtrTy = dyn_cast<PointerType>(PtrOp->getType());

    assert(PtrTy && "Only accept scalar pointer!");

    Type *Ty = PtrTy;
    gep_type_iterator GTI = gep_type_begin(GEP);
    for (auto OI = GEP->op_begin() + 1, E = GEP->op_end(); OI != E;
         ++OI, ++GTI) {
      Value *Idx = *OI;
      if (StructType *StTy = GTI.getStructTypeOrNull()) {
        unsigned Field =
            static_cast<unsigned>(cast<ConstantInt>(Idx)->getZExtValue());
        if (Field) {
          uint64_t Offset = DL->getStructLayout(StTy)->getElementOffset(Field);

          Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, OffsetValue, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
        Ty = StTy->getElementType(Field);
      } else {
        Ty = GTI.getIndexedType();
        if (const ConstantInt *CI = dyn_cast<ConstantInt>(Idx)) {
          if (!CI->isZero()) {
            uint64_t Offset = DL->getTypeAllocSize(Ty) * CI->getSExtValue();
            Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

            PointerValue = BinaryOperator::CreateAdd(PointerValue, OffsetValue,
                                                     "", InsPos);
            cast<llvm::Instruction>(PointerValue)
                ->setDebugLoc(InsPos->getDebugLoc());
          }
        } else {
          Value *NewIdx =
              CastInst::CreateTruncOrBitCast(Idx, int32Ty, "", InsPos);
          cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          APInt ElementSize =
              APInt((unsigned int)int32Ty->getPrimitiveSizeInBits(),
                    DL->getTypeAllocSize(Ty));

          if (ElementSize != 1) {
            NewIdx = BinaryOperator::CreateMul(
                NewIdx, ConstantInt::get(int32Ty, ElementSize), "", InsPos);
            cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          }

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, NewIdx, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
      }
    }
  }
  return PointerValue;
}

static Value *translateSVMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  auto DTy = LoadOp->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
        "svm.block.ld.unaligned";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {DTy, PtrV->getType()});
    RepI = IntrinsicInst::Create(NewFDecl, {PtrV}, "svm.block.ld.unaligned",
                                 LoadOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int RetLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, RetLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VPtrTy});
    auto VecPtr =
        CastInst::CreateBitOrPointerCast(PtrV, VPtrTy, PtrV->getName(), LoadOp);
    Instruction *IntrI = IntrinsicInst::Create(
        NewFDecl, {OnePred1, NumBlksC, VecPtr, UndefValue::get(VDTy)},
        "svm.gather", LoadOp);
    if (EltBytes == 2) {
      // cast 4xi8 to 2xi16
      auto I16x2Ty =
          llvm::FixedVectorType::get(IntegerType::getInt16Ty(CTX), 2);
      IntrI = CastInst::CreateBitOrPointerCast(
          IntrI, I16x2Ty, IntrI->getName() + ".cast", LoadOp);
    }
    RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
  }
  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Constant *getConstVector(Type *ITy, unsigned int n, unsigned int step) {
  std::vector<Constant *> vConsts;
  unsigned v = 0;
  for (unsigned i = 0; i < n; i++) {
    vConsts.push_back(ConstantInt::get(ITy, v));
    v += step;
  }
  return ConstantVector::get(vConsts);
}

static Value *translateSLMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  auto DTy = LoadOp->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, LoadOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  if (DTy->isVectorTy()) {
    auto VL = cast<VectorType>(DTy)->getNumElements();
    auto EltTy = cast<VectorType>(DTy)->getElementType();
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "gather.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
    int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
    // create constant for offset
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    // crease constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {DTy, PredVTy, VOffset->getType()});
    RepI = IntrinsicInst::Create(NewFDecl,
                                 {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset,
                                  VOffset, UndefValue::get(DTy)},
                                 "slm.block.gather", LoadOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "gather.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
    int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(DTy, 1);
    // create constant for offset
    auto VOffsetTy = llvm::FixedVectorType::get(CIntTy, 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VOffsetTy});
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), LoadOp);
    Instruction *IntrI =
        IntrinsicInst::Create(NewFDecl,
                              {OnePred1, NumBlksC, ScaleC, BTI, ZeroC,
                               VecOffset, UndefValue::get(VDTy)},
                              "slm.gather", LoadOp);
    RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
  }
  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Value *translateSVMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.block.st";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PtrV->getType(), DTy});
    RepI = IntrinsicInst::Create(
        NewFDecl, {PtrV, DTV},
        NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st", StoreOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int DataLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, DataLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VPtrTy, VDTy});
    auto VecPtr = CastInst::CreateBitOrPointerCast(PtrV, VPtrTy,
                                                   PtrV->getName(), StoreOp);
    Value *VecDTV = nullptr;
    if (EltBytes == 2) {
      // cast 2xi16 to 4xi8
      auto I16x2Ty = llvm::FixedVectorType::get(DTy, 2);
      VecDTV = Builder.CreateInsertElement(UndefValue::get(I16x2Ty), DTV, ZeroC,
                                           DTV->getName());
      VecDTV = CastInst::CreateBitOrPointerCast(
          VecDTV, VDTy, DTV->getName() + ".cast", StoreOp);
    } else
      VecDTV = Builder.CreateInsertElement(UndefValue::get(VDTy), DTV, ZeroC,
                                           DTV->getName());
    RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, VecPtr, VecDTV});
  }
  return RepI;
}

static Value *translateSLMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, StoreOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "scatter.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto VL = cast<VectorType>(DTy)->getNumElements();
    auto EltTy = cast<VectorType>(DTy)->getElementType();
    auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
    int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    // create constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredVTy, VOffset->getType(), DTy});
    RepI = Builder.CreateCall(
        NewFDecl, {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset, VOffset, DTV});
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "scatter.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
    int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(DTy, 1);
    auto VOffsetTy = llvm::FixedVectorType::get(SLMOffset->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), StoreOp);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VOffsetTy, VDTy});
    Value *VecDTV = Builder.CreateInsertElement(UndefValue::get(VDTy), DTV,
                                                ZeroC, DTV->getName());
    RepI = Builder.CreateCall(
        NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI, ZeroC, VecOffset, VecDTV});
  }
  return RepI;
}

static Value *translateLLVMInst(Instruction *Inst) {
  LLVMContext &CTX = Inst->getContext();
  IRBuilder<> Builder(Inst);
  if (auto CastOp = dyn_cast<llvm::CastInst>(Inst)) {
    llvm::Type *DstTy = CastOp->getDestTy();
    auto CastOpcode = CastOp->getOpcode();
    if ((CastOpcode == llvm::Instruction::FPToUI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() <= 32) ||
        (CastOpcode == llvm::Instruction::FPToSI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() < 32)) {
      llvm::Value *Src = CastOp->getOperand(0);
      auto TmpTy = llvm::FixedVectorType::get(
          llvm::Type::getInt32Ty(CTX),
          cast<FixedVectorType>(DstTy)->getNumElements());
      Src = Builder.CreateFPToSI(Src, TmpTy);

      llvm::Instruction::CastOps TruncOp = llvm::Instruction::Trunc;
      auto *NewDst = Builder.CreateCast(TruncOp, Src, DstTy);
      CastOp->replaceAllUsesWith(NewDst);
      return NewDst;
    }
    return CastOp;
  }
  if (auto LoadOp = dyn_cast<llvm::LoadInst>(Inst)) {
    auto AS = LoadOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS)
      return translateSVMLoad(LoadOp);
    else if (AS == SYCL_SLM_AS)
      return translateSLMLoad(LoadOp);
    return LoadOp;
  }
  if (auto StoreOp = dyn_cast<llvm::StoreInst>(Inst)) {
    auto AS = StoreOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS)
      return translateSVMStore(StoreOp);
    else if (AS == SYCL_SLM_AS)
      return translateSLMStore(StoreOp);
    return StoreOp;
  }
  if (auto CallOp = dyn_cast<llvm::IntrinsicInst>(Inst)) {
    // handle memory intrinsics
    // masked.gather, masked.scatter etc
    auto ID = CallOp->getIntrinsicID();
    switch (ID) {
    case Intrinsic::masked_load: {
      auto PtrV = CallOp->getArgOperand(0);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      if (AS == 0) { // thread private memory
                     // convert to load then select
        auto VecDT =
            Builder.CreateLoad(CallOp->getType(), PtrV, CallOp->getName());
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        auto DTy = CallOp->getType();
        // convert to unaligned-block-load then select
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTy, PtrV->getType()});
        auto VecDT = IntrinsicInst::Create(NewFDecl, {PtrV},
                                           "svm.block.ld.unaligned", CallOp);
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else if (AS == SYCL_SLM_AS) {
        auto DTy = CallOp->getType();
        auto SLMOffset = getSLMOffset(PtrV, CallOp);
        assert(SLMOffset);
        auto CIntTy = Type::getInt32Ty(CTX);
        auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
        auto VL = cast<VectorType>(DTy)->getNumElements();
        auto EltTy = cast<VectorType>(DTy)->getElementType();
        std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "gather.scaled";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
        assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
        int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
        // create constant for offset
        auto VOffset = getConstVector(CIntTy, VL, EltBytes);
        // create constant for predicate
        auto PredV = CallOp->getArgOperand(2);
        // crease constant for num-blocks
        auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
        auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
        // create the intrinsic call
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          CallOp->getModule(), ID, {DTy, PredV->getType(), VOffset->getType()});
        auto RepI = IntrinsicInst::Create(NewFDecl,
                                 {PredV, NumBlksC, ScaleC, BTI, SLMOffset,
                                  VOffset, UndefValue::get(DTy)},
                                 "slm.block.gather", CallOp);
	return RepI;
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_store: {
      auto PtrV = CallOp->getArgOperand(1);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      auto DTV = CallOp->getArgOperand(0);
      if (AS == 0) { // thread private memory
                     // convert to load then select then store
        auto VecDT = Builder.CreateLoad(DTV->getType(), PtrV);
        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);
        auto RepI = Builder.CreateStore(SelI, PtrV);
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        // convert to unaligned-block-load then select
        // then block-store
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTV->getType(), PtrV->getType()});
        auto VecDT = IntrinsicInst::Create(NewFDecl, {PtrV},
                                           "svm.block.ld.unaligned", CallOp);

        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);

        IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                   "svm.block.st";
        ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {PtrV->getType(), DTV->getType()});
        auto RepI = IntrinsicInst::Create(
            NewFDecl, {PtrV, SelI},
            NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st",
            CallOp);
        return RepI;
      } else if (AS == SYCL_SLM_AS) {
        auto DTy = DTV->getType();
        auto SLMOffset = getSLMOffset(PtrV, CallOp);
        assert(SLMOffset);
        auto CIntTy = Type::getInt32Ty(CTX);
        auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "scatter.scaled";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        auto VL = cast<VectorType>(DTy)->getNumElements();
        auto EltTy = cast<VectorType>(DTy)->getElementType();
        auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
        assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4);
        int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
        auto VOffset = getConstVector(CIntTy, VL, EltBytes);
        auto PredV = CallOp->getArgOperand(3);
        // create constant for num-blocks
        auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
        auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
        // create the intrinsic call
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            CallOp->getModule(), ID, {PredV->getType(), VOffset->getType(), DTy});
        auto RepI = Builder.CreateCall(
            NewFDecl, {PredV, NumBlksC, ScaleC, BTI, SLMOffset, VOffset, DTV});
	return RepI;
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_gather: {
      auto PtrV = CallOp->getArgOperand(0);
      auto MaskV = CallOp->getArgOperand(2);
      auto OldV = CallOp->getArgOperand(3);
      auto DTy = CallOp->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      assert (AS != SYCL_SLM_AS && "yet to support masked SLM gather");
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<VectorType>(DTy)->getElementType();
      auto NumElts = cast<VectorType>(DTy)->getNumElements();
      auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
      int EltsPerLane = 1;
      // for short or byte type, load-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        if (isa<UndefValue>(OldV))
          OldV = UndefValue::get(VDTy);
        else {
          // zext to i32 type
          auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
          auto ExtI = Builder.CreateZExt(OldV, VInt32, OldV->getName());
          // bitcast to 4xi8
          OldV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
        }
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {VDTy, MaskV->getType(), PtrV->getType()});
      RepI = IntrinsicInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, OldV},
                                   CallOp->getName(), CallOp);
      if (EltBytes < 4) {
        // bitcast to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto CastI = Builder.CreateBitCast(RepI, VInt32, RepI->getName());
        // trunc to the dst type
        RepI = Builder.CreateTrunc(CastI, DTy, CastI->getName());
      }
      CallOp->replaceAllUsesWith(RepI);
      return RepI;
    } break;
    case Intrinsic::masked_scatter: {
      auto PtrV = CallOp->getArgOperand(0);
      auto MaskV = CallOp->getArgOperand(2);
      auto DataV = CallOp->getArgOperand(3);
      auto DTy = DataV->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      assert (AS != SYCL_SLM_AS && "yet to support masked SLM scatter");
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<VectorType>(DTy)->getElementType();
      auto NumElts = cast<VectorType>(DTy)->getNumElements();
      auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
      int EltsPerLane = 1;
      // for short or byte type, store-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        // zext to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto ExtI = Builder.CreateZExt(DataV, VInt32, DataV->getName());
        // bitcast to 4xi8
        DataV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {MaskV->getType(), PtrV->getType(), VDTy});
      RepI = IntrinsicInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, DataV},
                                   CallOp->getName(), CallOp);
      return RepI;
    } break;
    case Intrinsic::vector_reduce_add:
    case Intrinsic::vector_reduce_mul:
    case Intrinsic::vector_reduce_xor:
    case Intrinsic::vector_reduce_or:
      return translateReduceOpIntrinsic(CallOp);
    default:
      break;
    }
    return CallOp;
  }
  return Inst;
}

static unsigned int assignSLMOffset(Module &M) {
  const DataLayout *DL = &M.getDataLayout();
  unsigned SLMSize = 0;
  for (auto &&GV : M.getGlobalList()) {
    auto Ty = dyn_cast<PointerType>(GV.getType());
    if (Ty && Ty->getAddressSpace() == SYCL_SLM_AS) {
      auto DTy = GV.getValueType();
      auto BufferSize = static_cast<size_t>(DL->getTypeAllocSize(DTy));
      auto align = GV.getAlignment();
      SLMSize = ((SLMSize + align - 1) / align) * align;
      if (!GV.hasAttribute(GENX_SLM_OFFSET))
        GV.addAttribute(GENX_SLM_OFFSET, std::to_string(SLMSize));
      SLMSize += BufferSize;
    }
  }
  return SLMSize;
}

PreservedAnalyses VPOParoptLowerSimdPass::run(Function &F,
                                              FunctionAnalysisManager &FAM) {
  // Only consider functions marked with !sycl_explicit_simd
  bool isOmpSpirKernel = false;

  if ((F.getMetadata("omp_simd_kernel") != nullptr) ||
      (F.getMetadata("sycl_explicit_simd") == nullptr &&
       F.getCallingConv() == CallingConv::SPIR_KERNEL)) {
    IRBuilder<> Builder(F.getEntryBlock().getFirstNonPHI());
    int simdWidth = 1;
    Metadata *AttrMDArgs[] = {
        ConstantAsMetadata::get(Builder.getInt32(simdWidth))};
    F.setMetadata("intel_reqd_sub_group_size",
                  MDNode::get(F.getContext(), AttrMDArgs));
    isOmpSpirKernel = true;
  } else
    return PreservedAnalyses::all();

  auto SLMSize = assignSLMOffset(*F.getParent());

  // add module-level meta-data for kernel functions
  if (isOmpSpirKernel) {
    auto M = F.getParent();
    auto MetaKernels = M->getOrInsertNamedMetadata(GENX_KERNEL_METADATA);

    LLVMContext &Ctx = M->getContext();
    Type *I32Ty = Type::getInt32Ty(Ctx);

    enum { AK_NORMAL, AK_SAMPLER, AK_SURFACE, AK_VME };
    enum { IK_NORMAL, IK_INPUT, IK_OUTPUT, IK_INPUT_OUTPUT };
    // Metadata node containing N i32s, where N is the number of kernel
    // arguments, and each i32 is the kind of argument,  one of:
    //     0 = general, 1 = sampler, 2 = surface, 3 = vme
    // (the same values as in the "kind" field of an "input_info"
    // record in a vISA kernel.
    SmallVector<Metadata *, 8> ArgKinds;
    // Optional, not supported for compute
    SmallVector<Metadata *, 8> ArgInOutKinds;
    // Metadata node describing N strings where N is the number of kernel
    // arguments, each string describing argument type in OpenCL.
    // required for running on top of OpenCL runtime.
    SmallVector<Metadata *, 8> ArgTypeDescs;
    auto *KernelArgTypes = F.getMetadata("kernel_arg_type");
    unsigned Idx = 0;

    // Iterate argument list to gather argument kinds and generate argument
    // descriptors.
    for (const Argument &Arg : F.args()) {
      int Kind = AK_NORMAL;
      int IKind = IK_NORMAL;

      auto ArgType = getMDString(KernelArgTypes, Idx);

      if (ArgType.find("image1d_t") != std::string::npos ||
          ArgType.find("image2d_t") != std::string::npos ||
          ArgType.find("image3d_t") != std::string::npos ||
          ArgType.find("image1d_buffer_t") != std::string::npos) {
        Kind = AK_SURFACE;
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgType));
      } else {
        StringRef ArgDesc = "";
        if (Arg.getType()->isPointerTy())
          ArgDesc = "svmptr_t";
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgDesc));
      }

      ArgKinds.push_back(ValueAsMetadata::get(ConstantInt::get(I32Ty, Kind)));
      ArgInOutKinds.push_back(
          ValueAsMetadata::get(ConstantInt::get(I32Ty, IKind)));
      Idx++;
    }
    MDNode *Kinds = MDNode::get(Ctx, ArgKinds);
    MDNode *IOKinds = MDNode::get(Ctx, ArgInOutKinds);
    MDNode *ArgDescs = MDNode::get(Ctx, ArgTypeDescs);

    Metadata *MDArgs[] = {ValueAsMetadata::get(&F),
                          MDString::get(Ctx, F.getName().str()),
                          Kinds,
                          ValueAsMetadata::get(llvm::ConstantInt::get(
                              I32Ty, SLMSize)), // SLM size in bytes
                          ValueAsMetadata::get(llvm::ConstantInt::getNullValue(
                              I32Ty)), // arg offsets
                          IOKinds,
                          ArgDescs};

    // Add this kernel to the root.
    MetaKernels->addOperand(MDNode::get(Ctx, MDArgs));
    F.addFnAttr("oclrt", "1");
    F.addFnAttr("CMGenxMain");
  }

  SmallVector<Instruction *, 8> SimdToErases;

  for (Instruction &I : instructions(F)) {
    auto replace = translateLLVMInst(&I);
    if (replace != &I) {
      SimdToErases.push_back(&I);
      continue;
    }

    auto *CI = dyn_cast<CallInst>(&I);
    Function *Callee = nullptr;
    if (!CI || !(Callee = CI->getCalledFunction()))
      continue;
    StringRef Name = Callee->getName();

    if (!Name.consume_front(SIMD_INTRIN_PREF0))
      continue;
    // now skip the digits
    Name = Name.drop_while([](char C) { return std::isdigit(C); });

    if (Name.consume_front(SPIRV_INTRIN_PREF)) {
      translateSpirvIntrinsic(CI, Name, SimdToErases);
      // For now: if no match, just let it go untranslated.
      continue;
    }
  }
  for (auto *CI : SimdToErases) {
    CI->eraseFromParent();
  }

  return PreservedAnalyses::none();
}

#endif // INTEL_COLLAB
