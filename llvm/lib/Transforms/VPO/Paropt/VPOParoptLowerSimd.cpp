#if INTEL_COLLAB
//
// INTEL CONFIDENTIAL
//
// Modifications, Copyright (C) 2021 Intel Corporation
//
// This software and the related documents are Intel copyrighted materials, and
// your use of them is governed by the express license under which they were
// provided to you ("License"). Unless the License provides otherwise, you may not
// use, modify, copy, publish, distribute, disclose or transmit this software or
// the related documents without Intel's prior written permission.
//
// This software and the related documents are provided as is, with no express
// or implied warranties, other than those that are expressly stated in the
// License.
//
//===-- VPOParoptLowerSimd.cpp - Lower SIMD Kernel for GenX architecture --===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// VPOParoptLowerSimd.cpp implements the interface to lower VPlan vectorizer
/// generated code and kernel for GenX vector-back.
///
//===----------------------------------------------------------------------===//

#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/GenXIntrinsics/GenXIntrinsics.h"
#include "llvm/IR/GetElementPtrTypeIterator.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/Module.h"
#include "llvm/IR/Operator.h"
#include "llvm/Pass.h"

#include "llvm/IR/Function.h"
#include "llvm/IR/LegacyPassManager.h"
#include "llvm/IR/PassManager.h"
#include "llvm/InitializePasses.h"
#include "llvm/Transforms/Scalar/InferAddressSpaces.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptLowerSimd.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptTransform.h"

using namespace llvm;

#define DEBUG_TYPE "vpo-paropt-lower-simd"

#define SLM_BTI 254

class VPOParoptLowerSimd : public FunctionPass {
public:
  static char ID; // Pass identification, replacement for typeid
  VPOParoptLowerSimd() : FunctionPass(ID) {
    initializeVPOParoptLowerSimdPass(*PassRegistry::getPassRegistry());
  }

  // run VPOParoptLowerSImd pass on the specified module
  bool runOnFunction(Function &F) override {
    // Infer generic AS
    auto *AC = &getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F);
    auto *DTWP = getAnalysisIfAvailable<DominatorTreeWrapperPass>();
    DominatorTree *DT = DTWP ? &DTWP->getDomTree() : nullptr;
    auto *TTI = &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    InferAddrSpaces(AC, DT, TTI, vpo::ADDRESS_SPACE_GENERIC, F);

    FunctionAnalysisManager FAM;
    auto PA = Impl.run(F, FAM);
    return !PA.areAllPreserved();
  }

  void getAnalysisUsage(AnalysisUsage &AU) const override {
    AU.addRequired<AssumptionCacheTracker>();
    AU.addRequired<TargetTransformInfoWrapperPass>();
  }

private:
  VPOParoptLowerSimdPass Impl;
};

char VPOParoptLowerSimd::ID = 0;
INITIALIZE_PASS_BEGIN(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                      "Lower SIMD code generated by VPlan vectorizer for GenX",
                      false, false)
INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                    "Lower SIMD code generated by VPlan vectorizer for GenX",
                    false, false)
// Public interface to the VPOParoptLowerSimdPass.
FunctionPass *llvm::createVPOParoptLowerSimdPass() {
  return new VPOParoptLowerSimd();
}

// The regexp for Simd intrinsics:
// /^_Z(\d+)__esimd_\w+/
static constexpr char SIMD_INTRIN_PREF0[] = "_Z";
static constexpr char SPIRV_INTRIN_PREF[] = "__spirv_";

static constexpr char GENX_KERNEL_METADATA[] = "genx.kernels";
static constexpr char GENX_SLM_OFFSET[] = "genx.slm.offset";

#define SYCL_GLOBAL_AS 1
#define SYCL_SLM_AS 3
#define SYCL_GENERIC_AS 4

static uint32_t getElementSizeInBytes(Type *ETy, DataLayout &DL) {
  uint32_t ebytes = (unsigned int)ETy->getPrimitiveSizeInBits() / 8;
  if (ETy->isPointerTy())
    ebytes = DL.getPointerTypeSize(ETy);
  return ebytes;
}

// Newly created GenX intrinsic might have different return type than expected.
// This helper function creates cast operation from GenX intrinsic return type
// to currently expected. Returns pointer to created cast instruction if it
// was created, otherwise returns NewI.
static Instruction *addCastInstIfNeeded(Instruction *OldI, Instruction *NewI) {
  Type *NITy = NewI->getType();
  Type *OITy = OldI->getType();
  if (OITy != NITy) {
    auto CastOpcode = CastInst::getCastOpcode(NewI, false, OITy, false);
    NewI = CastInst::Create(CastOpcode, NewI, OITy,
                            NewI->getName() + ".cast.ty", OldI);
  }
  return NewI;
}

static int getIndexForSuffix(StringRef Suff) {
  return llvm::StringSwitch<int>(Suff)
      .Case("x", 0)
      .Case("y", 1)
      .Case("z", 2)
      .Default(-1);
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that returns vector of coordinates.
// Example:
//   %call = call spir_func i64 @_Z23__spirv_WorkgroupSize_xv()
//     =>
//   %call.esimd = tail call <3 x i32> @llvm.genx.local.size.v3i32()
//   %wgsize.x = extractelement <3 x i32> %call.esimd, i32 0
//   %wgsize.x.cast.ty = zext i32 %wgsize.x to i64
static Instruction *generateVectorGenXForSpirv(CallInst &CI, StringRef Suff,
                                               const std::string &IntrinName,
                                               StringRef ValueName) {
  std::string IntrName =
      std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + IntrinName;
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  LLVMContext &Ctx = CI.getModule()->getContext();
  Type *I32Ty = Type::getInt32Ty(Ctx);
  Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
      CI.getModule(), ID, {FixedVectorType::get(I32Ty, 3)});
  Instruction *IntrI =
      CallInst::Create(NewFDecl, {}, CI.getName() + ".esimd", &CI);
  int ExtractIndex = getIndexForSuffix(Suff);
  assert(ExtractIndex != -1 && "Extract index is invalid.");
  Twine ExtractName = ValueName + Suff;
  Instruction *ExtrI = ExtractElementInst::Create(
      IntrI, ConstantInt::get(I32Ty, ExtractIndex), ExtractName, &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, ExtrI);
  return CastI;
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that has exact mapping.
// Example:
//   %call = call spir_func i64 @_Z21__spirv_WorkgroupId_xv()
//     =>
//   %group.id.x = tail call i32 @llvm.genx.group.id.x()
//   %group.id.x.cast.ty = zext i32 %group.id.x to i64
static Instruction *generateGenXForSpirv(CallInst &CI, StringRef Suff,
                                         const std::string &IntrinName) {
  std::string IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                         IntrinName + Suff.str();
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI.getModule(), ID, {});
  Instruction *IntrI =
      CallInst::Create(NewFDecl, {}, IntrinName + Suff.str(), &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, IntrI);
  return CastI;
}

static void translateBarrier(CallInst *CI) {
  auto ScopeV = CI->getArgOperand(0);
  assert(isa<ConstantInt>(ScopeV) &&
         "Barrier execution scope should be a constant");
  ConstantInt *ScopeC = cast<ConstantInt>(ScopeV);
  // if work-group scope
  //    generate a slm-fence then work-group-barrier
  // else if subgroup scope
  //    generate a slm-fence
  auto M = CI->getModule();
  if (ScopeC->equalsInt(2) || ScopeC->equalsInt(3)) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "fence";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(M, ID, {});
    // ESIMD_GLOBAL_COHERENT_FENCE | ESIMD_LOCAL_BARRIER
    // 0x01 | 0x20
    Type *I8Ty = Type::getInt8Ty(M->getContext());
    auto CtrlV = ConstantInt::get(I8Ty, 0x21);
    CallInst::Create(NewFDecl, {CtrlV}, "", CI);
  } else
    assert(false && "Unsupported barrier execution scope");

  if (ScopeC->equalsInt(2)) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "barrier";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(M, ID, {});
    CallInst::Create(NewFDecl, {}, "", CI);
  }
}

// This function translates SPIRV intrinsic into GenX intrinsic.
// TODO: Currently, we do not support mixing SYCL and Simd kernels.
// Later for Simd and SYCL kernels to coexist, we likely need to
// clone call graph that lead from Simd kernel to SPIRV intrinsic and
// translate SPIRV intrinsics to GenX intrinsics only in cloned subgraph.
static void
translateSpirvIntrinsic(CallInst *CI, StringRef SpirvIntrName,
                        SmallVector<Instruction *, 8> &SimdToErases) {

  if (SpirvIntrName.startswith("ControlBarrieriii")) {
    translateBarrier(CI);
    SimdToErases.push_back(CI);
    return;
  }

  auto translateSpirvIntr = [&SpirvIntrName, &SimdToErases,
                             CI](StringRef SpvIName, auto TranslateFunc) {
    if (SpirvIntrName.consume_front(SpvIName)) {
      Value *TranslatedV = TranslateFunc(*CI, SpirvIntrName.substr(1, 1));
      CI->replaceAllUsesWith(TranslatedV);
      SimdToErases.push_back(CI);
    }
  };

  translateSpirvIntr("WorkgroupSize", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
  });
  translateSpirvIntr("LocalInvocationId", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
  });
  translateSpirvIntr("WorkgroupId", [](CallInst &CI, StringRef Suff) {
    return generateGenXForSpirv(CI, Suff, "group.id.");
  });
  translateSpirvIntr("GlobalInvocationId", [](CallInst &CI, StringRef Suff) {
    // GlobalId = LocalId + WorkGroupSize * GroupId
    Instruction *LocalIdI =
        generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *GroupIdI = generateGenXForSpirv(CI, Suff, "group.id.");
    Instruction *MulI =
        BinaryOperator::CreateMul(WGSizeI, GroupIdI, "mul", &CI);
    return BinaryOperator::CreateAdd(LocalIdI, MulI, "add", &CI);
  });
  translateSpirvIntr("GlobalSize", [](CallInst &CI, StringRef Suff) {
    // GlobalSize = WorkGroupSize * NumWorkGroups
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *NumWGI = generateVectorGenXForSpirv(
        CI, Suff, "group.count.v3i32", "group_count.");
    return BinaryOperator::CreateMul(WGSizeI, NumWGI, "mul", &CI);
  });
  // TODO: Support GlobalOffset SPIRV intrinsics
  translateSpirvIntr("GlobalOffset", [](CallInst &CI, StringRef Suff) {
    return llvm::Constant::getNullValue(CI.getType());
  });
  translateSpirvIntr("NumWorkgroups", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "group.count.v3i32",
                                      "group_count.");
  });
}

static std::string getMDString(MDNode *N, unsigned I) {
  if (!N)
    return "";

  Metadata *Op = N->getOperand(I);
  if (!Op)
    return "";

  if (MDString *Str = dyn_cast<MDString>(Op)) {
    return Str->getString().str();
  }

  return "";
}
// Native sqrt doesn't support double type on GEN. We generate native sqrt only
// if fast flag is specified and no double type provided
static Value *translateSqrtOpIntrinsic(CallInst *CI) {
  assert(isa<llvm::IntrinsicInst>(CI) &&
         CI->getIntrinsicID() == Intrinsic::sqrt);

  GenXIntrinsic::ID GID = (CI->getType()->getScalarType()->isDoubleTy() ||
                           !CI->getFastMathFlags().isFast())
                              ? GenXIntrinsic::genx_ieee_sqrt
                              : GenXIntrinsic::genx_sqrt;

  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI->getModule(), GID, {CI->getType()});
  Value *RepI =
      CallInst::Create(NewFDecl, {CI->getOperand(0)}, CI->getName(), CI);
  CI->replaceAllUsesWith(RepI);
  return RepI;
}

static Value *translateReduceOpIntrinsic(IntrinsicInst *II, Instruction::BinaryOps ReduceOp) {
  auto *CI = cast<CallInst>(II);
  LLVMContext &CTX = CI->getContext();
  // TODO: introduce GenXIRBuilder helper
  bool IsFloatingPointReduction = CI->getType()->getScalarType()->isDoubleTy() ||
                                  CI->getType()->getScalarType()->isFloatTy();
  auto GetReduceSeq = [&](Value *Src, unsigned Size,
                          unsigned ElemOffset) -> Value * {
    if (Size == 1) {
      if (Src->getType()->isVectorTy()) {
        Value *Index = ConstantInt::get(Type::getInt32Ty(CTX), ElemOffset);
        return ExtractElementInst::Create(Src, Index, ".reduce.single",
                                          CI /*InsertBefore*/);
      }
      assert(ElemOffset == 0);
      return Src;
    }

    Type *SrcType = Src->getType();
    Type *SrcElementType = cast<FixedVectorType>(SrcType)->getElementType();

    Type *Tys[] = {FixedVectorType::get(SrcElementType, Size), SrcType,
                   Type::getInt16Ty(CTX)};
    auto *Decl = GenXIntrinsic::getGenXDeclaration(
        CI->getModule(), IsFloatingPointReduction ? GenXIntrinsic::genx_rdregionf :
                                                    GenXIntrinsic::genx_rdregioni, Tys);

    Value *Args[] = {
        Src,
        ConstantInt::get(Type::getInt32Ty(CTX), 0),    // VStride
        ConstantInt::get(Type::getInt32Ty(CTX), Size), // Width
        ConstantInt::get(Type::getInt32Ty(CTX), 1u),   // Stride
        ConstantInt::get(Type::getInt16Ty(CTX),
                         ElemOffset *
                             (SrcElementType->getPrimitiveSizeInBits() /
                              8)),                  // Offset in bytes
        ConstantInt::get(Type::getInt32Ty(CTX), 0), // Parent width (ignored)
    };

    return CallInst::Create(Decl, Args, Src->getName() + ".reduce.seq",
                            CI /*InsertBefore*/);
  };

  int OperandToReduceIdx = IsFloatingPointReduction ? 1 : 0;
  Value *OperandToReduce = CI->getOperand(OperandToReduceIdx);
  unsigned N =
      cast<FixedVectorType>(OperandToReduce->getType())->getNumElements();
  Value *CurrentReduceRes = nullptr;

  IRBuilder<> Builder(CI);

  // If reassoc is not allowed then do sequential operation for floating points.
  if (IsFloatingPointReduction && !CI->hasAllowReassoc()) {
    CurrentReduceRes = CI->getOperand(0);
    for (unsigned i = 0; i < N; i++) {
        auto *Extract = GetReduceSeq(OperandToReduce, 1, i);
        CurrentReduceRes = Builder.CreateBinOp(ReduceOp, CurrentReduceRes, Extract);
    }
    CI->replaceAllUsesWith(CurrentReduceRes);
    return CurrentReduceRes;
  }

  // Split vector into two parts. First part's size is alwayas a power of two
  // so we can simply apply reduction by spliiting it into parts. Reduction with
  // a second part is applicable it's size equals to the first part size
  unsigned N1, N2;
  if (llvm::isPowerOf2_32(N)) {
    N1 = N;
    N2 = 0;
  } else {
    N1 = 1u << llvm::Log2_32(N);
    N2 = N - N1;
  }

  CurrentReduceRes = OperandToReduce;
  for (unsigned i = N1 / 2; i >= 1; i /= 2) {
    auto *RSeq1 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq2 = GetReduceSeq(CurrentReduceRes, i, i);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq1, RSeq2);

    // Continue reducing if second part has suitable size, fall back
    // to reducing first part otherwise
    if (i > N2)
      continue;

    // Reduction across parts
    auto *RSeq21 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq22 = GetReduceSeq(OperandToReduce, i, N - N2);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq21, RSeq22);
    N2 -= i;
  }

  assert(CurrentReduceRes->getType() == CI->getType());

  if (IsFloatingPointReduction) {
    assert (CI->hasAllowReassoc());
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, CurrentReduceRes,  CI->getOperand(0));
  }

  CI->replaceAllUsesWith(CurrentReduceRes);
  return CurrentReduceRes;
}

//
// Trace the GetElementPtrInst[s] back to its base, which should be a SLM
// global variable. Convert the GEPs into multiple instructions that compute
// the byte offset from the base represented by these GEP instructions.
//
// Returns the final offset value if the GEP was able to be expanded to
// multiple instructions.
//
static Value *getSLMOffset(Value *V, Instruction *InsPos) {
  SmallVector<GEPOperator *, 4> GEPs;
  Value *base = V->stripPointerCasts();
  // gep : the last gep of pointer address, null if no GEP at all.
  while (GEPOperator *gep = dyn_cast<GEPOperator>(base)) {
    GEPs.push_back(gep);
    base = gep->getPointerOperand()->stripPointerCasts();
  }
  auto GV = dyn_cast<GlobalVariable>(base);
  // \TODO: what if the base is a PHINode?
  if (!GV)
    return nullptr;

  Module *M = GV->getParent();
  const DataLayout *DL = &M->getDataLayout();
  Type *int32Ty = Type::getInt32Ty(M->getContext());
  // obtain the offset of the global variable
  unsigned Offset;
  auto Invalid = GV->getAttribute(GENX_SLM_OFFSET)
                     .getValueAsString()
                     .getAsInteger(0, Offset);
  (void)Invalid; // INTEL
  assert(!Invalid);

  // initialize the PointerValue representing the offset
  Value *PointerValue = ConstantInt::get(int32Ty, Offset);
  const int nGEPs = GEPs.size();
  // GEPs is in the reverse order of execution! The last GEP is the first
  // one to execute.  For example:
  //    %37 = getelementptr inbounds float, float addrspace(1)* %signalw, i64
  //    16384 %38 = bitcast float addrspace(1)* %37 to[16 x[32 x[32 x float]]]
  //    addrspace(1)* %39 = getelementptr inbounds[16 x[32 x[32 x float]]], [16
  //    x[32 x[32 x float]]]
  //                        addrspace(1)* %38, i64 0, i64 % 34, i64 % 17, i64 %
  //                        18
  //    store float %36, float addrspace(1)* %39, align 4
  //
  //  GEPs = [%39, %37]  // GEPs[0] = %39, GEPs[1] = %37
  //
  for (int i = nGEPs; i > 0; --i) {
    GEPOperator *GEP = GEPs[i - 1];
    Value *PtrOp = GEP->getPointerOperand();
    PointerType *PtrTy = dyn_cast<PointerType>(PtrOp->getType());

    assert(PtrTy && "Only accept scalar pointer!");

    Type *Ty = PtrTy;
    gep_type_iterator GTI = gep_type_begin(GEP);
    for (auto OI = GEP->op_begin() + 1, E = GEP->op_end(); OI != E;
         ++OI, ++GTI) {
      Value *Idx = *OI;
      if (StructType *StTy = GTI.getStructTypeOrNull()) {
        unsigned Field =
            static_cast<unsigned>(cast<ConstantInt>(Idx)->getZExtValue());
        if (Field) {
          uint64_t Offset = DL->getStructLayout(StTy)->getElementOffset(Field);

          Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, OffsetValue, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
        Ty = StTy->getElementType(Field);
      } else {
        Ty = GTI.getIndexedType();
        if (const ConstantInt *CI = dyn_cast<ConstantInt>(Idx)) {
          if (!CI->isZero()) {
            uint64_t Offset = DL->getTypeAllocSize(Ty) * CI->getSExtValue();
            Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

            PointerValue = BinaryOperator::CreateAdd(PointerValue, OffsetValue,
                                                     "", InsPos);
            cast<llvm::Instruction>(PointerValue)
                ->setDebugLoc(InsPos->getDebugLoc());
          }
        } else {
          Value *NewIdx =
              CastInst::CreateTruncOrBitCast(Idx, int32Ty, "", InsPos);
          cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          APInt ElementSize =
              APInt((unsigned int)int32Ty->getPrimitiveSizeInBits(),
                    DL->getTypeAllocSize(Ty));

          if (ElementSize != 1) {
            NewIdx = BinaryOperator::CreateMul(
                NewIdx, ConstantInt::get(int32Ty, ElementSize), "", InsPos);
            cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          }

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, NewIdx, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
      }
    }
  }
  return PointerValue;
}

static Value *translateSVMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  auto DTy = LoadOp->getType();
  auto DL = LoadOp->getModule()->getDataLayout();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
        "svm.block.ld.unaligned";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {DTy, PtrV->getType()});
    RepI = CallInst::Create(NewFDecl, {PtrV}, "svm.block.ld.unaligned", LoadOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int RetLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, RetLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VPtrTy});
    auto VecPtr =
        CastInst::CreateBitOrPointerCast(PtrV, VPtrTy, PtrV->getName(), LoadOp);
    Instruction *IntrI = CallInst::Create(
        NewFDecl, {OnePred1, NumBlksC, VecPtr, UndefValue::get(VDTy)},
        "svm.gather", LoadOp);
    if (EltBytes == 2) {
      // cast 4xi8 to 2xi16
      auto I16x2Ty =
          llvm::FixedVectorType::get(IntegerType::getInt16Ty(CTX), 2);
      IntrI = CastInst::CreateBitOrPointerCast(
          IntrI, I16x2Ty, IntrI->getName() + ".cast", LoadOp);
    }
    RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
  }
  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Constant *getConstVector(Type *ITy, unsigned int n, unsigned int step) {
  std::vector<Constant *> vConsts;
  unsigned v = 0;
  for (unsigned i = 0; i < n; i++) {
    vConsts.push_back(ConstantInt::get(ITy, v));
    v += step;
  }
  return ConstantVector::get(vConsts);
}

static Instruction *createRdRegion(Value *Input, const Twine &Name,
                                   Instruction *InsertBefore, int NumElements,
                                   int StartIdx, int VStride, int Width,
                                   int Stride) {
  assert(isa<VectorType>(Input->getType()));
  IntegerType *I32Ty = Type::getInt32Ty(Input->getContext());
  Value *ParentWidthArg = UndefValue::get(I32Ty);
  Value *Args[] = {
      // Args to new rdregion:
      Input,                             // input to original rdregion
      ConstantInt::get(I32Ty, VStride),  // vstride
      ConstantInt::get(I32Ty, Width),    // width
      ConstantInt::get(I32Ty, Stride),   // stride
      ConstantInt::get(I32Ty, StartIdx), // start index (in bytes)
      ParentWidthArg                     // parent width
  };
  Type *ElTy = cast<VectorType>(Input->getType())->getElementType();
  auto RegionTy = llvm::FixedVectorType::get(ElTy, NumElements);
  Module *M = InsertBefore->getParent()->getParent()->getParent();
  auto IID = ElTy->isFloatingPointTy() ? GenXIntrinsic::genx_rdregionf
                                       : GenXIntrinsic::genx_rdregioni;
  Type *Tys[] = {RegionTy, Args[0]->getType(), Args[4]->getType()};
  auto Decl = GenXIntrinsic::getGenXDeclaration(M, IID, Tys);
  Instruction *NewInst = CallInst::Create(Decl, Args, Name, InsertBefore);
  return NewInst;
}

static Instruction *createWrRegion(Value *OldVal, Value *Input,
                                   const Twine &Name, Instruction *InsertBefore,
                                   int StartIdx, int VStride, int Width,
                                   int Stride, Value *PredV = nullptr) {
  assert(isa<VectorType>(Input->getType()));
  assert(isa<VectorType>(OldVal->getType()));
  assert(OldVal->getType()->getScalarType() ==
         Input->getType()->getScalarType());
  IntegerType *I32Ty = Type::getInt32Ty(Input->getContext());
  Value *ParentWidthArg = UndefValue::get(I32Ty);
  // if there is no predicate value, set mask to all-one
  Value *MaskArg =
      PredV ? PredV : ConstantInt::get(Type::getInt1Ty(Input->getContext()), 1);
  // Build the wrregion.
  Value *Args[] = {
      // Args to new wrregion:
      OldVal,                            // original vector
      Input,                             // value to write into subregion
      ConstantInt::get(I32Ty, VStride),  // vstride
      ConstantInt::get(I32Ty, Width),    // width
      ConstantInt::get(I32Ty, Stride),   // stride
      ConstantInt::get(I32Ty, StartIdx), // start index (in bytes)
      ParentWidthArg, // parent width (if variable start index)
      MaskArg         // mask
  };
  Type *ElTy = cast<VectorType>(Input->getType())->getElementType();
  auto IID = ElTy->isFloatingPointTy() ? GenXIntrinsic::genx_wrregionf
                                       : GenXIntrinsic::genx_wrregioni;
  Module *M = InsertBefore->getParent()->getParent()->getParent();
  Type *Tys[] = {Args[0]->getType(), Args[1]->getType(), Args[5]->getType(),
                 Args[7]->getType()};
  auto Decl = GenXIntrinsic::getGenXDeclaration(M, IID, Tys);
  Instruction *NewInst = CallInst::Create(Decl, Args, Name, InsertBefore);
  return NewInst;
}

// change vector from llllllllhhhhhhhh to lhlhlhlhlhlhlhlh
static Instruction *formDoubleVector(Value *InputVector,
                                     Instruction *InsertBefore,
                                     Value *PredV = nullptr,
                                     Value *OldVal = nullptr) {
  auto *VTy = dyn_cast<FixedVectorType>(InputVector->getType());
  assert(VTy);
  // if Pred-value is not null then old-value cannot be null
  assert(!PredV || OldVal);
  unsigned VL = VTy->getNumElements();
  assert(VL == 16 || VL == 32);
  Type *EltTy = VTy->getElementType();
  assert(!EltTy->isPointerTy());
  auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
  auto Width = VL / 2;
  // read low half
  auto LowHalf = createRdRegion(
      InputVector, InputVector->getName() + ".1stHalf", InsertBefore, Width,
      0 /*StartIdx*/, 0 /*VStride*/, Width, 1 /*HStride*/);
  // read high half
  auto HighHalf = createRdRegion(
      InputVector, InputVector->getName() + ".2ndHalf", InsertBefore, Width,
      Width * EltBytes /*startIdx*/, 0 /*VStride*/, Width, 1 /*HStride*/);
  if (OldVal == nullptr)
    OldVal = UndefValue::get(VTy);
  else
    assert(OldVal->getType() == VTy);
  // write low half
  auto Partial = createWrRegion(
      OldVal, LowHalf, InputVector->getName() + ".InsLow", InsertBefore,
      0 /*StartIdx*/, 0 /*VStride*/, Width, 2 /*HStride*/, PredV);
  // write high half
  auto Complete = createWrRegion(
      Partial, HighHalf, InputVector->getName() + ".InsHigh", InsertBefore,
      EltBytes /*StartIdx*/, 0 /*VStride*/, Width, 2 /*HStride*/, PredV);
  return Complete;
}

// change vector from lhlhlhlhlhlhlhlh to llllllllhhhhhhhh
static Instruction *disbandDoubleVector(Value *InputVector,
                                        Instruction *InsertBefore) {
  auto *VTy = dyn_cast<FixedVectorType>(InputVector->getType());
  assert(VTy);
  unsigned VL = VTy->getNumElements();
  assert(VL == 16 || VL == 32);
  Type *EltTy = VTy->getElementType();
  assert(!EltTy->isPointerTy());
  auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
  auto Width = VL / 2;
  // read low elements
  auto LowHalf = createRdRegion(InputVector, InputVector->getName() + "exLow",
                                InsertBefore, Width, 0 /*StartIdx*/,
                                0 /*VStride*/, Width, 2 /*HStride*/);
  // read high elements
  auto HighHalf = createRdRegion(InputVector, InputVector->getName() + "exHigh",
                                 InsertBefore, Width, EltBytes /*startIdx*/,
                                 0 /*VStride*/, Width, 2 /*HStride*/);
  auto Undef = UndefValue::get(VTy);
  // write low half
  auto Partial =
      createWrRegion(Undef, LowHalf, "", InsertBefore, 0 /*StartIdx*/,
                     0 /*VStride*/, Width, 1 /*HStride*/);
  // write high half
  auto Complete = createWrRegion(Partial, HighHalf, "", InsertBefore,
                                 Width * EltBytes /*StartIdx*/, 0 /*VStride*/,
                                 Width, 1 /*HStride*/);
  return Complete;
}

static Value *translateSLMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  Type *DTy = LoadOp->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, LoadOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  auto DL = LoadOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    unsigned VL = cast<FixedVectorType>(DTy)->getNumElements();
    Type *EltTy = cast<FixedVectorType>(DTy)->getElementType();
    auto EltBytes = getElementSizeInBytes(EltTy, DL);
    // create constant for offset
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "gather.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
      // crease constant for num-blocks
      auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          LoadOp->getModule(), ID, {DTy, PredVTy, VOffset->getType()});
      RepI = CallInst::Create(NewFDecl,
                              {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset,
                               VOffset, UndefValue::get(DTy)},
                              "slm.block.gather", LoadOp);
    } else if (EltBytes == 8) {
      // there is no surface load of i64 type, therefore we need to use gather4
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "gather4.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      // create constant for channel-mask GR-enabled
      auto ChanMask = ConstantInt::get(CIntTy, 3);
      // need to create the double-vec-type
      auto Dx2Ty =
          llvm::FixedVectorType::get(IntegerType::getInt32Ty(CTX), VL * 2);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          LoadOp->getModule(), ID, {Dx2Ty, PredVTy, VOffset->getType()});
      auto Gather4I =
          CallInst::Create(NewFDecl,
                           {OnePredV, ChanMask, ScaleC, BTI, SLMOffset, VOffset,
                            UndefValue::get(Dx2Ty)},
                           "slm.block.gather", LoadOp);
      // reorder the vector
      auto ShuffleV = formDoubleVector(Gather4I, LoadOp);
      // cast back to the original 64-bit type
      RepI = CastInst::CreateBitOrPointerCast(ShuffleV, DTy, LoadOp->getName(),
                                              LoadOp);
    } else
      assert(false);
  } else {
    // scalar load
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "gather.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 || EltBytes == 8);
    int NumBlks = (EltBytes >= 4) ? 2 : (EltBytes - 1);
    auto EltTy = (EltBytes == 8) ? CIntTy : DTy;
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, 1);
    // create constant for offset
    auto VOffsetTy = llvm::FixedVectorType::get(CIntTy, 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VOffsetTy});
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), LoadOp);
    auto GOffsetC = ConstantInt::get(CIntTy, 0);
    Instruction *IntrI =
        CallInst::Create(NewFDecl,
                         {OnePred1, NumBlksC, ScaleC, BTI, GOffsetC, VecOffset,
                          UndefValue::get(VDTy)},
                         "slm.gather", LoadOp);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    auto OneC = ConstantInt::get(CIntTy, 1);
    if (EltBytes <= 4)
      RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
    else {
      // load high-half
      auto ResTy = llvm::FixedVectorType::get(EltTy, 2);
      GOffsetC = ConstantInt::get(CIntTy, 4);
      Instruction *IntrI2 =
          CallInst::Create(NewFDecl,
                           {OnePred1, NumBlksC, ScaleC, BTI, GOffsetC,
                            VecOffset, UndefValue::get(VDTy)},
                           "slm.gather", LoadOp);
      auto LowHalf = Builder.CreateExtractElement(IntrI, ZeroC,
                                                  LoadOp->getName() + ".low");
      auto HighHalf = Builder.CreateExtractElement(IntrI2, ZeroC,
                                                   LoadOp->getName() + ".high");
      auto Partial =
          Builder.CreateInsertElement(UndefValue::get(ResTy), LowHalf, ZeroC);
      auto Complete = Builder.CreateInsertElement(Partial, HighHalf, OneC);
      RepI = CastInst::CreateBitOrPointerCast(Complete, DTy, LoadOp->getName(),
                                              LoadOp);
    }
  }

  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Value *translateSVMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  auto DL = StoreOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.block.st";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PtrV->getType(), DTy});
    RepI = CallInst::Create(
        NewFDecl, {PtrV, DTV},
        NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st", StoreOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int DataLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, DataLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VPtrTy, VDTy});
    auto VecPtr = CastInst::CreateBitOrPointerCast(PtrV, VPtrTy,
                                                   PtrV->getName(), StoreOp);
    Value *VecDTV = nullptr;
    if (EltBytes == 2) {
      // cast 2xi16 to 4xi8
      auto I16x2Ty = llvm::FixedVectorType::get(DTy, 2);
      VecDTV = Builder.CreateInsertElement(UndefValue::get(I16x2Ty), DTV, ZeroC,
                                           DTV->getName());
      VecDTV = CastInst::CreateBitOrPointerCast(
          VecDTV, VDTy, DTV->getName() + ".cast", StoreOp);
    } else
      VecDTV = Builder.CreateInsertElement(UndefValue::get(VDTy), DTV, ZeroC,
                                           DTV->getName());
    RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, VecPtr, VecDTV});
  }
  return RepI;
}

static Value *translateSLMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, StoreOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  auto DL = StoreOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    unsigned VL = cast<FixedVectorType>(DTy)->getNumElements();
    Type *EltTy = cast<FixedVectorType>(DTy)->getElementType();
    auto EltBytes = getElementSizeInBytes(EltTy, DL);
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "scatter.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
      // create constant for num-blocks
      auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          StoreOp->getModule(), ID, {PredVTy, VOffset->getType(), DTy});
      RepI = Builder.CreateCall(
          NewFDecl, {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset, VOffset, DTV});
    } else if (EltBytes == 8) {
      // cast it into i32 vector
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "scatter4.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      auto Dx2Ty = llvm::FixedVectorType::get(CIntTy, 2 * VL);
      auto VecDTV = CastInst::CreateBitOrPointerCast(
          DTV, Dx2Ty, DTV->getName() + ".cast", StoreOp);
      // shuffle it into 2-channel
      auto VectDTV2 = disbandDoubleVector(VecDTV, StoreOp);
      // create the scatter4
      // create channel-mask, red-green enabled
      auto ChanMask = ConstantInt::get(CIntTy, 3);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          StoreOp->getModule(), ID, {PredVTy, VOffset->getType(), Dx2Ty});
      RepI = Builder.CreateCall(NewFDecl, {OnePredV, ChanMask, ScaleC, BTI,
                                           SLMOffset, VOffset, VectDTV2});
    } else
      assert(false);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "scatter.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 || EltBytes == 8);
    int NumBlks = (EltBytes >= 4) ? 2 : (EltBytes - 1);
    auto EltTy = (EltBytes == 8) ? CIntTy : DTy;
    auto NumElts = (EltBytes == 8) ? 2 : 1;
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts);
    // cast data to vector
    auto VecDTV = CastInst::CreateBitOrPointerCast(
        DTV, VDTy, DTV->getName() + ".cast", StoreOp);
    auto VOffsetTy = llvm::FixedVectorType::get(SLMOffset->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto GOffsetC = ConstantInt::get(CIntTy, 0);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), StoreOp);
    // create the intrinsic call
    auto V1DTy = llvm::FixedVectorType::get(EltTy, 1);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VOffsetTy, V1DTy});
    if (NumElts == 1)
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, VecDTV});
    else {
      auto IndexC = ConstantInt::get(CIntTy, 0);
      auto DTVL =
          Builder.CreateExtractElement(VecDTV, IndexC, DTV->getName() + ".low");
      IndexC = ConstantInt::get(CIntTy, 1);
      auto DTVH = Builder.CreateExtractElement(VecDTV, IndexC,
                                               DTV->getName() + ".high");
      auto TmpDTV = CastInst::CreateBitOrPointerCast(
          DTVL, V1DTy, DTVL->getName() + ".cast", StoreOp);
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, TmpDTV});
      TmpDTV = CastInst::CreateBitOrPointerCast(
          DTVH, V1DTy, DTVH->getName() + ".cast", StoreOp);
      GOffsetC = ConstantInt::get(CIntTy, 4);
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, TmpDTV});
    }
  }
  return RepI;
}

// These two tables are used to change llvm math intrinsic names (left column)
// to the genx intrinsic (right column).
//
// f32 mapping
std::unordered_map<Intrinsic::ID, GenXIntrinsic::ID> GenXMath32 = {
    //  Basic functions
    //{Intrinsic::fma,        NA}, // llvm fma is supported by BE
    {Intrinsic::maxnum, GenXIntrinsic::genx_fmax},
    {Intrinsic::minnum, GenXIntrinsic::genx_fmin},

    //  Exponential functions
    {Intrinsic::exp, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::exp2, GenXIntrinsic::genx_exp},
    {Intrinsic::log, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log2, GenXIntrinsic::genx_log},
    {Intrinsic::log10, GenXIntrinsic::not_genx_intrinsic},

    //  Power functions
    {Intrinsic::pow, GenXIntrinsic::genx_pow},

    //  Trig & hyperbolic functions
    {Intrinsic::sin, GenXIntrinsic::genx_sin},
    {Intrinsic::cos, GenXIntrinsic::genx_cos},

    //  Rounding functions
    {Intrinsic::ceil, GenXIntrinsic::genx_rndu},
    {Intrinsic::floor, GenXIntrinsic::genx_rndd},
    {Intrinsic::trunc, GenXIntrinsic::genx_rndz},
    {Intrinsic::round, GenXIntrinsic::not_genx_intrinsic},

    //  Floating-point manipulation functions
    {Intrinsic::copysign, GenXIntrinsic::not_genx_intrinsic},
};

// f64 mapping
std::unordered_map<Intrinsic::ID, GenXIntrinsic::ID> GenXMath64 = {
    //  Basic functions
    {Intrinsic::fma, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::maxnum, GenXIntrinsic::genx_fmax},
    {Intrinsic::minnum, GenXIntrinsic::genx_fmin},

    //  Exponential functions
    {Intrinsic::exp, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::exp2, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log2, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log10, GenXIntrinsic::not_genx_intrinsic},

    //  Power functions
    {Intrinsic::pow, GenXIntrinsic::not_genx_intrinsic},

    //  Trig & hyperbolic functions
    {Intrinsic::sin, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::cos, GenXIntrinsic::not_genx_intrinsic},

    //  Rounding functions
    {Intrinsic::ceil, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::floor, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::trunc, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::round, GenXIntrinsic::not_genx_intrinsic},

    //  Floating-point manipulation functions
    {Intrinsic::copysign, GenXIntrinsic::not_genx_intrinsic}};

static Value *translateLLVMInst(Instruction *Inst) {
  LLVMContext &CTX = Inst->getContext();
  IRBuilder<> Builder(Inst);
  if (auto CastOp = dyn_cast<llvm::CastInst>(Inst)) {
    llvm::Type *DstTy = CastOp->getDestTy();
    auto CastOpcode = CastOp->getOpcode();
    if (isa<FixedVectorType>(DstTy) && (CastOpcode == llvm::Instruction::FPToUI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() <= 32) ||
        (CastOpcode == llvm::Instruction::FPToSI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() < 32)) {
      llvm::Value *Src = CastOp->getOperand(0);
      auto TmpTy = llvm::FixedVectorType::get(
          llvm::Type::getInt32Ty(CTX),
          cast<FixedVectorType>(DstTy)->getNumElements());
      Src = Builder.CreateFPToSI(Src, TmpTy);

      llvm::Instruction::CastOps TruncOp = llvm::Instruction::Trunc;
      auto *NewDst = Builder.CreateCast(TruncOp, Src, DstTy);
      CastOp->replaceAllUsesWith(NewDst);
      return NewDst;
    }
    return CastOp;
  }
  if (auto LoadOp = dyn_cast<llvm::LoadInst>(Inst)) {
    auto AS = LoadOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS)
      return translateSVMLoad(LoadOp);
    else if (AS == SYCL_SLM_AS)
      return translateSLMLoad(LoadOp);
    return LoadOp;
  }
  if (auto StoreOp = dyn_cast<llvm::StoreInst>(Inst)) {
    auto AS = StoreOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS)
      return translateSVMStore(StoreOp);
    else if (AS == SYCL_SLM_AS)
      return translateSLMStore(StoreOp);
    return StoreOp;
  }
  if (auto CallOp = dyn_cast<llvm::IntrinsicInst>(Inst)) {
    // handle memory intrinsics
    // masked.gather, masked.scatter etc
    auto ID = CallOp->getIntrinsicID();
    auto DL = CallOp->getModule()->getDataLayout();
    switch (ID) {
    case Intrinsic::masked_load: {
      auto PtrV = CallOp->getArgOperand(0);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      if (isa<AddrSpaceCastInst>(PtrV) && AS == SYCL_GENERIC_AS) {
        PtrV = cast<Instruction>(PtrV)->getOperand(0);
        AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      }
      if (AS == 0) { // thread private memory
                     // convert to load then select
        auto VecDT =
            Builder.CreateLoad(CallOp->getType(), PtrV, CallOp->getName());
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        auto DTy = CallOp->getType();
        // convert to unaligned-block-load then select
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTy, PtrV->getType()});
        auto VecDT = CallInst::Create(NewFDecl, {PtrV},
                                      "svm.block.ld.unaligned", CallOp);
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else if (AS == SYCL_SLM_AS) {
        auto *DTy = cast<FixedVectorType>(CallOp->getType());
        auto SLMOffset = getSLMOffset(PtrV, CallOp);
        assert(SLMOffset);
        auto CIntTy = Type::getInt32Ty(CTX);
        auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
        unsigned VL = DTy->getNumElements();
        Type *EltTy = DTy->getElementType();
        auto PredV = CallOp->getArgOperand(2);
        auto EltBytes = getElementSizeInBytes(EltTy, DL);
        // create constant for offset
        auto VOffset = getConstVector(CIntTy, VL, EltBytes);
        auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
        assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 ||
               EltBytes == 8);
        if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
          std::string IntrName =
              std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
              "gather.scaled";
          auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
          int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
          // crease constant for num-blocks
          auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
          // create the intrinsic call
          Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
              CallOp->getModule(), ID,
              {DTy, PredV->getType(), VOffset->getType()});
          auto RepI =
              IntrinsicInst::Create(NewFDecl,
                                    {PredV, NumBlksC, ScaleC, BTI, SLMOffset,
                                     VOffset, CallOp->getArgOperand(3)},
                                    "slm.block.gather", CallOp);
          return RepI;
        }
        if (EltBytes == 8) {
          // there is no surface load of i64 type,
          // therefore we need to use gather4
          std::string IntrName =
              std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
              "gather4.scaled";
          auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
          // create constant for channel-mask GR-enabled
          auto ChanMask = ConstantInt::get(CIntTy, 3);
          // need to create the double-vec-type
          auto Dx2Ty =
              llvm::FixedVectorType::get(IntegerType::getInt32Ty(CTX), VL * 2);
          // create the intrinsic call
          Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
              CallOp->getModule(), ID,
              {Dx2Ty, PredV->getType(), VOffset->getType()});
          auto Gather4I =
              IntrinsicInst::Create(NewFDecl,
                                    {PredV, ChanMask, ScaleC, BTI, SLMOffset,
                                     VOffset, UndefValue::get(Dx2Ty)},
                                    "slm.block.gather", CallOp);
          // cast the old value
          auto OldVal = CastInst::CreateBitOrPointerCast(
              CallOp->getArgOperand(3), Dx2Ty, "", CallOp);
          // reorder the vector
          auto ShuffleV = formDoubleVector(Gather4I, CallOp, PredV, OldVal);
          // cast back to the original 64-bit type
          auto RepI = CastInst::CreateBitOrPointerCast(
              ShuffleV, DTy, CallOp->getName(), CallOp);
          return RepI;
        }
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_store: {
      auto PtrV = CallOp->getArgOperand(1);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      if (isa<AddrSpaceCastInst>(PtrV) && AS == SYCL_GENERIC_AS) {
        PtrV = cast<Instruction>(PtrV)->getOperand(0);
        AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      }
      auto DTV = CallOp->getArgOperand(0);
      if (AS == 0) { // thread private memory
                     // convert to load then select then store
        auto VecDT = Builder.CreateLoad(DTV->getType(), PtrV);
        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);
        auto RepI = Builder.CreateStore(SelI, PtrV);
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        // convert to unaligned-block-load then select
        // then block-store
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTV->getType(), PtrV->getType()});
        auto VecDT = CallInst::Create(NewFDecl, {PtrV},
                                      "svm.block.ld.unaligned", CallOp);

        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);

        IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                   "svm.block.st";
        ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {PtrV->getType(), DTV->getType()});
        auto RepI = CallInst::Create(
            NewFDecl, {PtrV, SelI},
            NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st",
            CallOp);
        return RepI;
      } else if (AS == SYCL_SLM_AS) {
        auto *DTy = cast<FixedVectorType>(DTV->getType());
        auto SLMOffset = getSLMOffset(PtrV, CallOp);
        assert(SLMOffset);
        auto CIntTy = Type::getInt32Ty(CTX);
        auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
        unsigned VL = DTy->getNumElements();
        Type* EltTy = DTy->getElementType();
        auto EltBytes = getElementSizeInBytes(EltTy, DL);
        auto PredV = CallOp->getArgOperand(3);
        auto VOffset = getConstVector(CIntTy, VL, EltBytes);
        auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
        assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 ||
               EltBytes == 8);
        if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
          std::string IntrName =
              std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
              "scatter.scaled";
          auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
          int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
          // create constant for num-blocks
          auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
          // create the intrinsic call
          Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
              CallOp->getModule(), ID,
              {PredV->getType(), VOffset->getType(), DTy});
          auto RepI =
              Builder.CreateCall(NewFDecl, {PredV, NumBlksC, ScaleC, BTI,
                                            SLMOffset, VOffset, DTV});
          return RepI;
        }
        if (EltBytes == 8) {
          // cast it into i32 vector
          std::string IntrName =
              std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
              "scatter4.scaled";
          auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
          auto Dx2Ty = llvm::FixedVectorType::get(CIntTy, 2 * VL);
          auto VecDTV = CastInst::CreateBitOrPointerCast(
              DTV, Dx2Ty, DTV->getName() + ".cast", CallOp);
          // shuffle it into 2-channel
          auto VectDTV2 = disbandDoubleVector(VecDTV, CallOp);
          // create the scatter4
          // create channel-mask, red-green enabled
          auto ChanMask = ConstantInt::get(CIntTy, 3);
          // create the intrinsic call
          Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
              CallOp->getModule(), ID,
              {PredV->getType(), VOffset->getType(), Dx2Ty});
          auto RepI =
              Builder.CreateCall(NewFDecl, {PredV, ChanMask, ScaleC, BTI,
                                            SLMOffset, VOffset, VectDTV2});
          return RepI;
        }
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_gather: {
      auto PtrV = CallOp->getArgOperand(0);
      auto MaskV = CallOp->getArgOperand(2);
      auto OldV = CallOp->getArgOperand(3);
      auto DTy = CallOp->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      if (isa<AddrSpaceCastInst>(PtrV) && AS == SYCL_GENERIC_AS) {
        PtrV = cast<Instruction>(PtrV)->getOperand(0);
        PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
        AS = cast<PointerType>(PtrETy)->getAddressSpace();
      }
      assert(AS != SYCL_SLM_AS && "yet to support masked SLM gather");
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<FixedVectorType>(DTy)->getElementType();
      auto NumElts = cast<FixedVectorType>(DTy)->getNumElements();
      auto EltBytes = getElementSizeInBytes(EltTy, DL);
      int EltsPerLane = 1;
      // for short or byte type, load-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        if (isa<UndefValue>(OldV))
          OldV = UndefValue::get(VDTy);
        else {
          // zext to i32 type
          auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
          auto ExtI = Builder.CreateZExt(OldV, VInt32, OldV->getName());
          // bitcast to 4xi8
          OldV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
        }
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {VDTy, MaskV->getType(), PtrV->getType()});
      RepI = CallInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, OldV},
                              CallOp->getName(), CallOp);
      if (EltBytes < 4) {
        // bitcast to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto CastI = Builder.CreateBitCast(RepI, VInt32, RepI->getName());
        // trunc to the dst type
        RepI = Builder.CreateTrunc(CastI, DTy, CastI->getName());
      }
      CallOp->replaceAllUsesWith(RepI);
      return RepI;
    } break;
    case Intrinsic::masked_scatter: {
      auto DataV = CallOp->getArgOperand(0);
      auto PtrV = CallOp->getArgOperand(1);
      auto MaskV = CallOp->getArgOperand(3);
      auto DTy = DataV->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      if (isa<AddrSpaceCastInst>(PtrV) && AS == SYCL_GENERIC_AS) {
        PtrV = cast<Instruction>(PtrV)->getOperand(0);
        PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
        AS = cast<PointerType>(PtrETy)->getAddressSpace();
      }
      assert(AS != SYCL_SLM_AS && "yet to support masked SLM scatter");
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<FixedVectorType>(DTy)->getElementType();
      auto NumElts = cast<FixedVectorType>(DTy)->getNumElements();
      auto EltBytes = getElementSizeInBytes(EltTy, DL);
      int EltsPerLane = 1;
      // for short or byte type, store-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        // zext to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto ExtI = Builder.CreateZExt(DataV, VInt32, DataV->getName());
        // bitcast to 4xi8
        DataV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {MaskV->getType(), PtrV->getType(), VDTy});
      RepI = CallInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, DataV},
                              CallOp->getName(), CallOp);
      return RepI;
    } break;
    case Intrinsic::vector_reduce_add:
      return translateReduceOpIntrinsic(CallOp, Instruction::Add);
    case Intrinsic::vector_reduce_mul:
      return translateReduceOpIntrinsic(CallOp, Instruction::Mul);
    case Intrinsic::vector_reduce_xor:
      return translateReduceOpIntrinsic(CallOp, Instruction::Xor);
    case Intrinsic::vector_reduce_or:
      return translateReduceOpIntrinsic(CallOp, Instruction::Or);
    case Intrinsic::vector_reduce_fadd:
      return translateReduceOpIntrinsic(CallOp, Instruction::FAdd);
    case Intrinsic::sqrt:
      return translateSqrtOpIntrinsic(cast<CallInst>(CallOp));
    default: {
      auto DTy = CallOp->getType()->getScalarType();
      GenXIntrinsic::ID GID = GenXIntrinsic::not_genx_intrinsic;
      // find the intrinsic mapping
      if (DTy->isFloatTy()) {
        auto Map = GenXMath32.find(ID);
        if (Map != GenXMath32.end()) {
          GID = Map->second;
        }
      } else if (DTy->isDoubleTy()) {
        auto Map = GenXMath32.find(ID);
        if (Map != GenXMath64.end()) {
          GID = Map->second;
        }
      }
      // do the conversion
      if (GID != GenXIntrinsic::not_genx_intrinsic) {
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            CallOp->getModule(), GID, {CallOp->getType()});
        SmallVector<Value *, 2> ValueOperands(CallOp->args());
        Value *RepI = CallInst::Create(NewFDecl, ValueOperands,
                                       CallOp->getName(), CallOp);
        CallOp->replaceAllUsesWith(RepI);
        return RepI;
      }
    } break;
    }
    return CallOp;
  }
  return Inst;
}

static unsigned int assignSLMOffset(Module &M) {
  auto DL = M.getDataLayout();
  unsigned SLMSize = 0;
  for (auto &&GV : M.getGlobalList()) {
    auto Ty = dyn_cast<PointerType>(GV.getType());
    if (Ty && Ty->getAddressSpace() == SYCL_SLM_AS) {
      auto DTy = GV.getValueType();
      auto BufferSize = static_cast<size_t>(DL.getTypeAllocSize(DTy));
      auto align = GV.getAlignment();
      if (align == 0) {
        int EltBytes = getElementSizeInBytes(DTy->getScalarType(), DL);
        align = (EltBytes > 0 && EltBytes < 8) ? EltBytes : 8;
      }
      SLMSize = ((SLMSize + align - 1) / align) * align;
      if (!GV.hasAttribute(GENX_SLM_OFFSET))
        GV.addAttribute(GENX_SLM_OFFSET, std::to_string(SLMSize));
      SLMSize += BufferSize;
    }
  }
  return SLMSize;
}

PreservedAnalyses VPOParoptLowerSimdPass::run(Function &F,
                                              FunctionAnalysisManager &FAM) {
  // Only consider functions marked with !sycl_explicit_simd
  bool isOmpSpirKernel = false;

  if ((F.getMetadata("omp_simd_kernel") != nullptr) ||
      (F.getMetadata("sycl_explicit_simd") == nullptr &&
       F.getCallingConv() == CallingConv::SPIR_KERNEL)) {
    IRBuilder<> Builder(F.getEntryBlock().getFirstNonPHI());
    int simdWidth = 1;
    Metadata *AttrMDArgs[] = {
        ConstantAsMetadata::get(Builder.getInt32(simdWidth))};
    F.setMetadata("intel_reqd_sub_group_size",
                  MDNode::get(F.getContext(), AttrMDArgs));
    isOmpSpirKernel = true;
  } else
    return PreservedAnalyses::all();

  auto SLMSize = assignSLMOffset(*F.getParent());

  // add module-level meta-data for kernel functions
  if (isOmpSpirKernel) {
    auto M = F.getParent();
    auto MetaKernels = M->getOrInsertNamedMetadata(GENX_KERNEL_METADATA);

    LLVMContext &Ctx = M->getContext();
    Type *I32Ty = Type::getInt32Ty(Ctx);

    enum { AK_NORMAL, AK_SAMPLER, AK_SURFACE, AK_VME };
    enum { IK_NORMAL, IK_INPUT, IK_OUTPUT, IK_INPUT_OUTPUT };
    // Metadata node containing N i32s, where N is the number of kernel
    // arguments, and each i32 is the kind of argument,  one of:
    //     0 = general, 1 = sampler, 2 = surface, 3 = vme
    // (the same values as in the "kind" field of an "input_info"
    // record in a vISA kernel.
    SmallVector<Metadata *, 8> ArgKinds;
    // Optional, not supported for compute
    SmallVector<Metadata *, 8> ArgInOutKinds;
    // Metadata node describing N strings where N is the number of kernel
    // arguments, each string describing argument type in OpenCL.
    // required for running on top of OpenCL runtime.
    SmallVector<Metadata *, 8> ArgTypeDescs;
    auto *KernelArgTypes = F.getMetadata("kernel_arg_type");
    unsigned Idx = 0;

    // Iterate argument list to gather argument kinds and generate argument
    // descriptors.
    for (const Argument &Arg : F.args()) {
      int Kind = AK_NORMAL;
      int IKind = IK_NORMAL;

      auto ArgType = getMDString(KernelArgTypes, Idx);

      if (ArgType.find("image1d_t") != std::string::npos ||
          ArgType.find("image2d_t") != std::string::npos ||
          ArgType.find("image3d_t") != std::string::npos ||
          ArgType.find("image1d_buffer_t") != std::string::npos) {
        Kind = AK_SURFACE;
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgType));
      } else {
        StringRef ArgDesc = "";
        if (Arg.getType()->isPointerTy())
          ArgDesc = "svmptr_t";
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgDesc));
      }

      ArgKinds.push_back(ValueAsMetadata::get(ConstantInt::get(I32Ty, Kind)));
      ArgInOutKinds.push_back(
          ValueAsMetadata::get(ConstantInt::get(I32Ty, IKind)));
      Idx++;
    }
    MDNode *Kinds = MDNode::get(Ctx, ArgKinds);
    MDNode *IOKinds = MDNode::get(Ctx, ArgInOutKinds);
    MDNode *ArgDescs = MDNode::get(Ctx, ArgTypeDescs);

    Metadata *MDArgs[] = {ValueAsMetadata::get(&F),
                          MDString::get(Ctx, F.getName().str()),
                          Kinds,
                          ValueAsMetadata::get(llvm::ConstantInt::get(
                              I32Ty, SLMSize)), // SLM size in bytes
                          ValueAsMetadata::get(llvm::ConstantInt::getNullValue(
                              I32Ty)), // arg offsets
                          IOKinds,
                          ArgDescs};

    // Add this kernel to the root.
    MetaKernels->addOperand(MDNode::get(Ctx, MDArgs));
    F.addFnAttr("oclrt", "1");
    F.addFnAttr("CMGenxMain");
  }

  SmallVector<Instruction *, 8> SimdToErases;

  for (Instruction &I : instructions(F)) {
    auto replace = translateLLVMInst(&I);
    if (replace != &I) {
      SimdToErases.push_back(&I);
      continue;
    }

    auto *CI = dyn_cast<CallInst>(&I);
    Function *Callee = nullptr;
    if (!CI || !(Callee = CI->getCalledFunction()))
      continue;
    StringRef Name = Callee->getName();

    if (!Name.consume_front(SIMD_INTRIN_PREF0))
      continue;
    // now skip the digits
    Name = Name.drop_while([](char C) { return std::isdigit(C); });

    if (Name.consume_front(SPIRV_INTRIN_PREF)) {
      translateSpirvIntrinsic(CI, Name, SimdToErases);
      // For now: if no match, just let it go untranslated.
      continue;
    }
  }
  for (auto *CI : SimdToErases) {
    CI->eraseFromParent();
  }
  for (auto I = inst_begin(F), E = inst_end(F); I != E;) {
    auto *CI = dyn_cast<AddrSpaceCastInst>(&*I++);
    if (CI && CI->use_empty())
      CI->eraseFromParent();
  }

  return PreservedAnalyses::none();
}

#endif // INTEL_COLLAB
