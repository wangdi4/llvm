#if INTEL_COLLAB
//===-- VPOParoptLowerSimd.cpp - Lower SIMD Kernel for GenX architecture --===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// VPOParoptLowerSimd.cpp implements the interface to lower VPlan vectorizer
/// generated code and kernel for GenX vector-back.
///
//===----------------------------------------------------------------------===//

#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/GenXIntrinsics/GenXIntrinsics.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/Module.h"
#include "llvm/Pass.h"

#include "llvm/IR/Function.h"
#include "llvm/IR/LegacyPassManager.h"
#include "llvm/IR/PassManager.h"
#include "llvm/InitializePasses.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptLowerSimd.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptTransform.h"
#include "llvm/Transforms/Utils/InferAddressSpacesUtils.h"

using namespace llvm;

#define DEBUG_TYPE "vpo-paropt-lower-simd"

#define SLM_BTI 254

class VPOParoptLowerSimd : public FunctionPass {
public:
  static char ID; // Pass identification, replacement for typeid
  VPOParoptLowerSimd() : FunctionPass(ID) {
    initializeVPOParoptLowerSimdPass(*PassRegistry::getPassRegistry());
  }

  // run VPOParoptLowerSImd pass on the specified module
  bool runOnFunction(Function &F) override {
    // Infer generic AS
    auto &TTI = getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    InferAddrSpaces(TTI, vpo::ADDRESS_SPACE_GENERIC, F);

    FunctionAnalysisManager FAM;
    auto PA = Impl.run(F, FAM);
    return !PA.areAllPreserved();
  }

  void getAnalysisUsage(AnalysisUsage &AU) const {
   AU.addRequired<TargetTransformInfoWrapperPass>();
  }

private:
  VPOParoptLowerSimdPass Impl;
};

char VPOParoptLowerSimd::ID = 0;
INITIALIZE_PASS_BEGIN(VPOParoptLowerSimd, "VPOParoptLowerSimd",
  "Lower SIMD code generated by VPlan vectorizer for GenX", false, false)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VPOParoptLowerSimd, "VPOParoptLowerSimd",
  "Lower SIMD code generated by VPlan vectorizer for GenX", false, false)
// Public interface to the VPOParoptLowerSimdPass.
FunctionPass *llvm::createVPOParoptLowerSimdPass() {
  return new VPOParoptLowerSimd();
}

// The regexp for Simd intrinsics:
// /^_Z(\d+)__esimd_\w+/
static constexpr char SIMD_INTRIN_PREF0[] = "_Z";
static constexpr char SPIRV_INTRIN_PREF[] = "__spirv_";

static constexpr char GENX_KERNEL_METADATA[] = "genx.kernels";

// Turn a MDNode into llvm::value or its subclass.
// Return nullptr if the underlying value has type mismatch.
template <typename Ty = llvm::Value> Ty *getVal(llvm::Metadata *M) {
  if (auto VM = dyn_cast<llvm::ValueAsMetadata>(M))
    if (auto V = dyn_cast<Ty>(VM->getValue()))
      return V;
  return nullptr;
}

/// Return the MDNode that has the SLM size attribute.
static llvm::MDNode *getSLMSizeMDNode(llvm::Function *F) {
  llvm::NamedMDNode *Nodes =
      F->getParent()->getNamedMetadata(GENX_KERNEL_METADATA);
  assert(Nodes && "invalid genx.kernels metadata");
  for (auto Node : Nodes->operands()) {
    if (Node->getNumOperands() >= 4 && getVal(Node->getOperand(0)) == F)
      return Node;
  }
  // if F is not a kernel, keep looking into its callers
  while (!F->use_empty()) {
    auto CI = cast<CallInst>(F->use_begin()->getUser());
    auto UF = CI->getParent()->getParent();
    if (auto Node = getSLMSizeMDNode(UF))
      return Node;
  }
  return nullptr;
}

static inline llvm::Metadata *getMD(llvm::Value *V) {
  return llvm::ValueAsMetadata::get(V);
}

static void translateSLMInit(CallInst &CI) {
  auto F = CI.getParent()->getParent();

  auto *ArgV = CI.getArgOperand(0);
  if (!isa<ConstantInt>(ArgV)) {
    assert(false && "integral constant expected for slm size");
    return;
  }
  auto NewVal = cast<llvm::ConstantInt>(ArgV)->getZExtValue();
  assert(NewVal != 0 && "zero slm bytes being requested");

  // find the corresponding kernel metadata and set the SLM size.
  if (llvm::MDNode *Node = getSLMSizeMDNode(F)) {
    if (llvm::Value *OldSz = getVal(Node->getOperand(4))) {
      assert(isa<llvm::ConstantInt>(OldSz) && "integer constant expected");
      llvm::Value *NewSz = llvm::ConstantInt::get(OldSz->getType(), NewVal);
      uint64_t OldVal = cast<llvm::ConstantInt>(OldSz)->getZExtValue();
      if (OldVal < NewVal)
        Node->replaceOperandWith(3, getMD(NewSz));
    }
  } else {
    // We check whether this call is inside a kernel function.
    assert(false && "slm_init shall be called by a kernel");
  }
}


// Newly created GenX intrinsic might have different return type than expected.
// This helper function creates cast operation from GenX intrinsic return type
// to currently expected. Returns pointer to created cast instruction if it
// was created, otherwise returns NewI.
static Instruction *addCastInstIfNeeded(Instruction *OldI, Instruction *NewI) {
  Type *NITy = NewI->getType();
  Type *OITy = OldI->getType();
  if (OITy != NITy) {
    auto CastOpcode = CastInst::getCastOpcode(NewI, false, OITy, false);
    NewI = CastInst::Create(CastOpcode, NewI, OITy,
                            NewI->getName() + ".cast.ty", OldI);
  }
  return NewI;
}

static int getIndexForSuffix(StringRef Suff) {
  return llvm::StringSwitch<int>(Suff)
      .Case("x", 0)
      .Case("y", 1)
      .Case("z", 2)
      .Default(-1);
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that returns vector of coordinates.
// Example:
//   %call = call spir_func i64 @_Z23__spirv_WorkgroupSize_xv()
//     =>
//   %call.esimd = tail call <3 x i32> @llvm.genx.local.size.v3i32()
//   %wgsize.x = extractelement <3 x i32> %call.esimd, i32 0
//   %wgsize.x.cast.ty = zext i32 %wgsize.x to i64
static Instruction *generateVectorGenXForSpirv(CallInst &CI, StringRef Suff,
                                               const std::string &IntrinName,
                                               StringRef ValueName) {
  std::string IntrName =
      std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + IntrinName;
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  LLVMContext &Ctx = CI.getModule()->getContext();
  Type *I32Ty = Type::getInt32Ty(Ctx);
  Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
      CI.getModule(), ID, {FixedVectorType::get(I32Ty, 3)});
  Instruction *IntrI =
      IntrinsicInst::Create(NewFDecl, {}, CI.getName() + ".esimd", &CI);
  int ExtractIndex = getIndexForSuffix(Suff);
  assert(ExtractIndex != -1 && "Extract index is invalid.");
  Twine ExtractName = ValueName + Suff;
  Instruction *ExtrI = ExtractElementInst::Create(
      IntrI, ConstantInt::get(I32Ty, ExtractIndex), ExtractName, &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, ExtrI);
  return CastI;
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that has exact mapping.
// Example:
//   %call = call spir_func i64 @_Z21__spirv_WorkgroupId_xv()
//     =>
//   %group.id.x = tail call i32 @llvm.genx.group.id.x()
//   %group.id.x.cast.ty = zext i32 %group.id.x to i64
static Instruction *generateGenXForSpirv(CallInst &CI, StringRef Suff,
                                         const std::string &IntrinName) {
  std::string IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                         IntrinName + Suff.str();
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI.getModule(), ID, {});
  Instruction *IntrI =
      IntrinsicInst::Create(NewFDecl, {}, IntrinName + Suff.str(), &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, IntrI);
  return CastI;
}

// This function translates SPIRV intrinsic into GenX intrinsic.
// TODO: Currently, we do not support mixing SYCL and Simd kernels.
// Later for Simd and SYCL kernels to coexist, we likely need to
// clone call graph that lead from Simd kernel to SPIRV intrinsic and
// translate SPIRV intrinsics to GenX intrinsics only in cloned subgraph.
static void
translateSpirvIntrinsic(CallInst *CI, StringRef SpirvIntrName,
                        SmallVector<Instruction *, 8> &SimdToErases) {
  auto translateSpirvIntr = [&SpirvIntrName, &SimdToErases,
                             CI](StringRef SpvIName, auto TranslateFunc) {
    if (SpirvIntrName.consume_front(SpvIName)) {
      Value *TranslatedV = TranslateFunc(*CI, SpirvIntrName.substr(1, 1));
      CI->replaceAllUsesWith(TranslatedV);
      SimdToErases.push_back(CI);
    }
  };

  translateSpirvIntr("WorkgroupSize", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
  });
  translateSpirvIntr("LocalInvocationId", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
  });
  translateSpirvIntr("WorkgroupId", [](CallInst &CI, StringRef Suff) {
    return generateGenXForSpirv(CI, Suff, "group.id.");
  });
  translateSpirvIntr("GlobalInvocationId", [](CallInst &CI, StringRef Suff) {
    // GlobalId = LocalId + WorkGroupSize * GroupId
    Instruction *LocalIdI =
        generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *GroupIdI = generateGenXForSpirv(CI, Suff, "group.id.");
    Instruction *MulI =
        BinaryOperator::CreateMul(WGSizeI, GroupIdI, "mul", &CI);
    return BinaryOperator::CreateAdd(LocalIdI, MulI, "add", &CI);
  });
  translateSpirvIntr("GlobalSize", [](CallInst &CI, StringRef Suff) {
    // GlobalSize = WorkGroupSize * NumWorkGroups
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *NumWGI = generateVectorGenXForSpirv(
        CI, Suff, "group.count.v3i32", "group_count.");
    return BinaryOperator::CreateMul(WGSizeI, NumWGI, "mul", &CI);
  });
  // TODO: Support GlobalOffset SPIRV intrinsics
  translateSpirvIntr("GlobalOffset", [](CallInst &CI, StringRef Suff) {
    return llvm::Constant::getNullValue(CI.getType());
  });
  translateSpirvIntr("NumWorkgroups", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "group.count.v3i32",
                                      "group_count.");
  });
}

static std::string getMDString(MDNode *N, unsigned I) {
  if (!N)
    return "";

  Metadata *Op = N->getOperand(I);
  if (!Op)
    return "";

  if (MDString *Str = dyn_cast<MDString>(Op)) {
    return Str->getString().str();
  }

  return "";
}

static Value *translateReduceOpIntrinsic(CallInst *CI) {
  assert(isa<llvm::IntrinsicInst>(CI));
  LLVMContext &CTX = CI->getContext();
  // TODO: introduce GenXIRBuilder helper
  auto GetReduceSeq = [&](Value *Src, unsigned Size,
                          unsigned ElemOffset) -> Value * {
    if (Size == 1) {
      if (Src->getType()->isVectorTy()) {
        Value *Index = ConstantInt::get(Type::getInt32Ty(CTX), ElemOffset);
        return ExtractElementInst::Create(Src, Index, ".reduce.single",
                                          CI /*InsertBefore*/);
      }
      assert(ElemOffset == 0);
      return Src;
    }

    Type *SrcType = Src->getType();
    Type *SrcElementType = cast<FixedVectorType>(SrcType)->getElementType();

    Type *Tys[] = {FixedVectorType::get(SrcElementType, Size), SrcType,
                   Type::getInt16Ty(CTX)};
    auto *Decl = GenXIntrinsic::getGenXDeclaration(
        CI->getModule(), GenXIntrinsic::genx_rdregioni, Tys);

    Value *Args[] = {
        Src,
        ConstantInt::get(Type::getInt32Ty(CTX), 0),    // VStride
        ConstantInt::get(Type::getInt32Ty(CTX), Size), // Width
        ConstantInt::get(Type::getInt32Ty(CTX), 1u),   // Stride
        ConstantInt::get(Type::getInt16Ty(CTX),
                         ElemOffset *
                             (SrcElementType->getPrimitiveSizeInBits() /
                              8)),                  // Offset in bytes
        ConstantInt::get(Type::getInt32Ty(CTX), 0), // Parent width (ignored)
    };

    return CallInst::Create(Decl, Args, Src->getName() + ".reduce.seq",
                            CI /*InsertBefore*/);
  };

  Instruction::BinaryOps ReduceOp;
  switch (CI->getIntrinsicID()) {
  default:
    llvm_unreachable("unimplemented intrinsic");
  case Intrinsic::vector_reduce_add:
    ReduceOp = Instruction::Add;
    break;
  case Intrinsic::vector_reduce_mul:
    ReduceOp = Instruction::Mul;
    break;
  case Intrinsic::vector_reduce_xor:
    ReduceOp = Instruction::Xor;
    break;
  case Intrinsic::vector_reduce_or:
    ReduceOp = Instruction::Or;
    break;
  }

  Value *OperandToReduce = CI->getOperand(0);
  unsigned N =
      cast<FixedVectorType>(OperandToReduce->getType())->getNumElements();
  // Split vector into two parts. First part's size  is alwayas a power of two
  // so we can simply apply reduction by spliiting it into parts. Reduction with
  // a second part is applicable it's size equals to the first part size
  unsigned N1, N2;
  if (llvm::isPowerOf2_32(N)) {
    N1 = N;
    N2 = 0;
  } else {
    N1 = 1u << llvm::Log2_32(N);
    N2 = N - N1;
  }

  Value *CurrentReduceRes = OperandToReduce;
  IRBuilder<> Builder(CI);
  for (unsigned i = N1 / 2; i >= 1; i /= 2) {
    auto *RSeq1 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq2 = GetReduceSeq(CurrentReduceRes, i, i);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq1, RSeq2);

    unsigned CurrentReduceSize;
    if (auto *VT = dyn_cast<FixedVectorType>(CurrentReduceRes->getType()))
      CurrentReduceSize = VT->getNumElements();
    else
      CurrentReduceSize = 1;
    // Continue reducing if second part has suitable size, fall back
    // to reducing first part otherwise
    if (CurrentReduceSize > N2)
      continue;

    // Reduction across parts
    auto *RSeq21 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq22 = GetReduceSeq(OperandToReduce, i, N - N2);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq21, RSeq22);
    N2 -= i;
  }
  assert(CurrentReduceRes->getType() == CI->getType());
  CI->replaceAllUsesWith(CurrentReduceRes);
  return CurrentReduceRes;
}

#define SYCL_GLOBAL_AS 1
static Value *translateLLVMInst(Instruction *Inst) {
  LLVMContext &CTX = Inst->getContext();
  IRBuilder<> Builder(Inst);
  if (auto CastOp = dyn_cast<llvm::CastInst>(Inst)) {
    llvm::Type *DstTy = CastOp->getDestTy();
    auto CastOpcode = CastOp->getOpcode();
    if ((CastOpcode == llvm::Instruction::FPToUI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() <= 32) ||
        (CastOpcode == llvm::Instruction::FPToSI &&
         DstTy->getScalarType()->getPrimitiveSizeInBits() < 32)) {
      llvm::Value *Src = CastOp->getOperand(0);
      auto TmpTy = llvm::FixedVectorType::get(
          llvm::Type::getInt32Ty(CTX),
          cast<FixedVectorType>(DstTy)->getNumElements());
      Src = Builder.CreateFPToSI(Src, TmpTy);

      llvm::Instruction::CastOps TruncOp = llvm::Instruction::Trunc;
      auto *NewDst = Builder.CreateCast(TruncOp, Src, DstTy);
      CastOp->replaceAllUsesWith(NewDst);
      return NewDst;
    }
    return CastOp;
  }
  if (auto LoadOp = dyn_cast<llvm::LoadInst>(Inst)) {
    auto AS = LoadOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS) {
      auto PtrV = LoadOp->getPointerOperand();
      auto DTy = LoadOp->getType();
      if (!DTy->isSingleValueType()) {
        Twine Msg("unable to lower LoadInst with non-single-value type");
        llvm::report_fatal_error(Msg, false /*no crash diag*/);
      }
      Value *RepI = nullptr;
      if (DTy->isVectorTy()) {
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTy, PtrV->getType()});
        RepI = IntrinsicInst::Create(NewFDecl, {PtrV}, "svm.block.ld.unaligned",
                                     LoadOp);
      } else {
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
        int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
        int RetLen = 1;
        auto EltTy = DTy;
        if (EltBytes == 2) {
          NumBlks = 1;
          EltTy = Type::getInt8Ty(CTX);
          RetLen = 4; // return 4xi8
        } else if (EltBytes == 1) {
          EltTy = Type::getInt8Ty(CTX);
          RetLen = 4; // return 4xi8
        } else
          assert(EltBytes == 4 || EltBytes == 8);
        // create vector type
        auto VDTy = llvm::FixedVectorType::get(EltTy, RetLen);
        auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
        // create constant for predicate
        auto PredV1Ty =
            llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
        auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
        // crease constant for num-blocks
        auto CIntTy = Type::getInt32Ty(CTX);
        auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
        auto ZeroC = ConstantInt::get(CIntTy, 0);
        // create the intrinsic call
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {VDTy, PredV1Ty, VPtrTy});
        auto VecPtr = CastInst::CreateBitOrPointerCast(PtrV, VPtrTy,
                                                       PtrV->getName(), LoadOp);
        Instruction *IntrI = IntrinsicInst::Create(
            NewFDecl, {OnePred1, NumBlksC, VecPtr, UndefValue::get(VDTy)},
            "svm.gather", LoadOp);
        if (EltBytes == 2) {
          // cast 4xi8 to 2xi16
          auto I16x2Ty =
              llvm::FixedVectorType::get(IntegerType::getInt16Ty(CTX), 2);
          IntrI = CastInst::CreateBitOrPointerCast(
              IntrI, I16x2Ty, IntrI->getName() + ".cast", LoadOp);
        }
        RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
      }
      LoadOp->replaceAllUsesWith(RepI);
      return RepI;
    }
    return LoadOp;
  }
  if (auto StoreOp = dyn_cast<llvm::StoreInst>(Inst)) {
    auto AS = StoreOp->getPointerAddressSpace();
    if (AS == SYCL_GLOBAL_AS) {
      auto PtrV = StoreOp->getPointerOperand();
      auto DTV = StoreOp->getValueOperand();
      auto DTy = DTV->getType();
      if (!DTy->isSingleValueType()) {
        Twine Msg("unable to lower LoadInst with non-single-value type");
        llvm::report_fatal_error(Msg, false /*no crash diag*/);
      }
      Value *RepI = nullptr;
      if (DTy->isVectorTy()) {
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.st";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {PtrV->getType(), DTy});
        RepI = IntrinsicInst::Create(
             NewFDecl, {PtrV, DTV},
             NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st", StoreOp);
      } else {
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.scatter";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        auto EltBytes = DTy->getPrimitiveSizeInBits() / 8;
        int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
        int DataLen = 1;
        auto EltTy = DTy;
        if (EltBytes == 2) {
          NumBlks = 1;
          EltTy = Type::getInt8Ty(CTX);
          DataLen = 4; // return 4xi8
        } else if (EltBytes == 1) {
          EltTy = Type::getInt8Ty(CTX);
          DataLen = 4; // return 4xi8
        } else
          assert(EltBytes == 4 || EltBytes == 8);
        // create vector type
        auto VDTy = llvm::FixedVectorType::get(EltTy, DataLen);
        auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
        // create constant for predicate
        auto PredV1Ty =
            llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
        auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
        // crease constant for num-blocks
        auto CIntTy = Type::getInt32Ty(CTX);
        auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
        auto ZeroC = ConstantInt::get(CIntTy, 0);
        // create the intrinsic call
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {PredV1Ty, VPtrTy, VDTy});
        auto VecPtr = CastInst::CreateBitOrPointerCast(
            PtrV, VPtrTy, PtrV->getName(), StoreOp);
        Value *VecDTV = nullptr;
        if (EltBytes == 2) {
          // cast 2xi16 to 4xi8
          auto I16x2Ty = llvm::FixedVectorType::get(DTy, 2);
          VecDTV = Builder.CreateInsertElement(UndefValue::get(I16x2Ty), DTV,
                                               ZeroC, DTV->getName());
          VecDTV = CastInst::CreateBitOrPointerCast(
              VecDTV, VDTy, DTV->getName() + ".cast", StoreOp);
        } else
          VecDTV = Builder.CreateInsertElement(UndefValue::get(VDTy), DTV,
                                               ZeroC, DTV->getName());
        RepI =
            Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, VecPtr, VecDTV});
      }
      return RepI;
    }
    return StoreOp;
  }
  if (auto CallOp = dyn_cast<llvm::IntrinsicInst>(Inst)) {
    // handle memory intrinsics
    // masked.gather, masked.scatter etc
    auto ID = CallOp->getIntrinsicID();
    switch (ID) {
    case Intrinsic::masked_load: {
      auto PtrV = CallOp->getArgOperand(0);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      if (AS == 0) { // thread private memory
                     // convert to load then select
        auto VecDT =
            Builder.CreateLoad(CallOp->getType(), PtrV, CallOp->getName());
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        auto DTy = CallOp->getType();
        // convert to unaligned-block-load then select
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTy, PtrV->getType()});
        auto VecDT = IntrinsicInst::Create(NewFDecl, {PtrV},
                                           "svm.block.ld.unaligned", CallOp);
        auto RepI =
            Builder.CreateSelect(CallOp->getArgOperand(2), VecDT,
                                 CallOp->getArgOperand(3), CallOp->getName());
        return RepI;
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_store: {
      auto PtrV = CallOp->getArgOperand(1);
      assert(PtrV->getType()->isPointerTy());
      auto AS = cast<PointerType>(PtrV->getType())->getAddressSpace();
      auto DTV = CallOp->getArgOperand(0);
      if (AS == 0) { // thread private memory
                     // convert to load then select then store
        auto VecDT = Builder.CreateLoad(DTV->getType(), PtrV);
        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);
        auto RepI = Builder.CreateStore(SelI, PtrV);
        return RepI;
      } else if (AS == SYCL_GLOBAL_AS) {
        // convert to unaligned-block-load then select
        // then block-store
        std::string IntrName =
            std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
            "svm.block.ld.unaligned";
        auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {DTV->getType(), PtrV->getType()});
        auto VecDT = IntrinsicInst::Create(NewFDecl, {PtrV},
                                           "svm.block.ld.unaligned", CallOp);

        auto SelI = Builder.CreateSelect(CallOp->getArgOperand(3), DTV, VecDT);

        IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                   "svm.block.st";
        ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
        NewFDecl = GenXIntrinsic::getGenXDeclaration(
            Inst->getModule(), ID, {PtrV->getType(), DTV->getType()});
        auto RepI = IntrinsicInst::Create(
              NewFDecl, {PtrV, SelI},
              NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st", CallOp);
        return RepI;
      } else
        return CallOp;
    } break;
    case Intrinsic::masked_gather: {
      auto PtrV = CallOp->getArgOperand(0);
      auto MaskV = CallOp->getArgOperand(2);
      auto OldV = CallOp->getArgOperand(3);
      auto DTy = CallOp->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<VectorType>(DTy)->getElementType();
      auto NumElts = cast<VectorType>(DTy)->getNumElements();
      auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
      int EltsPerLane = 1;
      // for short or byte type, load-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        if (isa<UndefValue>(OldV))
          OldV = UndefValue::get(VDTy);
        else {
          // zext to i32 type
          auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
          auto ExtI = Builder.CreateZExt(OldV, VInt32, OldV->getName());
          // bitcast to 4xi8
          OldV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
        }
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {VDTy, MaskV->getType(), PtrV->getType()});
      RepI = IntrinsicInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, OldV},
                                   CallOp->getName(), CallOp);
      if (EltBytes < 4) {
        // bitcast to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto CastI = Builder.CreateBitCast(RepI, VInt32, RepI->getName());
        // trunc to the dst type
        RepI = Builder.CreateTrunc(CastI, DTy, CastI->getName());
      }
      CallOp->replaceAllUsesWith(RepI);
      return RepI;
    } break;
    case Intrinsic::masked_scatter: {
      auto PtrV = CallOp->getArgOperand(0);
      auto MaskV = CallOp->getArgOperand(2);
      auto DataV = CallOp->getArgOperand(3);
      auto DTy = DataV->getType();
      assert(PtrV->getType()->isVectorTy() && DTy->isVectorTy());
      auto PtrETy = cast<VectorType>(PtrV->getType())->getElementType();
      assert(PtrETy->isPointerTy());
      auto AS = cast<PointerType>(PtrETy)->getAddressSpace();
      if (AS != SYCL_GLOBAL_AS)
        return CallOp;
      auto EltTy = cast<VectorType>(DTy)->getElementType();
      auto NumElts = cast<VectorType>(DTy)->getNumElements();
      auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
      int EltsPerLane = 1;
      // for short or byte type, store-data is 4 bytes per lane
      if (EltBytes < 4) {
        EltTy = Type::getInt8Ty(CTX);
        EltsPerLane = 4;
      }
      auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts * EltsPerLane);
      auto CIntTy = Type::getInt32Ty(CTX);
      int nb = (EltBytes == 2) ? 1 : 0; // 0/1/2/3 means 1/2/4/8 blks
      auto NumBlksC = ConstantInt::get(CIntTy, nb);
      Value *RepI = nullptr;
      // need write-region for old-data in byte or short type
      if (EltBytes < 4) {
        // zext to i32 type
        auto VInt32 = llvm::FixedVectorType::get(CIntTy, NumElts);
        auto ExtI = Builder.CreateZExt(DataV, VInt32, DataV->getName());
        // bitcast to 4xi8
        DataV = Builder.CreateBitCast(ExtI, VDTy, ExtI->getName());
      }
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          Inst->getModule(), ID, {MaskV->getType(), PtrV->getType(), VDTy});
      RepI = IntrinsicInst::Create(NewFDecl, {MaskV, NumBlksC, PtrV, DataV},
                                   CallOp->getName(), CallOp);
      return RepI;
    } break;
    case Intrinsic::vector_reduce_add:
    case Intrinsic::vector_reduce_mul:
    case Intrinsic::vector_reduce_xor:
    case Intrinsic::vector_reduce_or:
      return translateReduceOpIntrinsic(CallOp);
    default:
      break;
    }
    return CallOp;
  }
  return Inst;
}

PreservedAnalyses VPOParoptLowerSimdPass::run(Function &F,
                                              FunctionAnalysisManager &FAM) {
  // Only consider functions marked with !sycl_explicit_simd
  bool isOmpSpirKernel = false;

  if ((F.getMetadata("omp_simd_kernel") != nullptr) ||
      (F.getMetadata("sycl_explicit_simd") == nullptr &&
       F.getCallingConv() == CallingConv::SPIR_KERNEL)) {
    IRBuilder<> Builder(F.getEntryBlock().getFirstNonPHI());
    int simdWidth = 1;
    Metadata *AttrMDArgs[] = {
        ConstantAsMetadata::get(Builder.getInt32(simdWidth))};
    F.setMetadata("intel_reqd_sub_group_size",
                  MDNode::get(F.getContext(), AttrMDArgs));
    isOmpSpirKernel = true;
  } else
    return PreservedAnalyses::all();

  // add module-level meta-data for kernel functions
  if (isOmpSpirKernel) {
    auto M = F.getParent();
    auto MetaKernels = M->getOrInsertNamedMetadata(GENX_KERNEL_METADATA);

    LLVMContext &Ctx = M->getContext();
    Type *I32Ty = Type::getInt32Ty(Ctx);

    enum { AK_NORMAL, AK_SAMPLER, AK_SURFACE, AK_VME };
    enum { IK_NORMAL, IK_INPUT, IK_OUTPUT, IK_INPUT_OUTPUT };
    // Metadata node containing N i32s, where N is the number of kernel
    // arguments, and each i32 is the kind of argument,  one of:
    //     0 = general, 1 = sampler, 2 = surface, 3 = vme
    // (the same values as in the "kind" field of an "input_info"
    // record in a vISA kernel.
    SmallVector<Metadata *, 8> ArgKinds;
    // Optional, not supported for compute
    SmallVector<Metadata *, 8> ArgInOutKinds;
    // Metadata node describing N strings where N is the number of kernel
    // arguments, each string describing argument type in OpenCL.
    // required for running on top of OpenCL runtime.
    SmallVector<Metadata *, 8> ArgTypeDescs;
    auto *KernelArgTypes = F.getMetadata("kernel_arg_type");
    unsigned Idx = 0;

    // Iterate argument list to gather argument kinds and generate argument
    // descriptors.
    for (const Argument &Arg : F.args()) {
      int Kind = AK_NORMAL;
      int IKind = IK_NORMAL;

      auto ArgType = getMDString(KernelArgTypes, Idx);

      if (ArgType.find("image1d_t") != std::string::npos ||
          ArgType.find("image2d_t") != std::string::npos ||
          ArgType.find("image3d_t") != std::string::npos ||
          ArgType.find("image1d_buffer_t") != std::string::npos) {
        Kind = AK_SURFACE;
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgType));
      } else {
        StringRef ArgDesc = "";
        if (Arg.getType()->isPointerTy())
          ArgDesc = "svmptr_t";
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgDesc));
      }

      ArgKinds.push_back(ValueAsMetadata::get(ConstantInt::get(I32Ty, Kind)));
      ArgInOutKinds.push_back(
          ValueAsMetadata::get(ConstantInt::get(I32Ty, IKind)));
      Idx++;
    }
    MDNode *Kinds = MDNode::get(Ctx, ArgKinds);
    MDNode *IOKinds = MDNode::get(Ctx, ArgInOutKinds);
    MDNode *ArgDescs = MDNode::get(Ctx, ArgTypeDescs);

    Metadata *MDArgs[] = {ValueAsMetadata::get(&F),
                          MDString::get(Ctx, F.getName().str()),
                          Kinds,
                          ValueAsMetadata::get(llvm::ConstantInt::getNullValue(
                              I32Ty)), // SLM size in bytes
                          ValueAsMetadata::get(llvm::ConstantInt::getNullValue(
                              I32Ty)), // arg offsets
                          IOKinds,
                          ArgDescs};

    // Add this kernel to the root.
    MetaKernels->addOperand(MDNode::get(Ctx, MDArgs));
    F.addFnAttr("oclrt", "1");
    F.addFnAttr("CMGenxMain");
  }

  SmallVector<Instruction *, 8> SimdToErases;

  for (Instruction &I : instructions(F)) {
    auto replace = translateLLVMInst(&I);
    if (replace != &I) {
      SimdToErases.push_back(&I);
      continue;
    }

    auto *CI = dyn_cast<CallInst>(&I);
    Function *Callee = nullptr;
    if (!CI || !(Callee = CI->getCalledFunction()))
      continue;
    StringRef Name = Callee->getName();

    if (!Name.consume_front(SIMD_INTRIN_PREF0))
      continue;
    // now skip the digits
    Name = Name.drop_while([](char C) { return std::isdigit(C); });

    if (Name.consume_front(SPIRV_INTRIN_PREF)) {
      translateSpirvIntrinsic(CI, Name, SimdToErases);
      // For now: if no match, just let it go untranslated.
      continue;
    }
  }
  for (auto *CI : SimdToErases) {
    CI->eraseFromParent();
  }

  return PreservedAnalyses::none();
}

#endif // INTEL_COLLAB
