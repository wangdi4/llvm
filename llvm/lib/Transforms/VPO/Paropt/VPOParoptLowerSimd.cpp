#if INTEL_COLLAB
//
// INTEL CONFIDENTIAL
//
// Modifications, Copyright (C) 2021 Intel Corporation
//
// This software and the related documents are Intel copyrighted materials, and
// your use of them is governed by the express license under which they were
// provided to you ("License"). Unless the License provides otherwise, you may
// not use, modify, copy, publish, distribute, disclose or transmit this
// software or the related documents without Intel's prior written permission.
//
// This software and the related documents are provided as is, with no express
// or implied warranties, other than those that are expressly stated in the
// License.
//
//===-- VPOParoptLowerSimd.cpp - Lower SIMD Kernel for GenX architecture --===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// VPOParoptLowerSimd.cpp implements the interface to lower VPlan vectorizer
/// generated code and kernel for GenX vector-back.
///
//===----------------------------------------------------------------------===//

#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/GenXIntrinsics/GenXIntrinsics.h"
#include "llvm/IR/GetElementPtrTypeIterator.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/Module.h"
#include "llvm/IR/Operator.h"
#include "llvm/Pass.h"

#include "llvm/IR/Function.h"
#include "llvm/IR/LegacyPassManager.h"
#include "llvm/IR/PassManager.h"
#include "llvm/InitializePasses.h"
#include "llvm/Transforms/Scalar/InferAddressSpaces.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptLowerSimd.h"
#include "llvm/Transforms/VPO/Paropt/VPOParoptTransform.h"

using namespace llvm;

#define DEBUG_TYPE "vpo-paropt-lower-simd"

#define SLM_BTI 254

class VPOParoptLowerSimd : public FunctionPass {
public:
  static char ID; // Pass identification, replacement for typeid
  VPOParoptLowerSimd() : FunctionPass(ID) {
    initializeVPOParoptLowerSimdPass(*PassRegistry::getPassRegistry());
  }

  // run VPOParoptLowerSImd pass on the specified module
  bool runOnFunction(Function &F) override {
    // Infer generic AS
    auto *AC = &getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F);
    auto *DTWP = getAnalysisIfAvailable<DominatorTreeWrapperPass>();
    DominatorTree *DT = DTWP ? &DTWP->getDomTree() : nullptr;
    auto *TTI = &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    InferAddrSpaces(AC, DT, TTI, vpo::ADDRESS_SPACE_GENERIC, F);

    FunctionAnalysisManager FAM;
    auto PA = Impl.run(F, FAM);
    return !PA.areAllPreserved();
  }

  void getAnalysisUsage(AnalysisUsage &AU) const override {
    AU.addRequired<AssumptionCacheTracker>();
    AU.addRequired<TargetTransformInfoWrapperPass>();
  }

private:
  VPOParoptLowerSimdPass Impl;
};

char VPOParoptLowerSimd::ID = 0;
INITIALIZE_PASS_BEGIN(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                      "Lower SIMD code generated by VPlan vectorizer for GenX",
                      false, false)
INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VPOParoptLowerSimd, "VPOParoptLowerSimd",
                    "Lower SIMD code generated by VPlan vectorizer for GenX",
                    false, false)
// Public interface to the VPOParoptLowerSimdPass.
FunctionPass *llvm::createVPOParoptLowerSimdPass() {
  return new VPOParoptLowerSimd();
}

// The regexp for Simd intrinsics:
// /^_Z(\d+)__esimd_\w+/
static constexpr char SIMD_INTRIN_PREF0[] = "_Z";
static constexpr char SPIRV_INTRIN_PREF[] = "__spirv_";

static constexpr char GENX_KERNEL_METADATA[] = "genx.kernels";
static constexpr char GENX_SLM_OFFSET[] = "genx.slm.offset";

static constexpr char BUILTIN_IB_PREF[] = "__builtin_IB_";

#define SYCL_GLOBAL_AS 1
#define SYCL_SLM_AS 3
#define SYCL_GENERIC_AS 4

static uint32_t getElementSizeInBytes(Type *ETy, DataLayout &DL) {
  uint32_t ebytes = (unsigned int)ETy->getPrimitiveSizeInBits() / 8;
  if (ETy->isPointerTy())
    ebytes = DL.getPointerTypeSize(ETy);
  return ebytes;
}

// Newly created GenX intrinsic might have different return type than expected.
// This helper function creates cast operation from GenX intrinsic return type
// to currently expected. Returns pointer to created cast instruction if it
// was created, otherwise returns NewI.
static Instruction *addCastInstIfNeeded(Instruction *OldI, Instruction *NewI) {
  Type *NITy = NewI->getType();
  Type *OITy = OldI->getType();
  if (OITy != NITy) {
    auto CastOpcode = CastInst::getCastOpcode(NewI, false, OITy, false);
    NewI = CastInst::Create(CastOpcode, NewI, OITy,
                            NewI->getName() + ".cast.ty", OldI);
  }
  return NewI;
}

static int getIndexForSuffix(StringRef Suff) {
  return llvm::StringSwitch<int>(Suff)
      .Case("x", 0)
      .Case("y", 1)
      .Case("z", 2)
      .Default(-1);
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that returns vector of coordinates.
// Example:
//   %call = call spir_func i64 @_Z23__spirv_WorkgroupSize_xv()
//     =>
//   %call.esimd = tail call <3 x i32> @llvm.genx.local.size.v3i32()
//   %wgsize.x = extractelement <3 x i32> %call.esimd, i32 0
//   %wgsize.x.cast.ty = zext i32 %wgsize.x to i64
static Instruction *generateVectorGenXForSpirv(CallInst &CI, StringRef Suff,
                                               const std::string &IntrinName,
                                               StringRef ValueName) {
  std::string IntrName =
      std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + IntrinName;
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  LLVMContext &Ctx = CI.getModule()->getContext();
  Type *I32Ty = Type::getInt32Ty(Ctx);
  Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
      CI.getModule(), ID, {FixedVectorType::get(I32Ty, 3)});
  Instruction *IntrI =
      CallInst::Create(NewFDecl, {}, CI.getName() + ".esimd", &CI);
  int ExtractIndex = getIndexForSuffix(Suff);
  assert(ExtractIndex != -1 && "Extract index is invalid.");
  Twine ExtractName = ValueName + Suff;
  Instruction *ExtrI = ExtractElementInst::Create(
      IntrI, ConstantInt::get(I32Ty, ExtractIndex), ExtractName, &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, ExtrI);
  return CastI;
}

// Helper function to convert SPIRV intrinsic into GenX intrinsic,
// that has exact mapping.
// Example:
//   %call = call spir_func i64 @_Z21__spirv_WorkgroupId_xv()
//     =>
//   %group.id.x = tail call i32 @llvm.genx.group.id.x()
//   %group.id.x.cast.ty = zext i32 %group.id.x to i64
static Instruction *generateGenXForSpirv(CallInst &CI, StringRef Suff,
                                         const std::string &IntrinName) {
  std::string IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                         IntrinName + Suff.str();
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI.getModule(), ID, {});
  Instruction *IntrI =
      CallInst::Create(NewFDecl, {}, IntrinName + Suff.str(), &CI);
  Instruction *CastI = addCastInstIfNeeded(&CI, IntrI);
  return CastI;
}

static void translateBarrier(CallInst *CI) {
  auto ScopeV = CI->getArgOperand(0);
  assert(isa<ConstantInt>(ScopeV) &&
         "Barrier execution scope should be a constant");
  ConstantInt *ScopeC = cast<ConstantInt>(ScopeV);
  // if work-group scope
  //    generate a slm-fence then work-group-barrier
  // else if subgroup scope
  //    generate a slm-fence
  auto M = CI->getModule();
  if (ScopeC->equalsInt(2) || ScopeC->equalsInt(3)) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "fence";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(M, ID, {});
    // ESIMD_GLOBAL_COHERENT_FENCE | ESIMD_LOCAL_BARRIER
    // 0x01 | 0x20
    Type *I8Ty = Type::getInt8Ty(M->getContext());
    auto CtrlV = ConstantInt::get(I8Ty, 0x21);
    CallInst::Create(NewFDecl, {CtrlV}, "", CI);
  } else
    assert(false && "Unsupported barrier execution scope");

  if (ScopeC->equalsInt(2)) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "barrier";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(M, ID, {});
    CallInst::Create(NewFDecl, {}, "", CI);
  }
}

// TODO: Currently, we do not support mixing SYCL and Simd kernels.
// Later for Simd and SYCL kernels to coexist, we likely need to
// clone call graph that lead from Simd kernel to SPIRV intrinsic and
// translate SPIRV intrinsics to GenX intrinsics only in cloned subgraph.
static void
translateSpirvIntrinsic(CallInst *CI, StringRef SpirvIntrName,
                        SmallVector<Instruction *, 8> &SimdToErases) {

  if (SpirvIntrName.startswith("ControlBarrieriii")) {
    translateBarrier(CI);
    SimdToErases.push_back(CI);
    return;
  }

  auto translateSpirvIntr = [&SpirvIntrName, &SimdToErases,
                             CI](StringRef SpvIName, auto TranslateFunc) {
    if (SpirvIntrName.consume_front(SpvIName)) {
      Value *TranslatedV = TranslateFunc(*CI, SpirvIntrName.substr(1, 1));
      CI->replaceAllUsesWith(TranslatedV);
      SimdToErases.push_back(CI);
    }
  };

  translateSpirvIntr("WorkgroupSize", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
  });
  translateSpirvIntr("LocalInvocationId", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
  });
  translateSpirvIntr("WorkgroupId", [](CallInst &CI, StringRef Suff) {
    return generateGenXForSpirv(CI, Suff, "group.id.");
  });
  translateSpirvIntr("GlobalInvocationId", [](CallInst &CI, StringRef Suff) {
    // GlobalId = LocalId + WorkGroupSize * GroupId
    Instruction *LocalIdI =
        generateVectorGenXForSpirv(CI, Suff, "local.id.v3i32", "local_id.");
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *GroupIdI = generateGenXForSpirv(CI, Suff, "group.id.");
    Instruction *MulI =
        BinaryOperator::CreateMul(WGSizeI, GroupIdI, "mul", &CI);
    return BinaryOperator::CreateAdd(LocalIdI, MulI, "add", &CI);
  });
  translateSpirvIntr("GlobalSize", [](CallInst &CI, StringRef Suff) {
    // GlobalSize = WorkGroupSize * NumWorkGroups
    Instruction *WGSizeI =
        generateVectorGenXForSpirv(CI, Suff, "local.size.v3i32", "wgsize.");
    Instruction *NumWGI = generateVectorGenXForSpirv(
        CI, Suff, "group.count.v3i32", "group_count.");
    return BinaryOperator::CreateMul(WGSizeI, NumWGI, "mul", &CI);
  });
  // TODO: Support GlobalOffset SPIRV intrinsics
  translateSpirvIntr("GlobalOffset", [](CallInst &CI, StringRef Suff) {
    return llvm::Constant::getNullValue(CI.getType());
  });
  translateSpirvIntr("NumWorkgroups", [](CallInst &CI, StringRef Suff) {
    return generateVectorGenXForSpirv(CI, Suff, "group.count.v3i32",
                                      "group_count.");
  });
}

// LSC_LDCC_DEFAULT      = 0,
// LSC_LDCC_L1UC_L3UC    = 1,   // L1 uncached and L3 uncached
// LSC_LDCC_L1UC_L3C     = 2,   // L1 uncached and L3 cached
// LSC_LDCC_L1C_L3UC     = 3,   // L1 cached and L3 uncached
// LSC_LDCC_L1C_L3C      = 4,   // L1 cached and L3 cached
// LSC_LDCC_L1S_L3UC     = 5,   // L1 streaming load and L3 uncached
// LSC_LDCC_L1S_L3C      = 6,   // L1 streaming load and L3 cached
// LSC_LDCC_L1IAR_L3C    = 7,   // L1 invalidate-after-read and L3 cached
//
// ### Cache mappings are:
//   - 0 -> .df (default)
//   - 1 -> .uc (uncached)
//   - 2 -> .ca (cached)
//   - 3 -> .wb (writeback)
//   - 4 -> .wt (writethrough)
//   - 5 -> .st (streaming)
//   - 6 -> .ri (read-invalidate)
static void mapCacheControl(unsigned in, unsigned char &outL1,
                            unsigned char &outL3) {
  switch (in) {
  case 0:
    outL1 = 0;
    outL3 = 0;
    break;
  case 1:
    outL1 = 1;
    outL3 = 1;
    break;
  case 2:
    outL1 = 1;
    outL3 = 2;
    break;
  case 3:
    outL1 = 2;
    outL3 = 1;
    break;
  case 4:
    outL1 = 2;
    outL3 = 2;
    break;
  case 5:
    outL1 = 5;
    outL3 = 1;
    break;
  case 6:
    outL1 = 5;
    outL3 = 2;
    break;
  case 7:
    outL1 = 6;
    outL3 = 2;
    break;
  default:
    assert(false);
  }
}

// Check if a prefetch builtin call is equivalent to the previous
static bool isRedundantPrefetch(CallInst *CI) {
  auto PI = CI->getPrevNonDebugInstruction();
  if (!PI)
    return false;
  auto PCI = dyn_cast<CallInst>(PI);
  if (!PCI)
    return false;
  // check if PI and CI call the same function
  if (PCI->getCalledFunction() != CI->getCalledFunction())
    return false;
  if (PCI->getNumOperands() != CI->getNumOperands())
    return false;
  // check if all operands are the same
  for (unsigned i = 0, n = PCI->getNumOperands(); i < n; i++) {
    if (PCI->getOperand(i) != CI->getOperand(i))
      return false;
  }
  return true;
}

// This function translates IGC builtin functions into GenX intrinsic.
// only prefetch for now.
static void
translateBuiltinIBIntrinsic(CallInst *CI, StringRef BuiltinIBIntrName,
                            SmallVector<Instruction *, 8> &SimdToErases) {
  if (!BuiltinIBIntrName.consume_front("lsc_prefetch_global"))
    return;
  // instruction is the same as the previous instruction
  if (isRedundantPrefetch(CI)) {
    SimdToErases.push_back(CI);
    return;
  }
  // count the number of redundant prefetch that follows
  unsigned count = 1;
  auto NI = dyn_cast<CallInst>(CI->getNextNonDebugInstruction());
  while (NI && isRedundantPrefetch(NI)) {
    count++;
    NI = dyn_cast<CallInst>(NI->getNextNonDebugInstruction());
  }
  // translate the prefetch
  std::string IntrName = std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
                         "lsc.prefetch.stateless";
  auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
  // builtin function has 3 operands, ptr, immed-offset, and cache-control
  assert(CI->getNumOperands() >= 3);
  LLVMContext &CTX = CI->getContext();
  IRBuilder<> Builder(CI);
  auto PtrV = CI->getOperand(0);
  assert(PtrV->getType()->isPointerTy());
  auto DL = CI->getModule()->getDataLayout();
  auto ImmedOffsetV = CI->getOperand(1);
  auto CacheCtrlV = CI->getOperand(2);
  assert(isa<ConstantInt>(ImmedOffsetV) && isa<ConstantInt>(CacheCtrlV));

   // arg0: {1,32}Xi1 predicate (overloaded)
   // arg1: i8 Subopcode, [MBZ]
   // arg2: i8 Caching behavior for L1, [MBC]
   // arg3: i8 Caching behavior for L3, [MBC]
   // arg4: i16 Address scale, [MBC]
   // arg5: i32 Immediate offset added to each address, [MBC]
   // arg6: i8 The dataum size, [MBC]
   // arg7: i8 Number of elements to load per address (vector size), [MBC]
   // arg8: i8 Indicates if the data is transposed during the transfer, [MBC]
   // arg9: i8 Channel mask for quad versions, [MBC]
   // arg10: {1,32}Xi{16,32,64} The vector register holding offsets (overloaded)
   //       for flat version Base Address + Offset[i] goes here
   // arg11: i32 surface to use for this operation.
   //       This can be an immediate or a register
   //         for flat and bindless version pass zero here

   //  Dataum size mapping is
   //   - 1 = :u8
   //   - 2 = :u16
   //   - 3 = :u32
   //   - 4 = :u64
   //   - 5 = :u8u32 (load 8b, zero extend to 32b; store the opposite),
   //   - 6 = :u16u32 (load 8b, zero extend to 32b; store the opposite),
   //   - 7 = :u16u32h (load 16b into high 16 of each 32b; store the high 16)

  auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
  auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
  // zero values
  auto CIntTy = Type::getInt32Ty(CTX);
  auto Zeroi32C = ConstantInt::get(CIntTy, 0);
  auto CI8Ty = Type::getInt8Ty(CTX);
  auto Zeroi8C = ConstantInt::get(CI8Ty, 0);
  auto Twoi8C = ConstantInt::get(CI8Ty, 2);
  auto CI16Ty = Type::getInt16Ty(CTX);
  auto Onei16C = ConstantInt::get(CI16Ty, 1);
  //
  unsigned EnumNumElems = 0;
  if (count <= 4)
    EnumNumElems = count;
  else if (count <= 8)
    EnumNumElems = 5;
  else if (count <= 16)
    EnumNumElems = 6;
  else if (count <= 32)
    EnumNumElems = 7;
  else if (count <= 64)
    EnumNumElems = 8;
  else
    assert(false);
  auto NumElemsV = ConstantInt::get(CI8Ty, EnumNumElems);
  //
  unsigned DatumSize = 0;
  if (BuiltinIBIntrName == "_uint")
    DatumSize = 3;
  else if (BuiltinIBIntrName == "_ulong")
    DatumSize = 4;
  else if (BuiltinIBIntrName == "_ushort")
    DatumSize = 2;
  else if (BuiltinIBIntrName == "_uchar")
    DatumSize = 1;
  else
    assert(false);
  auto DatumSizeV = ConstantInt::get(CI8Ty, DatumSize);
  //
  unsigned char L1Ctrl = 0, L3Ctrl = 0;
  unsigned Field =
      static_cast<unsigned>(cast<ConstantInt>(CacheCtrlV)->getZExtValue());
  mapCacheControl(Field, L1Ctrl, L3Ctrl);
  auto L1CtrlV = ConstantInt::get(CI8Ty, L1Ctrl);
  auto L3CtrlV = ConstantInt::get(CI8Ty, L3Ctrl);
  // create prefetch intrinsic call
  auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
  // create the intrinsic call
  Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(CI->getModule(), ID,
                                                         {PredV1Ty, VPtrTy});
  auto VecPtr =
      CastInst::CreateBitOrPointerCast(PtrV, VPtrTy, PtrV->getName(), CI);
  CallInst::Create(NewFDecl,
                   {OnePred1, Zeroi8C, L1CtrlV, L3CtrlV, Onei16C, ImmedOffsetV,
                    DatumSizeV, NumElemsV, Twoi8C, Zeroi8C, VecPtr, Zeroi32C},
                   "", CI);
  SimdToErases.push_back(CI);
}

static std::string getMDString(MDNode *N, unsigned I) {
  if (!N)
    return "";

  Metadata *Op = N->getOperand(I);
  if (!Op)
    return "";

  if (MDString *Str = dyn_cast<MDString>(Op)) {
    return Str->getString().str();
  }

  return "";
}
// Native sqrt doesn't support double type on GEN. We generate native sqrt only
// if fast flag is specified and no double type provided
static Value *translateSqrtOpIntrinsic(CallInst *CI) {
  assert(isa<llvm::IntrinsicInst>(CI) &&
         CI->getIntrinsicID() == Intrinsic::sqrt);

  GenXIntrinsic::ID GID = (CI->getType()->getScalarType()->isDoubleTy() ||
                           !CI->getFastMathFlags().isFast())
                              ? GenXIntrinsic::genx_ieee_sqrt
                              : GenXIntrinsic::genx_sqrt;

  Function *NewFDecl =
      GenXIntrinsic::getGenXDeclaration(CI->getModule(), GID, {CI->getType()});
  Value *RepI =
      CallInst::Create(NewFDecl, {CI->getOperand(0)}, CI->getName(), CI);
  CI->replaceAllUsesWith(RepI);
  return RepI;
}

static Value *translateReduceOpIntrinsic(IntrinsicInst *II,
                                         Instruction::BinaryOps ReduceOp) {
  auto *CI = cast<CallInst>(II);
  LLVMContext &CTX = CI->getContext();
  // TODO: introduce GenXIRBuilder helper
  bool IsFloatingPointReduction =
      CI->getType()->getScalarType()->isDoubleTy() ||
      CI->getType()->getScalarType()->isFloatTy();
  auto GetReduceSeq = [&](Value *Src, unsigned Size,
                          unsigned ElemOffset) -> Value * {
    if (Size == 1) {
      if (Src->getType()->isVectorTy()) {
        Value *Index = ConstantInt::get(Type::getInt32Ty(CTX), ElemOffset);
        return ExtractElementInst::Create(Src, Index, ".reduce.single",
                                          CI /*InsertBefore*/);
      }
      assert(ElemOffset == 0);
      return Src;
    }

    Type *SrcType = Src->getType();
    Type *SrcElementType = cast<FixedVectorType>(SrcType)->getElementType();

    Type *Tys[] = {FixedVectorType::get(SrcElementType, Size), SrcType,
                   Type::getInt16Ty(CTX)};
    auto *Decl = GenXIntrinsic::getGenXDeclaration(
        CI->getModule(),
        IsFloatingPointReduction ? GenXIntrinsic::genx_rdregionf
                                 : GenXIntrinsic::genx_rdregioni,
        Tys);

    Value *Args[] = {
        Src,
        ConstantInt::get(Type::getInt32Ty(CTX), 0),    // VStride
        ConstantInt::get(Type::getInt32Ty(CTX), Size), // Width
        ConstantInt::get(Type::getInt32Ty(CTX), 1u),   // Stride
        ConstantInt::get(Type::getInt16Ty(CTX),
                         ElemOffset *
                             (SrcElementType->getPrimitiveSizeInBits() /
                              8)),                  // Offset in bytes
        ConstantInt::get(Type::getInt32Ty(CTX), 0), // Parent width (ignored)
    };

    return CallInst::Create(Decl, Args, Src->getName() + ".reduce.seq",
                            CI /*InsertBefore*/);
  };

  int OperandToReduceIdx = IsFloatingPointReduction ? 1 : 0;
  Value *OperandToReduce = CI->getOperand(OperandToReduceIdx);
  unsigned N =
      cast<FixedVectorType>(OperandToReduce->getType())->getNumElements();
  Value *CurrentReduceRes = nullptr;

  IRBuilder<> Builder(CI);

  // If reassoc is not allowed then do sequential operation for floating points.
  if (IsFloatingPointReduction && !CI->hasAllowReassoc()) {
    CurrentReduceRes = CI->getOperand(0);
    for (unsigned i = 0; i < N; i++) {
      auto *Extract = GetReduceSeq(OperandToReduce, 1, i);
      CurrentReduceRes =
          Builder.CreateBinOp(ReduceOp, CurrentReduceRes, Extract);
    }
    CI->replaceAllUsesWith(CurrentReduceRes);
    return CurrentReduceRes;
  }

  // Split vector into two parts. First part's size is alwayas a power of two
  // so we can simply apply reduction by spliiting it into parts. Reduction with
  // a second part is applicable it's size equals to the first part size
  unsigned N1, N2;
  if (llvm::isPowerOf2_32(N)) {
    N1 = N;
    N2 = 0;
  } else {
    N1 = 1u << llvm::Log2_32(N);
    N2 = N - N1;
  }

  CurrentReduceRes = OperandToReduce;
  for (unsigned i = N1 / 2; i >= 1; i /= 2) {
    auto *RSeq1 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq2 = GetReduceSeq(CurrentReduceRes, i, i);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq1, RSeq2);

    // Continue reducing if second part has suitable size, fall back
    // to reducing first part otherwise
    if (i > N2)
      continue;

    // Reduction across parts
    auto *RSeq21 = GetReduceSeq(CurrentReduceRes, i, 0);
    auto *RSeq22 = GetReduceSeq(OperandToReduce, i, N - N2);
    CurrentReduceRes = Builder.CreateBinOp(ReduceOp, RSeq21, RSeq22);
    N2 -= i;
  }

  assert(CurrentReduceRes->getType() == CI->getType());

  if (IsFloatingPointReduction) {
    assert(CI->hasAllowReassoc());
    CurrentReduceRes =
        Builder.CreateBinOp(ReduceOp, CurrentReduceRes, CI->getOperand(0));
  }

  CI->replaceAllUsesWith(CurrentReduceRes);
  return CurrentReduceRes;
}

//
// Trace the GetElementPtrInst[s] back to its base, which should be a SLM
// global variable. Convert the GEPs into multiple instructions that compute
// the byte offset from the base represented by these GEP instructions.
//
// Returns the final offset value if the GEP was able to be expanded to
// multiple instructions.
//
static Value *getSLMOffset(Value *V, Instruction *InsPos) {
  SmallVector<GEPOperator *, 4> GEPs;
  Value *base = V->stripPointerCasts();
  // gep : the last gep of pointer address, null if no GEP at all.
  while (GEPOperator *gep = dyn_cast<GEPOperator>(base)) {
    GEPs.push_back(gep);
    base = gep->getPointerOperand()->stripPointerCasts();
  }
  auto GV = dyn_cast<GlobalVariable>(base);
  // \TODO: what if the base is a PHINode?
  if (!GV)
    return nullptr;

  Module *M = GV->getParent();
  const DataLayout *DL = &M->getDataLayout();
  Type *int32Ty = Type::getInt32Ty(M->getContext());
  // obtain the offset of the global variable
  unsigned Offset;
  auto Invalid = GV->getAttribute(GENX_SLM_OFFSET)
                     .getValueAsString()
                     .getAsInteger(0, Offset);
  (void)Invalid; // INTEL
  assert(!Invalid);

  // initialize the PointerValue representing the offset
  Value *PointerValue = ConstantInt::get(int32Ty, Offset);
  const int nGEPs = GEPs.size();
  // GEPs is in the reverse order of execution! The last GEP is the first
  // one to execute.  For example:
  //    %37 = getelementptr inbounds float, float addrspace(1)* %signalw, i64
  //    16384 %38 = bitcast float addrspace(1)* %37 to[16 x[32 x[32 x float]]]
  //    addrspace(1)* %39 = getelementptr inbounds[16 x[32 x[32 x float]]], [16
  //    x[32 x[32 x float]]]
  //                        addrspace(1)* %38, i64 0, i64 % 34, i64 % 17, i64 %
  //                        18
  //    store float %36, float addrspace(1)* %39, align 4
  //
  //  GEPs = [%39, %37]  // GEPs[0] = %39, GEPs[1] = %37
  //
  for (int i = nGEPs; i > 0; --i) {
    GEPOperator *GEP = GEPs[i - 1];
    Value *PtrOp = GEP->getPointerOperand();
    PointerType *PtrTy = dyn_cast<PointerType>(PtrOp->getType());

    assert(PtrTy && "Only accept scalar pointer!");

    Type *Ty = PtrTy;
    gep_type_iterator GTI = gep_type_begin(GEP);
    for (auto OI = GEP->op_begin() + 1, E = GEP->op_end(); OI != E;
         ++OI, ++GTI) {
      Value *Idx = *OI;
      if (StructType *StTy = GTI.getStructTypeOrNull()) {
        unsigned Field =
            static_cast<unsigned>(cast<ConstantInt>(Idx)->getZExtValue());
        if (Field) {
          uint64_t Offset = DL->getStructLayout(StTy)->getElementOffset(Field);

          Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, OffsetValue, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
      } else {
        Ty = GTI.getIndexedType();
        if (const ConstantInt *CI = dyn_cast<ConstantInt>(Idx)) {
          if (!CI->isZero()) {
            uint64_t Offset = DL->getTypeAllocSize(Ty) * CI->getSExtValue();
            Value *OffsetValue = ConstantInt::get(int32Ty, Offset);

            PointerValue = BinaryOperator::CreateAdd(PointerValue, OffsetValue,
                                                     "", InsPos);
            cast<llvm::Instruction>(PointerValue)
                ->setDebugLoc(InsPos->getDebugLoc());
          }
        } else {
          Value *NewIdx =
              CastInst::CreateTruncOrBitCast(Idx, int32Ty, "", InsPos);
          cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          APInt ElementSize =
              APInt((unsigned int)int32Ty->getPrimitiveSizeInBits(),
                    DL->getTypeAllocSize(Ty));

          if (ElementSize != 1) {
            NewIdx = BinaryOperator::CreateMul(
                NewIdx, ConstantInt::get(int32Ty, ElementSize), "", InsPos);
            cast<llvm::Instruction>(NewIdx)->setDebugLoc(InsPos->getDebugLoc());
          }

          PointerValue =
              BinaryOperator::CreateAdd(PointerValue, NewIdx, "", InsPos);
          cast<llvm::Instruction>(PointerValue)
              ->setDebugLoc(InsPos->getDebugLoc());
        }
      }
    }
  }
  return PointerValue;
}

static Value *translateSVMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  auto DTy = LoadOp->getType();
  auto DL = LoadOp->getModule()->getDataLayout();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
        "svm.block.ld.unaligned";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {DTy, PtrV->getType()});
    RepI = CallInst::Create(NewFDecl, {PtrV}, "svm.block.ld.unaligned", LoadOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.gather";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int RetLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      RetLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, RetLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VPtrTy});
    auto VecPtr =
        CastInst::CreateBitOrPointerCast(PtrV, VPtrTy, PtrV->getName(), LoadOp);
    Instruction *IntrI = CallInst::Create(
        NewFDecl, {OnePred1, NumBlksC, VecPtr, UndefValue::get(VDTy)},
        "svm.gather", LoadOp);
    if (EltBytes == 2) {
      // cast 4xi8 to 2xi16
      auto I16x2Ty =
          llvm::FixedVectorType::get(IntegerType::getInt16Ty(CTX), 2);
      IntrI = CastInst::CreateBitOrPointerCast(
          IntrI, I16x2Ty, IntrI->getName() + ".cast", LoadOp);
    }
    RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
  }
  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Constant *getConstVector(Type *ITy, unsigned int n, unsigned int step) {
  std::vector<Constant *> vConsts;
  unsigned v = 0;
  for (unsigned i = 0; i < n; i++) {
    vConsts.push_back(ConstantInt::get(ITy, v));
    v += step;
  }
  return ConstantVector::get(vConsts);
}

static Instruction *createRdRegion(Value *Input, const Twine &Name,
                                   Instruction *InsertBefore, int NumElements,
                                   int StartIdx, int VStride, int Width,
                                   int Stride) {
  assert(isa<VectorType>(Input->getType()));
  IntegerType *I32Ty = Type::getInt32Ty(Input->getContext());
  Value *ParentWidthArg = UndefValue::get(I32Ty);
  Value *Args[] = {
      // Args to new rdregion:
      Input,                             // input to original rdregion
      ConstantInt::get(I32Ty, VStride),  // vstride
      ConstantInt::get(I32Ty, Width),    // width
      ConstantInt::get(I32Ty, Stride),   // stride
      ConstantInt::get(I32Ty, StartIdx), // start index (in bytes)
      ParentWidthArg                     // parent width
  };
  Type *ElTy = cast<VectorType>(Input->getType())->getElementType();
  auto RegionTy = llvm::FixedVectorType::get(ElTy, NumElements);
  Module *M = InsertBefore->getParent()->getParent()->getParent();
  auto IID = ElTy->isFloatingPointTy() ? GenXIntrinsic::genx_rdregionf
                                       : GenXIntrinsic::genx_rdregioni;
  Type *Tys[] = {RegionTy, Args[0]->getType(), Args[4]->getType()};
  auto Decl = GenXIntrinsic::getGenXDeclaration(M, IID, Tys);
  Instruction *NewInst = CallInst::Create(Decl, Args, Name, InsertBefore);
  return NewInst;
}

static Instruction *createWrRegion(Value *OldVal, Value *Input,
                                   const Twine &Name, Instruction *InsertBefore,
                                   int StartIdx, int VStride, int Width,
                                   int Stride, Value *PredV = nullptr) {
  assert(isa<VectorType>(Input->getType()));
  assert(isa<VectorType>(OldVal->getType()));
  assert(OldVal->getType()->getScalarType() ==
         Input->getType()->getScalarType());
  IntegerType *I32Ty = Type::getInt32Ty(Input->getContext());
  Value *ParentWidthArg = UndefValue::get(I32Ty);
  // if there is no predicate value, set mask to all-one
  Value *MaskArg =
      PredV ? PredV : ConstantInt::get(Type::getInt1Ty(Input->getContext()), 1);
  // Build the wrregion.
  Value *Args[] = {
      // Args to new wrregion:
      OldVal,                            // original vector
      Input,                             // value to write into subregion
      ConstantInt::get(I32Ty, VStride),  // vstride
      ConstantInt::get(I32Ty, Width),    // width
      ConstantInt::get(I32Ty, Stride),   // stride
      ConstantInt::get(I32Ty, StartIdx), // start index (in bytes)
      ParentWidthArg, // parent width (if variable start index)
      MaskArg         // mask
  };
  Type *ElTy = cast<VectorType>(Input->getType())->getElementType();
  auto IID = ElTy->isFloatingPointTy() ? GenXIntrinsic::genx_wrregionf
                                       : GenXIntrinsic::genx_wrregioni;
  Module *M = InsertBefore->getParent()->getParent()->getParent();
  Type *Tys[] = {Args[0]->getType(), Args[1]->getType(), Args[5]->getType(),
                 Args[7]->getType()};
  auto Decl = GenXIntrinsic::getGenXDeclaration(M, IID, Tys);
  Instruction *NewInst = CallInst::Create(Decl, Args, Name, InsertBefore);
  return NewInst;
}

// change vector from llllllllhhhhhhhh to lhlhlhlhlhlhlhlh
static Instruction *formDoubleVector(Value *InputVector,
                                     Instruction *InsertBefore,
                                     Value *PredV = nullptr,
                                     Value *OldVal = nullptr) {
  auto *VTy = dyn_cast<FixedVectorType>(InputVector->getType());
  assert(VTy);
  // if Pred-value is not null then old-value cannot be null
  assert(!PredV || OldVal);
  unsigned VL = VTy->getNumElements();
  assert(VL == 16 || VL == 32);
  Type *EltTy = VTy->getElementType();
  assert(!EltTy->isPointerTy());
  auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
  auto Width = VL / 2;
  // read low half
  auto LowHalf = createRdRegion(
      InputVector, InputVector->getName() + ".1stHalf", InsertBefore, Width,
      0 /*StartIdx*/, 0 /*VStride*/, Width, 1 /*HStride*/);
  // read high half
  auto HighHalf = createRdRegion(
      InputVector, InputVector->getName() + ".2ndHalf", InsertBefore, Width,
      Width * EltBytes /*startIdx*/, 0 /*VStride*/, Width, 1 /*HStride*/);
  if (OldVal == nullptr)
    OldVal = UndefValue::get(VTy);
  else
    assert(OldVal->getType() == VTy);
  // write low half
  auto Partial = createWrRegion(
      OldVal, LowHalf, InputVector->getName() + ".InsLow", InsertBefore,
      0 /*StartIdx*/, 0 /*VStride*/, Width, 2 /*HStride*/, PredV);
  // write high half
  auto Complete = createWrRegion(
      Partial, HighHalf, InputVector->getName() + ".InsHigh", InsertBefore,
      EltBytes /*StartIdx*/, 0 /*VStride*/, Width, 2 /*HStride*/, PredV);
  return Complete;
}

// change vector from lhlhlhlhlhlhlhlh to llllllllhhhhhhhh
static Instruction *disbandDoubleVector(Value *InputVector,
                                        Instruction *InsertBefore) {
  auto *VTy = dyn_cast<FixedVectorType>(InputVector->getType());
  assert(VTy);
  unsigned VL = VTy->getNumElements();
  assert(VL == 16 || VL == 32);
  Type *EltTy = VTy->getElementType();
  assert(!EltTy->isPointerTy());
  auto EltBytes = EltTy->getPrimitiveSizeInBits() / 8;
  auto Width = VL / 2;
  // read low elements
  auto LowHalf = createRdRegion(InputVector, InputVector->getName() + "exLow",
                                InsertBefore, Width, 0 /*StartIdx*/,
                                0 /*VStride*/, Width, 2 /*HStride*/);
  // read high elements
  auto HighHalf = createRdRegion(InputVector, InputVector->getName() + "exHigh",
                                 InsertBefore, Width, EltBytes /*startIdx*/,
                                 0 /*VStride*/, Width, 2 /*HStride*/);
  auto Undef = UndefValue::get(VTy);
  // write low half
  auto Partial =
      createWrRegion(Undef, LowHalf, "", InsertBefore, 0 /*StartIdx*/,
                     0 /*VStride*/, Width, 1 /*HStride*/);
  // write high half
  auto Complete = createWrRegion(Partial, HighHalf, "", InsertBefore,
                                 Width * EltBytes /*StartIdx*/, 0 /*VStride*/,
                                 Width, 1 /*HStride*/);
  return Complete;
}

static Value *translateSLMLoad(LoadInst *LoadOp) {
  LLVMContext &CTX = LoadOp->getContext();
  IRBuilder<> Builder(LoadOp);
  auto PtrV = LoadOp->getPointerOperand();
  Type *DTy = LoadOp->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower LoadInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, LoadOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  auto DL = LoadOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    unsigned VL = cast<FixedVectorType>(DTy)->getNumElements();
    Type *EltTy = cast<FixedVectorType>(DTy)->getElementType();
    auto EltBytes = getElementSizeInBytes(EltTy, DL);
    // create constant for offset
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "gather.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
      // crease constant for num-blocks
      auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          LoadOp->getModule(), ID, {DTy, PredVTy, VOffset->getType()});
      RepI = CallInst::Create(NewFDecl,
                              {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset,
                               VOffset, UndefValue::get(DTy)},
                              "slm.block.gather", LoadOp);
    } else if (EltBytes == 8) {
      // there is no surface load of i64 type, therefore we need to use gather4
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "gather4.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      // create constant for channel-mask GR-enabled
      auto ChanMask = ConstantInt::get(CIntTy, 3);
      // need to create the double-vec-type
      auto Dx2Ty =
          llvm::FixedVectorType::get(IntegerType::getInt32Ty(CTX), VL * 2);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          LoadOp->getModule(), ID, {Dx2Ty, PredVTy, VOffset->getType()});
      auto Gather4I =
          CallInst::Create(NewFDecl,
                           {OnePredV, ChanMask, ScaleC, BTI, SLMOffset, VOffset,
                            UndefValue::get(Dx2Ty)},
                           "slm.block.gather", LoadOp);
      // reorder the vector
      auto ShuffleV = formDoubleVector(Gather4I, LoadOp);
      // cast back to the original 64-bit type
      RepI = CastInst::CreateBitOrPointerCast(ShuffleV, DTy, LoadOp->getName(),
                                              LoadOp);
    } else
      assert(false);
  } else {
    // scalar load
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "gather.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 || EltBytes == 8);
    int NumBlks = (EltBytes >= 4) ? 2 : (EltBytes - 1);
    auto EltTy = (EltBytes == 8) ? CIntTy : DTy;
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, 1);
    // create constant for offset
    auto VOffsetTy = llvm::FixedVectorType::get(CIntTy, 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // crease constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        LoadOp->getModule(), ID, {VDTy, PredV1Ty, VOffsetTy});
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), LoadOp);
    auto GOffsetC = ConstantInt::get(CIntTy, 0);
    Instruction *IntrI =
        CallInst::Create(NewFDecl,
                         {OnePred1, NumBlksC, ScaleC, BTI, GOffsetC, VecOffset,
                          UndefValue::get(VDTy)},
                         "slm.gather", LoadOp);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    auto OneC = ConstantInt::get(CIntTy, 1);
    if (EltBytes <= 4)
      RepI = Builder.CreateExtractElement(IntrI, ZeroC, LoadOp->getName());
    else {
      // load high-half
      auto ResTy = llvm::FixedVectorType::get(EltTy, 2);
      GOffsetC = ConstantInt::get(CIntTy, 4);
      Instruction *IntrI2 =
          CallInst::Create(NewFDecl,
                           {OnePred1, NumBlksC, ScaleC, BTI, GOffsetC,
                            VecOffset, UndefValue::get(VDTy)},
                           "slm.gather", LoadOp);
      auto LowHalf = Builder.CreateExtractElement(IntrI, ZeroC,
                                                  LoadOp->getName() + ".low");
      auto HighHalf = Builder.CreateExtractElement(IntrI2, ZeroC,
                                                   LoadOp->getName() + ".high");
      auto Partial =
          Builder.CreateInsertElement(UndefValue::get(ResTy), LowHalf, ZeroC);
      auto Complete = Builder.CreateInsertElement(Partial, HighHalf, OneC);
      RepI = CastInst::CreateBitOrPointerCast(Complete, DTy, LoadOp->getName(),
                                              LoadOp);
    }
  }

  LoadOp->replaceAllUsesWith(RepI);
  return RepI;
}

static Value *translateSVMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  Value *RepI = nullptr;
  auto DL = StoreOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.block.st";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PtrV->getType(), DTy});
    RepI = CallInst::Create(
        NewFDecl, {PtrV, DTV},
        NewFDecl->getReturnType()->isVoidTy() ? "" : "svm.block.st", StoreOp);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "svm.scatter";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    int NumBlks = 0; // (0/1/2/3 for num blocks 1/2/4/8)
    int DataLen = 1;
    auto EltTy = DTy;
    if (EltBytes == 2) {
      NumBlks = 1;
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else if (EltBytes == 1) {
      EltTy = Type::getInt8Ty(CTX);
      DataLen = 4; // return 4xi8
    } else
      assert(EltBytes == 4 || EltBytes == 8);
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, DataLen);
    auto VPtrTy = llvm::FixedVectorType::get(PtrV->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto CIntTy = Type::getInt32Ty(CTX);
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto ZeroC = ConstantInt::get(CIntTy, 0);
    // create the intrinsic call
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VPtrTy, VDTy});
    auto VecPtr = CastInst::CreateBitOrPointerCast(PtrV, VPtrTy,
                                                   PtrV->getName(), StoreOp);
    Value *VecDTV = nullptr;
    if (EltBytes == 2) {
      // cast 2xi16 to 4xi8
      auto I16x2Ty = llvm::FixedVectorType::get(DTy, 2);
      VecDTV = Builder.CreateInsertElement(UndefValue::get(I16x2Ty), DTV, ZeroC,
                                           DTV->getName());
      VecDTV = CastInst::CreateBitOrPointerCast(
          VecDTV, VDTy, DTV->getName() + ".cast", StoreOp);
    } else
      VecDTV = Builder.CreateInsertElement(UndefValue::get(VDTy), DTV, ZeroC,
                                           DTV->getName());
    RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, VecPtr, VecDTV});
  }
  return RepI;
}

static Value *translateSLMStore(StoreInst *StoreOp) {
  LLVMContext &CTX = StoreOp->getContext();
  IRBuilder<> Builder(StoreOp);
  auto PtrV = StoreOp->getPointerOperand();
  auto DTV = StoreOp->getValueOperand();
  auto DTy = DTV->getType();
  if (!DTy->isSingleValueType()) {
    Twine Msg("unable to lower StoreInst with non-single-value type");
    llvm::report_fatal_error(Msg, false /*no crash diag*/);
  }
  auto SLMOffset = getSLMOffset(PtrV, StoreOp);
  assert(SLMOffset);
  Value *RepI = nullptr;
  auto CIntTy = Type::getInt32Ty(CTX);
  auto BTI = ConstantInt::get(CIntTy, SLM_BTI);
  auto DL = StoreOp->getModule()->getDataLayout();
  if (DTy->isVectorTy()) {
    unsigned VL = cast<FixedVectorType>(DTy)->getNumElements();
    Type *EltTy = cast<FixedVectorType>(DTy)->getElementType();
    auto EltBytes = getElementSizeInBytes(EltTy, DL);
    auto VOffset = getConstVector(CIntTy, VL, EltBytes);
    // create constant for predicate
    auto PredVTy = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), VL);
    auto OnePredV = Constant::getAllOnesValue(PredVTy);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    if (EltBytes == 1 || EltBytes == 2 || EltBytes == 4) {
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "scatter.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      int NumBlks = (EltBytes == 4) ? 2 : (EltBytes - 1);
      // create constant for num-blocks
      auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          StoreOp->getModule(), ID, {PredVTy, VOffset->getType(), DTy});
      RepI = Builder.CreateCall(
          NewFDecl, {OnePredV, NumBlksC, ScaleC, BTI, SLMOffset, VOffset, DTV});
    } else if (EltBytes == 8) {
      // cast it into i32 vector
      std::string IntrName =
          std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) +
          "scatter4.scaled";
      auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
      auto Dx2Ty = llvm::FixedVectorType::get(CIntTy, 2 * VL);
      auto VecDTV = CastInst::CreateBitOrPointerCast(
          DTV, Dx2Ty, DTV->getName() + ".cast", StoreOp);
      // shuffle it into 2-channel
      auto VectDTV2 = disbandDoubleVector(VecDTV, StoreOp);
      // create the scatter4
      // create channel-mask, red-green enabled
      auto ChanMask = ConstantInt::get(CIntTy, 3);
      // create the intrinsic call
      Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
          StoreOp->getModule(), ID, {PredVTy, VOffset->getType(), Dx2Ty});
      RepI = Builder.CreateCall(NewFDecl, {OnePredV, ChanMask, ScaleC, BTI,
                                           SLMOffset, VOffset, VectDTV2});
    } else
      assert(false);
  } else {
    std::string IntrName =
        std::string(GenXIntrinsic::getGenXIntrinsicPrefix()) + "scatter.scaled";
    auto ID = GenXIntrinsic::lookupGenXIntrinsicID(IntrName);
    auto EltBytes = getElementSizeInBytes(DTy, DL);
    assert(EltBytes == 1 || EltBytes == 2 || EltBytes == 4 || EltBytes == 8);
    int NumBlks = (EltBytes >= 4) ? 2 : (EltBytes - 1);
    auto EltTy = (EltBytes == 8) ? CIntTy : DTy;
    auto NumElts = (EltBytes == 8) ? 2 : 1;
    // create vector type
    auto VDTy = llvm::FixedVectorType::get(EltTy, NumElts);
    // cast data to vector
    auto VecDTV = CastInst::CreateBitOrPointerCast(
        DTV, VDTy, DTV->getName() + ".cast", StoreOp);
    auto VOffsetTy = llvm::FixedVectorType::get(SLMOffset->getType(), 1);
    // create constant for predicate
    auto PredV1Ty = llvm::FixedVectorType::get(IntegerType::getInt1Ty(CTX), 1);
    auto OnePred1 = Constant::getAllOnesValue(PredV1Ty);
    // create constant for num-blocks
    auto NumBlksC = ConstantInt::get(CIntTy, NumBlks);
    auto GOffsetC = ConstantInt::get(CIntTy, 0);
    auto ScaleC = ConstantInt::get(Type::getInt16Ty(CTX), 0);
    auto VecOffset = CastInst::CreateBitOrPointerCast(
        SLMOffset, VOffsetTy, SLMOffset->getName(), StoreOp);
    // create the intrinsic call
    auto V1DTy = llvm::FixedVectorType::get(EltTy, 1);
    Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
        StoreOp->getModule(), ID, {PredV1Ty, VOffsetTy, V1DTy});
    if (NumElts == 1)
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, VecDTV});
    else {
      auto IndexC = ConstantInt::get(CIntTy, 0);
      auto DTVL =
          Builder.CreateExtractElement(VecDTV, IndexC, DTV->getName() + ".low");
      IndexC = ConstantInt::get(CIntTy, 1);
      auto DTVH = Builder.CreateExtractElement(VecDTV, IndexC,
                                               DTV->getName() + ".high");
      auto TmpDTV = CastInst::CreateBitOrPointerCast(
          DTVL, V1DTy, DTVL->getName() + ".cast", StoreOp);
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, TmpDTV});
      TmpDTV = CastInst::CreateBitOrPointerCast(
          DTVH, V1DTy, DTVH->getName() + ".cast", StoreOp);
      GOffsetC = ConstantInt::get(CIntTy, 4);
      RepI = Builder.CreateCall(NewFDecl, {OnePred1, NumBlksC, ScaleC, BTI,
                                           GOffsetC, VecOffset, TmpDTV});
    }
  }
  return RepI;
}

// These two tables are used to change llvm math intrinsic names (left column)
// to the genx intrinsic (right column).
//
// f32 mapping
std::unordered_map<Intrinsic::ID, GenXIntrinsic::ID> GenXMath32 = {
    //  Basic functions
    //{Intrinsic::fma,        NA}, // llvm fma is supported by BE

    //  Exponential functions
    {Intrinsic::exp, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::exp2, GenXIntrinsic::genx_exp},
    {Intrinsic::log, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log2, GenXIntrinsic::genx_log},
    {Intrinsic::log10, GenXIntrinsic::not_genx_intrinsic},

    //  Power functions
    {Intrinsic::pow, GenXIntrinsic::genx_pow},

    //  Trig & hyperbolic functions
    {Intrinsic::sin, GenXIntrinsic::genx_sin},
    {Intrinsic::cos, GenXIntrinsic::genx_cos},

    //  Rounding functions
    {Intrinsic::ceil, GenXIntrinsic::genx_rndu},
    {Intrinsic::floor, GenXIntrinsic::genx_rndd},
    {Intrinsic::trunc, GenXIntrinsic::genx_rndz},
    {Intrinsic::round, GenXIntrinsic::not_genx_intrinsic},

    //  Floating-point manipulation functions
    {Intrinsic::copysign, GenXIntrinsic::not_genx_intrinsic},
};

// f64 mapping
std::unordered_map<Intrinsic::ID, GenXIntrinsic::ID> GenXMath64 = {
    //  Basic functions
    {Intrinsic::fma, GenXIntrinsic::not_genx_intrinsic},

    //  Exponential functions
    {Intrinsic::exp, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::exp2, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log2, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::log10, GenXIntrinsic::not_genx_intrinsic},

    //  Power functions
    {Intrinsic::pow, GenXIntrinsic::not_genx_intrinsic},

    //  Trig & hyperbolic functions
    {Intrinsic::sin, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::cos, GenXIntrinsic::not_genx_intrinsic},

    //  Rounding functions
    {Intrinsic::ceil, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::floor, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::trunc, GenXIntrinsic::not_genx_intrinsic},
    {Intrinsic::round, GenXIntrinsic::not_genx_intrinsic},

    //  Floating-point manipulation functions
    {Intrinsic::copysign, GenXIntrinsic::not_genx_intrinsic}};

static Value *translateLLVMInst(Instruction *Inst) {
  LLVMContext &CTX = Inst->getContext();
  IRBuilder<> Builder(Inst);
  if (auto CastOp = dyn_cast<llvm::CastInst>(Inst)) {
    llvm::Type *DstTy = CastOp->getDestTy();
    auto CastOpcode = CastOp->getOpcode();
    if (isa<FixedVectorType>(DstTy) &&
        ((CastOpcode == llvm::Instruction::FPToUI &&
          DstTy->getScalarType()->getPrimitiveSizeInBits() <= 32) ||
         (CastOpcode == llvm::Instruction::FPToSI &&
          DstTy->getScalarType()->getPrimitiveSizeInBits() < 32))) {
      llvm::Value *Src = CastOp->getOperand(0);
      auto TmpTy = llvm::FixedVectorType::get(
          llvm::Type::getInt32Ty(CTX),
          cast<FixedVectorType>(DstTy)->getNumElements());
      Src = Builder.CreateFPToSI(Src, TmpTy);

      llvm::Instruction::CastOps TruncOp = llvm::Instruction::Trunc;
      auto *NewDst = Builder.CreateCast(TruncOp, Src, DstTy);
      CastOp->replaceAllUsesWith(NewDst);
      return NewDst;
    }
    return CastOp;
  }
  if (auto CallOp = dyn_cast<llvm::IntrinsicInst>(Inst)) {
    // handle memory intrinsics
    // masked.gather, masked.scatter etc
    auto ID = CallOp->getIntrinsicID();
    auto DL = CallOp->getModule()->getDataLayout();
    switch (ID) {
    case Intrinsic::vector_reduce_add:
      return translateReduceOpIntrinsic(CallOp, Instruction::Add);
    case Intrinsic::vector_reduce_mul:
      return translateReduceOpIntrinsic(CallOp, Instruction::Mul);
    case Intrinsic::vector_reduce_xor:
      return translateReduceOpIntrinsic(CallOp, Instruction::Xor);
    case Intrinsic::vector_reduce_or:
      return translateReduceOpIntrinsic(CallOp, Instruction::Or);
    case Intrinsic::vector_reduce_fadd:
      return translateReduceOpIntrinsic(CallOp, Instruction::FAdd);
    case Intrinsic::sqrt:
      return translateSqrtOpIntrinsic(cast<CallInst>(CallOp));
    default: {
      auto DTy = CallOp->getType()->getScalarType();
      GenXIntrinsic::ID GID = GenXIntrinsic::not_genx_intrinsic;
      // find the intrinsic mapping
      if (DTy->isFloatTy()) {
        auto Map = GenXMath32.find(ID);
        if (Map != GenXMath32.end()) {
          GID = Map->second;
        }
      } else if (DTy->isDoubleTy()) {
        auto Map = GenXMath64.find(ID);
        if (Map != GenXMath64.end()) {
          GID = Map->second;
        }
      }
      // do the conversion
      if (GID != GenXIntrinsic::not_genx_intrinsic) {
        Function *NewFDecl = GenXIntrinsic::getGenXDeclaration(
            CallOp->getModule(), GID, {CallOp->getType()});
        SmallVector<Value *, 2> ValueOperands(CallOp->args());
        Value *RepI = CallInst::Create(NewFDecl, ValueOperands,
                                       CallOp->getName(), CallOp);
        CallOp->replaceAllUsesWith(RepI);
        return RepI;
      }
    } break;
    }
    return CallOp;
  }
  return Inst;
}

static unsigned int assignSLMOffset(Module &M) {
  auto DL = M.getDataLayout();
  unsigned SLMSize = 0;
  for (auto &&GV : M.globals()) {
    auto Ty = dyn_cast<PointerType>(GV.getType());
    if (Ty && Ty->getAddressSpace() == SYCL_SLM_AS) {
      auto DTy = GV.getValueType();
      auto BufferSize = static_cast<size_t>(DL.getTypeAllocSize(DTy));
      auto align = GV.getAlignment();
      if (align == 0) {
        int EltBytes = getElementSizeInBytes(DTy->getScalarType(), DL);
        align = (EltBytes > 0 && EltBytes < 8) ? EltBytes : 8;
      }
      SLMSize = ((SLMSize + align - 1) / align) * align;
      if (!GV.hasAttribute(GENX_SLM_OFFSET))
        GV.addAttribute(GENX_SLM_OFFSET, std::to_string(SLMSize));
      SLMSize += BufferSize;
    }
  }
  return SLMSize;
}

PreservedAnalyses VPOParoptLowerSimdPass::run(Function &F,
                                              FunctionAnalysisManager &FAM) {
  // Only consider functions marked with !sycl_explicit_simd
  bool isOmpSpirKernel = false;

  if ((F.getMetadata("omp_simd_kernel") != nullptr) ||
      (F.getMetadata("omp_declare_target_simd_function") != nullptr) ||
      (F.getMetadata("sycl_explicit_simd") == nullptr &&
       F.getCallingConv() == CallingConv::SPIR_KERNEL)) {
    IRBuilder<> Builder(F.getEntryBlock().getFirstNonPHI());
    int simdWidth = 1;
    Metadata *AttrMDArgs[] = {
        ConstantAsMetadata::get(Builder.getInt32(simdWidth))};
    F.setMetadata("intel_reqd_sub_group_size",
                  MDNode::get(F.getContext(), AttrMDArgs));
    // for omp_declare_target_simd_function there's no need to generate all
    // kernel-related MD but still necessary to perform the lowering
    if (!F.getMetadata("omp_declare_target_simd_function"))
      isOmpSpirKernel = true;
  } else {
    return PreservedAnalyses::all();
  }

  auto SLMSize = assignSLMOffset(*F.getParent());

  // add module-level meta-data for kernel functions
  if (isOmpSpirKernel) {
    auto M = F.getParent();
    auto MetaKernels = M->getOrInsertNamedMetadata(GENX_KERNEL_METADATA);

    LLVMContext &Ctx = M->getContext();
    Type *I32Ty = Type::getInt32Ty(Ctx);

    enum { AK_NORMAL, AK_SAMPLER, AK_SURFACE, AK_VME };
    enum { IK_NORMAL, IK_INPUT, IK_OUTPUT, IK_INPUT_OUTPUT };
    // Metadata node containing N i32s, where N is the number of kernel
    // arguments, and each i32 is the kind of argument,  one of:
    //     0 = general, 1 = sampler, 2 = surface, 3 = vme
    // (the same values as in the "kind" field of an "input_info"
    // record in a vISA kernel.
    SmallVector<Metadata *, 8> ArgKinds;
    // Optional, not supported for compute
    SmallVector<Metadata *, 8> ArgInOutKinds;
    // Metadata node describing N strings where N is the number of kernel
    // arguments, each string describing argument type in OpenCL.
    // required for running on top of OpenCL runtime.
    SmallVector<Metadata *, 8> ArgTypeDescs;
    auto *KernelArgTypes = F.getMetadata("kernel_arg_type");
    unsigned Idx = 0;

    // Iterate argument list to gather argument kinds and generate argument
    // descriptors.
    for (const Argument &Arg : F.args()) {
      int Kind = AK_NORMAL;
      int IKind = IK_NORMAL;

      auto ArgType = getMDString(KernelArgTypes, Idx);

      if (ArgType.find("image1d_t") != std::string::npos ||
          ArgType.find("image2d_t") != std::string::npos ||
          ArgType.find("image3d_t") != std::string::npos ||
          ArgType.find("image1d_buffer_t") != std::string::npos) {
        Kind = AK_SURFACE;
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgType));
      } else {
        StringRef ArgDesc = "";
        if (Arg.getType()->isPointerTy())
          ArgDesc = "svmptr_t";
        ArgTypeDescs.push_back(MDString::get(Ctx, ArgDesc));
      }

      ArgKinds.push_back(ValueAsMetadata::get(ConstantInt::get(I32Ty, Kind)));
      ArgInOutKinds.push_back(
          ValueAsMetadata::get(ConstantInt::get(I32Ty, IKind)));
      Idx++;
    }
    MDNode *Kinds = MDNode::get(Ctx, ArgKinds);
    MDNode *IOKinds = MDNode::get(Ctx, ArgInOutKinds);
    MDNode *ArgDescs = MDNode::get(Ctx, ArgTypeDescs);

    Metadata *MDArgs[] = {ValueAsMetadata::get(&F),
                          MDString::get(Ctx, F.getName().str()),
                          Kinds,
                          ValueAsMetadata::get(llvm::ConstantInt::get(
                              I32Ty, SLMSize)), // SLM size in bytes
                          ValueAsMetadata::get(llvm::ConstantInt::getNullValue(
                              I32Ty)), // arg offsets
                          IOKinds,
                          ArgDescs};

    // Add this kernel to the root.
    MetaKernels->addOperand(MDNode::get(Ctx, MDArgs));
    F.addFnAttr("oclrt", "1");
    F.addFnAttr("CMGenxMain");
  }

  SmallVector<Instruction *, 8> SimdToErases;

  for (Instruction &I : instructions(F)) {
    auto replace = translateLLVMInst(&I);
    if (replace != &I) {
      SimdToErases.push_back(&I);
      continue;
    }

    auto *CI = dyn_cast<CallInst>(&I);
    Function *Callee = nullptr;
    if (!CI || !(Callee = CI->getCalledFunction()))
      continue;
    StringRef Name = Callee->getName();

    if (Name.consume_front(SIMD_INTRIN_PREF0)) {
      // now skip the digits
      Name = Name.drop_while([](char C) { return std::isdigit(C); });

      if (Name.consume_front(SPIRV_INTRIN_PREF)) {
        translateSpirvIntrinsic(CI, Name, SimdToErases);
        // For now: if no match, just let it go untranslated.
      }
    } else if (Name.consume_front(BUILTIN_IB_PREF)) {
      translateBuiltinIBIntrinsic(CI, Name, SimdToErases);
      // For now: if no match, just let it go untranslated.
    }
  }
  for (auto *CI : SimdToErases) {
    CI->eraseFromParent();
  }
  for (auto I = inst_begin(F), E = inst_end(F); I != E;) {
    auto *CI = dyn_cast<AddrSpaceCastInst>(&*I++);
    if (CI && CI->use_empty())
      CI->eraseFromParent();
  }

  return PreservedAnalyses::none();
}

#endif // INTEL_COLLAB
