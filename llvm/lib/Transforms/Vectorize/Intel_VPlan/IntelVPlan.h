//===- IntelVPlan.h - Represent A Vectorizer Plan -------------------------===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file contains the declarations of the Vectorization Plan base classes:
// 1. Specializations of GraphTraits that allow VPBasicBlock graphs to be treated
//    as proper graphs for generic algorithms;
// 2. VPInstruction and its sub-classes represent instructions contained within
//    VPBasicBlocks;
// 3. The VPlan class holding a candidate for vectorization;
// 4. The VPlanUtils class providing methods for building plans;
// 5. The VPlanPrinter class providing a way to print a plan in dot format.
// These are documented in docs/VectorizationPlan.rst.
//
//===----------------------------------------------------------------------===//

#ifndef LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H
#define LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H

#if INTEL_CUSTOMIZATION
#include "IntelVPlanValue.h"
#else
#include "VPlanValue.h"
#endif //INTEL_CUSTOMIZATION
#include "llvm/ADT/GraphTraits.h"
#include "llvm/ADT/SmallSet.h"
#include "llvm/ADT/ilist.h"
#include "llvm/ADT/ilist_node.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/Support/GenericDomTreeConstruction.h"
#include "llvm/Support/raw_ostream.h"
#include <atomic>

#if INTEL_CUSTOMIZATION
#include "IntelVPBasicBlock.h"
#include "IntelVPLoopAnalysis.h"
#include "IntelVPlanDivergenceAnalysis.h"
#include "IntelVPlanLoopInfo.h"
#include "VPlanHIR/IntelVPlanInstructionDataHIR.h"
#include "llvm/ADT/DepthFirstIterator.h"
#include "llvm/Analysis/Intel_LoopAnalysis/Framework/HIRFramework.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/Diag.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/HLGoto.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/HLInst.h"
#include "llvm/Analysis/Intel_OptReport/LoopOptReportBuilder.h"
#include "llvm/Support/FormattedStream.h"
#endif // INTEL_CUSTOMIZATION

namespace llvm {
// The (re)use of existing LoopVectorize classes is subject to future VPlan
// refactoring.
// namespace {
class InnerLoopVectorizer;
class LoopVectorizationLegality;
class LoopInfo;
//}

#if INTEL_CUSTOMIZATION
namespace loopopt {
class RegDDRef;
class HLLoop;
class OptReportDiag;
} // namespace loopopt

namespace vpo {
class SyncDependenceAnalysis;
class VPValue;
class VPlan;
class VPOCodeGen;
class VPOCodeGenHIR;
class VPOVectorizationLegality;
class VPBasicBlock;
class VPDominatorTree;
class VPPostDominatorTree;
#if INTEL_CUSTOMIZATION
class VPlanCostModel; // INTEL: to be later declared as a friend
class VPlanCostModelProprietary; // INTEL: to be later declared as a friend
class VPlanDivergenceAnalysis;
class VPlanBranchDependenceAnalysis;
#endif // INTEL_CUSTOMIZATION
typedef SmallPtrSet<VPValue *, 8> UniformsTy;

// Class names mapping to minimize the diff:
#define InnerLoopVectorizer VPOCodeGen
#define LoopVectorizationLegality VPOVectorizationLegality

struct TripCountInfo;
#endif // INTEL_CUSTOMIZATION

/// In what follows, the term "input IR" refers to code that is fed into the
/// vectorizer whereas the term "output IR" refers to code that is generated by
/// the vectorizer.

/// VPIteration represents a single point in the iteration space of the output
/// (vectorized and/or unrolled) IR loop.
struct VPIteration {
  unsigned Part; ///< in [0..UF)
  unsigned Lane; ///< in [0..VF)
  VPIteration(unsigned Part, unsigned Lane) : Part(Part), Lane(Lane) {}
  VPIteration(unsigned Part) : Part(Part), Lane(ALL_LANES()) {}
  static unsigned ALL_LANES() { return -1; }
};

/// This class is used to enable the VPlan to invoke a method of ILV. This is
/// needed until the method is refactored out of ILV and becomes reusable.
struct VPCallback {
  virtual ~VPCallback() {}
  virtual Value *getOrCreateVectorValues(Value *V, unsigned Part) = 0;
};

/// VPTransformState holds information passed down when "executing" a VPlan,
/// needed for generating the output IR.
struct VPTransformState {
  VPTransformState(unsigned VF, unsigned UF, LoopInfo *LI,
                   class DominatorTree *DT, IRBuilder<> &Builder,
                   InnerLoopVectorizer *ILV, VPCallback &Callback,
                   VPLoopInfo *VPLI)
      : VF(VF), UF(UF), Instance(), LI(LI), DT(DT), Builder(Builder), ILV(ILV),
        Callback(Callback), VPLI(VPLI) {}

  /// The chosen Vectorization and Unroll Factors of the loop being vectorized.
  unsigned VF;
  unsigned UF;

  /// Hold the indices to generate specific scalar instructions. Null indicates
  /// that all instances are to be generated, using either scalar or vector
  /// instructions.
  Optional<VPIteration> Instance;

  struct DataState {
    /// A type for vectorized values in the new loop. Each value from the
    /// original loop, when vectorized, is represented by UF vector values in
    /// the new unrolled loop, where UF is the unroll factor.
    typedef SmallVector<Value *, 2> PerPartValuesTy;

    DenseMap<VPValue *, PerPartValuesTy> PerPartOutput;
  } Data;

  /// Get the generated Value for a given VPValue and a given Part. If such a
  /// Value does not exist by the time this method is called, then this Def
  /// represents Values that are still generated by ILV via ValueMap.
  Value *get(VPValue *Def, unsigned Part) {
    if (Data.PerPartOutput.count(Def))
      return Data.PerPartOutput[Def][Part];
    // Bring the Values from ValueMap.
    return Callback.getOrCreateVectorValues(VPValue2Value[Def], Part);
  }

  /// Set the generated Value for a given VPValue and a given Part.
  void set(VPValue *Def, Value *V, unsigned Part) {
    if (!Data.PerPartOutput.count(Def)) {
      DataState::PerPartValuesTy Entry(UF);
      Data.PerPartOutput[Def] = Entry;
    }
    Data.PerPartOutput[Def][Part] = V;
  }
  /// Hold state information used when constructing the CFG of the output IR,
  /// traversing the VPBasicBlocks and generating corresponding IR BasicBlocks.
  struct CFGState {
    /// The previous VPBasicBlock visited. Initially set to null.
    VPBasicBlock *PrevVPBB = nullptr;
    /// The previous IR BasicBlock created or used. Initially set to the new
    /// header BasicBlock.
    BasicBlock *PrevBB = nullptr;
    /// The last IR BasicBlock in the output IR. Set to the new latch
    /// BasicBlock, used for placing the newly created BasicBlocks.
    BasicBlock *InsertBefore = nullptr;
    /// A mapping of each VPBasicBlock to the corresponding BasicBlock. In case
    /// of replication, maps the BasicBlock of the last replica created.
    SmallDenseMap<VPBasicBlock *, BasicBlock *> VPBB2IRBB;
    /// Vector of VPBasicBlocks whose terminator instruction needs to be fixed
    /// up at the end of vector code generation.
    SmallVector<VPBasicBlock *, 8> VPBBsToFix;
  } CFG;

  /// Hold a pointer to LoopInfo to register new basic blocks in the loop.
  class LoopInfo *LI;

  /// Hold a pointer to Dominator Tree to register new basic blocks in the loop.
  class DominatorTree *DT;

  /// Hold a reference to the IRBuilder used to generate output IR code.
  IRBuilder<> &Builder;

  /// Hold a reference to a mapping between VPValues in VPlan and original
  /// Values they correspond to.
  VPValue2ValueTy VPValue2Value;

  /// Hold a pointer to InnerLoopVectorizer to reuse its IR generation methods.
  class InnerLoopVectorizer *ILV;

  VPCallback &Callback;
#if INTEL_CUSTOMIZATION
  VPLoopInfo *VPLI;
#endif
};

/// Class to model a single VPlan-level instruction - it may generate a sequence
/// of IR instructions when executed, these instructions would always form a
/// single-def expression as the VPInstruction is also a single def-use vertex.
///
#if INTEL_CUSTOMIZATION
/// For HIR, we classify VPInstructions into 3 sub-types:
///   1) Master VPInstruction: It has underlying HIR data attached and its
///      operands could have been decomposed or not. If so, this VPInstruction
///      is the last one in the UD chain of the group of decomposed
///      VPInstructions. If this VPInstruction or any of its decomposed ones are
///      modified, the HIR will automatically be marked as invalid.
///   2) Decomposed VPInstruction: It's created as a result of decomposing a
///      master VPInstruction. It doesn't have underlying HIR directly attached
///      but it has a pointer to its master VPInstruction holding it. In order
///      to check whether the underlying HIR of a decomposed VPInstruction is
///      valid, its master VPInstruction must be checked.
///   3) New VPInstruction: It's created as a result of a VPlan-to-VPlan
///      transformation, excluding decomposition. It doesn't have underlying HIR
///      or master VPInstruction attached. New VPInstructions also exist in the
///      LLVM-IR path.
///
/// DESIGN PRINCIPLE: access to the underlying IR is forbidden by default. Only
/// the front-end and back-end of VPlan should have access to the underlying IR
/// and be aware of the VPInstruction sub-type (master/decomposed/new)
/// instructions. Some well-delimited VPlan analyses, previous design review
/// approval, may also need to have access to the underlying IR and
/// VPInstruction sub-types to bring analysis information computed on the input
/// IR to VPlan. The remaining VPlan algorithms should process all the
/// VPInstructions ignoring their underlying IR and sub-type. For these
/// reasons, adding new friends to this class must be very well justified.
///
/// DESIGN DECISION: for VPO, we decided to pay the memory and design cost of
/// having LLVM-IR data (Inst) HIR data (MasterData) and their respective
/// interfaces in the same class in favor of minimizing divergence with the
/// community. We know that this is not the best design but creating a
/// VPInstructionHIR sub-class would be complicated because VPInstruction also
/// has sub-classes (VPCmpInst, VPPHINode, etc.) that would need to be
/// replicated under the VPInstructionHIR.
#endif
class VPInstruction : public VPUser,
  public ilist_node_with_parent<VPInstruction, VPBasicBlock> {
#if INTEL_CUSTOMIZATION
  friend class HIRSpecifics;
  friend class VPBasicBlock;
  friend class VPBuilder;
  friend class VPBranchInst;
  friend class VPBuilderHIR;
  friend class VPDecomposerHIR;
  // To get underlying HIRData until we have proper VPType.
  friend class VPVLSClientMemrefHIR;
  friend class VPlanCostModel;
  friend class VPlanCostModelProprietary;
  friend class VPlanDivergenceAnalysis;
  friend class VPlanIdioms;
  friend class VPlanVLSAnalysis;
  friend class VPlanVLSAnalysisHIR;
  friend class VPlanVerifier;
  friend class VPOCodeGen;
  friend class VPOCodeGenHIR;
  friend class VPCloneUtils;
  friend class VPValueMapper;
  friend class VPLoopEntityList;
  friend class VPValue;

  /// Hold all the HIR-specific data and interfaces for a VPInstruction.
  class HIRSpecifics {
    friend class VPValue;

  private:
    /// Return true if the underlying HIR data is valid. If it's a decomposed
    /// VPInstruction, the HIR of the attached master VPInstruction is checked.
    bool isValid() const {
      if (isMaster() || isDecomposed())
        return getVPInstData()->isValid();

      // For other VPInstructions without underlying HIR.
      assert(!isSet() && "HIR data must be unset!");
      return false;
    }

    /// Invalidate underlying HIR deta. If decomposed VPInstruction, the HIR of
    /// its master VPInstruction is invalidated.
    void invalidate() {
      if (isMaster() || isDecomposed())
        getVPInstData()->setInvalid();
    }

  public:
    HIRSpecifics() {}
    ~HIRSpecifics() {
      if (isMaster())
        delete getVPInstData();
    }

    // DESIGN PRINCIPLE: IR-independent algorithms don't need to know about
    // HIR-specific master, decomposed and new VPInstructions or underlying HIR
    // information. For that reason, access to the following HIR-specific
    // methods must be restricted. We achieve that goal by making
    // VPInstruction's HIRSpecifics member private.

    // Hold the underlying HIR information related to the LHS operand of this
    // VPInstruction.
    std::unique_ptr<VPOperandHIR> LHSHIROperand;

    // Used to save the symbase of the scalar memref corresponding to a
    // load/store instruction. Vector memref generated during vector CG is
    // assigned the same symbase.
    unsigned Symbase = loopopt::InvalidSymbase;

    /// Pointer to access the underlying HIR data attached to this
    /// VPInstruction, if any, depending on its sub-type:
    ///   1) Master VPInstruction: MasterData points to a VPInstDataHIR holding
    ///      the actual HIR data.
    ///   2) Decomposed VPInstruction: MasterData points to master VPInstruction
    ///      holding the actual HIR data.
    ///   3) Other VPInstruction (!Master and !Decomposed): MasterData is null.
    ///      We use a void pointer to represent this case.
    PointerUnion<MasterVPInstData *, VPInstruction *, void *> MasterData =
        (int *)nullptr;

    // Return the VPInstruction data of this VPInstruction if it's a master or
    // decomposed. Return nullptr otherwise.
    MasterVPInstData *getVPInstData() {
      if (isMaster())
        return MasterData.get<MasterVPInstData *>();
      if (isDecomposed())
        return getMaster()->HIR.getVPInstData();
      // New VPInstructions don't have VPInstruction data.
      return nullptr;
    }
    const MasterVPInstData *getVPInstData() const {
      return const_cast<HIRSpecifics *>(this)->getVPInstData();
    }

    void verifyState() const {
      if (MasterData.is<MasterVPInstData *>())
        assert(!MasterData.isNull() &&
               "MasterData can't be null for master VPInstruction!");
      else if (MasterData.is<VPInstruction *>())
        assert(!MasterData.isNull() &&
               "MasterData can't be null for decomposed VPInstruction!");
      else
        assert(MasterData.is<void *>() && MasterData.isNull() &&
               "MasterData must be null for VPInstruction that is not master "
               "or decomposed!");
    }

    /// Return true if this is a master VPInstruction.
    bool isMaster() const {
      verifyState();
      return MasterData.is<MasterVPInstData *>();
    }

    /// Return true if this is a decomposed VPInstruction.
    bool isDecomposed() const {
      verifyState();
      return MasterData.is<VPInstruction *>();
    }

    // Return true if MasterData contains actual HIR data.
    bool isSet() const {
      verifyState();
      return !MasterData.is<void *>();
    }

    /// Return the underlying HIR attached to this master VPInstruction. Return
    /// nullptr if the VPInstruction doesn't have underlying HIR.
    loopopt::HLNode *getUnderlyingNode() {
      MasterVPInstData *MastData = getVPInstData();
      if (!MastData)
        return nullptr;
      return MastData->getNode();
    }
    loopopt::HLNode *getUnderlyingNode() const {
      return const_cast<HIRSpecifics *>(this)->getUnderlyingNode();
    }

    /// Attach \p UnderlyingNode to this VPInstruction and turn it into a master
    /// VPInstruction.
    void setUnderlyingNode(loopopt::HLNode *UnderlyingNode) {
      assert(!isSet() && "MasterData is already set!");
      MasterData = new MasterVPInstData(UnderlyingNode);
    }

    /// Attach \p Def to this VPInstruction as its VPOperandHIR.
    void setOperandDDR(const loopopt::DDRef *Def) {
      assert(!LHSHIROperand && "LHSHIROperand is already set!");
      LHSHIROperand.reset(new VPBlob(Def));
    }

    /// Attach \p IVLevel to this VPInstruction as its VPOperandHIR.
    void setOperandIV(unsigned IVLevel) {
      assert(!LHSHIROperand && "LHSHIROperand is already set!");
      LHSHIROperand.reset(new VPIndVar(IVLevel));
    }

    /// Return the VPOperandHIR with the underlying HIR information of the LHS
    /// operand.
    VPOperandHIR *getOperandHIR() const { return LHSHIROperand.get(); }

    /// Return the master VPInstruction attached to a decomposed VPInstruction.
    VPInstruction *getMaster() {
      assert(isDecomposed() && "Only decomposed VPInstructions have a pointer "
                               "to a master VPInstruction!");
      return MasterData.get<VPInstruction *>();
    }
    VPInstruction *getMaster() const {
      return const_cast<HIRSpecifics *>(this)->getMaster();
    }

    /// Attach \p MasterVPI as master VPInstruction of a decomposed
    /// VPInstruction.
    void setMaster(VPInstruction *MasterVPI) {
      assert(MasterVPI && "Master VPInstruction cannot be set to null!");
      assert(!isMaster() &&
             "A master VPInstruction can't point to a master VPInstruction!");
      assert(!isSet() && "Master VPInstruction is already set!");
      MasterData = MasterVPI;
    }

    /// Mark the underlying HIR data as valid.
    void setValid() {
      assert(isMaster() && "Only a master VPInstruction must set HIR!");
      getVPInstData()->setValid();
    }

    /// Print HIR-specific flags. It's mainly for debugging purposes.
    void printHIRFlags(raw_ostream &OS) const {
      OS << "IsMaster=" << isMaster() << " IsDecomp=" << isDecomposed()
         << " IsNew=" << !isSet() << " HasValidHIR= " << isValid() << "\n";
    }

    void setSymbase(unsigned SB) { Symbase = SB; }
    unsigned getSymbase(void) const { return Symbase; }

    void cloneFrom(const HIRSpecifics &HIR) {
      if (HIR.isMaster()) {
        setUnderlyingNode(HIR.getUnderlyingNode());
        if (HIR.isValid())
          setValid();
      } else if (HIR.isDecomposed())
        setMaster(HIR.getMaster());

      // Copy the operand.
      if (VPOperandHIR *HIROperand = HIR.getOperandHIR()) {
        if (VPBlob *Blob = dyn_cast<VPBlob>(HIROperand))
          setOperandDDR(Blob->getBlob());
        else {
          VPIndVar *IV = cast<VPIndVar>(HIROperand);
          setOperandIV(IV->getIVLevel());
        }
      }
      setSymbase(HIR.getSymbase());

      // Verify correctness of the cloned HIR.
      assert(isMaster() == HIR.isMaster() &&
             "Cloned isMaster() value should be equal to the original one");
      assert(isDecomposed() == HIR.isDecomposed() &&
             "Cloned isDecomposed() value should be equal to the original one");
      assert(isSet() == HIR.isSet() &&
             "Cloned isSet() value should be equal to the original one");
      if (isSet())
        assert(HIR.isValid() == isValid() &&
               "Cloned isValid() value should be equal to the original one");
    }
  };
#endif // INTEL_CUSTOMIZATION

public:
#if INTEL_CUSTOMIZATION
  /// VPlan opcodes, extending LLVM IR with idiomatics instructions.
  enum {
      Not = Instruction::OtherOpsEnd + 1,
      AllZeroCheck,
      Pred,
      SMax,
      UMax,
      FMax,
      SMin,
      UMin,
      FMin,
      InductionInit,
      InductionInitStep,
      InductionFinal,
      ReductionInit,
      ReductionFinal,
      AllocatePrivate,
      Subscript,
  };
#else
  enum { Not = Instruction::OtherOpsEnd + 1 };
#endif

private:
  typedef unsigned char OpcodeTy;
  OpcodeTy Opcode;

  /// Each VPInstruction belongs to a single VPBasicBlock.
  VPBasicBlock *Parent = nullptr;

#if INTEL_CUSTOMIZATION
  // Hold the underlying HIR information, if any, attached to this
  // VPInstruction.
  HIRSpecifics HIR;

  void setSymbase(unsigned Symbase) {
    assert(Opcode == Instruction::Store ||
           Opcode == Instruction::Load &&
               "setSymbase called for invalid VPInstruction");
    assert(Symbase != loopopt::InvalidSymbase &&
           "Unexpected invalid symbase assignment");
    HIR.setSymbase(Symbase);
  }

  unsigned getSymbase(void) const {
    assert(Opcode == Instruction::Store ||
           Opcode == Instruction::Load &&
               "getSymbase called for invalid VPInstruction");
    assert(HIR.Symbase != loopopt::InvalidSymbase &&
           "Unexpected invalid symbase");
    return HIR.getSymbase();
  }
#endif

  /// Utility method serving execute(): generates a single instance of the
  /// modeled instruction.
  void generateInstruction(VPTransformState &State, unsigned Part);

  void copyUnderlyingFrom(const VPInstruction &Inst) {
#if INTEL_CUSTOMIZATION
    HIR.cloneFrom(Inst.HIR);
#endif // INTEL_CUSTOMIZATION
    Value *V = Inst.getUnderlyingValue();
    if (V)
      setUnderlyingValue(*V);
    if (!Inst.isUnderlyingIRValid())
      invalidateUnderlyingIR();
  }

#if INTEL_CUSTOMIZATION
protected:
  /// Return the underlying Instruction attached to this VPInstruction. Return
  /// null if there is no Instruction attached. This interface is similar to
  /// getValue() but it hides the cast when we are working with VPInstruction
  /// pointers.
  Instruction *getInstruction() const {
    assert((!getUnderlyingValue() || isa<Instruction>(getUnderlyingValue())) &&
           "Expected Instruction as underlying Value.");
    return cast_or_null<Instruction>(getUnderlyingValue());
  }

  /// Return true if this is a new VPInstruction (i.e., an VPInstruction that is
  /// not coming from the underlying IR.
  bool isNew() const { return getUnderlyingValue() == nullptr && !HIR.isSet(); }
#endif

  virtual VPInstruction *cloneImpl() const {
    VPInstruction *Cloned = new VPInstruction(Opcode, getType(), {});
    for (auto &O : operands()) {
      Cloned->addOperand(O);
    }
    return Cloned;
  }

public:
  VPInstruction(unsigned Opcode, Type *BaseTy, ArrayRef<VPValue *> Operands)
      : VPUser(VPValue::VPInstructionSC, Operands, BaseTy), Opcode(Opcode) {
    assert(BaseTy && "BaseTy can't be null!");
  }
  VPInstruction(unsigned Opcode, Type *BaseTy,
                std::initializer_list<VPValue *> Operands)
      : VPUser(VPValue::VPInstructionSC, Operands, BaseTy), Opcode(Opcode) {
    assert(BaseTy && "BaseTy can't be null!");
  }

  /// Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return V->getVPValueID() == VPValue::VPInstructionSC;
  }

  unsigned getOpcode() const { return Opcode; }
#if INTEL_CUSTOMIZATION
  // FIXME: Temporary workaround for TTI problems that make the cost
  // modeling incorrect. The getCMType() returns nullptr in case the underlying
  // instruction is not set and this makes the cost of this instruction
  // undefined (i.e. 0). Non-null return value causes calculation by TTI with
  // incorrect result.
  virtual Type *getCMType() const override {
    if (getUnderlyingValue())
      return getUnderlyingValue()->getType();

    if (!HIR.isMaster())
      return nullptr;

    const loopopt::HLNode *Node = HIR.getUnderlyingNode();
    const loopopt::HLInst *Inst = dyn_cast_or_null<loopopt::HLInst>(Node);

    if (!Inst)
      return nullptr;

    const Instruction *LLVMInst = Inst->getLLVMInstruction();
    if (!LLVMInst)
      return nullptr;

    return LLVMInst->getType();
  }

  // Return true if this VPInstruction represents a cast operation.
  bool isCast() const { return Instruction::isCast(getOpcode()); }

  bool mayHaveSideEffects() const;
#endif // INTEL_CUSTOMIZATION

  /// \return the VPBasicBlock which this VPInstruction belongs to.
  VPBasicBlock *getParent() { return Parent; }
  const VPBasicBlock *getParent() const { return Parent; }
  void setParent(VPBasicBlock *NewParent) { Parent = NewParent; }

  /// Generate the instruction.
  /// TODO: We currently execute only per-part unless a specific instance is
  /// provided.
  virtual void execute(VPTransformState &State);
#if INTEL_CUSTOMIZATION
  virtual void executeHIR(VPOCodeGenHIR *CG);
#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Dump the VPInstruction.
  void dump(raw_ostream &O) const override { dump(O, nullptr); };
  void dump(raw_ostream &O, const VPlanDivergenceAnalysis *DA) const;

  void dump() const override { dump(errs()); }
  void dump(const VPlanDivergenceAnalysis *DA) const { dump(dbgs(), DA); }
#endif // !NDEBUG || LLVM_ENABLE_DUMP
#endif

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Print the VPInstruction.
  virtual void print(raw_ostream &O, const Twine &Indent) const;

  /// Print the VPInstruction.
  void print(raw_ostream &O, const VPlanDivergenceAnalysis *DA = nullptr) const;
  static const char *getOpcodeName(unsigned Opcode);
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  VPInstruction *clone() const {
    VPInstruction *Cloned = cloneImpl();
    Cloned->copyUnderlyingFrom(*this);
    return Cloned;
  }
};

#if INTEL_CUSTOMIZATION
/// Concrete class for comparison. Represents both integers and floats.
class VPCmpInst : public VPInstruction {
public:
  typedef CmpInst::Predicate Predicate;
  /// Create VPCmpInst with its two operands, a pred and a BaseType.
  /// Operands \p LHS and \p RHS must not have conflicting base types.
  VPCmpInst(VPValue *LHS, VPValue *RHS, Predicate Pred)
      : VPInstruction(inferOpcodeFromPredicate(Pred),
                      CmpInst::makeCmpResultType(LHS->getType()),
                      ArrayRef<VPValue *>({LHS, RHS})),
        Pred(Pred) {}

  /// Return the predicate for this instruction
  Predicate getPredicate() const { return Pred; }

  /// Methods for supporting type inquiry through isa, cast, and
  /// dyn_cast:
  static bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::ICmp ||
           VPI->getOpcode() == Instruction::FCmp;
  }
  static bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  virtual VPCmpInst *cloneImpl() const final {
    return new VPCmpInst(getOperand(0), getOperand(1), getPredicate());
  }

private:
  Predicate Pred;

  // Return the opcode that corresponds to predicate
  unsigned inferOpcodeFromPredicate(Predicate Pred) {
    // Infer Opcode from Pred
    if (CmpInst::isIntPredicate(Pred))
      return Instruction::ICmp;
    if (CmpInst::isFPPredicate(Pred))
      return Instruction::FCmp;
    llvm_unreachable("Integer/Float predicate expected");
  }
};

/// Concrete class to represent branch instruction in VPlan.
class VPBranchInst : public VPInstruction {
public:
  explicit VPBranchInst(Type *BaseTy)
      : VPInstruction(Instruction::Br, BaseTy, {}) {}

  const loopopt::HLGoto *getHLGoto() const {
    return cast<loopopt::HLGoto>(HIR.getUnderlyingNode());
  }

  /// Return target block of unconditional VPBranchInst. If VPBranchInst was
  /// created for HLGoto, return its external target block or nullptr otherwise.
  const BasicBlock *getTargetBlock() const {
    if (getHLGoto())
      return getHLGoto()->getTargetBBlock();
    // TODO: LLVM-IR implementation is needed.
    return nullptr;
  }

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  void print(raw_ostream &O) const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  static inline bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::Br;
  }

  virtual VPBranchInst *cloneImpl() const final {
    return new VPBranchInst(getType());
  }
};

/// Concrete class for PHI instruction.
class VPPHINode : public VPInstruction {
private:
  SmallVector<VPBasicBlock *, 2> VPBBUsers;

  // Used to indicate that this PHI needs to be blended using selects during
  // vector code generation. This can be removed once we make the phi->select
  // transformation explicit in VPlan.
  bool Blend = false;

public:
  void setBlend(bool B) { Blend = B; }
  bool getBlend() const { return Blend; }
  /// Sort the incoming blocks of the blend phi according to their execution
  /// order in the linearized CFG. Required to be performed prior to code
  /// generation for the blend phis.
  ///
  /// \p BlockIndexInRPOTOrNull is an optional parameter with the mapping of the
  /// blocks in \p this phi's parent region to that blocks' RPOT numbers. If not
  /// provided, it will be calculated inside the method.
  //
  // TODO: As an optimization, the sorting can be done once per block, but that
  // should be done at the caller side complicating the code.
  void sortIncomingBlocksForBlend(
      DenseMap<const VPBasicBlock *, int> *BlockIndexInRPOTOrNull = nullptr);

  using vpblock_iterator = SmallVectorImpl<VPBasicBlock *>::iterator;
  using const_vpblock_iterator =
      SmallVectorImpl<VPBasicBlock *>::const_iterator;

  explicit VPPHINode(Type *BaseTy)
      : VPInstruction(Instruction::PHI, BaseTy, ArrayRef<VPValue *>()) {}

  vpblock_iterator block_begin(void) { return VPBBUsers.begin(); }

  vpblock_iterator block_end(void) { return VPBBUsers.end(); }

  const_vpblock_iterator block_begin(void) const {
    return VPBBUsers.begin();
  }

  const_vpblock_iterator block_end(void) const {
    return VPBBUsers.end();
  }

  iterator_range<vpblock_iterator> blocks(void) {
    return make_range(block_begin(), block_end());
  }

  iterator_range<const_vpblock_iterator> blocks(void) const {
    return make_range(block_begin(), block_end());
  }

  operand_range incoming_values(void) { return operands(); }
  const_operand_range incoming_values(void) const { return operands(); }

  /// Return number of incoming values.
  unsigned getNumIncomingValues(void) const { return getNumOperands(); }

  /// Return VPValue that corresponds to Idx'th incomming VPBasicBlock.
  VPValue *getIncomingValue(const unsigned Idx) const { return getOperand(Idx); }

  /// Return VPValue that corresponds to incomming VPBB.
  VPValue *getIncomingValue(const VPBasicBlock *VPBB) const {
    auto Idx = getBlockIndexOrNone(VPBB);
    assert(Idx && "Cannot find value for a given BB.");
    return getIncomingValue(Idx.getValue());
  }

  void setIncomingValue(const unsigned Idx, VPValue *Value) {
    assert(Value && "Value must not be null.");
    setOperand(Idx, Value);
  }

  /// Set the incoming value for a specific basic block.
  void setIncomingValue(VPBasicBlock *VPBB, VPValue *Value) {
    assert(Value && VPBB && "Value and VPBB must not be null.");
    auto Idx = getBlockIndex(VPBB);
    assert(Idx >= 0 && "VPBB should have a valid index.");
    setIncomingValue(Idx, Value);
  }

  /// Add an incoming value to the end of the PHI list
  void addIncoming(VPValue *Value, VPBasicBlock *BB) {
    assert(Value && "Value must not be null.");
    assert(BB && "Block must not be null.");
    addOperand(Value);
    VPBBUsers.push_back(BB);
  }

  /// Return incoming basic block number \p Idx.
  VPBasicBlock *getIncomingBlock(const unsigned Idx) const {
    return VPBBUsers[Idx];
  }

  /// Return incoming basic block corresponding to \p Value.
  VPBasicBlock *getIncomingBlock(const VPValue *Value) const {
    auto It = llvm::find(make_range(op_begin(), op_end()), Value);
    return getIncomingBlock(std::distance(op_begin(), It));
  }

  /// Set incoming basic block as \p Block corresponding to basic block number
  /// \p Idx.
  void setIncomingBlock(const unsigned Idx, VPBasicBlock *BB) {
    assert(BB && "VPPHI node got a null basic block");
    VPBBUsers[Idx] = BB;
  }

  /// Remove a block from the phi node.
  void removeIncomingValue(const VPBasicBlock *BB) {
    unsigned Idx = getBlockIndex(BB);
    assert(Idx <= VPBBUsers.size() && "Index is out of range");
    auto it = VPBBUsers.begin();
    std::advance(it, Idx);
    VPBBUsers.erase(it);
    removeOperand(Idx);
  }

  /// Return index for a given \p Block.
  int getBlockIndex(const VPBasicBlock *BB) const {
    auto It = llvm::find(make_range(block_begin(), block_end()), BB);
    if (It != block_end())
      return std::distance(block_begin(), It);
    return -1;
  }

  /// Return Optional index for a given basic block \p Block.
  Optional<unsigned> getBlockIndexOrNone(const VPBasicBlock *BB) const {
    int Idx = getBlockIndex(BB);
    return Idx != -1 ? Optional<unsigned>(Idx) : None;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == Instruction::PHI;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  /// Create new PHINode and copy original incoming values to the newly created
  /// PHINode. Caller is responsible to replace these values with what is
  /// needed.
  virtual VPPHINode *cloneImpl() const final {
    VPPHINode *Cloned = new VPPHINode(getType());
    for (unsigned i = 0, e = getNumIncomingValues(); i != e; ++i) {
      Cloned->addIncoming(getIncomingValue(i), getIncomingBlock(i));
    }
    return Cloned;
  }

  /// Checks whether the specified PHI node always merges together the same
  /// value, assuming that undefs result in the same value as non-undefs.
  /// Adapted from the LLVM-source PHINode::hasConstantOrUndefValue().
  bool hasConstantOrUndefValue() const {
    VPValue *ConstantValue = nullptr;
    for (int I = 0, E = getNumIncomingValues(); I != E; ++I) {
      VPValue *Incoming = getIncomingValue(I);
      const VPConstant *ConstOp = dyn_cast<VPConstant>(Incoming);
      bool IsUndef = ConstOp && isa<UndefValue>(ConstOp->getConstant());
      if (Incoming != this && !IsUndef) {
        if (ConstantValue && ConstantValue != Incoming)
          return false;
        ConstantValue = Incoming;
      }
    }
    return true;
  }
};

/// Concrete class to represent GEP instruction in VPlan.
class VPGEPInstruction : public VPInstruction {
  friend class VPlanVerifier;

private:
  // Trailing struct offsets for a GEP are tracked via a vector of bools called
  // OperandIsStructOffset. This vector will always be consistent with the
  // number of operands stored in a given GEP instruction. An operand's
  // corresponding index entry in OperandIsStructOffset is set to true if it's a
  // trailing struct offset, false otherwise. NOTE: This information is needed
  // only if the GEP is created via HIR-path. For LLVM-IR path the vector will
  // always be false for all entries.
  //
  // Examples:
  // 1. (@a)[0].1[i1] --> gep %a, 0, 1, %vpi1
  //
  //    Vector will look like:
  //    <false, false, true, false>
  //
  //    i.e. operand at index 2 is a struct offset corresponding to the previous
  //    last-found non struct offset operand (index 1).
  //
  // 2. (@a)[0].2.1[5][i1] --> gep %a, 0, 2, 1, 5, %vpi1
  //
  //    Vector will look like:
  //    <false, false, true, true, false, false>
  //

  bool InBounds;
  SmallVector<bool, 4> OperandIsStructOffset;

public:
  /// Default constructor for VPGEPInstruction. The default value for \p
  /// InBounds is false.
  VPGEPInstruction(Type *BaseTy, VPValue *Ptr, ArrayRef<VPValue *> IdxList,
                   bool InBounds = false)
      : VPInstruction(Instruction::GetElementPtr, BaseTy, {}),
        InBounds(InBounds) {
    assert(Ptr && "Base pointer operand of GEP cannot be null.");
    // Base pointer should be the first operand of GEP followed by index
    // operands
    assert(!getNumOperands() &&
           "GEP instruction already has operands before base pointer.");
    VPInstruction::addOperand(Ptr);
    for (auto Idx : IdxList)
      VPInstruction::addOperand(Idx);
    // Track all operands as non struct offsets, since that information is not
    // available at this point.
    OperandIsStructOffset.resize(1 + IdxList.size(), false);
  }

  /// Setter and getter functions for InBounds.
  void setIsInBounds(bool IsInBounds) { InBounds = IsInBounds; }
  bool isInBounds() const { return InBounds; }

  /// Get the base pointer operand of given VPGEPInstruction.
  VPValue *getPointerOperand() const { return VPInstruction::getOperand(0); }

  /// Overloaded method for adding an operand \p Operand. The struct offset
  /// tracker is accordingly updated after operand addition.
  void addOperand(VPValue *Operand, bool IsStructOffset = false) {
    VPInstruction::addOperand(Operand);
    OperandIsStructOffset.push_back(IsStructOffset);
    assert(OperandIsStructOffset.size() == getNumOperands() &&
           "Number of operands and struct offset tracker sizes don't match.");
  }

  /// Overloaded method for setting index \p Idx with operand \p Operand. The
  /// struct offset tracker is accordingly updated after operand is set.
  void setOperand(const unsigned Idx, VPValue *Operand,
                  bool IsStructOffset = false) {
    assert((Idx > 1 || !IsStructOffset) &&
           "Base pointer and first index operand of GEP cannot be a struct "
           "offset.");
    VPInstruction::setOperand(Idx, Operand);
    OperandIsStructOffset[Idx] = IsStructOffset;
  }

  /// Overloaded method for removing an operand at index \p Idx. The struct
  /// offset tracker is accordingly updated after operand removal.
  void removeOperand(const unsigned Idx) {
    VPInstruction::removeOperand(Idx);
    OperandIsStructOffset.erase(OperandIsStructOffset.begin() + Idx);
    assert(OperandIsStructOffset.size() == getNumOperands() &&
           "Number of operands and struct offset tracker sizes don't match.");
  }

  /// Check if a given operand \p Operand of this GEP is a struct offset.
  bool isOperandStructOffset(VPValue *Operand) const {
    auto It = llvm::find(operands(), Operand);
    assert(It != op_end() && "Operand not found in VPGEPInstruction.");
    return OperandIsStructOffset[std::distance(op_begin(), It)];
  }

  /// Check if operand at index \p Idx of this GEP is a struct offset.
  bool isOperandStructOffset(const unsigned Idx) const {
    assert(Idx < OperandIsStructOffset.size() &&
           "Operand index out of bounds.");
    return OperandIsStructOffset[Idx];
  }

  /// Methods for supporting type inquiry through isa, cast and dyn_cast:
  static bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::GetElementPtr;
  }

  static bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  virtual VPGEPInstruction *cloneImpl() const final {
    VPGEPInstruction *Cloned =
        new VPGEPInstruction(getType(), getOperand(0), {}, isInBounds());
    for (auto *O : make_range(op_begin()+1, op_end())) {
      Cloned->addOperand(O, isOperandStructOffset(O));
    }
    return Cloned;
  }
};

/// Concrete class to represent a single or multi-dimensional array access
/// implemented using llvm.intel.subscript intrinsic calls in VPlan.
// Consider the following two examples:
//
// 1. Single-dimension access
//
//    If the incoming IR contains:
//    %0 = llvm.intel.subscript...(i8 %Rank, i32 %Lower, i32 %Stride,
//                                 i32* %Base, i32 %Idx)
//      (or)
//    %0 = @(%Base)[%Idx]
//
//    The corresponding VPSubscriptInst in VPlan looks like:
//    i32* %vp0 = subscript i32* %Base, i32 %Lower, i32 %Stride, i32 %Idx
//
//    where,
//     getRank(0) = %Rank
//     getLower(0) = %Lower
//     getStride(0) = %Stride
//     getIndex(0) = %Idx
//
// 2. Combined multi-dimensional access
//
//    If the incoming IR contains:
//    %0 = llvm.intel.subscript...(i8 1, i32 %L1, i32 %S1, i32* %Base, i32 %I1)
//    %1 = llvm.intel.subscript...(i8 0, i32 %L0, i32 %S0, i32* %0, i32 %I0)
//      (or)
//    %1 = @(%Base)[%I1][%I0]
//
//    The corresponding VPSubscriptInst in VPlan for combined access looks like:
//    i32* %vp1 = subscript i32* %Base, i32 %L1, i32 %S1, i32 %I1,
//                                      i32 %L0, i32 %S0, i32 %I0
//
//    where,
//     getRank(1) = 1
//     getLower(1) = %L1
//     getStride(1) = %S1
//     getIndex(1) = %I1
//    and,
//     getRank(0) = 0
//     getLower(0) = %L0
//     getStride(0) = %S0
//     getIndex(0) = %I0
class VPSubscriptInst final : public VPInstruction {
public:
  using DimStructOffsetsMapTy = DenseMap<unsigned, SmallVector<unsigned, 4>>;

private:
  // TODO: Below SmallVectors are currently assuming that dimensions are added
  // in a fixed order i.e. outer-most dimension to inner-most dimension. To
  // support flexibility in this ordering, DenseMaps will be needed. For
  // example, for a 3-dimensional array access, Ranks = < 2, 1, 0 >.
  SmallVector<unsigned, 4> Ranks;
  // Struct offsets associated with each dimension of this array access. For
  // example, incoming HIR contains:
  // %1 = @(%Base)[%I1].0.1[%I2].0[%I3]
  //
  // then the map below will have following entries:
  // Dim  --->  Offsets
  //  2         {0, 1}
  //  1          {0}
  //  0          {}
  //
  // NOTE: This information is needed only if the subscript instruction
  // is created via HIR-path. For LLVM-IR path the map will always be empty.
  DimStructOffsetsMapTy DimStructOffsets;

  /// Add a new dimension to represent the array access for this subscript
  /// instruction.
  void addDimension(unsigned Rank, VPValue *Lower, VPValue *Stride,
                    VPValue *Index) {
    if (Ranks.size() > 0) {
      assert(Rank == Ranks.back() - 1 &&
             "Dimension is being added in out-of-order fashion.");
    }
    assert(getNumOperands() > 0 &&
           "Dimension is being added without base pointer.");
    Ranks.push_back(Rank);
    addOperand(Lower);
    addOperand(Stride);
    addOperand(Index);
  }

  unsigned getDimensionArrayIndex(unsigned Dim) const {
    assert(Dim < getNumDimensions() && "Invalid dimension.");
    unsigned Idx = getNumDimensions() - 1 - Dim;
    return Idx;
  }

  /// Explicit copy constructor to copy the operand list, Ranks and struct
  /// offsets map.
  VPSubscriptInst(const VPSubscriptInst &Other)
      : VPInstruction(VPInstruction::Subscript, Other.getType(), {}) {
    for (auto *Op : Other.operands())
      addOperand(Op);
    Ranks = Other.Ranks;
    DimStructOffsets = Other.DimStructOffsets;
  }

public:
  /// Constructor to allow clients to create a VPSubscriptInst with a single
  /// dimension only. Type of the instruction will be same as the type of the
  /// base pointer operand.
  VPSubscriptInst(unsigned Rank, VPValue *Lower, VPValue *Stride, VPValue *Base,
                  VPValue *Index)
      : VPInstruction(VPInstruction::Subscript, Base->getType(),
                      {Base, Lower, Stride, Index}) {
    Ranks.push_back(Rank);
  }

  /// Constructor to create a VPSubscriptInst that represents a combined
  /// multi-dimensional array access. The fields are expected to be ordered from
  /// highest-dimension to lowest-dimension.
  VPSubscriptInst(unsigned NumDims, ArrayRef<VPValue *> Lowers,
                  ArrayRef<VPValue *> Strides, VPValue *Base,
                  ArrayRef<VPValue *> Indices)
      : VPInstruction(VPInstruction::Subscript, Base->getType(), {Base}) {
    assert((Lowers.size() == NumDims && Strides.size() == NumDims &&
            Indices.size() == NumDims) &&
           "Inconsistent parameters for multi-dimensional subscript access.");
    for (unsigned I = 0; I < NumDims; ++I) {
      unsigned Rank = NumDims - 1 - I;
      addDimension(Rank, Lowers[I], Strides[I], Indices[I]);
    }
  }

  /// Constructor to create a VPSubscriptInst that represents a combined
  /// multi-dimensional array access when each dimension has corresponding
  /// struct offsets. The fields are expected to be ordered from
  /// highest-dimension to lowest-dimension.
  VPSubscriptInst(unsigned NumDims, ArrayRef<VPValue *> Lowers,
                  ArrayRef<VPValue *> Strides, VPValue *Base,
                  ArrayRef<VPValue *> Indices,
                  DimStructOffsetsMapTy StructOffsets)
      : VPSubscriptInst(NumDims, Lowers, Strides, Base, Indices) {
    DimStructOffsets = std::move(StructOffsets);
  }

  /// Get trailing struct offsets for given dimension.
  ArrayRef<unsigned> getStructOffsets(unsigned Dim) const {
    auto Iter = DimStructOffsets.find(Dim);
    if (Iter != DimStructOffsets.end())
      return Iter->second;
    return ArrayRef<unsigned>(None);
  }

  /// Get number of dimensions associated with this array access.
  unsigned getNumDimensions() const { return Ranks.size(); }

  /// Get the rank for given dimension. For multi-dimensional accesses this
  /// should be trivial as Dimension == Rank.
  unsigned getRank(unsigned Dim) const {
    return Ranks[getDimensionArrayIndex(Dim)];
  }
  /// Get the lower bound of index for given dimension.
  VPValue *getLower(unsigned Dim) const {
    unsigned OpIdx = (getDimensionArrayIndex(Dim) * 3) + 1;
    return getOperand(OpIdx);
  }
  /// Get the stride of access for given dimension.
  VPValue *getStride(unsigned Dim) const {
    unsigned OpIdx = (getDimensionArrayIndex(Dim) * 3) + 2;
    return getOperand(OpIdx);
  }
  /// Get the base pointer operand.
  VPValue *getPointerOperand() const { return getOperand(0); }
  /// Get the index operand for given dimension.
  VPValue *getIndex(unsigned Dim) const {
    unsigned OpIdx = (getDimensionArrayIndex(Dim) * 3) + 3;
    return getOperand(OpIdx);
  }

  /// Methods for supporting type inquiry through isa, cast and dyn_cast:
  static bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == VPInstruction::Subscript;
  }

  static bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  virtual VPSubscriptInst *cloneImpl() const {
    VPSubscriptInst *Cloned = new VPSubscriptInst(*this);
    return Cloned;
  }
};

// VPInstruction to initialize vector for induction variable.
// It's initialized depending on the binary operation,
// For +/-   : broadcast(start) + step*{0, 1,..,VL -1}
// For */div : broadcast(start) * pow(step,{0, 1,..,VL -1})
// Other binary operations are not induction-compatible.
class VPInductionInit : public VPInstruction {
public:
  VPInductionInit(VPValue *Start, VPValue *Step, Instruction::BinaryOps Opc)
      : VPInstruction(VPInstruction::InductionInit, Start->getType(),
                      {Start, Step}),
        BinOpcode(Opc) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionInit;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

  VPValue *getStep() const { return getOperand(1); }

private:
  Instruction::BinaryOps BinOpcode;
};

// VPInstruction to initialize vector for induction step.
// It's initialized depending on the binary operation,
// For +/-   : broadcast(step*VL)
// For */div : broadcast(pow(step,VL))
// Other binary operations are not induction-compatible.
class VPInductionInitStep : public VPInstruction {
public:
  VPInductionInitStep(VPValue *Step, Instruction::BinaryOps Opcode)
      : VPInstruction(VPInstruction::InductionInitStep, Step->getType(),
                      {Step}),
        BinOpcode(Opcode) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionInitStep;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }
  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

private:
  Instruction::BinaryOps BinOpcode = Instruction::BinaryOpsEnd;
};

// VPInstruction for induction last value calculation.
// It's calculated depending on the binary operation,
// For +/-   : lv = start OP step*count
// For */div : lv = start OP pow(step, count)
// Other binary operations are not induction-compatible.
//
// We should choose the optimal way for that - probably, for mul/div we should
// prefer scalar calculations in the loop body or extraction from the final
// vector.
class VPInductionFinal : public VPInstruction {
public:
  /// Constructor to extract last lane (for */div).
  VPInductionFinal(VPValue *InducVec)
      : VPInstruction(VPInstruction::InductionFinal, InducVec->getType(),
                      {InducVec}) {}

  /// Constructor to calculate using close-form (start+step*rounded_tc). The
  /// rounded trip count is known at code generation.
  VPInductionFinal(VPValue *Start, VPValue *Step, Instruction::BinaryOps Opcode)
      : VPInstruction(VPInstruction::InductionFinal, Start->getType(),
                      {Start, Step}),
        BinOpcode(Opcode) {}

  /// Return operand that corresponds to the reducing value.
  VPValue *getInductionOperand() const {
    assert(getNumOperands() == 1 && "Incorrect operand request");
    return getOperand(0);
  }

  /// Return operand that corresponds to the start value.
  VPValue *getStartValueOperand() const {
    assert(getNumOperands() == 2 && "Incorrect operand request");
    return getOperand(0);
  }

  /// Return operand that corresponds to the step value.
  VPValue *getStepOperand() const {
    assert(getNumOperands() == 2 && "Incorrect operand request");
    return getOperand(1);
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionFinal;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

private:
  Instruction::BinaryOps BinOpcode = Instruction::BinaryOpsEnd;
};

// VPInstruction for reduction initialization.
// It can be done in two ways and should be aligned with last value
// calculation  The first way is just broadcast(identity), the second one is
// to calculate vector of {start_value, identity,...,identity}. The second way
// is acceptable for some reductions (+,-,*) and allows eliminating one scalar
// operation during last value calculation. Though, that is ineffective for
// multiplication, while for summation the movd/movq x86 instructions
// perfectly fit this way.
//
class VPReductionInit : public VPInstruction {
public:
  VPReductionInit(VPValue *Identity)
      : VPInstruction(VPInstruction::ReductionInit, Identity->getType(),
                      {Identity}) {}

  VPReductionInit(VPValue *Identity, VPValue *StartValue)
      : VPInstruction(VPInstruction::ReductionInit, Identity->getType(),
                      {Identity, StartValue}) {}

  /// Return operand that corresponds to the indentity value.
  VPValue *getIdentityOperand() const { return getOperand(0);}

  /// Return operand that corresponds to the start value. Can be nullptr for
  /// optimized reduce.
  VPValue *getStartValueOperand() const {
    return getNumOperands() > 1 ? getOperand(1) : nullptr;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::ReductionInit;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }
};

// VPInstruction for reduction last value calculation.
// It's calculated depending on the binary operation. A general sequence
// is generated:
// v_tmp = shuffle(value,m1) // v_tmp contains the upper half of value
// v_tmp = value OP v_tmp;
// v_tmp2 = shuffle(v_tmp, m2) // v_tmp2 contains the upper half of v_tmp1
// v_tmp2 = v_tmp1 OP v_tmp2;
// ...
// res = (is_minmax || optimized_plus) ? vtmp_N : start OP vtp_N;
// (For minmax and optimized summation (see VPReductionInit) we don't
// need operation in the last step.)
//
// A special way is required for min/max+index reductions. The index
// part of the reduction has a link to the main, min/max, part and code
// generation for it requires two values of the main part, the last vector
// value and the last scalar value. They can be accesses having a link
// to the main instruction, which is passed as an additional argument to
// the index part.
//
class VPReductionFinal : public VPInstruction {
public:
  /// General constructor
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec, VPValue *StartValue,
                   bool Sign)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec, StartValue}),
        BinOpcode(BinOp), Signed(Sign) {}

  /// Constructor for optimized summation
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec}),
        BinOpcode(BinOp), Signed(false) {}

  /// Constructor for index part of min/max+index reduction.
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec, VPValue *ParentExit,
                   VPReductionFinal *ParentFinal, bool Sign)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec, ParentExit, ParentFinal}),
        BinOpcode(BinOp), Signed(Sign) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::ReductionFinal;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  unsigned getBinOpcode() const { return BinOpcode; }

  bool isSigned() const { return Signed; }

  /// Return operand that corresponds to the reducing value.
  VPValue *getReducingOperand() const { return getOperand(0);}

  /// Return operand that corresponds to the start value. Can be nullptr for
  /// optimized reduce.
  VPValue *getStartValueOperand() const {
    return getNumOperands() == 2 ? getOperand(1) : nullptr;
  }

  /// Return operand that corrresponds to min/max parent vector value.
  VPValue *getParentExitValOperand() const {
    return getNumOperands() == 3 ? getOperand(1) : nullptr;
  }

  /// Return operand that corrresponds to min/max parent final value.
  VPValue *getParentFinalValOperand() const {
    return getNumOperands() == 3 ? getOperand(2) : nullptr;
  }

  /// Return true if this instruction is for last value calculation of an index
  /// part of min/max+index idiom.
  bool isMinMaxIndex() const {
    return getParentExitValOperand() != nullptr;
  }

  /// Return ID of the corresponding reduce intrinsic.
  Intrinsic::ID getVectorReduceIntrinsic() const {
    switch (BinOpcode) {
    case Instruction::Add:
    case Instruction::Sub:
      return Intrinsic::experimental_vector_reduce_add;
    case Instruction::FAdd:
    case Instruction::FSub:
      return Intrinsic::experimental_vector_reduce_v2_fadd;
    case Instruction::Mul:
      return Intrinsic::experimental_vector_reduce_mul;
    case Instruction::FMul:
      return Intrinsic::experimental_vector_reduce_v2_fmul;
    case Instruction::And:
      return Intrinsic::experimental_vector_reduce_and;
    case Instruction::Or:
      return Intrinsic::experimental_vector_reduce_or;
    case Instruction::Xor:
      return Intrinsic::experimental_vector_reduce_xor;
    case VPInstruction::UMin:
      return Intrinsic::experimental_vector_reduce_umin;
    case VPInstruction::SMin:
      return Intrinsic::experimental_vector_reduce_smin;
    case VPInstruction::UMax:
      return Intrinsic::experimental_vector_reduce_umax;
    case VPInstruction::SMax:
      return Intrinsic::experimental_vector_reduce_smax;
    case VPInstruction::FMax:
      return Intrinsic::experimental_vector_reduce_fmax;
    case VPInstruction::FMin:
      return Intrinsic::experimental_vector_reduce_fmin;
    default:
      llvm_unreachable("Vector reduction opcode not supported.");
    }
  }

private:
  unsigned BinOpcode;
  bool Signed;
};

// VPInstruction to allocate private memory. This is translated into
// allocation of a private memory in the function entry block. This instruction
// is not supposed to vectorize alloca instructions that appear inside the loop
// for arrays of a variable size.
class VPAllocatePrivate : public VPInstruction {
public:
  VPAllocatePrivate(Type *Ty)
      : VPInstruction(VPInstruction::AllocatePrivate, Ty, {}), IsSOASafe(false),
        IsSOAProfitable(false) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::AllocatePrivate;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  /// Return true if doing SOA-layout transformation for the given memory is
  /// both safe and profitable.
  bool isSOALayout() const { return IsSOASafe && IsSOAProfitable; }

  /// Return true if memory is safe for SOA, i.e. all uses inside the loop
  /// are known and there are no layout-casts.
  bool isSOASafe() const { return IsSOASafe; }

  /// Return true if it's profitable to do SOA transformation, i.e. there
  /// is at least one uniform/unit-stride load/store to that memory (in case of
  /// private array), or the memory is a scalar structure
  bool isSOAProfitable() const { return IsSOAProfitable; }

  /// Set the property of the memory to be SOA-safe.
  void setSOASafe() { IsSOASafe = true; }

  /// Set the memory to be profitable for SOA-layout.
  void setSOAProfitable() { IsSOAProfitable = true; }

private:
  bool IsSOASafe;
  bool IsSOAProfitable;
};
#endif // INTEL_CUSTOMIZATION

/// VPlan models a candidate for vectorization, encoding various decisions take
/// to produce efficient output IR, including which branches, basic-blocks and
/// output IR instructions to generate, and their cost.
class VPlan {
  friend class VPlanPrinter;

private:
  LLVMContext *Context = nullptr;
  std::unique_ptr<VPLoopInfo> VPLInfo;
  std::unique_ptr<VPlanDivergenceAnalysis> VPlanDA;
  const DataLayout *DL = nullptr;

  /// Holds the entry vpbasicblock of the Plan.
  VPBasicBlock *EntryBB = nullptr;

  /// Holds the exit vpbasicblock of the Plan.
  VPBasicBlock *ExitBB = nullptr;

  /// Holds the number of VPBasicBlocks within the Plan. It is necessary for
  /// dominator tree.
  unsigned Size = 0;

  /// Traverse all the region VPBasicBlocks to recompute Size
  void recomputeSize();

  /// Dominator Tree for the Plan.
  std::unique_ptr<VPDominatorTree> PlanDT;

  /// Post-Dominator Tree for the Plan.
  std::unique_ptr<VPPostDominatorTree> PlanPDT;

  // We need to force full linearization for certain cases. Currently this
  // happens for cases where while-loop canonicalization or merge loop exits
  // transformation break SSA or for HIR vector code generation which needs
  // to be extended to preserve uniform control flow. This flag is set to true
  // when we need to force full linearization. Full linearization can still
  // kick in when this flag is false such as cases where we use a command
  // line option to do the same.
  bool FullLinearizationForced = false;

  // The flag shows whether all steps needed for the loop entities privatization
  // are finished. Particularly, all VPInstructions for private memory
  // allocation are generated and the needed replacements in the VPLoop code are
  // done.
  bool LoopEntitiesPrivatizationIsDone = false;

  /// Holds the name of the VPlan, for printing.
  std::string Name;

  /// Holds all the VPConstants created for this VPlan.
  DenseMap<Constant *, std::unique_ptr<VPConstant>> VPConstants;

  /// Holds all the external definitions representing an underlying Value
  /// in this VPlan. CFGBuilder ensures these are unique.
  SmallVector<std::unique_ptr<VPExternalDef>, 16> VPExternalDefs;

  /// Holds all the external definitions representing an HIR underlying entity
  /// in this VPlan. The hash is based on the underlying HIR information that
  /// uniquely identifies each external definition.
  FoldingSet<VPExternalDef> VPExternalDefsHIR;

  /// Holds all the external uses in this VPlan representing an underlying
  /// Value. The key is the underlying Value that uniquely identifies each
  /// external use.
  DenseMap<Value *, std::unique_ptr<VPExternalUse>> VPExternalUses;

  /// Holds all the external uses representing an HIR underlying entity
  /// in this VPlan. The key is the underlying HIR information that uniquely
  /// identifies each external use.
  FoldingSet<VPExternalUse> VPExternalUsesHIR;

  /// Holds all the VPMetadataAsValues created for this VPlan.
  DenseMap<MetadataAsValue *, std::unique_ptr<VPMetadataAsValue>>
      VPMetadataAsValues;

  DenseMap<const VPLoop *, std::unique_ptr<VPLoopEntityList>> LoopEntities;

  // Holds the instructions that need to be deleted by VPlan's destructor.
  SmallVector<std::unique_ptr<VPInstruction>, 2> UnlinkedVPInsns;

public:
  VPlan(LLVMContext *Context, const DataLayout *DL);

  ~VPlan();

  /// Generate the IR code for this VPlan.
  void execute(struct VPTransformState *State);
#if INTEL_CUSTOMIZATION
  void executeHIR(VPOCodeGenHIR *CG);
#endif // INTEL_CUSTOMIZATION

  VPLoopInfo *getVPLoopInfo() { return VPLInfo.get(); }

  const VPLoopInfo *getVPLoopInfo() const { return VPLInfo.get(); }

  void setVPLoopInfo(std::unique_ptr<VPLoopInfo> VPLI) {
    VPLInfo = std::move(VPLI);
  }

  void setVPlanDA(std::unique_ptr<VPlanDivergenceAnalysis> VPDA) {
    VPlanDA = std::move(VPDA);
  }

  LLVMContext *getLLVMContext(void) const { return Context; }

  VPlanDivergenceAnalysis *getVPlanDA() const { return VPlanDA.get(); }

  void markFullLinearizationForced() { FullLinearizationForced = true; }
  bool isFullLinearizationForced() const { return FullLinearizationForced; }

  const DataLayout *getDataLayout() const { return DL; }

  /// Return an existing or newly created LoopEntities for the loop \p L.
  VPLoopEntityList *getOrCreateLoopEntities(const VPLoop *L) {
    // Sanity check
    VPBasicBlock *HeaderBB = L->getHeader(); (void)HeaderBB;
    assert(VPLInfo->getLoopFor(HeaderBB) == L &&
           "the loop does not exist in VPlan");

    std::unique_ptr<VPLoopEntityList> &Ptr = LoopEntities[L];
    if (!Ptr) {
      VPLoopEntityList *E =
          new VPLoopEntityList(*this, *(const_cast<VPLoop *>(L)));
      Ptr.reset(E);
    }
    return Ptr.get();
  }

  /// Return LoopEntities list for the loop \p L. The nullptr is returned if
  /// the descriptors were not created for the loop.
  const VPLoopEntityList *getLoopEntities(const VPLoop *L) const {
    auto Iter = LoopEntities.find(L);
    if (Iter == LoopEntities.end())
      return nullptr;
    return Iter->second.get();
  }

  unsigned getSize() const { return Size; }

  void setSize(unsigned Sz) { Size = Sz; }

  /// Getters for Dominator Tree
  VPDominatorTree *getDT() { return PlanDT.get(); }
  const VPDominatorTree *getDT() const { return PlanDT.get(); }
  /// Getter for Post-Dominator Tree
  VPPostDominatorTree *getPDT() { return PlanPDT.get(); }

  /// Compute the Dominator Tree for this Plan.
  void computeDT();

  /// Compute the Post-Dominator Tree for this Plan.
  void computePDT();

  VPBasicBlock *getEntryBlock() { return EntryBB; }
  const VPBasicBlock *getEntryBlock() const { return EntryBB; }

  void setEntryBlock(VPBasicBlock *BB) { EntryBB = BB; }

  VPBasicBlock *getExitBlock() { return ExitBB; }

  const VPBasicBlock *getExitBlock() const { return ExitBB; }

  void setExitBlock(VPBasicBlock *BB) { ExitBB = BB; }

  // TODO: This is weird. For some reason, DominatorTreeBase is using
  // A->getParent()->front() instead of using GraphTraints::getEntry. We may
  // need to report it.
  VPBasicBlock &front() const { return *EntryBB; }

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Print (in text format) VPlan blocks in order based on dominator tree.
  void dump(raw_ostream &OS, bool DumpDA = false) const;
  void dump() const;
  void dumpLivenessInfo(raw_ostream &OS) const;
  void print(raw_ostream &OS, unsigned Indent, bool DumpDA) const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  const std::string &getName() const { return Name; }

  void setName(const Twine &newName) { Name = newName.str(); }

  /// Create a new VPConstant for \p Const if it doesn't exist or retrieve the
  /// existing one.
  VPConstant *getVPConstant(Constant *Const) {
    std::unique_ptr<VPConstant> &UPtr = VPConstants[Const];
    if (!UPtr)
      // Const is a new VPConstant to be inserted in the map.
      UPtr.reset(new VPConstant(Const));

    return UPtr.get();
  }

  /// Create or retrieve a VPExternalDef for a given Value \p ExtVal.
  VPExternalDef *getVPExternalDef(Value *ExtDef) {
    VPExternalDefs.emplace_back(new VPExternalDef(ExtDef));
    return VPExternalDefs.back().get();
  }

  /// Create or retrieve a VPExternalDef for a given non-decomposable DDRef \p
  /// DDR.
  VPExternalDef *getVPExternalDefForDDRef(const loopopt::DDRef *DDR) {
    return getExternalItemForDDRef(VPExternalDefsHIR, DDR);
  }

  /// Retrieve the VPExternalDef for given HIR symbase \p Symbase. If no
  /// external definition exists then a nullptr is returned.
  VPExternalDef *getVPExternalDefForSymbase(unsigned Symbase) {
    return getExternalItemForSymbase(VPExternalDefsHIR, Symbase);
  }

  /// Create or retrieve a VPExternalDef for an HIR IV identified by its \p
  /// IVLevel.
  VPExternalDef *getVPExternalDefForIV(unsigned IVLevel, Type *BaseTy) {
    return getExternalItemForIV(VPExternalDefsHIR, IVLevel, BaseTy);
  }

  /// Create or retrieve a VPExternalUse for a given Value \p ExtVal.
  VPExternalUse *getVPExternalUse(PHINode *ExtDef) {
    return getExternalItem(VPExternalUses, ExtDef);
  }

  /// Create or retrieve a VPExternalUse for a given non-decomposable DDRef \p
  /// DDR.
  VPExternalUse *getVPExternalUseForDDRef(const loopopt::DDRef *DDR) {
    return getExternalItemForDDRef(VPExternalUsesHIR, DDR);
  }

  /// Create or retrieve a VPExternalUse for an HIR IV identified by its \p
  /// IVLevel.
  VPExternalUse *getVPExternalUseForIV(unsigned IVLevel, Type *BaseTy) {
    return getExternalItemForIV(VPExternalUsesHIR, IVLevel, BaseTy);
  }

  /// Create a new VPMetadataAsValue for \p MDAsValue if it doesn't exist or
  /// retrieve the existing one.
  VPMetadataAsValue *getVPMetadataAsValue(MetadataAsValue *MDAsValue) {
    std::unique_ptr<VPMetadataAsValue> &UPtr = VPMetadataAsValues[MDAsValue];
    if (!UPtr)
      // MDAsValue is a new VPMetadataAsValue to be inserted in the map.
      UPtr.reset(new VPMetadataAsValue(MDAsValue));

    return UPtr.get();
  }

  /// Create a new VPMetadataAsValue for Metadata \p MD if it doesn't exist or
  /// retrieve the existing one.
  VPMetadataAsValue *getVPMetadataAsValue(Metadata *MD) {
    // TODO: implement this method when needed.
    llvm_unreachable("Not implemented yet!");
  }

  // Verify that VPConstants are unique in the pool and that the map keys are
  // consistent with the underlying IR information of each VPConstant.
  void verifyVPConstants() const;

  // Verify that VPExternalDefs are unique in the pool and that the map keys are
  // consistent with the underlying IR information of each VPExternalDef.
  void verifyVPExternalDefs() const;

  // Verify that VPExternalDefs are unique in the pool and that the map keys are
  // consistent with the underlying HIR information of each VPExternalDef.
  void verifyVPExternalDefsHIR() const;

  // Verify that VPMetadataAsValues are unique in the pool and that the map keys
  // are consistent with the underlying IR information of each
  // VPMetadataAsValue.
  void verifyVPMetadataAsValues() const;

  // Add a VPInstruction that needs to be erased in UnlinkedVPInsns vector.
  void addUnlinkedVPInst(VPInstruction *I) { UnlinkedVPInsns.emplace_back(I); }

private:
  /// Add to the given dominator tree the header block and every new basic block
  /// that was created between it and the latch block, inclusive.
  static void updateDominatorTree(class DominatorTree *DT,
                                  BasicBlock *LoopPreHeaderBB,
                                  BasicBlock *LoopLatchBB);

  // Create or retrieve an external item from \p Table for a given Value \p
  // ExtVal.
  template <typename T>
  typename T::mapped_type::element_type *getExternalItem(T &Table,
                                                         PHINode *ExtVal) {
    using Def = typename T::mapped_type::element_type;
    typename T::mapped_type &UPtr = Table[ExtVal];
    if (!UPtr)
      // Def is a new external item to be inserted in the map.
      UPtr.reset(new Def(ExtVal));
    return UPtr.get();
  }

  // Retrieve an external item from \p Table for given HIR symbase \p Symbase.
  // If no external item is found, then a nullptr is returned
  template <typename Def>
  Def *getExternalItemForSymbase(FoldingSet<Def> &Table, unsigned Symbase) {
    FoldingSetNodeID ID;
    ID.AddInteger(Symbase);
    ID.AddInteger(0 /*IVLevel*/);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    // No Def found in table
    return nullptr;
  }

  // Create or retrieve an external item from \p Table for given HIR unitary
  // DDRef \p DDR.
  template <typename Def>
  Def *getExternalItemForDDRef(FoldingSet<Def> &Table,
                               const loopopt::DDRef *DDR) {
    assert(DDR->isNonDecomposable() && "Expected non-decomposable DDRef!");
    FoldingSetNodeID ID;
    ID.AddInteger(DDR->getSymbase());
    ID.AddInteger(0 /*IVLevel*/);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    Def *ExtDef = new Def(DDR);
    Table.InsertNode(ExtDef, IP);
    return ExtDef;
  }

  // Create or retrieve an external item from \p Table for an HIR IV identified
  // by its \p IVLevel.
  template <typename Def>
  Def *getExternalItemForIV(FoldingSet<Def> &Table, unsigned IVLevel,
                            Type *BaseTy) {
    FoldingSetNodeID ID;
    ID.AddInteger(0 /*Symbase*/);
    ID.AddInteger(IVLevel);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    Def *ExtDef = new Def(IVLevel, BaseTy);
    Table.InsertNode(ExtDef, IP);
    return ExtDef;
  }

  VPlan *clone(void) const {
    llvm_unreachable("Implement after VPlan redesign.");
  }
};

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
/// VPlanPrinter prints a given VPlan to a given output stream. The printing is
/// indented and follows the dot format.
class VPlanPrinter {
  friend inline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan);

private:
  raw_ostream &OS;
  const VPlan &Plan;
  unsigned Depth;
  unsigned TabWidth = 2;
  std::string Indent;

  unsigned BID = 0;

  SmallDenseMap<const VPBasicBlock *, unsigned> BlockID;

  /// Handle indentation.
  void bumpIndent(int b) { Indent = std::string((Depth += b) * TabWidth, ' '); }

  /// Print the information related to the CFG edges going out of a given
  /// \p Block, followed by printing the successor blocks themselves.
  void dumpEdges(const VPBasicBlock *BB);

  /// Print a given \p BasicBlock, including its instructions, followed by
  /// printing its successor blocks.
  void dumpBasicBlock(const VPBasicBlock *BB);

  unsigned getOrCreateBID(const VPBasicBlock *BB) {
    return BlockID.count(BB) ? BlockID[BB] : BlockID[BB] = BID++;
  }

  const Twine getOrCreateName(const VPBasicBlock *BB);

  const Twine getUID(const VPBasicBlock *BB);

  /// Print the information related to a CFG edge between two VPBasicBlocks.
  void drawEdge(const VPBasicBlock *From, const VPBasicBlock *To, bool Hidden,
                const Twine &Label);

  VPlanPrinter(raw_ostream &O, const VPlan &P) : OS(O), Plan(P) {}

  void dump();
};

inline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan) {
  VPlanPrinter Printer(OS, Plan);
  Printer.dump();
  return OS;
}

// Set of print functions
inline raw_ostream &operator<<(raw_ostream &OS, const VPInstruction &I) {
  I.dump(OS);
  return OS;
}
inline raw_ostream &operator<<(raw_ostream &OS, const VPBasicBlock &BB) {
  BB.print(OS, 2);
  return OS;
}
#endif // !NDEBUG || LLVM_ENABLE_DUMP

//===----------------------------------------------------------------------===//
// VPlan Utilities
//===----------------------------------------------------------------------===//
/// The VPlanUtils class provides common interfaces and functions that are
/// required across different VPlan classes e.g. VPBasicBlock.
class VPlanUtils {
private:
  /// Unique ID generator.
  static std::atomic<unsigned> NextOrdinal;

public:
  VPlanUtils() = delete;

  /// Create a unique name for a new VPlan entity such as a VPBasicBlock.
  static std::string createUniqueName(const llvm::StringRef &Prefix) {
    std::string S;
    raw_string_ostream RSO(S);
    RSO << Prefix << NextOrdinal++;
    return RSO.str();
  }
};

/// A wrapper class to add VPlan related remarks for opt-report. Currently
/// the implementation is naive with a single method to add a remark for
/// a given loop (can be HLLoop or llvm::Loop).
//  TODO:
/// In the future this will be extended to record all vectorization related
/// remarks emitted by VPlan by mapping the remarks to underlying VPlan data
/// structures that represent a loop. For example:
///
/// VPLoopRegion MainLoop --> {"LOOP WAS VECTORIZED", "vector length: 4"}
/// VPLoopRegion RemainderLoop --> {"remainder loop was not vectorized"}
class VPlanOptReportBuilder {
  LoopOptReportBuilder &LORBuilder;
  // LORB needs the LoopInfo while adding remarks for llvm::Loop. This will be
  // nullptr for HLLoop.
  LoopInfo *LI;

public:
  VPlanOptReportBuilder(LoopOptReportBuilder &LORB, LoopInfo *LI = nullptr)
      : LORBuilder(LORB), LI(LI) {}

  /// Add a vectorization related remark for the HIR loop \p Lp. The remark
  /// message is identified by \p MsgID.
  template <typename... Args>
  void addRemark(loopopt::HLLoop *Lp, OptReportVerbosity::Level Verbosity,
                 unsigned MsgID, Args &&... args);

  /// Add a vectorization related remark for the LLVM loop \p Lp. The remark
  /// message is identified by \p MsgID.
  template <typename... Args>
  void addRemark(Loop *Lp, OptReportVerbosity::Level Verbosity, unsigned MsgID,
                 Args &&... args);
};

} // namespace vpo

//===----------------------------------------------------------------------===//
// GraphTraits specializations for VPBasicBlock                               //
//===----------------------------------------------------------------------===//

// The following template specializations are implemented to support GraphTraits
// for VPBasicBlocks.
template <> struct GraphTraits<vpo::VPBasicBlock *> {
  using NodeRef = vpo::VPBasicBlock *;
  using ChildVectorType = SmallVectorImpl<vpo::VPBasicBlock *>;
  using ChildIteratorType = ChildVectorType::iterator;

  static NodeRef getEntryNode(NodeRef N) { return N; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getSuccessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getSuccessors().end();
  }
};

// This specialization is for the ChildrenGetterTy from
// GenericIteratedDominanceFrontier.h. Clang's GraphTraits for clang::CFGBlock
// do the same trick.
// TODO: Consider fixing GenericIteratedDominanceFrontier.h during upstreaming
// instead.
template <>
struct GraphTraits<vpo::VPBasicBlock>
    : public GraphTraits<vpo::VPBasicBlock *> {};

// GraphTraits specialization for VPBasicBlocks using constant iterator.
template <> struct GraphTraits<const vpo::VPBasicBlock *> {
  using NodeRef = const vpo::VPBasicBlock *;
  using ChildVectorType = const SmallVectorImpl<vpo::VPBasicBlock *>;
  using ChildIteratorType = ChildVectorType::const_iterator;

  static NodeRef getEntryNode(NodeRef N) { return N; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getSuccessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getSuccessors().end();
  }
};

// GraphTraits specialization for VPBasicBlocks using inverse iterator.
template <> struct GraphTraits<Inverse<vpo::VPBasicBlock *>> {
  using NodeRef = vpo::VPBasicBlock *;
  using ChildVectorType = SmallVectorImpl<vpo::VPBasicBlock *>;
  using ChildIteratorType = ChildVectorType::iterator;

  static NodeRef getEntryNode(Inverse<NodeRef> N) { return N.Graph; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getPredecessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getPredecessors().end();
  }
};

// The following template specializations are implemented to support GraphTraits
// for VPlan.
template <>
struct GraphTraits<vpo::VPlan *> : public GraphTraits<vpo::VPBasicBlock *> {

  using nodes_iterator = df_iterator<NodeRef>;

  static NodeRef getEntryNode(vpo::VPlan *Plan) {
    return Plan->getEntryBlock();
  }

  static inline nodes_iterator nodes_begin(vpo::VPlan *Plan) {
    return nodes_iterator::begin(Plan->getEntryBlock());
  }

  static inline nodes_iterator nodes_end(vpo::VPlan *Plan) {
    // df_iterator returns an empty iterator so the node used doesn't matter.
    return nodes_iterator::end(Plan->getExitBlock());
  }

  static size_t size(vpo::VPlan *Plan) { return Plan->getSize(); }
};

} // namespace llvm

#endif // LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H
