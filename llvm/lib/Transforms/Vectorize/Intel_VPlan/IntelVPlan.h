//===- IntelVPlan.h - Represent A Vectorizer Plan -------------------------===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file contains the declarations of the Vectorization Plan base classes:
// 1. VPBasicBlock and VPRegionBlock that inherit from a common pure virtual
//    VPBlockBase, together implementing a Hierarchical CFG;
// 2. Specializations of GraphTraits that allow VPBlockBase graphs to be treated
//    as proper graphs for generic algorithms;
// 3. VPInstruction and its  sub-classes represent instructions contained within
//    VPBasicBlocks;
// 4. The VPlan class holding a candidate for vectorization;
// 5. The VPlanUtils class providing methods for building plans;
// 6. The VPlanPrinter class providing a way to print a plan in dot format.
// These are documented in docs/VectorizationPlan.rst.
//
//===----------------------------------------------------------------------===//

#ifndef LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H
#define LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H

#if INTEL_CUSTOMIZATION
#include "IntelVPlanValue.h"
#else
#include "VPlanValue.h"
#endif //INTEL_CUSTOMIZATION
#include "llvm/ADT/GraphTraits.h"
#include "llvm/ADT/SmallSet.h"
#include "llvm/ADT/ilist.h"
#include "llvm/ADT/ilist_node.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/Support/GenericDomTreeConstruction.h"
#include "llvm/Support/raw_ostream.h"
#include <atomic>

#if INTEL_CUSTOMIZATION
#include "IntelVPLoopAnalysis.h"
#include "IntelVPlanDivergenceAnalysis.h"
#include "IntelVPlanLoopInfo.h"
#include "VPlanHIR/IntelVPlanInstructionDataHIR.h"
#include "llvm/ADT/DepthFirstIterator.h"
#include "llvm/Analysis/Intel_LoopAnalysis/Framework/HIRFramework.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/Diag.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/HLGoto.h"
#include "llvm/Analysis/Intel_LoopAnalysis/IR/HLInst.h"
#include "llvm/Analysis/Intel_OptReport/LoopOptReportBuilder.h"
#include "llvm/Support/FormattedStream.h"
#endif // INTEL_CUSTOMIZATION

namespace llvm {
// The (re)use of existing LoopVectorize classes is subject to future VPlan
// refactoring.
// namespace {
class InnerLoopVectorizer;
class LoopVectorizationLegality;
class LoopInfo;
//}

#if INTEL_CUSTOMIZATION
namespace loopopt {
class RegDDRef;
class HLLoop;
class OptReportDiag;
} // namespace loopopt

namespace vpo {
class SyncDependenceAnalysis;
class VPValue;
class VPlan;
class VPBlockBase;
class VPOCodeGen;
class VPOCodeGenHIR;
class VPOVectorizationLegality;
class VPBasicBlock;
class VPDominatorTree;
class VPPostDominatorTree;
#if INTEL_CUSTOMIZATION
class VPlanCostModel; // INTEL: to be later declared as a friend
class VPlanCostModelProprietary; // INTEL: to be later declared as a friend
class VPlanDivergenceAnalysis;
class VPlanBranchDependenceAnalysis;
#endif // INTEL_CUSTOMIZATION
typedef SmallPtrSet<VPValue *, 8> UniformsTy;

// Class names mapping to minimize the diff:
#define InnerLoopVectorizer VPOCodeGen
#define LoopVectorizationLegality VPOVectorizationLegality

struct TripCountInfo;
#endif // INTEL_CUSTOMIZATION

/// In what follows, the term "input IR" refers to code that is fed into the
/// vectorizer whereas the term "output IR" refers to code that is generated by
/// the vectorizer.

/// VPIteration represents a single point in the iteration space of the output
/// (vectorized and/or unrolled) IR loop.
struct VPIteration {
  unsigned Part; ///< in [0..UF)
  unsigned Lane; ///< in [0..VF)
  VPIteration(unsigned Part, unsigned Lane) : Part(Part), Lane(Lane) {}
  VPIteration(unsigned Part) : Part(Part), Lane(ALL_LANES()) {}
  static unsigned ALL_LANES() { return -1; }
};

/// This class is used to enable the VPlan to invoke a method of ILV. This is
/// needed until the method is refactored out of ILV and becomes reusable.
struct VPCallback {
  virtual ~VPCallback() {}
  virtual Value *getOrCreateVectorValues(Value *V, unsigned Part) = 0;
};

/// VPTransformState holds information passed down when "executing" a VPlan,
/// needed for generating the output IR.
struct VPTransformState {
  VPTransformState(unsigned VF, unsigned UF, LoopInfo *LI,
                   class DominatorTree *DT, IRBuilder<> &Builder,
                   InnerLoopVectorizer *ILV, VPCallback &Callback,
                   VPLoopInfo *VPLI)
      : VF(VF), UF(UF), Instance(), LI(LI), DT(DT), Builder(Builder), ILV(ILV),
        Callback(Callback), VPLI(VPLI) {}

  /// The chosen Vectorization and Unroll Factors of the loop being vectorized.
  unsigned VF;
  unsigned UF;

  /// Hold the indices to generate specific scalar instructions. Null indicates
  /// that all instances are to be generated, using either scalar or vector
  /// instructions.
  Optional<VPIteration> Instance;

  struct DataState {
    /// A type for vectorized values in the new loop. Each value from the
    /// original loop, when vectorized, is represented by UF vector values in
    /// the new unrolled loop, where UF is the unroll factor.
    typedef SmallVector<Value *, 2> PerPartValuesTy;

    DenseMap<VPValue *, PerPartValuesTy> PerPartOutput;
  } Data;

  /// Get the generated Value for a given VPValue and a given Part. If such a
  /// Value does not exist by the time this method is called, then this Def
  /// represents Values that are still generated by ILV via ValueMap.
  Value *get(VPValue *Def, unsigned Part) {
    if (Data.PerPartOutput.count(Def))
      return Data.PerPartOutput[Def][Part];
    // Bring the Values from ValueMap.
    return Callback.getOrCreateVectorValues(VPValue2Value[Def], Part);
  }

  /// Set the generated Value for a given VPValue and a given Part.
  void set(VPValue *Def, Value *V, unsigned Part) {
    if (!Data.PerPartOutput.count(Def)) {
      DataState::PerPartValuesTy Entry(UF);
      Data.PerPartOutput[Def] = Entry;
    }
    Data.PerPartOutput[Def][Part] = V;
  }
  /// Hold state information used when constructing the CFG of the output IR,
  /// traversing the VPBasicBlocks and generating corresponding IR BasicBlocks.
  struct CFGState {
    /// The previous VPBasicBlock visited. Initially set to null.
    VPBasicBlock *PrevVPBB = nullptr;
    /// The previous IR BasicBlock created or used. Initially set to the new
    /// header BasicBlock.
    BasicBlock *PrevBB = nullptr;
    /// The last IR BasicBlock in the output IR. Set to the new latch
    /// BasicBlock, used for placing the newly created BasicBlocks.
    BasicBlock *LastBB = nullptr;
    /// A mapping of each VPBasicBlock to the corresponding BasicBlock. In case
    /// of replication, maps the BasicBlock of the last replica created.
    SmallDenseMap<VPBasicBlock *, BasicBlock *> VPBB2IRBB;
    /// Vector of VPBasicBlocks whose terminator instruction needs to be fixed
    /// up at the end of vector code generation.
    SmallVector<VPBasicBlock *, 8> VPBBsToFix;
  } CFG;

  /// Hold a pointer to LoopInfo to register new basic blocks in the loop.
  class LoopInfo *LI;

  /// Hold a pointer to Dominator Tree to register new basic blocks in the loop.
  class DominatorTree *DT;

  /// Hold a reference to the IRBuilder used to generate output IR code.
  IRBuilder<> &Builder;

  /// Hold a reference to a mapping between VPValues in VPlan and original
  /// Values they correspond to.
  VPValue2ValueTy VPValue2Value;

  /// Hold a pointer to InnerLoopVectorizer to reuse its IR generation methods.
  class InnerLoopVectorizer *ILV;

  VPCallback &Callback;
#if INTEL_CUSTOMIZATION
  VPLoopInfo *VPLI;
#endif
};

/// Class to model a single VPlan-level instruction - it may generate a sequence
/// of IR instructions when executed, these instructions would always form a
/// single-def expression as the VPInstruction is also a single def-use vertex.
///
#if INTEL_CUSTOMIZATION
/// For HIR, we classify VPInstructions into 3 sub-types:
///   1) Master VPInstruction: It has underlying HIR data attached and its
///      operands could have been decomposed or not. If so, this VPInstruction
///      is the last one in the UD chain of the group of decomposed
///      VPInstructions. If this VPInstruction or any of its decomposed ones are
///      modified, the HIR will automatically be marked as invalid.
///   2) Decomposed VPInstruction: It's created as a result of decomposing a
///      master VPInstruction. It doesn't have underlying HIR directly attached
///      but it has a pointer to its master VPInstruction holding it. In order
///      to check whether the underlying HIR of a decomposed VPInstruction is
///      valid, its master VPInstruction must be checked.
///   3) New VPInstruction: It's created as a result of a VPlan-to-VPlan
///      transformation, excluding decomposition. It doesn't have underlying HIR
///      or master VPInstruction attached. New VPInstructions also exist in the
///      LLVM-IR path.
///
/// DESIGN PRINCIPLE: access to the underlying IR is forbidden by default. Only
/// the front-end and back-end of VPlan should have access to the underlying IR
/// and be aware of the VPInstruction sub-type (master/decomposed/new)
/// instructions. Some well-delimited VPlan analyses, previous design review
/// approval, may also need to have access to the underlying IR and
/// VPInstruction sub-types to bring analysis information computed on the input
/// IR to VPlan. The remaining VPlan algorithms should process all the
/// VPInstructions ignoring their underlying IR and sub-type. For these
/// reasons, adding new friends to this class must be very well justified.
///
/// DESIGN DECISION: for VPO, we decided to pay the memory and design cost of
/// having LLVM-IR data (Inst) HIR data (MasterData) and their respective
/// interfaces in the same class in favor of minimizing divergence with the
/// community. We know that this is not the best design but creating a
/// VPInstructionHIR sub-class would be complicated because VPInstruction also
/// has sub-classes (VPCmpInst, VPPHINode, etc.) that would need to be
/// replicated under the VPInstructionHIR.
#endif
class VPInstruction : public VPUser,
  public ilist_node_with_parent<VPInstruction, VPBasicBlock> {
#if INTEL_CUSTOMIZATION
  friend class HIRSpecifics;
  friend class VPBasicBlock;
  friend class VPBuilder;
  friend class VPBranchInst;
  friend class VPBuilderHIR;
  friend class VPDecomposerHIR;
  // To get underlying HIRData until we have proper VPType.
  friend class VPVLSClientMemrefHIR;
  friend class VPlanCostModel;
  friend class VPlanCostModelProprietary;
  friend class VPlanDivergenceAnalysis;
  friend class VPlanIdioms;
  friend class VPlanVLSAnalysis;
  friend class VPlanVLSAnalysisHIR;
  friend class VPlanVerifier;
  friend class VPOCodeGen;
  friend class VPOCodeGenHIR;
  friend class VPCloneUtils;
  friend class VPValueMapper;
  friend class VPLoopEntityList;
  friend class VPValue;

  /// Hold all the HIR-specific data and interfaces for a VPInstruction.
  class HIRSpecifics {
    friend class VPValue;

  private:
    /// Return true if the underlying HIR data is valid. If it's a decomposed
    /// VPInstruction, the HIR of the attached master VPInstruction is checked.
    bool isValid() const {
      if (isMaster() || isDecomposed())
        return getVPInstData()->isValid();

      // For other VPInstructions without underlying HIR.
      assert(!isSet() && "HIR data must be unset!");
      return false;
    }

    /// Invalidate underlying HIR deta. If decomposed VPInstruction, the HIR of
    /// its master VPInstruction is invalidated.
    void invalidate() {
      if (isMaster() || isDecomposed())
        getVPInstData()->setInvalid();
    }

  public:
    HIRSpecifics() {}
    ~HIRSpecifics() {
      if (isMaster())
        delete getVPInstData();
    }

    // DESIGN PRINCIPLE: IR-independent algorithms don't need to know about
    // HIR-specific master, decomposed and new VPInstructions or underlying HIR
    // information. For that reason, access to the following HIR-specific
    // methods must be restricted. We achieve that goal by making
    // VPInstruction's HIRSpecifics member private.

    // Hold the underlying HIR information related to the LHS operand of this
    // VPInstruction.
    std::unique_ptr<VPOperandHIR> LHSHIROperand;

    // Used to save the symbase of the scalar memref corresponding to a
    // load/store instruction. Vector memref generated during vector CG is
    // assigned the same symbase.
    unsigned Symbase = loopopt::InvalidSymbase;

    /// Pointer to access the underlying HIR data attached to this
    /// VPInstruction, if any, depending on its sub-type:
    ///   1) Master VPInstruction: MasterData points to a VPInstDataHIR holding
    ///      the actual HIR data.
    ///   2) Decomposed VPInstruction: MasterData points to master VPInstruction
    ///      holding the actual HIR data.
    ///   3) Other VPInstruction (!Master and !Decomposed): MasterData is null.
    ///      We use a void pointer to represent this case.
    PointerUnion<MasterVPInstData *, VPInstruction *, void *> MasterData =
        (int *)nullptr;

    // Return the VPInstruction data of this VPInstruction if it's a master or
    // decomposed. Return nullptr otherwise.
    MasterVPInstData *getVPInstData() {
      if (isMaster())
        return MasterData.get<MasterVPInstData *>();
      if (isDecomposed())
        return getMaster()->HIR.getVPInstData();
      // New VPInstructions don't have VPInstruction data.
      return nullptr;
    }
    const MasterVPInstData *getVPInstData() const {
      return const_cast<HIRSpecifics *>(this)->getVPInstData();
    }

    void verifyState() const {
      if (MasterData.is<MasterVPInstData *>())
        assert(!MasterData.isNull() &&
               "MasterData can't be null for master VPInstruction!");
      else if (MasterData.is<VPInstruction *>())
        assert(!MasterData.isNull() &&
               "MasterData can't be null for decomposed VPInstruction!");
      else
        assert(MasterData.is<void *>() && MasterData.isNull() &&
               "MasterData must be null for VPInstruction that is not master "
               "or decomposed!");
    }

    /// Return true if this is a master VPInstruction.
    bool isMaster() const {
      verifyState();
      return MasterData.is<MasterVPInstData *>();
    }

    /// Return true if this is a decomposed VPInstruction.
    bool isDecomposed() const {
      verifyState();
      return MasterData.is<VPInstruction *>();
    }

    // Return true if MasterData contains actual HIR data.
    bool isSet() const {
      verifyState();
      return !MasterData.is<void *>();
    }

    /// Return the underlying HIR attached to this master VPInstruction. Return
    /// nullptr if the VPInstruction doesn't have underlying HIR.
    loopopt::HLNode *getUnderlyingNode() {
      MasterVPInstData *MastData = getVPInstData();
      if (!MastData)
        return nullptr;
      return MastData->getNode();
    }
    loopopt::HLNode *getUnderlyingNode() const {
      return const_cast<HIRSpecifics *>(this)->getUnderlyingNode();
    }

    /// Attach \p UnderlyingNode to this VPInstruction and turn it into a master
    /// VPInstruction.
    void setUnderlyingNode(loopopt::HLNode *UnderlyingNode) {
      assert(!isSet() && "MasterData is already set!");
      MasterData = new MasterVPInstData(UnderlyingNode);
    }

    /// Attach \p Def to this VPInstruction as its VPOperandHIR.
    void setOperandDDR(const loopopt::DDRef *Def) {
      assert(!LHSHIROperand && "LHSHIROperand is already set!");
      LHSHIROperand.reset(new VPBlob(Def));
    }

    /// Attach \p IVLevel to this VPInstruction as its VPOperandHIR.
    void setOperandIV(unsigned IVLevel) {
      assert(!LHSHIROperand && "LHSHIROperand is already set!");
      LHSHIROperand.reset(new VPIndVar(IVLevel));
    }

    /// Return the VPOperandHIR with the underlying HIR information of the LHS
    /// operand.
    VPOperandHIR *getOperandHIR() const { return LHSHIROperand.get(); }

    /// Return the master VPInstruction attached to a decomposed VPInstruction.
    VPInstruction *getMaster() {
      assert(isDecomposed() && "Only decomposed VPInstructions have a pointer "
                               "to a master VPInstruction!");
      return MasterData.get<VPInstruction *>();
    }
    VPInstruction *getMaster() const {
      return const_cast<HIRSpecifics *>(this)->getMaster();
    }

    /// Attach \p MasterVPI as master VPInstruction of a decomposed
    /// VPInstruction.
    void setMaster(VPInstruction *MasterVPI) {
      assert(MasterVPI && "Master VPInstruction cannot be set to null!");
      assert(!isMaster() &&
             "A master VPInstruction can't point to a master VPInstruction!");
      assert(!isSet() && "Master VPInstruction is already set!");
      MasterData = MasterVPI;
    }

    /// Mark the underlying HIR data as valid.
    void setValid() {
      assert(isMaster() && "Only a master VPInstruction must set HIR!");
      getVPInstData()->setValid();
    }

    /// Print HIR-specific flags. It's mainly for debugging purposes.
    void printHIRFlags(raw_ostream &OS) const {
      OS << "IsMaster=" << isMaster() << " IsDecomp=" << isDecomposed()
         << " IsNew=" << !isSet() << " HasValidHIR= " << isValid() << "\n";
    }

    void setSymbase(unsigned SB) { Symbase = SB; }
    unsigned getSymbase(void) const { return Symbase; }

    void cloneFrom(const HIRSpecifics &HIR) {
      if (HIR.isMaster()) {
        setUnderlyingNode(HIR.getUnderlyingNode());
        if (HIR.isValid())
          setValid();
      } else if (HIR.isDecomposed())
        setMaster(HIR.getMaster());

      // Copy the operand.
      if (VPOperandHIR *HIROperand = HIR.getOperandHIR()) {
        if (VPBlob *Blob = dyn_cast<VPBlob>(HIROperand))
          setOperandDDR(Blob->getBlob());
        else {
          VPIndVar *IV = cast<VPIndVar>(HIROperand);
          setOperandIV(IV->getIVLevel());
        }
      }
      setSymbase(HIR.getSymbase());

      // Verify correctness of the cloned HIR.
      assert(isMaster() == HIR.isMaster() &&
             "Cloned isMaster() value should be equal to the original one");
      assert(isDecomposed() == HIR.isDecomposed() &&
             "Cloned isDecomposed() value should be equal to the original one");
      assert(isSet() == HIR.isSet() &&
             "Cloned isSet() value should be equal to the original one");
      if (isSet())
        assert(HIR.isValid() == isValid() &&
               "Cloned isValid() value should be equal to the original one");
    }
  };
#endif // INTEL_CUSTOMIZATION

public:
#if INTEL_CUSTOMIZATION
  /// VPlan opcodes, extending LLVM IR with idiomatics instructions.
  enum {
      Not = Instruction::OtherOpsEnd + 1,
      AllZeroCheck,
      Pred,
      SMax,
      UMax,
      FMax,
      SMin,
      UMin,
      FMin,
      InductionInit,
      InductionInitStep,
      InductionFinal,
      ReductionInit,
      ReductionFinal,
      AllocatePrivate,
  };
#else
  enum { Not = Instruction::OtherOpsEnd + 1 };
#endif

private:
  typedef unsigned char OpcodeTy;
  OpcodeTy Opcode;

  /// Each VPInstruction belongs to a single VPBasicBlock.
  VPBasicBlock *Parent = nullptr;

#if INTEL_CUSTOMIZATION
  // Hold the underlying HIR information, if any, attached to this
  // VPInstruction.
  HIRSpecifics HIR;

  void setSymbase(unsigned Symbase) {
    assert(Opcode == Instruction::Store ||
           Opcode == Instruction::Load &&
               "setSymbase called for invalid VPInstruction");
    assert(Symbase != loopopt::InvalidSymbase &&
           "Unexpected invalid symbase assignment");
    HIR.setSymbase(Symbase);
  }

  unsigned getSymbase(void) const {
    assert(Opcode == Instruction::Store ||
           Opcode == Instruction::Load &&
               "getSymbase called for invalid VPInstruction");
    assert(HIR.Symbase != loopopt::InvalidSymbase &&
           "Unexpected invalid symbase");
    return HIR.getSymbase();
  }
#endif

  /// Utility method serving execute(): generates a single instance of the
  /// modeled instruction.
  void generateInstruction(VPTransformState &State, unsigned Part);

  void copyUnderlyingFrom(const VPInstruction &Inst) {
#if INTEL_CUSTOMIZATION
    HIR.cloneFrom(Inst.HIR);
#endif // INTEL_CUSTOMIZATION
    Value *V = Inst.getUnderlyingValue();
    if (V)
      setUnderlyingValue(*V);
    if (!Inst.isUnderlyingIRValid())
      invalidateUnderlyingIR();
  }

#if INTEL_CUSTOMIZATION
protected:
  /// Return the underlying Instruction attached to this VPInstruction. Return
  /// null if there is no Instruction attached. This interface is similar to
  /// getValue() but it hides the cast when we are working with VPInstruction
  /// pointers.
  Instruction *getInstruction() const {
    assert((!getUnderlyingValue() || isa<Instruction>(getUnderlyingValue())) &&
           "Expected Instruction as underlying Value.");
    return cast_or_null<Instruction>(getUnderlyingValue());
  }

  /// Return true if this is a new VPInstruction (i.e., an VPInstruction that is
  /// not coming from the underlying IR.
  bool isNew() const { return getUnderlyingValue() == nullptr && !HIR.isSet(); }
#endif

  virtual VPInstruction *cloneImpl() const {
    VPInstruction *Cloned = new VPInstruction(Opcode, getType(), {});
    for (auto &O : operands()) {
      Cloned->addOperand(O);
    }
    return Cloned;
  }

public:
  VPInstruction(unsigned Opcode, Type *BaseTy, ArrayRef<VPValue *> Operands)
      : VPUser(VPValue::VPInstructionSC, Operands, BaseTy), Opcode(Opcode) {
    assert(BaseTy && "BaseTy can't be null!");
  }
  VPInstruction(unsigned Opcode, Type *BaseTy,
                std::initializer_list<VPValue *> Operands)
      : VPUser(VPValue::VPInstructionSC, Operands, BaseTy), Opcode(Opcode) {
    assert(BaseTy && "BaseTy can't be null!");
  }

  /// Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return V->getVPValueID() == VPValue::VPInstructionSC;
  }

  unsigned getOpcode() const { return Opcode; }
#if INTEL_CUSTOMIZATION
  // FIXME: Temporary workaround for TTI problems that make the cost
  // modeling incorrect. The getCMType() returns nullptr in case the underlying
  // instruction is not set and this makes the cost of this instruction
  // undefined (i.e. 0). Non-null return value causes calculation by TTI with
  // incorrect result.
  virtual Type *getCMType() const override {
    if (getUnderlyingValue())
      return getUnderlyingValue()->getType();

    if (!HIR.isMaster())
      return nullptr;

    const loopopt::HLNode *Node = HIR.getUnderlyingNode();
    const loopopt::HLInst *Inst = dyn_cast_or_null<loopopt::HLInst>(Node);

    if (!Inst)
      return nullptr;

    const Instruction *LLVMInst = Inst->getLLVMInstruction();
    if (!LLVMInst)
      return nullptr;

    return LLVMInst->getType();
  }

  // Return true if this VPInstruction represents a cast operation.
  bool isCast() const { return Instruction::isCast(getOpcode()); }

  bool mayHaveSideEffects() const;
#endif // INTEL_CUSTOMIZATION

  /// \return the VPBasicBlock which this VPInstruction belongs to.
  VPBasicBlock *getParent() { return Parent; }
  const VPBasicBlock *getParent() const { return Parent; }
  void setParent(VPBasicBlock *NewParent) { Parent = NewParent; }

  /// Generate the instruction.
  /// TODO: We currently execute only per-part unless a specific instance is
  /// provided.
  virtual void execute(VPTransformState &State);
#if INTEL_CUSTOMIZATION
  virtual void executeHIR(VPOCodeGenHIR *CG);
#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Dump the VPInstruction.
  void dump(raw_ostream &O) const override { dump(O, nullptr); };
  void dump(raw_ostream &O, const VPlanDivergenceAnalysis *DA) const;

  void dump() const override { dump(errs()); }
  void dump(const VPlanDivergenceAnalysis *DA) const { dump(dbgs(), DA); }
#endif // !NDEBUG || LLVM_ENABLE_DUMP
#endif

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Print the VPInstruction.
  virtual void print(raw_ostream &O, const Twine &Indent) const;

  /// Print the VPInstruction.
  void print(raw_ostream &O, const VPlanDivergenceAnalysis *DA = nullptr) const;
  static const char *getOpcodeName(unsigned Opcode);
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  VPInstruction *clone() const {
    VPInstruction *Cloned = cloneImpl();
    Cloned->copyUnderlyingFrom(*this);
    return Cloned;
  }
};

#if INTEL_CUSTOMIZATION
/// Concrete class for comparison. Represents both integers and floats.
class VPCmpInst : public VPInstruction {
public:
  typedef CmpInst::Predicate Predicate;
  /// \brief Create VPCmpInst with its two operands, a pred and a BaseType.
  /// Operands \p LHS and \p RHS must not have conflicting base types.
  VPCmpInst(VPValue *LHS, VPValue *RHS, Predicate Pred)
      : VPInstruction(inferOpcodeFromPredicate(Pred),
                      CmpInst::makeCmpResultType(LHS->getType()),
                      ArrayRef<VPValue *>({LHS, RHS})),
        Pred(Pred) {}

  /// \brief Return the predicate for this instruction
  Predicate getPredicate() const { return Pred; }

  /// \brief Methods for supporting type inquiry through isa, cast, and
  /// dyn_cast:
  static bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::ICmp ||
           VPI->getOpcode() == Instruction::FCmp;
  }
  static bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  virtual VPCmpInst *cloneImpl() const final {
    return new VPCmpInst(getOperand(0), getOperand(1), getPredicate());
  }

private:
  Predicate Pred;

  // Return the opcode that corresponds to predicate
  unsigned inferOpcodeFromPredicate(Predicate Pred) {
    // Infer Opcode from Pred
    if (CmpInst::isIntPredicate(Pred))
      return Instruction::ICmp;
    if (CmpInst::isFPPredicate(Pred))
      return Instruction::FCmp;
    llvm_unreachable("Integer/Float predicate expected");
  }
};

/// Concrete class to represent branch instruction in VPlan.
class VPBranchInst : public VPInstruction {
public:
  explicit VPBranchInst(Type *BaseTy)
      : VPInstruction(Instruction::Br, BaseTy, {}) {}

  const loopopt::HLGoto *getHLGoto() const {
    return cast<loopopt::HLGoto>(HIR.getUnderlyingNode());
  }

  /// Return target block of unconditional VPBranchInst. If VPBranchInst was
  /// created for HLGoto, return its external target block or nullptr otherwise.
  const BasicBlock *getTargetBlock() const {
    if (getHLGoto())
      return getHLGoto()->getTargetBBlock();
    // TODO: LLVM-IR implementation is needed.
    return nullptr;
  }

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  void print(raw_ostream &O) const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  static inline bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::Br;
  }

  virtual VPBranchInst *cloneImpl() const final {
    return new VPBranchInst(getType());
  }
};

/// Concrete class for PHI instruction.
class VPPHINode : public VPInstruction {
private:
  SmallVector<VPBasicBlock *, 2> VPBBUsers;

  // Used to indicate that this PHI needs to be blended using selects during
  // vector code generation. This can be removed once we make the phi->select
  // transformation explicit in VPlan.
  bool Blend = false;

public:
  void setBlend(bool B) { Blend = B; }
  bool getBlend() const { return Blend; }
  /// Sort the incoming blocks of the blend phi according to their execution
  /// order in the linearized CFG. Required to be performed prior to code
  /// generation for the blend phis.
  ///
  /// \p BlockIndexInRPOTOrNull is an optional parameter with the mapping of the
  /// blocks in \p this phi's parent region to that blocks' RPOT numbers. If not
  /// provided, it will be calculated inside the method.
  //
  // TODO: As an optimization, the sorting can be done once per block, but that
  // should be done at the caller side complicating the code.
  void sortIncomingBlocksForBlend(
      DenseMap<const VPBlockBase *, int> *BlockIndexInRPOTOrNull = nullptr);

  using vpblock_iterator = SmallVectorImpl<VPBasicBlock *>::iterator;
  using const_vpblock_iterator =
      SmallVectorImpl<VPBasicBlock *>::const_iterator;

  explicit VPPHINode(Type *BaseTy)
      : VPInstruction(Instruction::PHI, BaseTy, ArrayRef<VPValue *>()) {}

  vpblock_iterator block_begin(void) { return VPBBUsers.begin(); }

  vpblock_iterator block_end(void) { return VPBBUsers.end(); }

  const_vpblock_iterator block_begin(void) const {
    return VPBBUsers.begin();
  }

  const_vpblock_iterator block_end(void) const {
    return VPBBUsers.end();
  }

  iterator_range<vpblock_iterator> blocks(void) {
    return make_range(block_begin(), block_end());
  }

  iterator_range<const_vpblock_iterator> blocks(void) const {
    return make_range(block_begin(), block_end());
  }

  operand_range incoming_values(void) { return operands(); }
  const_operand_range incoming_values(void) const { return operands(); }

  /// Return number of incoming values.
  unsigned getNumIncomingValues(void) const { return getNumOperands(); }

  /// Return VPValue that corresponds to Idx'th incomming VPBasicBlock.
  VPValue *getIncomingValue(const unsigned Idx) const { return getOperand(Idx); }

  /// Return VPValue that corresponds to incomming VPBB.
  VPValue *getIncomingValue(const VPBasicBlock *VPBB) const {
    auto Idx = getBlockIndexOrNone(VPBB);
    assert(Idx && "Cannot find value for a given BB.");
    return getIncomingValue(Idx.getValue());
  }

  void setIncomingValue(const unsigned Idx, VPValue *Value) {
    assert(Value && "Value must not be null.");
    setOperand(Idx, Value);
  }

  /// Set the incoming value for a specific basic block.
  void setIncomingValue(VPBasicBlock *VPBB, VPValue *Value) {
    assert(Value && VPBB && "Value and VPBB must not be null.");
    auto Idx = getBlockIndex(VPBB);
    assert(Idx >= 0 && "VPBB should have a valid index.");
    setIncomingValue(Idx, Value);
  }

  /// Add an incoming value to the end of the PHI list
  void addIncoming(VPValue *Value, VPBasicBlock *Block) {
    assert(Value && "Value must not be null.");
    assert(Block && "Block must not be null.");
    addOperand(Value);
    VPBBUsers.push_back(Block);
  }

  /// Return incoming basic block number \p Idx.
  VPBasicBlock *getIncomingBlock(const unsigned Idx) const {
    return VPBBUsers[Idx];
  }

  /// Return incoming basic block corresponding to \p Value.
  VPBasicBlock *getIncomingBlock(const VPValue *Value) const {
    auto It = llvm::find(make_range(op_begin(), op_end()), Value);
    return getIncomingBlock(std::distance(op_begin(), It));
  }

  /// Set incoming basic block as \p Block corresponding to basic block number
  /// \p Idx.
  void setIncomingBlock(const unsigned Idx, VPBasicBlock *Block) {
    assert(Block && "VPPHI node got a null basic block");
    VPBBUsers[Idx] = Block;
  }

  /// Remove a block from the phi node.
  void removeIncomingValue(const VPBasicBlock *Block) {
    unsigned Idx = getBlockIndex(Block);
    assert(Idx <= VPBBUsers.size() && "Index is out of range");
    auto it = VPBBUsers.begin();
    std::advance(it, Idx);
    VPBBUsers.erase(it);
    removeOperand(Idx);
  }

  /// Return index for a given \p Block.
  int getBlockIndex(const VPBasicBlock *Block) const {
    auto It = llvm::find(make_range(block_begin(), block_end()), Block);
    if (It != block_end())
      return std::distance(block_begin(), It);
    return -1;
  }

  /// Return Optional index for a given basic block \p Block.
  Optional<unsigned> getBlockIndexOrNone(const VPBasicBlock *Block) const {
    int Idx = getBlockIndex(Block);
    return Idx != -1 ? Optional<unsigned>(Idx) : None;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == Instruction::PHI;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  /// Create new PHINode and copy original incoming values to the newly created
  /// PHINode. Caller is responsible to replace these values with what is
  /// needed.
  virtual VPPHINode *cloneImpl() const final {
    VPPHINode *Cloned = new VPPHINode(getType());
    for (unsigned i = 0, e = getNumIncomingValues(); i != e; ++i) {
      Cloned->addIncoming(getIncomingValue(i), getIncomingBlock(i));
    }
    return Cloned;
  }

  /// Checks whether the specified PHI node always merges together the same
  /// value, assuming that undefs result in the same value as non-undefs.
  /// Adapted from the LLVM-source PHINode::hasConstantOrUndefValue().
  bool hasConstantOrUndefValue() const {
    VPValue *ConstantValue = nullptr;
    for (int I = 0, E = getNumIncomingValues(); I != E; ++I) {
      VPValue *Incoming = getIncomingValue(I);
      const VPConstant *ConstOp = dyn_cast<VPConstant>(Incoming);
      bool IsUndef = ConstOp && isa<UndefValue>(ConstOp->getConstant());
      if (Incoming != this && !IsUndef) {
        if (ConstantValue && ConstantValue != Incoming)
          return false;
        ConstantValue = Incoming;
      }
    }
    return true;
  }
};

/// Concrete class to represent GEP instruction in VPlan.
class VPGEPInstruction : public VPInstruction {
  friend class VPlanVerifier;

private:
  // Trailing struct offsets for a GEP are tracked via a vector of bools called
  // OperandIsStructOffset. This vector will always be consistent with the
  // number of operands stored in a given GEP instruction. An operand's
  // corresponding index entry in OperandIsStructOffset is set to true if it's a
  // trailing struct offset, false otherwise. NOTE: This information is needed
  // only if the GEP is created via HIR-path. For LLVM-IR path the vector will
  // always be false for all entries.
  //
  // Examples:
  // 1. (@a)[0].1[i1] --> gep %a, 0, 1, %vpi1
  //
  //    Vector will look like:
  //    <false, false, true, false>
  //
  //    i.e. operand at index 2 is a struct offset corresponding to the previous
  //    last-found non struct offset operand (index 1).
  //
  // 2. (@a)[0].2.1[5][i1] --> gep %a, 0, 2, 1, 5, %vpi1
  //
  //    Vector will look like:
  //    <false, false, true, true, false, false>
  //

  bool InBounds;
  SmallVector<bool, 4> OperandIsStructOffset;

public:
  /// Default constructor for VPGEPInstruction. The default value for \p
  /// InBounds is false.
  VPGEPInstruction(Type *BaseTy, VPValue *Ptr, ArrayRef<VPValue *> IdxList,
                   bool InBounds = false)
      : VPInstruction(Instruction::GetElementPtr, BaseTy, {}),
        InBounds(InBounds) {
    assert(Ptr && "Base pointer operand of GEP cannot be null.");
    // Base pointer should be the first operand of GEP followed by index
    // operands
    assert(!getNumOperands() &&
           "GEP instruction already has operands before base pointer.");
    VPInstruction::addOperand(Ptr);
    for (auto Idx : IdxList)
      VPInstruction::addOperand(Idx);
    // Track all operands as non struct offsets, since that information is not
    // available at this point.
    OperandIsStructOffset.resize(1 + IdxList.size(), false);
  }

  /// Setter and getter functions for InBounds.
  void setIsInBounds(bool IsInBounds) { InBounds = IsInBounds; }
  bool isInBounds() const { return InBounds; }

  /// Get the base pointer operand of given VPGEPInstruction.
  VPValue *getPointerOperand() const { return VPInstruction::getOperand(0); }

  /// Overloaded method for adding an operand \p Operand. The struct offset
  /// tracker is accordingly updated after operand addition.
  void addOperand(VPValue *Operand, bool IsStructOffset = false) {
    VPInstruction::addOperand(Operand);
    OperandIsStructOffset.push_back(IsStructOffset);
    assert(OperandIsStructOffset.size() == getNumOperands() &&
           "Number of operands and struct offset tracker sizes don't match.");
  }

  /// Overloaded method for setting index \p Idx with operand \p Operand. The
  /// struct offset tracker is accordingly updated after operand is set.
  void setOperand(const unsigned Idx, VPValue *Operand,
                  bool IsStructOffset = false) {
    assert((Idx > 1 || !IsStructOffset) &&
           "Base pointer and first index operand of GEP cannot be a struct "
           "offset.");
    VPInstruction::setOperand(Idx, Operand);
    OperandIsStructOffset[Idx] = IsStructOffset;
  }

  /// Overloaded method for removing an operand at index \p Idx. The struct
  /// offset tracker is accordingly updated after operand removal.
  void removeOperand(const unsigned Idx) {
    VPInstruction::removeOperand(Idx);
    OperandIsStructOffset.erase(OperandIsStructOffset.begin() + Idx);
    assert(OperandIsStructOffset.size() == getNumOperands() &&
           "Number of operands and struct offset tracker sizes don't match.");
  }

  /// Check if a given operand \p Operand of this GEP is a struct offset.
  bool isOperandStructOffset(VPValue *Operand) const {
    auto It = llvm::find(operands(), Operand);
    assert(It != op_end() && "Operand not found in VPGEPInstruction.");
    return OperandIsStructOffset[std::distance(op_begin(), It)];
  }

  /// Check if operand at index \p Idx of this GEP is a struct offset.
  bool isOperandStructOffset(const unsigned Idx) const {
    assert(Idx < OperandIsStructOffset.size() &&
           "Operand index out of bounds.");
    return OperandIsStructOffset[Idx];
  }

  /// Methods for supporting type inquiry through isa, cast and dyn_cast:
  static bool classof(const VPInstruction *VPI) {
    return VPI->getOpcode() == Instruction::GetElementPtr;
  }

  static bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  virtual VPGEPInstruction *cloneImpl() const final {
    VPGEPInstruction *Cloned =
        new VPGEPInstruction(getType(), getOperand(0), {}, isInBounds());
    for (auto *O : make_range(op_begin()+1, op_end())) {
      Cloned->addOperand(O, isOperandStructOffset(O));
    }
    return Cloned;
  }
};

// VPInstruction to initialize vector for induction variable.
// It's initialized depending on the binary operation,
// For +/-   : broadcast(start) + step*{0, 1,..,VL -1}
// For */div : broadcast(start) * pow(step,{0, 1,..,VL -1})
// Other binary operations are not induction-compatible.
class VPInductionInit : public VPInstruction {
public:
  VPInductionInit(VPValue *Start, VPValue *Step, Instruction::BinaryOps Opc)
      : VPInstruction(VPInstruction::InductionInit, Start->getType(),
                      {Start, Step}),
        BinOpcode(Opc) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionInit;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

private:
  Instruction::BinaryOps BinOpcode;
};

// VPInstruction to initialize vector for induction step.
// It's initialized depending on the binary operation,
// For +/-   : broadcast(step*VL)
// For */div : broadcast(pow(step,VL))
// Other binary operations are not induction-compatible.
class VPInductionInitStep : public VPInstruction {
public:
  VPInductionInitStep(VPValue *Step, Instruction::BinaryOps Opcode)
      : VPInstruction(VPInstruction::InductionInitStep, Step->getType(),
                      {Step}),
        BinOpcode(Opcode) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionInitStep;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }
  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

private:
  Instruction::BinaryOps BinOpcode = Instruction::BinaryOpsEnd;
};

// VPInstruction for induction last value calculation.
// It's calculated depending on the binary operation,
// For +/-   : lv = start OP step*count
// For */div : lv = start OP pow(step, count)
// Other binary operations are not induction-compatible.
//
// We should choose the optimal way for that - probably, for mul/div we should
// prefer scalar calculations in the loop body or extraction from the final
// vector.
class VPInductionFinal : public VPInstruction {
public:
  /// Constructor to extract last lane (for */div).
  VPInductionFinal(VPValue *InducVec)
      : VPInstruction(VPInstruction::InductionFinal, InducVec->getType(),
                      {InducVec}) {}

  /// Constructor to calculate using close-form (start+step*rounded_tc). The
  /// rounded trip count is known at code generation.
  VPInductionFinal(VPValue *Start, VPValue *Step, Instruction::BinaryOps Opcode)
      : VPInstruction(VPInstruction::InductionFinal, Start->getType(),
                      {Start, Step}),
        BinOpcode(Opcode) {}

  /// Return operand that corresponds to the reducing value.
  VPValue *getInductionOperand() const {
    assert(getNumOperands() == 1 && "Incorrect operand request");
    return getOperand(0);
  }

  /// Return operand that corresponds to the start value.
  VPValue *getStartValueOperand() const {
    assert(getNumOperands() == 2 && "Incorrect operand request");
    return getOperand(0);
  }

  /// Return operand that corresponds to the step value.
  VPValue *getStepOperand() const {
    assert(getNumOperands() == 2 && "Incorrect operand request");
    return getOperand(1);
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::InductionFinal;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  Instruction::BinaryOps getBinOpcode() const { return BinOpcode; }

private:
  Instruction::BinaryOps BinOpcode = Instruction::BinaryOpsEnd;
};

// VPInstruction for reduction initialization.
// It can be done in two ways and should be aligned with last value
// calculation  The first way is just broadcast(identity), the second one is
// to calculate vector of {start_value, identity,...,identity}. The second way
// is acceptable for some reductions (+,-,*) and allows eliminating one scalar
// operation during last value calculation. Though, that is ineffective for
// multiplication, while for summation the movd/movq x86 instructions
// perfectly fit this way.
//
class VPReductionInit : public VPInstruction {
public:
  VPReductionInit(VPValue *Identity)
      : VPInstruction(VPInstruction::ReductionInit, Identity->getType(),
                      {Identity}) {}

  VPReductionInit(VPValue *Identity, VPValue *StartValue)
      : VPInstruction(VPInstruction::ReductionInit, Identity->getType(),
                      {Identity, StartValue}) {}

  /// Return operand that corresponds to the indentity value.
  VPValue *getIdentityOperand() const { return getOperand(0);}

  /// Return operand that corresponds to the start value. Can be nullptr for
  /// optimized reduce.
  VPValue *getStartValueOperand() const {
    return getNumOperands() > 1 ? getOperand(1) : nullptr;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::ReductionInit;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }
};

// VPInstruction for reduction last value calculation.
// It's calculated depending on the binary operation. A general sequence
// is generated:
// v_tmp = shuffle(value,m1) // v_tmp contains the upper half of value
// v_tmp = value OP v_tmp;
// v_tmp2 = shuffle(v_tmp, m2) // v_tmp2 contains the upper half of v_tmp1
// v_tmp2 = v_tmp1 OP v_tmp2;
// ...
// res = (is_minmax || optimized_plus) ? vtmp_N : start OP vtp_N;
// (For minmax and optimized summation (see VPReductionInit) we don't
// need operation in the last step.)
//
// A special way is required for min/max+index reductions. The index
// part of the reduction has a link to the main, min/max, part and code
// generation for it requires two values of the main part, the last vector
// value and the last scalar value. They can be accesses having a link
// to the main instruction, which is passed as an additional argument to
// the index part.
//
class VPReductionFinal : public VPInstruction {
public:
  /// General constructor
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec, VPValue *StartValue,
                   bool Sign)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec, StartValue}),
        BinOpcode(BinOp), Signed(Sign) {}

  /// Constructor for optimized summation
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec}),
        BinOpcode(BinOp), Signed(false) {}

  /// Constructor for index part of min/max+index reduction.
  VPReductionFinal(unsigned BinOp, VPValue *ReducVec, VPValue *ParentExit,
                   VPReductionFinal *ParentFinal, bool Sign)
      : VPInstruction(VPInstruction::ReductionFinal, ReducVec->getType(),
                      {ReducVec, ParentExit, ParentFinal}),
        BinOpcode(BinOp), Signed(Sign) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::ReductionFinal;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  unsigned getBinOpcode() const { return BinOpcode; }

  bool isSigned() const { return Signed; }

  /// Return operand that corresponds to the reducing value.
  VPValue *getReducingOperand() const { return getOperand(0);}

  /// Return operand that corresponds to the start value. Can be nullptr for
  /// optimized reduce.
  VPValue *getStartValueOperand() const {
    return getNumOperands() == 2 ? getOperand(1) : nullptr;
  }

  /// Return operand that corrresponds to min/max parent vector value.
  VPValue *getParentExitValOperand() const {
    return getNumOperands() == 3 ? getOperand(1) : nullptr;
  }

  /// Return operand that corrresponds to min/max parent final value.
  VPValue *getParentFinalValOperand() const {
    return getNumOperands() == 3 ? getOperand(2) : nullptr;
  }

  /// Return true if this instruction is for last value calculation of an index
  /// part of min/max+index idiom.
  bool isMinMaxIndex() const {
    return getParentExitValOperand() != nullptr;
  }

  /// Return ID of the corresponding reduce intrinsic.
  Intrinsic::ID getVectorReduceIntrinsic() const {
    switch (BinOpcode) {
    case Instruction::Add:
    case Instruction::Sub:
      return Intrinsic::experimental_vector_reduce_add;
    case Instruction::FAdd:
    case Instruction::FSub:
      return Intrinsic::experimental_vector_reduce_v2_fadd;
    case Instruction::Mul:
      return Intrinsic::experimental_vector_reduce_mul;
    case Instruction::FMul:
      return Intrinsic::experimental_vector_reduce_v2_fmul;
    case Instruction::And:
      return Intrinsic::experimental_vector_reduce_and;
    case Instruction::Or:
      return Intrinsic::experimental_vector_reduce_or;
    case Instruction::Xor:
      return Intrinsic::experimental_vector_reduce_xor;
    case VPInstruction::UMin:
      return Intrinsic::experimental_vector_reduce_umin;
    case VPInstruction::SMin:
      return Intrinsic::experimental_vector_reduce_smin;
    case VPInstruction::UMax:
      return Intrinsic::experimental_vector_reduce_umax;
    case VPInstruction::SMax:
      return Intrinsic::experimental_vector_reduce_smax;
    case VPInstruction::FMax:
      return Intrinsic::experimental_vector_reduce_fmax;
    case VPInstruction::FMin:
      return Intrinsic::experimental_vector_reduce_fmin;
    default:
      llvm_unreachable("Vector reduction opcode not supported.");
    }
  }

private:
  unsigned BinOpcode;
  bool Signed;
};

// VPInstruction to allocate private memory. This is translated into
// allocation of a private memory in the function entry block. This instruction
// is not supposed to vectorize alloca instructions that appear inside the loop
// for arrays of a variable size.
class VPAllocatePrivate : public VPInstruction {
public:
  VPAllocatePrivate(Type *Ty)
      : VPInstruction(VPInstruction::AllocatePrivate, Ty, {}), IsSOASafe(false),
        IsSOAProfitable(false) {}

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPInstruction *V) {
    return V->getOpcode() == VPInstruction::AllocatePrivate;
  }

  // Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPValue *V) {
    return isa<VPInstruction>(V) && classof(cast<VPInstruction>(V));
  }

  /// Return true if doing SOA-layout transformation for the given memory is
  /// both safe and profitable.
  bool isSOALayout() const { return IsSOASafe && IsSOAProfitable; }

  /// Return true if memory is safe for SOA, i.e. all uses inside the loop
  /// are known and there are no layout-casts.
  bool isSOASafe() const { return IsSOASafe; }

  /// Return true if it's profitable to do SOA transformation, i.e. there
  /// is at least one uniform/unit-stride load/store to that memory (in case of
  /// private array), or the memory is a scalar structure
  bool isSOAProfitable() const { return IsSOAProfitable; }

  /// Set the property of the memory to be SOA-safe.
  void setSOASafe() { IsSOASafe = true; }

  /// Set the memory to be profitable for SOA-layout.
  void setSOAProfitable() { IsSOAProfitable = true; }

private:
  bool IsSOASafe;
  bool IsSOAProfitable;
};
#endif // INTEL_CUSTOMIZATION

/// VPBlockBase is the building block of the Hierarchical CFG. A VPBlockBase
/// can be either a VPBasicBlock or a VPRegionBlock.
///
/// The Hierarchical CFG is a control-flow graph whose nodes are basic-blocks
/// or Hierarchical CFG's. The Hierarchical CFG data structure we use is similar
/// to the Tile Tree [1], where cross-Tile edges are lifted to connect Tiles
/// instead of the original basic-blocks as in Sharir [2], promoting the Tile
/// encapsulation. We use the terms Region and Block rather than Tile [1] to
/// avoid confusion with loop tiling.
///
/// [1] "Register Allocation via Hierarchical Graph Coloring", David Callahan
/// and Brian Koblenz, PLDI 1991
///
/// [2] "Structural analysis: A new approach to flow analysis in optimizing
/// compilers", M. Sharir, Journal of Computer Languages, Jan. 1980
///
/// Note that in contrast to the IR BasicBlock, a VPBlockBase models its
/// control-flow edges with successor and predecessor VPBlockBase directly,
/// rather than through a Terminator branch or through predecessor branches that
/// Use the VPBlockBase.
class VPBlockBase {
  friend class VPBlockUtils;

private:
  const unsigned char VBID; // Subclass identifier (for isa/dyn_cast).

  std::string Name;

  /// The immediate VPRegionBlock which this VPBlockBase belongs to, or null if
  /// it is a topmost VPBlockBase.
  class VPRegionBlock *Parent = nullptr;

  /// List of predecessor blocks.
  SmallVector<VPBlockBase *, 2> Predecessors;

  /// List of successor blocks.
  SmallVector<VPBlockBase *, 2> Successors;

  /// Successor selector, null for zero or single successor blocks.
  VPValue *CondBit = nullptr;

  /// Current block predicate - null if the block does not need a predicate.
  VPValue *Predicate = nullptr;

#if INTEL_CUSTOMIZATION
public:
#endif
  /// \brief Add \p Successor as the last successor to this block.
  void appendSuccessor(VPBlockBase *Successor) {
    assert(Successor && "Cannot add nullptr successor!");
    Successors.push_back(Successor);
  }

  /// \brief Add \p Predecessor as the last predecessor to this block.
  void appendPredecessor(VPBlockBase *Predecessor) {
    assert(Predecessor && "Cannot add nullptr predecessor!");
    Predecessors.push_back(Predecessor);
  }

  /// \brief Remove \p Predecessor from the predecessors of this block.
  void removePredecessor(VPBlockBase *Predecessor) {
    auto Pos = std::find(Predecessors.begin(), Predecessors.end(), Predecessor);
    assert(Pos && "Predecessor does not exist");
    Predecessors.erase(Pos);
  }

  /// \brief Remove \p Successor from the successors of this block.
  void removeSuccessor(VPBlockBase *Successor) {
    auto Pos = std::find(Successors.begin(), Successors.end(), Successor);
    assert(Pos && "Successor does not exist");
    Successors.erase(Pos);
  }

protected:
  VPBlockBase(const unsigned char SC, const std::string &N)
      : VBID(SC), Name(N) {}

public:
  /// An enumeration for keeping track of the concrete subclass of VPBlockBase
  /// that is actually instantiated. Values of this enumeration are kept in the
  /// VPBlockBase classes VBID field. They are used for concrete type
  /// identification.
#if INTEL_CUSTOMIZATION
  typedef enum {
    VPBasicBlockSC,
    VPRegionBlockSC,
    VPLoopRegionSC,
    VPLoopRegionHIRSC
  } VPBlockTy;
#else
  typedef enum { VPBasicBlockSC, VPRegionBlockSC } VPBlockTy;
#endif
  virtual ~VPBlockBase() {}

  const std::string &getName() const { return Name; }

  void setName(const Twine &newName) { Name = newName.str(); }

  /// \return an ID for the concrete type of this object.
  /// This is used to implement the classof checks. This should not be used
  /// for any other purpose, as the values may change as LLVM evolves.
  unsigned getVPBlockID() const { return VBID; }

  VPRegionBlock *getParent() { return Parent; }
  const VPRegionBlock *getParent() const { return Parent; }

  void setParent(VPRegionBlock *P) { Parent = P; }

  /// \return the VPBasicBlock that is the entry of this VPBlockBase,
  /// recursively, if the latter is a VPRegionBlock. Otherwise, if this
  /// VPBlockBase is a VPBasicBlock, it is returned.
  const class VPBasicBlock *getEntryBasicBlock() const;
  class VPBasicBlock *getEntryBasicBlock();

  /// \return the VPBasicBlock that is the exit of this VPBlockBase,
  /// recursively, if the latter is a VPRegionBlock. Otherwise, if this
  /// VPBlockBase is a VPBasicBlock, it is returned.
  const class VPBasicBlock *getExitBasicBlock() const;
  class VPBasicBlock *getExitBasicBlock();

  const SmallVectorImpl<VPBlockBase *> &getSuccessors() const {
    return Successors;
  }

  const SmallVectorImpl<VPBlockBase *> &getPredecessors() const {
    return Predecessors;
  }

  SmallVectorImpl<VPBlockBase *> &getSuccessors() { return Successors; }

  SmallVectorImpl<VPBlockBase *> &getPredecessors() { return Predecessors; }

#if INTEL_CUSTOMIZATION
  VPBlockBase *getSingleSuccessor() {
    if (Successors.size() != 1)
      return nullptr;
    return *Successors.begin();
  }

  VPBlockBase *getSinglePredecessor() {
    if (Predecessors.size() != 1)
      return nullptr;
    return *Predecessors.begin();
  }

  size_t getNumSuccessors() const { return Successors.size(); }

  size_t getNumPredecessors() const { return Predecessors.size(); }
#endif // INTEL_CUSTOMIZATION

  /// \return the successor of this VPBlockBase if it has a single successor.
  /// Otherwise return a null pointer.
  VPBlockBase *getSingleSuccessor() const {
    return (Successors.size() == 1 ? *Successors.begin() : nullptr);
  }

  /// \return the predecessor of this VPBlockBase if it has a single
  /// predecessor. Otherwise return a null pointer.
  VPBlockBase *getSinglePredecessor() const {
    return (Predecessors.size() == 1 ? *Predecessors.begin() : nullptr);
  }

#if INTEL_CUSTOMIZATION
  /// If this basic block has a unique predecessor block, return the block,
  /// otherwise return a null pointer. Note that unique predecessor doesn't
  /// mean single edge, there can be multiple edges from the unique predecessor
  /// to this block (for example a switch statement with multiple cases having
  /// the same destination).
  const VPBlockBase *getUniquePredecessor() const {
    auto PI = Predecessors.begin();
    auto E = Predecessors.end();
    if (PI == E)
      return nullptr; // No preds.
    const VPBlockBase *PredBB = *PI;
    ++PI;
    for (; PI != E; ++PI) {
      if (*PI != PredBB)
        return nullptr;
      // The same predecessor appears multiple times in the predecessor list.
      // This is OK.
    }
    return PredBB;
  }
#endif // INTEL_CUSTOMIZATION

  VPValue *getPredicate() { return Predicate; }

  const VPValue *getPredicate() const { return Predicate; }

  void setPredicate(VPValue *Pred) { Predicate = Pred; }

#if INTEL_CUSTOMIZATION
  // Please, do not use setSuccessor and setTwoSuccessors in VPO Vectorizer.
  // Depending on what you need, you may want to use connectBlocks or
  // insertBlockBefore and insertBlockAfter. appendBlockSuccessor,
  // appendBlockPredecessor, setBlockSuccessor and setBlockTwoSuccessors are
  // also available but their use should be less common. Original setSuccessor
  // and setTwoSuccessors are also setting predecessors and parent, which is
  // known to cause problems:
  //  1. Predecessors for original LLVM CFG must be appended in the right order
  //  and not following successors' order.
  //  2. If Block's parent is wrong, it will be propagated to Successor anyway.
#endif

  /// connectBlocks should be used instead of this function when possible.
  /// Set a given VPBlockBase \p Successor as the single successor of this
  /// VPBlockBase. This VPBlockBase is not added as predecessor of \p Successor.
  /// This VPBlockBase must have no successors.
  void setOneSuccessor(VPBlockBase *Successor) {
    assert(Successors.empty() && "Setting one successor when others exist.");
    appendSuccessor(Successor);
  }

  /// connectBlocks should be used instead of this function when possible.
  /// Set two given VPBlockBases \p IfTrue and \p IfFalse to be the two
  /// successors of this VPBlockBase. This VPBlockBase is not added as
  /// predecessor of \p IfTrue or \p IfFalse. This VPBlockBase must have no
  /// successors. \p ConditionV is set as successor selector.
  void setTwoSuccessors(VPValue *ConditionV, VPBlockBase *IfTrue,
                        VPBlockBase *IfFalse);

  /// Set each VPBasicBlock in \p NewPreds as predecessor of this VPBlockBase.
  /// This VPBlockBase must have no predecessors. This VPBlockBase is not added
  /// as successor of any VPBasicBlock in \p NewPreds.
  void setPredecessors(ArrayRef<VPBlockBase *> NewPreds) {
    assert(Predecessors.empty() && "Block predecessors already set.");
    for (auto *Pred : NewPreds)
      appendPredecessor(Pred);
  }

#if INTEL_CUSTOMIZATION
  /// Remove all the predecessor of this block.
  void clearPredecessors() { Predecessors.clear(); }

  /// Remove all the successors of this block and set its condition bit to null.
  void clearSuccessors() {
    Successors.clear();
    CondBit = nullptr;
  }
#endif

  /// \return the condition bit selecting the successor.
  VPValue *getCondBit() { return CondBit; }

  const VPValue *getCondBit() const { return CondBit; }

  void setCondBit(VPValue *CB) { CondBit = CB; }

  /// The method which generates all new IR instructions that correspond to
  /// this VPBlockBase in the vectorized version, thereby "executing" the VPlan.
  virtual void execute(struct VPTransformState *State) = 0;
#if INTEL_CUSTOMIZATION
  virtual void executeHIR(VPOCodeGenHIR *CG) = 0;
#endif


  // Delete all blocks reachable from a given VPBlockBase, inclusive.
  static void deleteCFG(VPBlockBase *Entry);

  // Quick hack to get pulldown to compile. This needs more serious
  // consideration.
  bool isLegalToHoistInto() { return true; }

#if INTEL_CUSTOMIZATION
#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  void printAsOperand(raw_ostream &OS, bool PrintType) const {
    (void)PrintType;
    OS << getName();
  }

  void print(raw_ostream &OS, unsigned Indent = 0,
             const VPlanDivergenceAnalysis *DA = nullptr,
             const Twine &NamePrefix = "") const;

  void dump() const { print(dbgs()); };
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  // Iterators and types to access Successors of a VPBlockBase
  using SuccType = SmallVector<VPBlockBase *, 2>;
  using succ_iterator = SuccType::iterator;
  using succ_const_iterator = SuccType::const_iterator;
  using succ_reverse_iterator = SuccType::reverse_iterator;
  using succ_const_reverse_iterator = SuccType::const_reverse_iterator;

  inline succ_iterator succ_begin() { return Successors.begin(); }
  inline succ_iterator succ_end() { return Successors.end(); }
  inline succ_const_iterator succ_begin() const { return Successors.begin(); }
  inline succ_const_iterator succ_end() const { return Successors.end(); }
  inline succ_reverse_iterator succ_rbegin() { return Successors.rbegin(); }
  inline succ_reverse_iterator succ_rend() { return Successors.rend(); }
  inline succ_const_reverse_iterator succ_rbegin() const {
    return Successors.rbegin();
  }
  inline succ_const_reverse_iterator succ_rend() const {
    return Successors.rend();
  }
#endif // INTEL_CUSTOMIZATION
};

/// VPBasicBlock serves as the leaf of the Hierarchical CFG. It represents a
/// sequence of instructions that will appear consecutively in a basic block
/// of the vectorized version. The VPBasicBlock takes care of the control-flow
/// relations with other VPBasicBlock's and Regions. It holds a sequence of zero
/// or more VPInstructions that take care of representing the instructions.
/// A VPBasicBlock that holds no instructions: this may happen, e.g., to support
/// disjoint Regions and to ensure Regions have a single exit, possibly an empty
/// one.
///
/// Note that in contrast to the IR BasicBlock, a VPBasicBlock models its
/// control-flow edges with successor and predecessor VPBlockBase directly,
/// rather than through a Terminator branch or through predecessor branches that
/// "use" the VPBasicBlock.
class VPBasicBlock : public VPBlockBase {
  friend class VPBlockUtils;
public:
  using VPInstructionListTy = iplist<VPInstruction>;

private:
  /// The list of VPInstructions, held in order of instructions to generate.
  VPInstructionListTy Instructions;

public:
  /// Instruction iterators...
  using iterator = VPInstructionListTy::iterator;
  using const_iterator = VPInstructionListTy::const_iterator;
  using reverse_iterator = VPInstructionListTy::reverse_iterator;
  using const_reverse_iterator = VPInstructionListTy::const_reverse_iterator;

  //===--------------------------------------------------------------------===//
  /// Instruction iterator methods
  ///
  inline iterator begin() { return Instructions.begin(); }
  inline const_iterator begin() const { return Instructions.begin(); }
  inline iterator end() { return Instructions.end(); }
  inline const_iterator end() const { return Instructions.end(); }

  inline reverse_iterator rbegin() { return Instructions.rbegin(); }
  inline const_reverse_iterator rbegin() const { return Instructions.rbegin(); }
  inline reverse_iterator rend() { return Instructions.rend(); }
  inline const_reverse_iterator rend() const { return Instructions.rend(); }

  inline size_t size() const { return Instructions.size(); }
  inline bool empty() const { return Instructions.empty(); }
  inline const VPInstruction &front() const { return Instructions.front(); }
  inline VPInstruction &front() { return Instructions.front(); }
  inline const VPInstruction &back() const { return Instructions.back(); }
  inline VPInstruction &back() { return Instructions.back(); }

  /// \brief Returns a pointer to a member of the instruction list.
  static VPInstructionListTy VPBasicBlock::*getSublistAccess(VPInstruction *) {
    return &VPBasicBlock::Instructions;
  }

  auto getVPPhis() {
    auto AsVPPHINode = [](VPInstruction &Instruction) -> VPPHINode & {
      return cast<VPPHINode>(Instruction);
    };

    // If the block is empty or if it has no PHIs, return null range
    if (empty() || !isa<VPPHINode>(begin()))
      return map_range(make_range(end(), end()), AsVPPHINode);

    // Increment iterator till a non PHI VPInstruction is found
    iterator It = begin();
    while (It != end() && isa<VPPHINode>(It))
      ++It;

    return map_range(make_range(begin(), It), AsVPPHINode);
  }

  auto getVPPhis() const {
    auto AsVPPHINode = [](const VPInstruction &Instruction) -> const VPPHINode & {
      return cast<VPPHINode>(Instruction);
    };

    // If the block is empty or if it has no PHIs, return null range
    if (empty() || !isa<VPPHINode>(begin()))
      return map_range(make_range(end(), end()), AsVPPHINode);

    // Increment iterator till a non PHI VPInstruction is found
    const_iterator It = begin();
    while (It != end() && isa<VPPHINode>(It))
      ++It;

    return map_range(make_range(begin(), It), AsVPPHINode);
  }

  VPBasicBlock(const std::string &Name, VPInstruction *Instruction = nullptr)
      : VPBlockBase(VPBasicBlockSC, Name), CBlock(nullptr), TBlock(nullptr),
        FBlock(nullptr), OriginalBB(nullptr) {
    if (Instruction)
      appendInstruction(Instruction);
  }

  ~VPBasicBlock() { Instructions.clear(); }

  /// Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPBlockBase *V) {
    return V->getVPBlockID() == VPBlockBase::VPBasicBlockSC;
  }

  void insert(VPInstruction *Instruction, iterator InsertPt) {
    assert(Instruction && "No instruction to append.");
    assert(!Instruction->Parent && "Instruction already in VPlan");
    Instruction->Parent = this;
    Instructions.insert(InsertPt, Instruction);
  }

  /// Augment the existing instructions of a VPBasicBlock with an additional
  /// \p Instruction as the last Instruction.
  void appendInstruction(VPInstruction *Instruction) {
    insert(Instruction, end());
  }

#if INTEL_CUSTOMIZATION
  /// Add \p Instruction after \p After. If \p After is null, \p Instruction
  /// will be inserted as the first instruction.
  void addInstructionAfter(VPInstruction *Instruction, VPInstruction *After) {
    Instruction->Parent = this;
    if (!After) {
      Instructions.insert(Instructions.begin(), Instruction);
    } else {
      Instructions.insertAfter(After->getIterator(), Instruction);
    }
  }

  /// Augment the existing instructions of a VPBasicBlock with an additional
  /// \p Instruction at a position given by an existing instruction \p Before.
  /// \p If Before is null, \p Instruction is appended as the last instruction.
  void addInstruction(VPInstruction *Instruction,
                      VPInstruction *Before = nullptr) {
    Instruction->Parent = this;
    if (!Before) {
      Instructions.insert(Instructions.end(), Instruction);
    } else {
      assert(Before->Parent == this &&
             "Insertion before point not in this basic block.");
      Instructions.insert(Before->getIterator(), Instruction);
    }
  }

  void moveConditionalEOBTo(VPBasicBlock *ToBB);
#endif

  /// Remove the instruction from VPBasicBlock's instructions.
  void removeInstruction(VPInstruction *Instruction) {
    Instructions.remove(Instruction);
  }

  /// Remove the instruction from VPBasicBlock's instructions and destroy
  /// Instruction object.
  void eraseInstruction(VPInstruction *Instruction) {
    // We need to remove all instruction operands before erasing the
    // VPInstruction. Else this breaks the use-def chains.
    while (Instruction->getNumOperands())
      Instruction->removeOperand(0);

    Instructions.erase(Instruction);
  }

  /// The method which generates all new IR instructions that correspond to
  /// this VPBasicBlock in the vectorized version, thereby "executing" the
  /// VPlan.
  void execute(struct VPTransformState *State) override;
#if INTEL_CUSTOMIZATION
  void executeHIR(VPOCodeGenHIR *CG) override;
#endif

  /// Retrieve the list of VPInstructions that belong to this VPBasicBlock.
  const VPInstructionListTy &getInstructions() const { return Instructions; }
  VPInstructionListTy &getInstructions() { return Instructions; }
#if INTEL_CUSTOMIZATION
  /// Returns a range that iterates over non predicator related instructions
  /// in the VPBasicBlock.
  auto getNonPredicateInstructions() const {
    // New predicator uses VPInstructions to generate the block predicate.
    // Skip instructions until block-predicate instruction is seen if the block
    // has a predicate.

    // No predicate instruction, return immediately.
    if(getPredicate() == nullptr)
      return make_range(begin(), end());

    auto It = begin();
    auto ItEnd = end();

    assert(It != ItEnd &&
           "VPBasicBlock without VPInstructions can't have a predicate!");

    // Skip until predicate.
    while (It->getOpcode() != VPInstruction::Pred) {
      assert(It != ItEnd && "Predicate VPInstruction not found!");
      ++It;
    }

    // Skip the predicate itself.
    ++It;

    return make_range(It, ItEnd);
  }
#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  void print(raw_ostream &OS, unsigned Indent = 0,
             const VPlanDivergenceAnalysis *DA = nullptr,
             const Twine &NamePrefix = "") const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP
  void setCBlock(BasicBlock *CB) { CBlock = CB; }
  void setFBlock(BasicBlock *FB) { FBlock = FB; }
  void setTBlock(BasicBlock *TB) { TBlock = TB; }
  BasicBlock *getCBlock() { return CBlock; }
  BasicBlock *getTBlock() { return TBlock; }
  BasicBlock *getFBlock() { return FBlock; }

  bool hasTrueEdge() { return CBlock && TBlock; }
  bool hasFalseEdge() { return CBlock && FBlock; }

  void setOriginalBB(BasicBlock *BB) { OriginalBB = BB; }
  BasicBlock *getOriginalBB() const { return OriginalBB; }

  TripCountInfo *getTripCountInfo() { return TCInfo.get(); }
  const TripCountInfo *getTripCountInfo() const { return TCInfo.get(); }
  void setTripCountInfo(std::unique_ptr<TripCountInfo> TCInfo) {
    assert(!this->TCInfo && "Trip count info alread set!");
    this->TCInfo = std::move(TCInfo);
  }
  void moveTripCountInfoFrom(VPBasicBlock *OtherBB) {
    TCInfo = std::move(OtherBB->TCInfo);
  }

private:
  /// Create an IR BasicBlock to hold the instructions vectorized from this
  /// VPBasicBlock, and return it. Update the CFGState accordingly.
  BasicBlock *createEmptyBasicBlock(VPTransformState *State);

  /// Split this block before the VPInstruction pointer by the \p I, or before
  /// the implicit [conditional] jump represented by the successors.
  ///
  /// This routine also updates the CFG accordingly, i.e. moves
  /// successors/condition bits to the newly created block. If \p I points to a
  /// VPPHINode, the split happens after the last VPPHINode in the current block
  /// to preserve correctness.
  ///
  /// Block predicate instruction is NOT cloned, if original block contained,
  /// only one of the blocks will have it.
  ///
  /// VPHINodes in the successors of this block are also updated. In case of
  /// Blends, incoming blocks are updated in such a way that the block
  /// containing the block-predicate instruction after the split is used.
  VPBasicBlock *splitBlock(iterator I, const Twine &NewBBName = "");

private:
  BasicBlock *CBlock;
  BasicBlock *TBlock;
  BasicBlock *FBlock;
  BasicBlock *OriginalBB;

  // TODO: Not sure what other types of loop metadata we'd need. Most probably,
  // we need some abstraction on top of TripCountInfo (and maybe that struct
  // itself should be split in some way). The idea about this field is to have
  // something similar to LLVM IR's loop metadata on the backedge branch
  // instruction, so it will be filled for the latches only.
  std::unique_ptr<TripCountInfo> TCInfo;
#endif
};

/// VPRegionBlock represents a collection of VPBasicBlocks and VPRegionBlocks
/// which form a single-entry-single-exit subgraph of the CFG in the vectorized
/// code.
///
/// A VPRegionBlock may indicate that its contents are to be replicated several
/// times. This is designed to support predicated scalarization, in which a
/// scalar if-then code structure needs to be generated VF * UF times. Having
/// this replication indicator helps to keep a single VPlan for multiple
/// candidate VF's; the actual replication takes place only once the desired VF
/// and UF have been determined.
///
/// **Design principle:** when some additional information relates to an SESE
/// set of VPBlockBase, we use a VPRegionBlock to wrap them and attach the
/// information to it. For example, a VPRegionBlock can be used to indicate that
/// a scalarized SESE region is to be replicated, and that a vectorized SESE
/// region can retain its internal control-flow, independent of the control-flow
/// external to the region.
class VPRegionBlock : public VPBlockBase {
  friend class VPBlockUtils;

private:
  /// Hold the Single Entry of the SESE region represented by the VPRegionBlock.
  VPBlockBase *Entry;

  /// Hold the Single Exit of the SESE region represented by the VPRegionBlock.
  VPBlockBase *Exit;

#if INTEL_CUSTOMIZATION
  /// Holds the number of VPBasicBlocks within the region. It is necessary for
  /// dominator tree.
  unsigned Size;

  /// Traverse all the region VPBasicBlocks to recompute Size
  void recomputeSize();
#endif

#if INTEL_CUSTOMIZATION
  /// Dominator Tree for the region
  VPDominatorTree *RegionDT;
  /// Post-Dominator Tree for the region
  VPPostDominatorTree *RegionPDT;
#endif

public:
  /// An enumeration for keeping track of the concrete subclass of VPRegionBlock
  /// that is actually instantiated. Values of this enumeration are kept in the
  /// VPRegionBlock classes VRID field. They are used for concrete type
  /// identification.
  VPRegionBlock(const unsigned char SC, const std::string &Name)
      : VPBlockBase(SC, Name), Entry(nullptr), Exit(nullptr), Size(0),
        RegionDT(nullptr), RegionPDT(nullptr) {}

  ~VPRegionBlock();

  /// Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPBlockBase *V) {
#if INTEL_CUSTOMIZATION
    return V->getVPBlockID() == VPBlockBase::VPRegionBlockSC ||
           V->getVPBlockID() == VPBlockBase::VPLoopRegionSC ||
           V->getVPBlockID() == VPBlockBase::VPLoopRegionHIRSC;
#else
    return V->getVPBlockID() == VPBlockBase::VPRegionBlockSC;
#endif
  }

  const VPBlockBase *getEntry() const { return Entry; }
  VPBlockBase *getEntry() { return Entry; }

  /// Set \p EntryBlock as the entry VPBlockBase of this VPRegionBlock. \p
  /// EntryBlock must have no predecessors.
  void setEntry(VPBlockBase *EntryBlock) {
    assert(EntryBlock->getPredecessors().empty() &&
           "Entry block cannot have predecessors.");
    Entry = EntryBlock;
    EntryBlock->setParent(this);
  }

  const VPBlockBase *getExit() const { return Exit; }
  VPBlockBase *getExit() { return Exit; }

  /// Set \p ExitBlock as the exit VPBlockBase of this VPRegionBlock. \p
  /// ExitBlock must have no successors.
  void setExit(VPBlockBase *ExitBlock) {
    assert(ExitBlock->getSuccessors().empty() &&
           "Exit block cannot have successors.");
    Exit = ExitBlock;
    ExitBlock->setParent(this);
  }

#if INTEL_CUSTOMIZATION
  unsigned getSize() const { return Size; }

  void setSize(unsigned Sz) { Size = Sz; }

  // TODO: This is weird. For some reason, DominatorTreeBase is using
  // A->getParent()->front() instead of using GraphTraints::getEntry. We may
  // need to report it.
  VPBlockBase &front() const { return *Entry; }
#endif

#if INTEL_CUSTOMIZATION
  /// Getters for Dominator Tree
  VPDominatorTree *getDT(void) { return RegionDT; }
  const VPDominatorTree *getDT(void) const { return RegionDT; }
  /// Getter for Post-Dominator Tree
  VPPostDominatorTree *getPDT(void) { return RegionPDT; }

  /// Compute the Dominator Tree for this region
  void computeDT(void);

  /// Compute the Post-Dominator Tree for this region
  void computePDT(void);

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  void print(raw_ostream &OS, unsigned Indent = 0,
             const VPlanDivergenceAnalysis *DA = nullptr,
             const Twine &NamePrefix = "") const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP
#endif

  /// The method which generates the new IR instructions that correspond to
  /// this VPRegionBlock in the vectorized version, thereby "executing" the
  /// VPlan.
  void execute(struct VPTransformState *State) override;
#if INTEL_CUSTOMIZATION
  void executeHIR(VPOCodeGenHIR *CG) override;
#endif
};

/// VPlan models a candidate for vectorization, encoding various decisions take
/// to produce efficient output IR, including which branches, basic-blocks and
/// output IR instructions to generate, and their cost. VPlan holds a
/// Hierarchical-CFG of VPBasicBlocks and VPRegionBlocks rooted at an Entry
/// VPBlock.
class VPlan {
  friend class VPlanPrinter;

private:
#if INTEL_CUSTOMIZATION
  LLVMContext *Context = nullptr;
  std::unique_ptr<VPLoopInfo> VPLInfo;
  std::unique_ptr<VPlanDivergenceAnalysis> VPlanDA;
  const DataLayout *DL = nullptr;

  // We need to force full linearization for certain cases. Currently this
  // happens for cases where while-loop canonicalization or merge loop exits
  // transformation break SSA or for HIR vector code generation which needs
  // to be extended to preserve uniform control flow. This flag is set to true
  // when we need to force full linearization. Full linearization can still
  // kick in when this flag is false such as cases where we use a command
  // line option to do the same.
  bool FullLinearizationForced = false;

  // The flag shows whether all steps needed for the loop entities privatization
  // are finished. Particularly, all VPInstructions for private memory
  // allocation are generated and the needed replacements in the VPLoop code are
  // done.
  bool LoopEntitiesPrivatizationIsDone = false;
#endif

#if INTEL_CUSTOMIZATION
protected:
#endif
  /// Hold the single entry to the Hierarchical CFG of the VPlan.
  std::unique_ptr<VPRegionBlock> Entry;

  /// Holds the name of the VPlan, for printing.
  std::string Name;

#if INTEL_CUSTOMIZATION
  /// Holds all the VPConstants created for this VPlan.
  DenseMap<Constant *, std::unique_ptr<VPConstant>> VPConstants;

  /// Holds all the external definitions representing an underlying Value
  /// in this VPlan. CFGBuilder ensures these are unique.
  SmallVector<std::unique_ptr<VPExternalDef>, 16> VPExternalDefs;

  /// Holds all the external definitions representing an HIR underlying entity
  /// in this VPlan. The hash is based on the underlying HIR information that
  /// uniquely identifies each external definition.
  FoldingSet<VPExternalDef> VPExternalDefsHIR;

  /// Holds all the external uses in this VPlan representing an underlying
  /// Value. The key is the underlying Value that uniquely identifies each
  /// external use.
  DenseMap<Value *, std::unique_ptr<VPExternalUse>> VPExternalUses;

  /// Holds all the external uses representing an HIR underlying entity
  /// in this VPlan. The key is the underlying HIR information that uniquely
  /// identifies each external use.
  FoldingSet<VPExternalUse> VPExternalUsesHIR;

  /// Holds all the VPMetadataAsValues created for this VPlan.
  DenseMap<MetadataAsValue *, std::unique_ptr<VPMetadataAsValue>>
      VPMetadataAsValues;

  DenseMap<const VPLoop *, std::unique_ptr<VPLoopEntityList>> LoopEntities;
#else
  /// Holds a mapping between Values and their corresponding VPValue inside
  /// VPlan.
  Value2VPValueTy Value2VPValue;
#endif

public:
  VPlan(LLVMContext *Context, const DataLayout *DL)
      : Context(Context), DL(DL) {}

  ~VPlan() = default;

  /// Generate the IR code for this VPlan.
  void execute(struct VPTransformState *State);
#if INTEL_CUSTOMIZATION
  void executeHIR(VPOCodeGenHIR *CG);
  VPLoopInfo *getVPLoopInfo() { return VPLInfo.get(); }

  const VPLoopInfo *getVPLoopInfo() const { return VPLInfo.get(); }

  void setVPLoopInfo(std::unique_ptr<VPLoopInfo> VPLI) {
    VPLInfo = std::move(VPLI);
  }

  void setVPlanDA(std::unique_ptr<VPlanDivergenceAnalysis> VPDA) {
    VPlanDA = std::move(VPDA);
  }

  LLVMContext *getLLVMContext(void) const { return Context; }

  VPlanDivergenceAnalysis *getVPlanDA() const { return VPlanDA.get(); }

  void markFullLinearizationForced() { FullLinearizationForced = true; }
  bool isFullLinearizationForced() const { return FullLinearizationForced; }

  const DataLayout* getDataLayout() const { return DL; }

  /// Return an existing or newly created LoopEntities for the loop \p L.
  VPLoopEntityList *getOrCreateLoopEntities(const VPLoop *L) {
    // Sanity check
    VPBlockBase *HeaderBB = L->getHeader(); (void)HeaderBB;
    assert(VPLInfo->getLoopFor(HeaderBB) == L &&
           "the loop does not exist in VPlan");

    std::unique_ptr<VPLoopEntityList> &Ptr = LoopEntities[L];
    if (!Ptr) {
      VPLoopEntityList *E =
          new VPLoopEntityList(*this, *(const_cast<VPLoop *>(L)));
      Ptr.reset(E);
    }
    return Ptr.get();
  }

  /// Return LoopEntities list for the loop \p L. The nullptr is returned if
  /// the descriptors were not created for the loop.
  const VPLoopEntityList* getLoopEntities(const VPLoop* L) const {
    auto Iter = LoopEntities.find(L);
    if (Iter == LoopEntities.end())
      return nullptr;
    return Iter->second.get();
  }

  /// Return \p true if we have finished importing of loop-entities and
  /// generated appropriate VPInstructions.
  bool isLoopEntitiesPrivatizationDone() const {
    return LoopEntitiesPrivatizationIsDone;
  }

  /// Mark the flag that indicates that importing of loop entities is done.
  void setLoopEntitiesPrivatizationDone(bool LEImportsDone) {
    LoopEntitiesPrivatizationIsDone = LEImportsDone;
  }
#endif // INTEL_CUSTOMIZATION

  VPRegionBlock *getEntry() { return Entry.get(); }
  const VPRegionBlock *getEntry() const { return Entry.get(); }

  void setEntry(std::unique_ptr<VPRegionBlock> Block) {
    Entry = std::move(Block);
  }

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
  /// Print (in text format) VPlan blocks in order based on dominator tree.
  void dump(raw_ostream &OS, bool DumpDA = false) const;
  void dump() const;
  void dumpLivenessInfo(raw_ostream &OS) const;
#endif // !NDEBUG || LLVM_ENABLE_DUMP

  const std::string &getName() const { return Name; }

  void setName(const Twine &newName) { Name = newName.str(); }

#if INTEL_CUSTOMIZATION
  /// Create a new VPConstant for \p Const if it doesn't exist or retrieve the
  /// existing one.
  VPConstant *getVPConstant(Constant *Const) {
    std::unique_ptr<VPConstant> &UPtr = VPConstants[Const];
    if (!UPtr)
      // Const is a new VPConstant to be inserted in the map.
      UPtr.reset(new VPConstant(Const));

    return UPtr.get();
  }

  /// Create or retrieve a VPExternalDef for a given Value \p ExtVal.
  VPExternalDef *getVPExternalDef(Value *ExtDef) {
    VPExternalDefs.emplace_back(new VPExternalDef(ExtDef));
    return VPExternalDefs.back().get();
  }

  /// Create or retrieve a VPExternalDef for a given non-decomposable DDRef \p
  /// DDR.
  VPExternalDef *getVPExternalDefForDDRef(const loopopt::DDRef *DDR) {
    return getExternalItemForDDRef(VPExternalDefsHIR, DDR);
  }

  /// Retrieve the VPExternalDef for given HIR symbase \p Symbase. If no
  /// external definition exists then a nullptr is returned.
  VPExternalDef *getVPExternalDefForSymbase(unsigned Symbase) {
    return getExternalItemForSymbase(VPExternalDefsHIR, Symbase);
  }

  /// Create or retrieve a VPExternalDef for an HIR IV identified by its \p
  /// IVLevel.
  VPExternalDef *getVPExternalDefForIV(unsigned IVLevel, Type *BaseTy) {
    return getExternalItemForIV(VPExternalDefsHIR, IVLevel, BaseTy);
  }

  /// Create or retrieve a VPExternalUse for a given Value \p ExtVal.
  VPExternalUse *getVPExternalUse(Value *ExtDef) {
    return getExternalItem(VPExternalUses, ExtDef);
  }

  /// Create or retrieve a VPExternalUse for a given non-decomposable DDRef \p
  /// DDR.
  VPExternalUse *getVPExternalUseForDDRef(const loopopt::DDRef *DDR) {
    return getExternalItemForDDRef(VPExternalUsesHIR, DDR);
  }

  /// Create or retrieve a VPExternalUse for an HIR IV identified by its \p
  /// IVLevel.
  VPExternalUse *getVPExternalUseForIV(unsigned IVLevel, Type *BaseTy) {
    return getExternalItemForIV(VPExternalUsesHIR, IVLevel, BaseTy);
  }

  /// Create a new VPMetadataAsValue for \p MDAsValue if it doesn't exist or
  /// retrieve the existing one.
  VPMetadataAsValue *getVPMetadataAsValue(MetadataAsValue *MDAsValue) {
    std::unique_ptr<VPMetadataAsValue> &UPtr = VPMetadataAsValues[MDAsValue];
    if (!UPtr)
      // MDAsValue is a new VPMetadataAsValue to be inserted in the map.
      UPtr.reset(new VPMetadataAsValue(MDAsValue));

    return UPtr.get();
  }

  /// Create a new VPMetadataAsValue for Metadata \p MD if it doesn't exist or
  /// retrieve the existing one.
  VPMetadataAsValue *getVPMetadataAsValue(Metadata *MD) {
    // TODO: implement this method when needed.
    llvm_unreachable("Not implemented yet!");
  }

  // Verify that VPConstants are unique in the pool and that the map keys are
  // consistent with the underlying IR information of each VPConstant.
  void verifyVPConstants() const;

  // Verify that VPExternalDefs are unique in the pool and that the map keys are
  // consistent with the underlying IR information of each VPExternalDef.
  void verifyVPExternalDefs() const;

  // Verify that VPExternalDefs are unique in the pool and that the map keys are
  // consistent with the underlying HIR information of each VPExternalDef.
  void verifyVPExternalDefsHIR() const;

  // Verify that VPMetadataAsValues are unique in the pool and that the map keys
  // are consistent with the underlying IR information of each
  // VPMetadataAsValue.
  void verifyVPMetadataAsValues() const;
#else
  void addVPValue(Value &V) {
    if (!Value2VPValue.count(&V))
      Value2VPValue[&V] = new VPValue();
  }

  VPValue *getVPValue(Value &V) { return Value2VPValue[&V]; }
#endif

private:
  /// Add to the given dominator tree the header block and every new basic block
  /// that was created between it and the latch block, inclusive.
  static void updateDominatorTree(class DominatorTree *DT,
                                  BasicBlock *LoopPreHeaderBB,
                                  BasicBlock *LoopLatchBB);

  // Create or retrieve an external item from \p Table for a given Value \p
  // ExtVal.
  template <typename T>
  typename T::mapped_type::element_type *getExternalItem(T &Table,
                                                         Value *ExtVal) {
    using Def = typename T::mapped_type::element_type;
    typename T::mapped_type &UPtr = Table[ExtVal];
    if (!UPtr)
      // Def is a new external item to be inserted in the map.
      UPtr.reset(new Def(ExtVal));
    return UPtr.get();
  }

  // Retrieve an external item from \p Table for given HIR symbase \p Symbase.
  // If no external item is found, then a nullptr is returned
  template <typename Def>
  Def *getExternalItemForSymbase(FoldingSet<Def> &Table, unsigned Symbase) {
    FoldingSetNodeID ID;
    ID.AddInteger(Symbase);
    ID.AddInteger(0 /*IVLevel*/);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    // No Def found in table
    return nullptr;
  }

  // Create or retrieve an external item from \p Table for given HIR unitary
  // DDRef \p DDR.
  template <typename Def>
  Def *getExternalItemForDDRef(FoldingSet<Def> &Table,
                               const loopopt::DDRef *DDR) {
    assert(DDR->isNonDecomposable() && "Expected non-decomposable DDRef!");
    FoldingSetNodeID ID;
    ID.AddInteger(DDR->getSymbase());
    ID.AddInteger(0 /*IVLevel*/);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    Def *ExtDef = new Def(DDR);
    Table.InsertNode(ExtDef, IP);
    return ExtDef;
  }

  // Create or retrieve an external item from \p Table for an HIR IV identified
  // by its \p IVLevel.
  template <typename Def>
  Def *getExternalItemForIV(FoldingSet<Def> &Table, unsigned IVLevel,
                            Type *BaseTy) {
    FoldingSetNodeID ID;
    ID.AddInteger(0 /*Symbase*/);
    ID.AddInteger(IVLevel);
    void *IP = nullptr;
    if (Def *ExtDef = Table.FindNodeOrInsertPos(ID, IP))
      return ExtDef;
    Def *ExtDef = new Def(IVLevel, BaseTy);
    Table.InsertNode(ExtDef, IP);
    return ExtDef;
  }

  VPlan *clone(void) const {
    llvm_unreachable("Implement after VPlan redesign.");
  }
};

#if INTEL_CUSTOMIZATION
class VPLoopRegion : public VPRegionBlock {
  friend class VPValueMapper;
private:
  // Pointer to VPLoopInfo analysis information for this loop region
  VPLoop *VPLp;

protected:
  VPLoopRegion(const unsigned char SC, const std::string &Name, VPLoop *Lp)
      : VPRegionBlock(SC, Name), VPLp(Lp) {}

  void setVPLoop(VPLoop *Lp) { VPLp = Lp; }

public:
  VPLoopRegion(const std::string &Name, VPLoop *Lp)
      : VPRegionBlock(VPLoopRegionSC, Name), VPLp(Lp) {}

  const VPLoop *getVPLoop() const { return VPLp; }
  VPLoop *getVPLoop() { return VPLp; }

  /// Method to support type inquiry through isa, cast, and dyn_cast.
  static inline bool classof(const VPBlockBase *B) {
    return B->getVPBlockID() == VPBlockBase::VPLoopRegionSC ||
           B->getVPBlockID() == VPBlockBase::VPLoopRegionHIRSC;
  }
};

#endif

#if !defined(NDEBUG) || defined(LLVM_ENABLE_DUMP)
/// VPlanPrinter prints a given VPlan to a given output stream. The printing is
/// indented and follows the dot format.
class VPlanPrinter {
  friend inline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan);
  friend inline raw_ostream &operator<<(raw_ostream &OS,
                                        const struct VPlanIngredient &I);

private:
  raw_ostream &OS;
  const VPlan &Plan;
  unsigned Depth;
  unsigned TabWidth = 2;
  std::string Indent;

  unsigned BID = 0;

  SmallDenseMap<const VPBlockBase *, unsigned> BlockID;

  /// Handle indentation.
  void bumpIndent(int b) { Indent = std::string((Depth += b) * TabWidth, ' '); }

  /// Print a given \p Block of the Plan.
  void dumpBlock(const VPBlockBase *Block);

  /// Print the information related to the CFG edges going out of a given
  /// \p Block, followed by printing the successor blocks themselves.
  void dumpEdges(const VPBlockBase *Block);

  /// Print a given \p BasicBlock, including its instructions, followed by
  /// printing its successor blocks.
  void dumpBasicBlock(const VPBasicBlock *BasicBlock);

  /// Print a given \p Region of the Plan.
  void dumpRegion(const VPRegionBlock *Region);

  unsigned getOrCreateBID(const VPBlockBase *Block) {
    return BlockID.count(Block) ? BlockID[Block] : BlockID[Block] = BID++;
  }

  const Twine getOrCreateName(const VPBlockBase *Block);

  const Twine getUID(const VPBlockBase *Block);

  /// Print the information related to a CFG edge between two VPBlockBases.
  void drawEdge(const VPBlockBase *From, const VPBlockBase *To, bool Hidden,
                const Twine &Label);

  VPlanPrinter(raw_ostream &O, const VPlan &P) : OS(O), Plan(P) {}

  void dump();

  static void printAsIngredient(raw_ostream &O, Value *V);
};

#if !INTEL_CUSTOMIZATION
inline raw_ostream &operator<<(raw_ostream &OS, const VPlanIngredient &I) {
  VPlanPrinter::printAsIngredient(OS, I.V);
  return OS;
}
#endif

inline raw_ostream &operator<<(raw_ostream &OS, const VPlan &Plan) {
  VPlanPrinter Printer(OS, Plan);
  Printer.dump();
  return OS;
}

#if INTEL_CUSTOMIZATION
// Set of print functions
inline raw_ostream &operator<<(raw_ostream &OS, const VPInstruction &I) {
  I.dump(OS);
  return OS;
}
inline raw_ostream &operator<<(raw_ostream &OS, const VPBlockBase &BB) {
  BB.print(OS, 2);
  return OS;
}
#endif // INTEL_CUSTOMIZATION
#endif // !NDEBUG || LLVM_ENABLE_DUMP

//===----------------------------------------------------------------------===//
// VPlan Utilities
//===----------------------------------------------------------------------===//

/// Class that provides utilities for VPBlockBases in VPlan.
class VPBlockUtils {
public:
  VPBlockUtils() = delete;

#if INTEL_CUSTOMIZATION
  /// Insert NewBlock in the HCFG before BlockPtr and update parent region
  /// accordingly.
  static void insertBlockBefore(VPBlockBase *NewBlock, VPBlockBase *BlockPtr) {
    insertBlockBefore(NewBlock, BlockPtr, BlockPtr->getPredecessors());
  }

  /// Insert NewBlock in the HCFG before BlockPtr, connect NewBlock with
  /// given Preds of BlockPtr and update parent region accordingly.
  /// Predecessors of BlockPtr that are not in the \p Preds will stay attached
  /// to BlockPtr.
  static void insertBlockBefore(VPBlockBase *NewBlock, VPBlockBase *BlockPtr,
                                SmallVectorImpl<VPBlockBase *> &Preds) {
    VPRegionBlock *ParentRegion = BlockPtr->getParent();

    movePredecessors(BlockPtr, NewBlock, Preds);
    NewBlock->setParent(ParentRegion);
    connectBlocks(NewBlock, BlockPtr);
    ++BlockPtr->Parent->Size;

    // If BlockPtr is parent region's entry, set BlockPtr as parent region's
    // entry
    if (ParentRegion->getEntry() == BlockPtr)
      ParentRegion->setEntry(NewBlock);
  }

  /// Insert NewBlock in the HCFG after BlockPtr and update parent region
  /// accordingly. If BlockPtr has more that one successors, its CondBit is
  /// propagated to NewBlock.
  static void insertBlockAfter(VPBlockBase *NewBlock, VPBlockBase *BlockPtr) {

    VPRegionBlock *ParentRegion = BlockPtr->getParent();
    moveSuccessors(BlockPtr, NewBlock);
    NewBlock->setParent(ParentRegion);
    connectBlocks(BlockPtr, NewBlock);
    ++BlockPtr->Parent->Size;

    // If BlockPtr is parent region's exit, set BlockPtr as parent region's
    // exit
    if (ParentRegion->getExit() == BlockPtr)
      ParentRegion->setExit(NewBlock);
  }
#endif

  /// Connect VPBlockBases \p From and \p To bi-directionally. Append \p To to
  /// the successors of \p From and \p From to the predecessors of \p To. Both
  /// VPBlockBases must have the same parent, which can be null. Both
  /// VPBlockBases can be already connected to other VPBlockBases.
  static void connectBlocks(VPBlockBase *From, VPBlockBase *To) {
    assert((From->getParent() == To->getParent()) &&
           "Can't connect two block with different parents");
    assert(From->getNumSuccessors() < 2 &&
           "Blocks can't have more than two successors.");
    From->appendSuccessor(To);
    To->appendPredecessor(From);
  }

#if INTEL_CUSTOMIZATION
  /// Connect \p From to \p IfTrue and \p IfFalse bi-directionally. \p IfTrue
  /// and \p IfFalse are set as successors of \p From. \p From is set as
  /// predecessor of \p IfTrue and \p IfFalse. \p From must have no successors.
  static void connectBlocks(VPBlockBase *From, VPValue *ConditionV,
                            VPBlockBase *IfTrue, VPBlockBase *IfFalse) {
    From->setTwoSuccessors(ConditionV, IfTrue, IfFalse);
    IfTrue->appendPredecessor(From);
    IfFalse->appendPredecessor(From);
  }
#endif

  /// Disconnect VPBlockBases \p From and \p To bi-directionally. Remove \p To
  /// from the successors of \p From and \p From from the predecessors of \p To.
  static void disconnectBlocks(VPBlockBase *From, VPBlockBase *To) {
    assert(To && "Successor to disconnect is null.");
    From->removeSuccessor(To);
    To->removePredecessor(From);
  }

#if INTEL_CUSTOMIZATION
  // Replace \p OldSuccessor by \p NewSuccessor in Block's successor list.
  // \p NewSuccessor will be inserted in the same position as \p OldSuccessor.
  static void replaceBlockSuccessor(VPBlockBase *Block,
                                    VPBlockBase *OldSuccessor,
                                    VPBlockBase *NewSuccessor) {
    // Replace successor
    // TODO: Add VPBlockBase::replaceSuccessor. Let's not modify VPlan.h too
    // much by now
    auto &Successors = Block->getSuccessors();
    auto SuccIt = std::find(Successors.begin(), Successors.end(), OldSuccessor);
    assert(SuccIt != Successors.end() && "Successor not found");
    SuccIt = Successors.erase(SuccIt);
    Successors.insert(SuccIt, NewSuccessor);
  }

  // Replace \p OldPredecessor by \p NewPredecessor in Block's predecessor list.
  // \p NewPredecessor will be inserted in the same position as \p
  // OldPredecessor.
  static void replaceBlockPredecessor(VPBlockBase *Block,
                                      VPBlockBase *OldPredecessor,
                                      VPBlockBase *NewPredecessor) {
    // Replace predecessor
    // TODO: Add VPBlockBase::replacePredecessor. Let's not modify VPlan.h too
    // much by now
    auto &Predecessors = Block->getPredecessors();
    auto PredIt =
        std::find(Predecessors.begin(), Predecessors.end(), OldPredecessor);
    assert(PredIt != Predecessors.end() && "Predecessor not found");
    PredIt = Predecessors.erase(PredIt);
    Predecessors.insert(PredIt, NewPredecessor);
  }

  static void movePredecessor(VPBlockBase *Pred, VPBlockBase *From,
                              VPBlockBase *To) {
    replaceBlockSuccessor(Pred, From /*OldSuccessor*/, To /*NewSuccessor*/);
    To->appendPredecessor(Pred);
    From->removePredecessor(Pred);
  }

  static void movePredecessors(VPBlockBase *From, VPBlockBase *To,
                               SmallVectorImpl<VPBlockBase *> &Predecessors) {
    for (auto &Pred : Predecessors) {
      replaceBlockSuccessor(Pred, From, To);
      To->appendPredecessor(Pred);
      From->removePredecessor(Pred);
    }
  }

  static void movePredecessors(VPBlockBase *From, VPBlockBase *To) {
    movePredecessors(From, To, From->getPredecessors());
  }

  static void moveSuccessors(VPBlockBase *From, VPBlockBase *To) {
    auto &Successors = From->getSuccessors();

    for (auto &Succ : Successors) {
      replaceBlockPredecessor(Succ, From, To);
      To->appendSuccessor(Succ);
    }

    // Remove successors from From
    Successors.clear();
  }

  /// Returns true if the edge \p FromBlock -> \p ToBlock is a back-edge.
  static bool isBackEdge(const VPBlockBase *FromBlock,
                         const VPBlockBase *ToBlock, const VPLoopInfo *VPLI) {

    if (!isa<VPBasicBlock>(FromBlock) || !isa<VPBasicBlock>(ToBlock))
      // Back edge can exist only between BBs, not between BB/Region.
      return false;

    assert(FromBlock->getParent() == ToBlock->getParent() &&
           FromBlock->getParent() != nullptr && "Must be in same region");
    const VPLoop *FromLoop = VPLI->getLoopFor(FromBlock);
    const VPLoop *ToLoop = VPLI->getLoopFor(ToBlock);
    if (FromLoop == nullptr || ToLoop == nullptr || FromLoop != ToLoop) {
      return false;
    }
    // A back-edge is latch->header
    return (ToBlock == ToLoop->getHeader() && ToLoop->isLoopLatch(FromBlock));
  }

  /// Returns true if \p Block is a loop latch
  static bool blockIsLoopLatch(const VPBlockBase *Block,
                               const VPLoopInfo *VPLInfo) {

    if (const VPLoop *ParentVPL = VPLInfo->getLoopFor(Block)) {
      return ParentVPL->isLoopLatch(Block);
    }

    return false;
  }

private:
  static VPBasicBlock *splitBlock(VPBasicBlock *Block,
                                  VPBasicBlock::iterator BeforeIt,
                                  VPLoopInfo *VPLInfo,
                                  VPDominatorTree *DomTree = nullptr,
                                  VPPostDominatorTree *PostDomTree = nullptr);

public:
  static VPBasicBlock *
  splitBlockBegin(VPBlockBase *Block, VPLoopInfo *VPLInfo,
                  VPDominatorTree *DomTree = nullptr,
                  VPPostDominatorTree *PostDomTree = nullptr);
  static VPBasicBlock *
  splitBlockEnd(VPBlockBase *Block, VPLoopInfo *VPLInfo,
                VPDominatorTree *DomTree = nullptr,
                VPPostDominatorTree *PostDomTree = nullptr);

#endif // INTEL_CUSTOMIZATION
};

/// The VPlanUtils class provides common interfaces and functions that are
/// required across different VPlan classes like VPBlockBase, VPRegionBlock.
class VPlanUtils {
private:
  /// Unique ID generator.
  static std::atomic<unsigned> NextOrdinal;

public:
  VPlanUtils() = delete;

  /// Create a unique name for a new VPlan entity such as a VPBasicBlock or
  /// VPRegionBlock.
  static std::string createUniqueName(const llvm::StringRef &Prefix) {
    std::string S;
    raw_string_ostream RSO(S);
    RSO << Prefix << NextOrdinal++;
    return RSO.str();
  }
};

/// A wrapper class to add VPlan related remarks for opt-report. Currently
/// the implementation is naive with a single method to add a remark for
/// a given loop (can be HLLoop or llvm::Loop).
//  TODO:
/// In the future this will be extended to record all vectorization related
/// remarks emitted by VPlan by mapping the remarks to underlying VPlan data
/// structures that represent a loop. For example:
///
/// VPLoopRegion MainLoop --> {"LOOP WAS VECTORIZED", "vector length: 4"}
/// VPLoopRegion RemainderLoop --> {"remainder loop was not vectorized"}
class VPlanOptReportBuilder {
  LoopOptReportBuilder &LORBuilder;
  // LORB needs the LoopInfo while adding remarks for llvm::Loop. This will be
  // nullptr for HLLoop.
  LoopInfo *LI;

public:
  VPlanOptReportBuilder(LoopOptReportBuilder &LORB, LoopInfo *LI = nullptr)
      : LORBuilder(LORB), LI(LI) {}

  /// Add a vectorization related remark for the HIR loop \p Lp. The remark
  /// message is identified by \p MsgID.
  template <typename... Args>
  void addRemark(loopopt::HLLoop *Lp, OptReportVerbosity::Level Verbosity,
                 unsigned MsgID, Args &&... args);

  /// Add a vectorization related remark for the LLVM loop \p Lp. The remark
  /// message is identified by \p MsgID.
  template <typename... Args>
  void addRemark(Loop *Lp, OptReportVerbosity::Level Verbosity, unsigned MsgID,
                 Args &&... args);
};

} // namespace vpo

//===----------------------------------------------------------------------===//
// GraphTraits specializations for VPlan H-CFG Control-Flow Graphs            //
//===----------------------------------------------------------------------===//

// The following set of template specializations implement GraphTraits to treat
// any VPBlockBase as a node in a graph of VPBlockBases. It's important to note
// that VPBlockBase traits don't recurse into VPRegionBlocks, i.e., if the
// VPBlockBase is a VPRegionBlock, this specialization provides access to its
// successors/predecessors but not to the blocks inside the region.

template <> struct GraphTraits<vpo::VPBlockBase *> {
  using NodeRef = vpo::VPBlockBase *;
  using ChildIteratorType = SmallVectorImpl<vpo::VPBlockBase *>::iterator;

  static NodeRef getEntryNode(NodeRef N) { return N; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getSuccessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getSuccessors().end();
  }
};

// This specialization is for the ChildrenGetterTy from
// GenericIteratedDominanceFrontier.h. Clang's GraphTraits for clang::CFGBlock
// do the same trick.
// TODO: Consider fixing GenericIteratedDominanceFrontier.h during upstreaming
// instead.
template <>
struct GraphTraits<vpo::VPBlockBase> : public GraphTraits<vpo::VPBlockBase *> {
};

template <> struct GraphTraits<const vpo::VPBlockBase *> {
  using NodeRef = const vpo::VPBlockBase *;
  using ChildIteratorType = SmallVectorImpl<vpo::VPBlockBase *>::const_iterator;

  static NodeRef getEntryNode(NodeRef N) { return N; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getSuccessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getSuccessors().end();
  }
};

// Inverse order specialization for VPBlockBases. Predecessors are used instead
// of successors for the inverse traversal.
template <> struct GraphTraits<Inverse<vpo::VPBlockBase *>> {
  using NodeRef = vpo::VPBlockBase *;
  using ChildIteratorType = SmallVectorImpl<vpo::VPBlockBase *>::iterator;

  static NodeRef getEntryNode(Inverse<NodeRef> B) { return B.Graph; }

  static inline ChildIteratorType child_begin(NodeRef N) {
    return N->getPredecessors().begin();
  }

  static inline ChildIteratorType child_end(NodeRef N) {
    return N->getPredecessors().end();
  }
};

// The following set of template specializations implement GraphTraits to
// treat VPRegionBlock as a graph and recurse inside its nodes. It's important
// to note that the blocks inside the VPRegionBlock are treated as VPBlockBases
// (i.e., no dyn_cast is performed, VPBlockBases specialization is used), so
// there won't be automatic recursion into other VPBlockBases that turn to be
// VPRegionBlocks.

template <>
struct GraphTraits<vpo::VPRegionBlock *>
    : public GraphTraits<vpo::VPBlockBase *> {
  using GraphRef = vpo::VPRegionBlock *;
  using nodes_iterator = df_iterator<NodeRef>;

  static NodeRef getEntryNode(GraphRef N) { return N->getEntry(); }

  static nodes_iterator nodes_begin(GraphRef N) {
    return nodes_iterator::begin(N->getEntry());
  }

  static nodes_iterator nodes_end(GraphRef N) {
    // df_iterator returns an empty iterator so the node used doesn't matter.
    return nodes_iterator::end(N);
  }

  static unsigned size(GraphRef N) { return N->getSize(); }
};

template <>
struct GraphTraits<const vpo::VPRegionBlock *>
    : public GraphTraits<const vpo::VPBlockBase *> {
  using GraphRef = const vpo::VPRegionBlock *;
  using nodes_iterator = df_iterator<NodeRef>;

  static NodeRef getEntryNode(GraphRef N) { return N->getEntry(); }

  static nodes_iterator nodes_begin(GraphRef N) {
    return nodes_iterator::begin(N->getEntry());
  }

  static nodes_iterator nodes_end(GraphRef N) {
    // df_iterator returns an empty iterator so the node used doesn't matter.
    return nodes_iterator::end(N);
  }

  static unsigned size(GraphRef N) { return N->getSize(); }
};

template <>
struct GraphTraits<Inverse<vpo::VPRegionBlock *>>
    : public GraphTraits<Inverse<vpo::VPBlockBase *>> {
  using GraphRef = vpo::VPRegionBlock *;
  using nodes_iterator = df_iterator<NodeRef>;

  static NodeRef getEntryNode(Inverse<GraphRef> N) {
    return N.Graph->getExit();
  }

  static nodes_iterator nodes_begin(GraphRef N) {
    return nodes_iterator::begin(N->getExit());
  }

  static nodes_iterator nodes_end(GraphRef N) {
    // df_iterator returns an empty iterator so the node used doesn't matter.
    return nodes_iterator::end(N);
  }

  static unsigned size(GraphRef N) { return N->getSize(); }
};
} // namespace llvm

#endif // LLVM_TRANSFORMS_VECTORIZE_INTEL_VPLAN_INTELVPLAN_H
