//===--------- HIRCodeGen.cpp - Implements HIRCodeGen class ---------------===//
//
// Copyright (C) 2015-2020 Intel Corporation. All rights reserved.
//
// The information and source code contained herein is the exclusive
// property of Intel Corporation and may not be disclosed, examined
// or reproduced in whole or in part without explicit written authorization
// from the company.
//
//===----------------------------------------------------------------------===//
//
// This file implements HIRCodeGen class, used to convert HIR to LLVM IR
// New LLVM IR is generated by visiting each HIR node, and attached regular and
// blob ddrefs. Once the new IR is generated, the CFG is manipulated to ensure
// the old LLVM IR for a region is unreachable, with new IR reached instead.
//
// The emitted LLVM IR is mostly unoptimized. For example, IV's are represented
// as load/stores to memory. While this greatly simplifies the implementation
// of this pass, it does require some passes to be run afterwards, such as
// mem2reg, GVN, and simplifycfg
//
//===----------------------------------------------------------------------===//

#include "llvm/Transforms/Intel_LoopTransforms/HIRCodeGen.h"

#include "llvm/Analysis/GlobalsModRef.h"
#include "llvm/Analysis/Intel_Andersens.h"

#include "llvm/Analysis/Intel_LoopAnalysis/Utils/BlobUtils.h"
#include "llvm/Analysis/Intel_LoopAnalysis/Utils/DDRefUtils.h"
#include "llvm/Transforms/Intel_LoopTransforms/Passes.h"
#include "llvm/Transforms/VPO/Utils/VPOUtils.h"

#include "llvm/Analysis/ScalarEvolution.h"
#include "llvm/Analysis/ScalarEvolutionExpander.h"

#include "llvm/Analysis/Intel_OptReport/LoopOptReportBuilder.h"
#include "llvm/Analysis/Intel_OptReport/OptReportOptionsPass.h"

#include "llvm/Analysis/Utils/Local.h"
#include "llvm/IR/DebugInfoMetadata.h"
#include "llvm/IR/Verifier.h"
#include "llvm/InitializePasses.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Support/MathExtras.h"
#include "llvm/Transforms/Utils/BasicBlockUtils.h"
#include "llvm/Transforms/Utils/Local.h"

#include "llvm/Analysis/Intel_LoopAnalysis/IR/HIRVisitor.h"
// TODO audit includes
#define DEBUG_TYPE "hir-cg"

using namespace llvm;
using namespace llvm::loopopt;
using namespace llvm::vpo;

static cl::opt<bool> forceHIRCG("force-hir-cg", cl::init(false), cl::Hidden,
                                cl::desc("forces CodeGen on all HIR regions"));

static cl::opt<unsigned> HIRDebugRegion(
    "hir-cg-x-region-only", cl::Optional,
    cl::desc("HIRCG the x'th region only, regardless of modification status"),
    cl::Hidden, cl::value_desc("number"), cl::init(0));

namespace {

// Maps region's old exit edge to new exiting bblocks.
struct OldToNewExits {
  BasicBlock *OldExitingBB;
  BasicBlock *ExitBB;
  SmallVector<BasicBlock *, 8> NewExitingBBs;

  OldToNewExits(BasicBlock *OldExitingBB, BasicBlock *ExitBB)
      : OldExitingBB(OldExitingBB), ExitBB(ExitBB) {}
};

// This does the real work of llvm ir cg
// Uses IRBuilder to generate LLVM IR for each HIR construct
// visited
// Note that the visitor used is not the standard HLNodeVisitor
// HIR code gen visits HIR in an unusual order, so a different
// visitor class which does not define a traversal order is used.
// TODO: probably could use a return of void
// TODO: check if HLNodeVisitor can substitute HIRVisitor so we can eliminate
// HIRVisitor alltogether.
class CGVisitor : public HIRVisitor<CGVisitor, Value *> {
public:
  // following are extra functions not part of visitor
  // ddref and ce cg generates many intermediate inst and values
  // but each one logically represents a single resulting value
  // which is returned
  Value *visitCanonExpr(CanonExpr *CE);

  // While a ddref represents a single value, the ddref for A[i] has a
  // different meaning whether lval or rval. For Lvals, we are storing into
  // &A[i], and so we need to return a value representing that address to use
  // in a store instruction. For rvals, we want a value for what is stored at
  // that address to use as operand of an instruction. This function will
  // return an address or a load of that address depending on rval/lval of
  // ref. If MaskVal is not null, we generate a masked load/gather instead
  // of the load.
  Value *visitRegDDRef(RegDDRef *Ref, Value *MaskVal = nullptr);
  Value *visitScalar(RegDDRef *Ref);

  Value *visitRegion(HLRegion *R);
  Value *visitLoop(HLLoop *L);
  // IVAdd and IVAlloca are passed to generate store for the iv update inside
  // the bottom test of unknown loops.
  // LoopID is passed so it can be attached to bottom test of unknown loops.
  Value *visitIf(HLIf *I, Value *IVAdd = nullptr,
                 AllocaInst *IVAlloca = nullptr, MDNode *LoopID = nullptr);

  Value *visitSwitch(HLSwitch *S);

  Value *visitInst(HLInst *I);

  Value *visitGoto(HLGoto *G);
  Value *visitLabel(HLLabel *L);
  BasicBlock *getBBlockForLabel(HLLabel *L);

  // any client should have used visit(node), this function is used as a
  // fallback when visitXXX couldnt be found for an hlnode of type XXX
  Value *visitHLNode(HLNode *Node) {
    llvm_unreachable("Unknown HIR type in CG");
  }

  CGVisitor(HIRFramework &HIRF, ScalarEvolution &SE, LoopOptReportBuilder &LORB)
      : F(HIRF.getFunction()), HIRF(HIRF), SE(SE), CurRegion(nullptr),
        CurLoopIsNSW(false), Builder(HIRF.getContext()),
        Expander(SE, HIRF.getDataLayout(), "i", *this), LORBuilder(LORB) {}

private:
  // Performs preprocessing for \p Reg before we start generating code for it.
  void preprocess(HLRegion *Reg);

  // Regions have a list of live in values and their corresponding symbase
  // Any use of the value in HIR region is represented by a temp with some
  // symbase
  // CG turns those temps into load/stores of memory corresponding ot symbase.
  // However we must store the initial value into symbase's memory slot or the
  // first use of live in value will remain as load of uninitialized memory
  void initializeLiveins();

  // Regions have a list of liveout values and their symbase. We must ensure
  // that all uses of liveout value are replaced by a load of symbase's
  // memory slot.

  // Handles liveouts for the region.
  void processLiveouts();

  // Makes the CFG point to the new region. The old region becomes
  // unreachable.
  void replaceOldRegion(BasicBlock *RegionEntry);

  // Add an entry from old region exiting block to new region exiting block.
  void addOldToNewExitBlockEntry(BasicBlock *OldExitingBB, BasicBlock *ExitBB,
                                 BasicBlock *NewExitingBB);

  // Returns the set of new extiing blocks corresponding to the edge:
  // (OldExitingBB -> ExitBB).
  const SmallVector<BasicBlock *, 8> *
  getNewExitingBlocks(BasicBlock *OldExitingBB, BasicBlock *ExitBB) const;

  // Adds an unconditional branch from the current insertion point to \p ToBB
  // if the last inserted instruction is not an unconditional branch.
  void generateBranchIfRequired(BasicBlock *ToBB);

  // Set the metadata for the instruction using the passed in MDNodes.
  static void setMetadata(Value *Val, const RegDDRef *Ref);
  static void setMetadata(Instruction *Inst, const RegDDRef *Ref);
  static void setMetadata(Instruction *Inst, const RegDDRef::MDNodesTy &MDs);

  // Generate llvm.dbg.declare for the dbg intrinstic \p DbgInfoIntrin.
  void generateDeclareValue(AllocaInst *Alloca,
                            const DbgInfoIntrinsic *DbgInfoIntrin);

  // Generate llvm.dbg.declare with the specific \p LocalVariable, \p
  // Expression and \p Location.
  void generateDeclareValue(AllocaInst *Alloca, DILocalVariable *LocalVariable,
                            DIExpression *Expression, DILocation *Location);

  // Generates eventual store for an lval HLInst once all the operands have
  // been CG'd.
  void generateLvalStore(const HLInst *HInst, Value *StorePtr, Value *StoreVal);

  // Returns a value representing the summation of all coef*blob pairs.
  Value *sumBlobs(CanonExpr *CE);

  // For a canon expr of form c_1 * i_1[ + c_2 *i_2 + ...] + c_0 + blob
  // return a value representing ONLY the summation of c_n * i_n pairs where
  // c_i is a constant and i_n is an induction variable
  Value *sumIV(CanonExpr *CE);

  /// Generates a bool value* representing truth value of HLIf's
  /// predicate(s)
  Value *generateAllPredicates(HLIf *HIf);

  /// Generates a bool value for predicate in HLIf
  Value *generatePredicate(HLIf *HIf, HLIf::const_pred_iterator P);

  // Creates and returns icmp or fcmp instuction(depending on lhs type)
  // at current IP
  Value *createCmpInst(const HLPredicate &P, Value *LHS, Value *RHS,
                       const Twine &Name);

  // Return a value for blob corresponding to BlobIdx multiplied by Coeff.
  // We normally expect Blob type to match CE type. The only exception is
  // ptr blobs. Ptrs are converted to int of argument type, which should be
  // CE src type
  Value *getBlobValue(int64_t Coeff, unsigned BlobIdx, Type *Ty);

  // TODO blobs are reprsented by scev with some caveats
  const SCEV *getBlobSCEV(unsigned BlobIdx) {
    return HIRF.getBlobUtils().getBlob(BlobIdx);
  }

  Value *IVCoefCG(CanonExpr *CE, CanonExpr::iv_iterator IVIt) {
    return getBlobValue(CE->getIVConstCoeff(IVIt), CE->getIVBlobCoeff(IVIt),
                        CE->getSrcType());
  }

  // Return value for blobCoeff * constCoeff * iv with IV at level
  Value *IVPairCG(CanonExpr *CE, CanonExpr::iv_iterator IVIt, Type *Ty);

  // Return value for coeff*V
  Value *CoefCG(int64_t Coeff, Value *V);

  // Returns value for blobCoeff*blob in <blobidx,coeff> pair
  Value *BlobPairCG(CanonExpr *CE, CanonExpr::blob_iterator BlobIt) {
    auto BlobVal = getBlobValue(CE->getBlobCoeff(BlobIt),
                                CE->getBlobIndex(BlobIt), CE->getSrcType());
    return BlobVal;
  }

  // Applies cast to Val according to CE's dest type, if applicable.
  Value *castToDestType(CanonExpr *CE, Value *Val);

  // Creates a stack allocation of size with name at entry of
  // current func. used for allocs that we expect to regisiterize
  AllocaInst *CreateEntryBlockAlloca(const std::string &VarName, Type *Ty,
                                     Value *size = 0) {
    IRBuilder<> TmpB(&F.getEntryBlock(), F.getEntryBlock().begin());
    return TmpB.CreateAlloca(Ty, size, VarName.c_str());
  }

  // HIRCG's considers iv names is iN.ty where N is nesting level and
  // ty is type
  static std::string getIVName(int NestingLevel, Type *Ty) {
    return "i" + std::to_string(NestingLevel) + ".i" +
           std::to_string(Ty->getPrimitiveSizeInBits());
  }

  static std::string getIVName(const HLLoop *L) {
    return getIVName(L->getNestingLevel(), L->getIVType());
  }

  // Temps are named in format of tN where N is the symbase
  static std::string getTempName(unsigned Symbase) {
    return "t" + std::to_string(Symbase);
  }

  static std::string getTempName(RegDDRef *Ref) {
    assert(!Ref->hasGEPInfo() && "Non scalar ref accessed as scalar");
    return getTempName(Ref->getSymbase());
  }

  // Gets an allocation for name of type T, creating a new allocation
  // if necessary. Allocation is a alloca at function entry
  AllocaInst *getLvalTerminalAlloca(RegDDRef *Ref);
  AllocaInst *getSymbaseAlloca(unsigned Symbase, Type *Ty,
                               HLRegion *Region = nullptr);

  AllocaInst *getLoopIVAlloca(const HLLoop *Loop);

  Value *getTokenVal(unsigned Symbase) const;
  void addTokenEntry(unsigned Symbase, Value *TokenVal);

  // Handles special casing for SCEVUnknowns possibly representing blobs
  // within HIR framework
  // We don't want to replicate logic for handling add exprs and the like
  // so we inherit from SCEVExpander and override visitUnknown and expand()
  // Also SCEVExpander caches expanded values per IP, and attempts to hoist
  // values as far up as possible out of loop. Overridden functions do
  // not have this behavior.
  class HIRSCEVExpander : public SCEVExpander {
  public:
    HIRSCEVExpander(ScalarEvolution &SE, const DataLayout &DL, const char *Name,
                    CGVisitor &CurCG)
        : SCEVExpander(SE, DL, Name), CG(CurCG) {}

    ~HIRSCEVExpander() {}

  private:
    // provides access to named value map, blob table and ir builder
    CGVisitor &CG;

    // Some blobs require a load from memory. If an SCEVUnknown has a
    // value of type Instruction, it is assumed to be created within HIR and
    // requires a load of that blob's symbase's memory slot
    Value *visitUnknown(const SCEVUnknown *S) override;

    // This is called by expandCodeFor(). Default implementation hoists
    // generated values out of loop and caches generated values. This version
    // has no caching, and no loop lookups.
    Value *expand(const SCEV *S) override { return visit(S); }
  };

  class ScopeDbgLoc {
    CGVisitor &Visitor;
    DebugLoc OldDbgLoc;

  public:
    ScopeDbgLoc(ScopeDbgLoc &&Scope) : Visitor(Scope.Visitor) {}
    ScopeDbgLoc(const ScopeDbgLoc &) = delete;

    ScopeDbgLoc(CGVisitor &Visitor, const DebugLoc &Loc) : Visitor(Visitor) {
      OldDbgLoc = Visitor.Builder.getCurrentDebugLocation();

      if (Loc) {
        Visitor.Builder.SetCurrentDebugLocation(Loc);
      }
    }

    ~ScopeDbgLoc() { Visitor.Builder.SetCurrentDebugLocation(OldDbgLoc); }
  };

  Function &F;
  HIRFramework &HIRF;
  ScalarEvolution &SE;
  HLRegion *CurRegion;
  bool CurLoopIsNSW;
  IRBuilder<> Builder;
  HIRSCEVExpander Expander;
  const LoopOptReportBuilder &LORBuilder;

  // keep track of our mem allocs. Only IV and temps atm
  std::map<std::string, AllocaInst *> NamedValues;

  // maps internal labels to bblocks. Needed if we encounter "goto Label"
  // before the label itself
  SmallDenseMap<HLLabel *, BasicBlock *, 16> InternalLabels;

  // A stack of IV memory slots for current loop nest. Creating a load
  // of value at CurIVValues[1] will return the IV for loop level 1 of
  // current loop nest
  SmallVector<Value *, MaxLoopNestLevel + 1> CurIVValues;

  // Maps the old region exiting edges to a vector of new ones. We can have
  // multiple new exits for one original exit due to replication (unrolling,
  // for example).
  SmallVector<OldToNewExits, 8> OldToNewRegionExitingBlocks;

  // Set of region exit bblocks.
  SmallPtrSet<BasicBlock *, 8> ExitBBs;

  // Stores mapping of symbase to token values.
  SmallDenseMap<unsigned, Value *> TokenMap;
};

class HIRCodeGen {
private:
  ScalarEvolution &SE;
  HIRFramework &HIRF;
  LoopOptReportBuilder &LORBuilder;

  // Clears HIR related metadata from instructions. Returns true if any
  // instruction was cleared.
  bool clearHIRMetadata(HLRegion *Reg) const;

  // Returns true if we need to generate code for this region.
  bool shouldGenCode(HLRegion *Reg, unsigned RegionIdx) const;

public:
  HIRCodeGen(ScalarEvolution &SE, HIRFramework &HIRF)
      : SE(SE), HIRF(HIRF), LORBuilder(HIRF.getLORBuilder()) {}

  bool run();

  /// Erases all the dummy instructions.
  void eraseDummyInstructions(HLNodeUtils &HNU);
};

class HIRCodeGenWrapperPass : public FunctionPass {
public:
  static char ID;

  HIRCodeGenWrapperPass() : FunctionPass(ID) {
    initializeHIRCodeGenWrapperPassPass(*PassRegistry::getPassRegistry());
  }

  bool runOnFunction(Function &F) override {
    return HIRCodeGen(getAnalysis<ScalarEvolutionWrapperPass>().getSE(),
                      getAnalysis<HIRFrameworkWrapperPass>().getHIR())
        .run();
  }
  void getAnalysisUsage(AnalysisUsage &AU) const override {
    AU.addRequired<ScalarEvolutionWrapperPass>();
    AU.addRequired<HIRFrameworkWrapperPass>();

    AU.addPreserved<GlobalsAAWrapperPass>();
    AU.addPreserved<AndersensAAWrapperPass>();
  }
};

} // namespace

FunctionPass *llvm::createHIRCodeGenWrapperPass() {
  return new HIRCodeGenWrapperPass();
}

char HIRCodeGenWrapperPass::ID = 0;
INITIALIZE_PASS_BEGIN(HIRCodeGenWrapperPass, "hir-cg", "HIR Code Generation",
                      false, false)
INITIALIZE_PASS_DEPENDENCY(ScalarEvolutionWrapperPass)
INITIALIZE_PASS_DEPENDENCY(HIRFrameworkWrapperPass)
INITIALIZE_PASS_END(HIRCodeGenWrapperPass, "hir-cg", "HIR Code Generation",
                    false, false)

PreservedAnalyses HIRCodeGenPass::run(Function &F,
                                      FunctionAnalysisManager &AM) {
  bool Transformed = HIRCodeGen(AM.getResult<ScalarEvolutionAnalysis>(F),
                                AM.getResult<HIRFrameworkAnalysis>(F))
                         .run();

  if (!Transformed) {
    return PreservedAnalyses::all();
  }

  PreservedAnalyses PA;
  PA.preserve<GlobalsAA>();
  PA.preserve<AndersensAA>();
  return PA;
}

bool HIRCodeGen::run() {
  LLVM_DEBUG(dbgs().write_escaped(HIRF.getFunction().getName()) << "\n");
  LLVM_DEBUG(HIRF.getFunction().dump());

  // generate code
  CGVisitor CG(HIRF, SE, LORBuilder);
  bool Transformed = false;
  unsigned RegionIdx = 1;

  for (auto I = HIRF.hir_begin(), E = HIRF.hir_end(); I != E;
       ++I, ++RegionIdx) {
    HLRegion *Reg = cast<HLRegion>(&*I);

    if (shouldGenCode(Reg, RegionIdx)) {
      LLVM_DEBUG(dbgs() << "Starting the code gen for " << RegionIdx << "\n");
      LLVM_DEBUG(Reg->dump());
      CG.visit(Reg);
      Transformed = true;
    } else {
      // Clear HIR related metadata.
      bool Cleared = clearHIRMetadata(Reg);
      Transformed = Transformed || Cleared;
    }
  }

  eraseDummyInstructions(HIRF.getHLNodeUtils());

  // No longer need to suppress scalar optimizations.
  HIRF.getFunction().resetPreLoopOpt();

  return Transformed;
}

bool HIRCodeGen::shouldGenCode(HLRegion *Reg, unsigned RegionIdx) const {

  if (HIRDebugRegion) {
    if (HIRDebugRegion == RegionIdx) {
      return true;
    } else {
      return false;
    }
  }

  if (forceHIRCG) {
    return true;
  }

  if (Reg->shouldGenCode()) {
    return true;
  }

  return false;
}

bool HIRCodeGen::clearHIRMetadata(HLRegion *Reg) const {
  bool Cleared = false;
  unsigned LiveInID = SE.getHIRMDKindID(ScalarEvolution::HIRLiveKind::LiveIn);
  unsigned LiveOutID = SE.getHIRMDKindID(ScalarEvolution::HIRLiveKind::LiveOut);
  unsigned LiveRangeID =
      SE.getHIRMDKindID(ScalarEvolution::HIRLiveKind::LiveRange);

  for (auto BB = Reg->bb_begin(), End = Reg->bb_end(); BB != End; ++BB) {
    for (auto &Inst : **BB) {
      Instruction &NonConstInst = const_cast<Instruction &>(Inst);

      if (Inst.getMetadata(LiveInID)) {
        Cleared = true;
        NonConstInst.setMetadata(LiveInID, nullptr);

      } else if (Inst.getMetadata(LiveOutID)) {
        Cleared = true;
        NonConstInst.setMetadata(LiveOutID, nullptr);

      } else if (Inst.getMetadata(LiveRangeID)) {
        Cleared = true;
        NonConstInst.setMetadata(LiveRangeID, nullptr);
      }
    }
  }

  return Cleared;
}

Value *CGVisitor::HIRSCEVExpander::visitUnknown(const SCEVUnknown *S) {
  // Blobs represented by SCEVUnknowns whose value is constexpr or
  // globals can have their values directly returned. For example a blob
  // for a global ptr or function arg we can return the scevunknown's
  // value.
  Value *V = S->getValue();
  if (!isa<Instruction>(V))
    return V;

  // Blobs represented by an scevunknown whose value is an instruction
  // are represented by load and stores to a memory location corresponding
  // to the blob's symbase. Blobs are always rvals, and so loaded
  unsigned BlobSymbase = CG.HIRF.getBlobUtils().findTempBlobSymbase(S);

  // SCEVExpander can create its own SCEVs as intermediates which are
  // then expanded. One example is expandAddToGep which replaces
  // adds of ptr types with a SCEV for a gep instead of ptrtoints
  // and adds. These new scevunknowns have an instruction but no
  // corresponding blob. For those, return their underlying value
  if (BlobSymbase == InvalidBlobIndex) {
    return V;
  }

  AllocaInst *TempAddr = CG.getSymbaseAlloca(BlobSymbase, S->getType());
  // Be careful to use scevexpanders builder, not CGVisitor's
  // otherwise some insertions may be in wrong place.
  // There will be many loads of same temp name, so a . is added to end
  // to distinguish second load of t2 vs first load of t22
  return Builder.CreateLoad(TempAddr, TempAddr->getName() + ".");
}

void CGVisitor::preprocess(HLRegion *Reg) {
  CurRegion = Reg;
  NamedValues.clear();
  InternalLabels.clear();
  OldToNewRegionExitingBlocks.clear();
  ExitBBs.clear();

  // Gather all loops for processing.
  SmallVector<HLLoop *, 64> Loops;
  Reg->getHLNodeUtils().gatherAllLoops(Reg, Loops);

  // Extract ztt, preheader and postexit.
  for (auto &I : Loops) {
    I->extractZtt();
    I->extractPreheaderAndPostexit();
  }
}

Value *CGVisitor::castToDestType(CanonExpr *CE, Value *Val) {

  auto DestTy = CE->getDestType();
  Type *CastToTy;

  // If Val is a scalar and DestType is a vector, apply the cast operation
  // before the broadcast - this is important for good performance
  if (DestTy->isVectorTy() && !Val->getType()->isVectorTy()) {
    CastToTy = DestTy->getScalarType();
  } else {
    CastToTy = DestTy;
  }

  if (CE->isSExt()) {
    Val = Builder.CreateSExt(Val, CastToTy);
  } else if (CE->isZExt()) {
    Val = Builder.CreateZExt(Val, CastToTy);
  } else if (CE->isTrunc()) {
    Val = Builder.CreateTrunc(Val, CastToTy);
  }

  // If the cast value is a scalar type and dest type is a vector, we need
  // to do a broadcast.
  if (DestTy->isVectorTy() && !Val->getType()->isVectorTy()) {
    Val = Builder.CreateVectorSplat(cast<VectorType>(DestTy)->getNumElements(),
                                    Val);
  }

  return Val;
}

Value *CGVisitor::createCmpInst(const HLPredicate &P, Value *LHS, Value *RHS,
                                const Twine &Name) {
  Value *CmpInst = nullptr;

  ScopeDbgLoc DbgLoc(*this, P.DbgLoc);

  // Account for vector type
  auto LType = LHS->getType()->getScalarType();

  assert(P != UNDEFINED_PREDICATE && "invalid predicate for cmp/sel in HIRCG");

  if (LType->isIntegerTy() || LType->isPointerTy()) {
    CmpInst = Builder.CreateICmp(P, LHS, RHS, Name);
  } else if (LType->isFloatingPointTy()) {
    Builder.setFastMathFlags(P.FMF);
    CmpInst = Builder.CreateFCmp(P, LHS, RHS, Name);
    Builder.clearFastMathFlags();
  } else {
    llvm_unreachable("unknown predicate type in HIRCG");
  }

  return CmpInst;
}

Value *CGVisitor::getBlobValue(int64_t Coeff, unsigned BlobIdx, Type *Ty) {

  auto *BlobSCEV = getBlobSCEV(BlobIdx);
  bool Negate = false;
  bool IsIntTy = BlobSCEV->getType()->isIntegerTy();

  if (IsIntTy && (Coeff != 1)) {
    // For negative power of 2 coefficients, reassociate (-Coeff * Blob) to
    // -(Coeff * Blob) so we generate a (shift + sub) instead of a mul. Ideally,
    // this reassociation should be done by the regular reassociation pass but
    // adding it to the cleanup pipeline degrades some benchmarks.
    if ((Coeff < 0) && (Coeff != LLONG_MIN) && isPowerOf2_64(-Coeff)) {
      Coeff = -Coeff;
      Negate = true;
    }

    auto *Const = SE.getConstant(BlobSCEV->getType(), Coeff, true);
    BlobSCEV = SE.getMulExpr(Const, BlobSCEV);
  }

  // TODO: Rather than creating unreachable for each blob, create once per basic
  // block or once per function which gets reused for each basic block.
  //
  // SCEVExpander instruction generator references the insertion point's
  // parent.
  // If the IP is bblock.end(), undefined behavior results because the
  // parent of that "instruction" is invalid. We work around this by
  // adding temporary instruction to use as our IP, and remove it after
  Instruction *TmpIP = Builder.CreateUnreachable();
  Value *Blob = Expander.expandCodeFor(BlobSCEV, nullptr, TmpIP);

  // Expander shouldnt create new Bblocks, new IP is end of current bblock
  Builder.SetInsertPoint(TmpIP->getParent());
  TmpIP->eraseFromParent();
  Type *BType = Blob->getType();

  if (Negate) {
    Blob = Builder.CreateNeg(Blob);
  }

  if (BType->isPointerTy() && BType != Ty) {
    // A version of this should be in verifier, but we want to test CG'd
    // type.
    auto ScalarTy = Ty->getScalarType();
    if (ScalarTy->isPointerTy()) {
      assert((ScalarTy == BType) && "Pointer blob type mismatch!");
    } else {
      assert((ScalarTy->getPrimitiveSizeInBits() ==
              F.getParent()->getDataLayout().getPointerTypeSizeInBits(BType)) &&
             "Pointer size and CE size mismatch");
      Blob = Builder.CreatePtrToInt(Blob, ScalarTy);
    }
  }

  return IsIntTy ? Blob : CoefCG(Coeff, Blob);
}

Value *CGVisitor::visitCanonExpr(CanonExpr *CE) {
  Value *BlobSum = nullptr, *IVSum = nullptr, *C0Value = nullptr,
        *DenomVal = nullptr;

  ScopeDbgLoc DbgLoc(*this, CE->getDebugLoc());

  auto SrcType = CE->getSrcType();

  LLVM_DEBUG(dbgs() << "cg for CE ");
  LLVM_DEBUG(CE->dump());
  LLVM_DEBUG(dbgs() << "\n");

  if (CE->isNull()) {
    return ConstantPointerNull::get(cast<PointerType>(SrcType));
  }

  if (CE->isNullVector()) {
    auto PtrType = cast<PointerType>(SrcType->getScalarType());

    auto NullVal = ConstantPointerNull::get(PtrType);
    return Builder.CreateVectorSplat(
        cast<VectorType>(SrcType)->getNumElements(), NullVal);
  }

  BlobSum = sumBlobs(CE);
  IVSum = sumIV(CE);

  // Broadcast scalar value only when absolutely needed. The broadcast is
  // needed when both BlobSum and IVSum are non-null and one of BlobSum/IVSum
  // is a vector and the other is a scalar.
  if (SrcType->isVectorTy()) {
    if ((BlobSum && BlobSum->getType()->isVectorTy()) ||
        (IVSum && IVSum->getType()->isVectorTy())) {
      if (BlobSum && !(BlobSum->getType()->isVectorTy())) {
        BlobSum = Builder.CreateVectorSplat(
            cast<VectorType>(SrcType)->getNumElements(), BlobSum);
      }
      if (IVSum && !(IVSum->getType()->isVectorTy())) {
        IVSum = Builder.CreateVectorSplat(
            cast<VectorType>(SrcType)->getNumElements(), IVSum);
      }
    } else {
      // Both BlobSum/IVSum are scalar, for C0/Denom use Scalar type.
      // Any necessary broadcast is done in castToDestType.
      SrcType = SrcType->getScalarType();
    }
  }

  int64_t C0 = CE->getConstant();
  int64_t Denom = CE->getDenominator();

  // TODO I dunno about htis more specially a pointer?
  // ie [i32 X 10] for type of base ptr what type to use?
  if (C0) {
    if (isa<StructType>(SrcType) || isa<ArrayType>(SrcType) ||
        isa<VectorType>(SrcType)) {
      // We should be generating a GEP for a pointer base with an offset. For
      // struct types, we need to follow the structure layout.
      assert("Pointer base with offset not handled!");
    }
    C0Value = ConstantInt::getSigned(SrcType, C0);
  }

  // combine the blob, const, and ivs into one value
  Value *Res = nullptr;
  if (BlobSum && IVSum) {
    Res = Builder.CreateAdd(BlobSum, IVSum);
  } else {
    Res = IVSum ? IVSum : BlobSum;
  }
  if (Res) {
    Res = C0Value ? Builder.CreateAdd(Res, C0Value) : Res;
  } else {
    Res = C0Value;
  }

  if (!Res) {
    // assert c0 is 0. no iv no blob
    if (CE->hasIV() || CE->hasBlob() || C0 != 0)
      llvm_unreachable("failed to cg IV or blob");
    Res = ConstantInt::getSigned(SrcType, C0);
  }

  if (Denom != 1) {
    DenomVal = ConstantInt::getSigned(SrcType, Denom);

    if (CE->isSignedDiv()) {
      Res = Builder.CreateSDiv(Res, DenomVal);
    } else {
      Res = Builder.CreateUDiv(Res, DenomVal);
    }
  }

  Res = castToDestType(CE, Res);

  return Res;
}

AllocaInst *CGVisitor::getLvalTerminalAlloca(RegDDRef *Ref) {
  assert(Ref->isLval() && "Ref is expected to be Lval");
  assert(Ref->isTerminalRef() && "Ref is expected to be terminal");

  return getSymbaseAlloca(Ref->getSymbase(), Ref->getDestType(), CurRegion);
}

AllocaInst *CGVisitor::getSymbaseAlloca(unsigned Symbase, Type *Ty,
                                        HLRegion *Region) {
  AllocaInst *Alloca;
  std::string Name = getTempName(Symbase);
  if (!NamedValues.count(Name)) {
    assert(Region && "Region should be defined for uninitialized symbase");

    Alloca = CreateEntryBlockAlloca(Name, Ty);

    auto Iter = Region->getDebugIntrinMap().find(Symbase);
    if (Iter != Region->getDebugIntrinMap().end()) {
      auto &DbgInstVector = Iter->second;

      for (const DbgInfoIntrinsic *DbgInfoInst : DbgInstVector) {
        generateDeclareValue(Alloca, DbgInfoInst);
      }
    }

    NamedValues[Name] = Alloca;
  } else {
    Alloca = NamedValues[Name];
    assert(Ty == Alloca->getAllocatedType() && "Mismatch alloca type request");
  }
  return Alloca;
}

Value *CGVisitor::getTokenVal(unsigned Symbase) const {
  auto It = TokenMap.find(Symbase);

  assert(It != TokenMap.end() && "Uninitialized token found!");
  return It->second;
}

void CGVisitor::addTokenEntry(unsigned Symbase, Value *TokenVal) {
  assert(TokenMap.find(Symbase) == TokenMap.end() &&
         "Redefinition of token found!");
  assert(TokenVal->getType()->isTokenTy() && "Token type value expected!");
  TokenMap[Symbase] = TokenVal;
}

AllocaInst *CGVisitor::getLoopIVAlloca(const HLLoop *Loop) {
  AllocaInst *Alloca;

  std::string Name = getIVName(Loop);
  if (!NamedValues.count(Name)) {
    Alloca = CreateEntryBlockAlloca(Name, Loop->getIVType());
    NamedValues[Name] = Alloca;
  } else {
    Alloca = NamedValues[Name];
    assert(Loop->getIVType() == Alloca->getAllocatedType() &&
           "Mismatch alloca type request");
  }

  return Alloca;
}

Value *CGVisitor::visitScalar(RegDDRef *Ref) {
  CanonExpr *ScalarCE = Ref->getSingleCanonExpr();

  // For rval temps, we generate value directly from CE
  if (Ref->isRval()) {
    return visitCanonExpr(ScalarCE);
  }

  ScopeDbgLoc DbgLoc(*this, ScalarCE->getDebugLoc());

  // For lvals return address of temp
  return getLvalTerminalAlloca(Ref);
}

Value *CGVisitor::visitRegDDRef(RegDDRef *Ref, Value *MaskVal) {
  assert(Ref && " Reference is null.");
  LLVM_DEBUG(dbgs() << "cg for RegRef ");
  LLVM_DEBUG(Ref->dump());
  LLVM_DEBUG(dbgs() << " Symbase: " << Ref->getSymbase() << " \n");

  if (Ref->isTerminalRef()) {
    return visitScalar(Ref);
  }

  ScopeDbgLoc GepDbgLoc(*this, Ref->getGepDebugLoc());

  Value *BaseV = visitCanonExpr(Ref->getBaseCE());

  bool AnyVector = false;
  unsigned DimNum = Ref->getNumDimensions();

  auto BitCastDestTy = Ref->getBitCastDestType();

  for (auto CEI = Ref->canon_begin(), CEE = Ref->canon_end(); CEI != CEE;
       ++CEI) {
    if ((*CEI)->getDestType()->isVectorTy()) {
      AnyVector = true;
      break;
    }
  }

  // A GEP instruction is allowed to have a mix of scalar and vector operands.
  // However, not all optimizations(especially LLVM loop unroller) are handling
  // such cases. To workaround, the base pointer value needs to be broadcast.
  // If Ref's dest type is a vector, we need to do a broadcast.
  if (!BaseV->getType()->isVectorTy()) {
    if (AnyVector || (BitCastDestTy && BitCastDestTy->isVectorTy())) {
      auto VL = cast<VectorType>(Ref->getDestType())->getNumElements();
      BaseV = Builder.CreateVectorSplat(VL, BaseV);
    }
  }

  std::function<Value *(Value *, ArrayRef<Value *>)> CreateGEPInBoundsHelper =
      [this](Value *BasePtr, ArrayRef<Value *> IndexV) {
        return Builder.CreateInBoundsGEP(BasePtr, IndexV, "arrayIdx");
      };

  std::function<Value *(Value *, ArrayRef<Value *>)> CreateGEPHelper =
      [this](Value *BasePtr, ArrayRef<Value *> IndexV) {
        return Builder.CreateGEP(BasePtr, IndexV, "arrayIdx");
      };

  auto CreateGEP =
      Ref->isInBounds() ? CreateGEPInBoundsHelper : CreateGEPHelper;

  auto &DL = HIRF.getDataLayout();
  Value *GEPVal = BaseV;

  // Ref either looks like t[0] or &t[0]. In such cases we don't need a GEP, we
  // can simply use the base value. Also, for opaque (forward declared) struct
  // types, LLVM doesn't allow any indices even if it is just a zero.
  if ((DimNum == 1) && !Ref->hasTrailingStructOffsets() &&
      (*Ref->canon_begin())->isZero()) {
    // GEP is not needed.
  } else {
    assert(DimNum && "No dimensions");
    SmallVector<Value *, 4> IndexV;

    // stored as A[canon3][canon2][canon1], but gep requires them in reverse
    // order
    for (; DimNum > 0; --DimNum) {
      auto *IndexVal = visitCanonExpr(Ref->getDimensionIndex(DimNum));
      auto *LowerVal = visitCanonExpr(Ref->getDimensionLower(DimNum));
      auto *StrideVal = visitCanonExpr(Ref->getDimensionStride(DimNum));

      Type *GEPElementTy = Ref->getDimensionElementType(DimNum);

      // Create a GEP offset that corresponds to the current dimension.
      Value *OffsetVal = emitBaseOffset(&Builder, DL, GEPElementTy, GEPVal,
                                        LowerVal, IndexVal, StrideVal);

      // Adding an index to the GEP will change the result type to the element
      // of an aggregate:
      //   getelementptr [10 x float]*, 0    -> [10 x float]
      //   getelementptr [10 x float]*, 0, 0 -> float
      //
      // For non-array dimensions instead of adding offset as a new index,
      // another GEP should be created:
      //
      //   %p[0][i][j][k]
      //        /   |   \
      //      %vs1 %vs2 [10 x float];    %vs1, %vs2 - variable strides
      //
      //   %0 = getelementptr [10 x float]* %p, 0
      //   %1 = getelementptr [10 x float]* %0, %offset1
      //   %2 = getelementptr [10 x float]* %1, %offset2, %k
      //
      // The following check emits the indices for the previous dimension if the
      // current dimension is not an array type.
      if (!Ref->isDimensionLLVMArray(DimNum) && !IndexV.empty()) {
        GEPVal = CreateGEP(GEPVal, IndexV);
        IndexV.clear();
      }

      IndexV.push_back(OffsetVal);

      // Push back indices for dimensions's trailing offsets.
      auto Offsets = Ref->getTrailingStructOffsets(DimNum);

      if (!Offsets.empty()) {
        // Structure fields are always i32 type.
        auto I32Ty = Type::getInt32Ty(F.getContext());

        for (auto OffsetVal : Offsets) {
          auto OffsetIndex = ConstantInt::get(I32Ty, OffsetVal);
          IndexV.push_back(OffsetIndex);
        }
      }
    }

    GEPVal = CreateGEP(GEPVal, IndexV);
  }

  if (GEPVal->getType()->isVectorTy() && isa<PointerType>(BitCastDestTy)) {
    // When we have a vector of pointers and base src and dest types do not
    // match, we need to bitcast from vector of pointers of src type to vector
    // of pointers of dest type. Example case, Src type is int * and Dest type
    // is <4 x float>*, we will have pointer vector <4 x int*>. This vector
    // needs to be bitcast to <4 x float*> so that the gather/scatter
    // loads/stores <4 x float>.
    auto PtrDestTy = cast<PointerType>(BitCastDestTy); // <4 x float>*
    auto DestElTy = PtrDestTy->getElementType();       // <4 x float>
    auto DestScTy = DestElTy->getScalarType();         // float
    auto DestScPtrTy = PointerType::get(DestScTy,      // float *
                                        PtrDestTy->getAddressSpace());

    if (GEPVal->getType()->getScalarType() != DestScPtrTy) {
      auto VL = cast<VectorType>(DestElTy)->getNumElements();

      // We have a vector of pointers of BaseSrcType. We need to convert it to
      // vector of pointers of DestScType.
      GEPVal = Builder.CreateBitCast(GEPVal, VectorType::get(DestScPtrTy, VL));
    }
  } else if (BitCastDestTy) {
    // Base CE could have different src and dest types in which case we need a
    // bitcast. Can occur from llvm's canonicalization of store/load of float
    // to int by bitcast. Note that bitcast of  something like int * to
    // <4 x int>* is also handled here.
    GEPVal = Builder.CreateBitCast(GEPVal, BitCastDestTy);
  }

  if (Ref->isAddressOf()) {
    return GEPVal;
  }

  ScopeDbgLoc DbgLoc(*this, Ref->getMemDebugLoc());

  // Ref is A[i], but meaning differs for lval vs rval. On rhs, we want the
  // value of A[i], ie a load. For lval, we will store into &A[i], so we
  // want the address, the gep
  if (Ref->isRval()) {
    Instruction *LInst;

    if (GEPVal->getType()->isVectorTy()) {
      LInst = VPOUtils::createMaskedGatherCall(GEPVal, Builder,
                                               Ref->getAlignment(), MaskVal);
    } else if (MaskVal) {
      LInst = VPOUtils::createMaskedLoadCall(GEPVal, Builder,
                                             Ref->getAlignment(), MaskVal);
    } else {
      LInst = Builder.CreateAlignedLoad(GEPVal, MaybeAlign(Ref->getAlignment()),
                                        Ref->isVolatile(), "gepload");
    }

    setMetadata(LInst, Ref);

    return LInst;
  }

  return GEPVal;
}

void CGVisitor::processLiveouts() {
  // Liveout handling for multi-exit region requires mapping old exits to new
  // ones. This is done in visitGoto() for early exits and visitRegion() for
  // normal exit. Using this map we patch every phi in a region exit block.
  // Region liveout set is used to map old liveout value to its memory slot.
  //
  // NOTE: The liveout logic is dependent on incoming LLVM IR to be in LCSSA
  // form for correctness. Handling liveouts for multi-exit loops without LCSSA
  // form is complicated.

  for (auto ExitBB : ExitBBs) {

    for (auto &ExitInst : (*ExitBB)) {
      auto Phi = dyn_cast<PHINode>(&ExitInst);

      if (!Phi) {
        break;
      }

      for (unsigned I = 0, E = Phi->getNumIncomingValues(); I < E; ++I) {
        auto NewExits = getNewExitingBlocks(Phi->getIncomingBlock(I), ExitBB);

        // Exit was optimized away.
        if (!NewExits) {
          continue;
        }

        AllocaInst *SymSlot = nullptr;
        Value *ReplVal = nullptr;
        Value *IncomingVal = Phi->getIncomingValue(I);

        if (!isa<Instruction>(IncomingVal)) {
          // Old liveout value is not an instruction, use the same value for the
          // new region.
          ReplVal = IncomingVal;
        } else {
          auto Inst = cast<Instruction>(IncomingVal);
          if (!CurRegion->containsBBlock(Inst->getParent())) {
            // Old liveout instruction wasn't defined inside the region so it
            // must be defined before the region. We can use the same liveout
            // instruction for the new region.
            ReplVal = IncomingVal;
          } else {
            // Materialize the old liveout instruction into a load.
            unsigned Symbase = CurRegion->getLiveOutSymbase(Inst);
            SymSlot = getSymbaseAlloca(Symbase, Inst->getType(), CurRegion);
          }
        }

        for (auto NewExitingBB : *NewExits) {
          if (SymSlot) {
            Builder.SetInsertPoint(NewExitingBB->getTerminator());
            ReplVal = Builder.CreateLoad(SymSlot);
          }
          // NOTE: it should be okay to update phi while traversing the old
          // operands as the operand order should not change with addition.
          Phi->addIncoming(ReplVal, NewExitingBB);
        }
      }
    }
  }
}

void CGVisitor::generateDeclareValue(AllocaInst *Alloca,
                                     const DbgInfoIntrinsic *DbgInfoIntrin) {
  if (const DbgValueInst *ValueInst = dyn_cast<DbgValueInst>(DbgInfoIntrin)) {
    generateDeclareValue(Alloca, ValueInst->getVariable(),
                         ValueInst->getExpression(), ValueInst->getDebugLoc());
  } else if (const DbgDeclareInst *DeclareInst =
                 dyn_cast<DbgDeclareInst>(DbgInfoIntrin)) {
    generateDeclareValue(Alloca, DeclareInst->getVariable(),
                         DeclareInst->getExpression(),
                         DeclareInst->getDebugLoc());
  } else {
    llvm_unreachable("Unexpected debug intrinsic type");
  }
}

void CGVisitor::generateDeclareValue(AllocaInst *Alloca,
                                     DILocalVariable *LocalVariable,
                                     DIExpression *Expression,
                                     DILocation *Location) {
  assert(LocalVariable &&
         "empty or invalid DILocalVariable* passed to dbg.declare");
  assert(Location && "Expected debug loc");
  assert(Location->getScope()->getSubprogram() ==
             LocalVariable->getScope()->getSubprogram() &&
         "Expected matching subprograms");

  Function *DeclareFn =
      Intrinsic::getDeclaration(F.getParent(), Intrinsic::dbg_declare);

  auto &Context = F.getContext();
  Value *Args[] = {MetadataAsValue::get(Context, ValueAsMetadata::get(Alloca)),
                   MetadataAsValue::get(Context, LocalVariable),
                   MetadataAsValue::get(Context, Expression)};

  Instruction *CallInst = CallInst::Create(DeclareFn, Args);
  CallInst->setDebugLoc(Location);

  CallInst->insertAfter(Alloca);
}

void CGVisitor::initializeLiveins() {

  auto &BU = CurRegion->getBlobUtils();

  for (auto I = CurRegion->live_in_begin(), E = CurRegion->live_in_end();
       I != E; ++I) {

    LLVM_DEBUG(dbgs() << "Symbase " << I->first
                      << " is livein with initial value ");
    LLVM_DEBUG(I->second->dump());
    LLVM_DEBUG(dbgs() << " \n");

    unsigned Symbase = I->first;
    unsigned BlobIndex = BU.findTempBlobIndex(Symbase);

    // Some liveins are redundant as they are eliminated during parsing. IVs are
    // an example.
    // Also, there is no need to generate alloca for non-instruction liveins as
    // the original value is used directly.
    if (!CurRegion->isLiveOut(Symbase) &&
        ((BlobIndex == InvalidBlobIndex) || !BU.isInstBlob(BlobIndex))) {
      continue;
    }

    AllocaInst *SymSlot =
        getSymbaseAlloca(Symbase, I->second->getType(), CurRegion);

    Value *Val = const_cast<Value *>(I->second);
    Builder.CreateStore(Val, SymSlot);
  }
}

void CGVisitor::replaceOldRegion(BasicBlock *RegionEntry) {
  // Patch up predecessor(s) to region entry bblock
  // We do this by splitting the region entry bblock, with first block having
  // original label, but only a br to the second block, with second bblock
  // with the original instructions. Then the br to second block is replaced by
  // a cond br with true branch jumping to our new region'a entry and
  // false jumping to old code(second bblock), but cond always being true.
  // We end on valid IR but must call some form of pred opt to remove old code

  // Save entry and succ fields, these get invalidated once block is split
  BasicBlock *EntryFirstHalf = CurRegion->getEntryBBlock();

  // Split the block if the region entry is the same as function entry
  // TODO - As mentioned in discussions with Pankaj, the framework should
  // handle this splitting as splitting here can cause problems.
  if (&(F.getEntryBlock()) == EntryFirstHalf) {
    EntryFirstHalf = EntryFirstHalf->splitBasicBlock(
        EntryFirstHalf->getTerminator(), "entry.split");
  }

  BasicBlock *EntrySecondHalf =
      SplitBlock(EntryFirstHalf, &*(EntryFirstHalf->begin()));

  Instruction *Term = EntryFirstHalf->getTerminator();
  BasicBlock::iterator It(Term);
  BranchInst *RegionBranch = BranchInst::Create(
      RegionEntry, EntrySecondHalf,
      ConstantInt::get(IntegerType::get(F.getContext(), 1), 1));
  ReplaceInstWithInst(Term->getParent()->getInstList(), It, RegionBranch);
}

Value *CGVisitor::visitRegion(HLRegion *Reg) {

  assert(CurIVValues.empty() && "IV list not empty at region start");

  preprocess(Reg);

  // push back one null so iv for level 1 is at array position 1
  // in other words, make this vector 1 indexed
  CurIVValues.push_back(nullptr);
  // create new bblock for region entry
  BasicBlock *RegionEntry = BasicBlock::Create(
      F.getContext(), "region." + std::to_string(Reg->getNumber()), &F);
  Builder.SetInsertPoint(RegionEntry);

  initializeLiveins();

  LoopOptReport RegOptReport = Reg->getOptReport();
  if (RegOptReport) {
    // Optreports for outermost lost loop are stored as first child of the
    // region. So it looks like:
    // !1 = distinct !{!"llvm.loop.optreport", !2}
    // !2 = distinct !{!"intel.loop.optreport", !3}
    // !3 = !{!"intel.optreport.first_child", !4}
    // !4 = distinct !{!"llvm.loop.optreport", !5}
    // !5 = distinct !{!"intel.loop.optreport", !6, !8}
    // !6 = !{!"intel.optreport.remarks", !7}
    // !7 = !{!"intel.optreport.remark", !"Loop completely unrolled"}
    //
    // Our aim now is to attach it to the function. We do that the same way as
    // for the region. Except for the function may have multiple outermost
    // loops. In this case all the following loops as stored as next siblings of
    // the first child. E.g.
    // !1 = distinct !{!"llvm.loop.optreport", !2}
    // !2 = distinct !{!"intel.loop.optreport", !3}
    // !3 = !{!"intel.optreport.first_child", !4}
    // !4 = distinct !{!"llvm.loop.optreport", !5}
    // !5 = distinct !{!"intel.loop.optreport", !6, !8}
    // !6 = !{!"intel.optreport.remarks", !7}
    // !7 = !{!"intel.optreport.remark", !"Loop completely unrolled"}
    // !8 = !{!"intel.optreport.next_sibling", !9}
    // !9 = distinct !{!"llvm.loop.optreport", !10}
    // !10 = distinct !{!"intel.loop.optreport", !6, !11}
    // !11 = !{!"intel.optreport.next_sibling", !12}
    // !12 = distinct !{!"llvm.loop.optreport", !13}
    // !13 = distinct !{!"intel.loop.optreport", !6}

    // TODO: We reattach all region-attached loops to a function and
    // this is not always correct: the proper way would be to find
    // a previous sibling of the loop first, even if this sibling
    // is located in another region.
    LORBuilder(F).addChild(RegOptReport.firstChild());
  }

  // Onto children cg
  for (auto It = Reg->child_begin(), E = Reg->child_end(); It != E; ++It) {
    visit(*It);
  }

  BasicBlock *RegionSuccessor = Reg->getSuccBBlock();

  // current insertion point is at end of region, add jump to successor
  // and we are done
  if (RegionSuccessor) {
    addOldToNewExitBlockEntry(Reg->getExitBBlock(), RegionSuccessor,
                              Builder.GetInsertBlock());
    generateBranchIfRequired(RegionSuccessor);

  } else if (Reg->isFunctionLevel()) {
    // It is possible that the function exiting statements for function level
    // region are embedded inside the top level if statement. In this case we
    // emit an unreachable instruction so that the last merge bblock ends with
    // a terminator instruction.
    if (!Reg->exitsFunction()) {
      Builder.CreateUnreachable();
    }

    assert(!Reg->hasLiveOuts() &&
           "Function level region cannot have liveouts!");
  } else {
    assert(Reg->exitsFunction() && "no successor block to region!");
    assert(!Reg->hasLiveOuts() && "Unsupported liveout for multiexit region!");
  }

  processLiveouts();

  replaceOldRegion(RegionEntry);

  // LLVM_DEBUG(F.dump());
  // Remove null value used for indexing
  CurIVValues.pop_back();
  return nullptr;
}

Value *CGVisitor::generatePredicate(HLIf *HIf, HLIf::const_pred_iterator P) {
  Value *CurPred = nullptr;
  Value *LHSVal, *RHSVal;

  RegDDRef *LHSRef = HIf->getPredicateOperandDDRef(P, true);
  RegDDRef *RHSRef = HIf->getPredicateOperandDDRef(P, false);

  // For undef predicate, we don't need to CG operands since end result
  // is undef anyway.
  if (*P == UNDEFINED_PREDICATE) {
    // TODO icmp/fcmp with nonvector args return a boolean, i1 but
    // vector types would require cmp to return vector of i1
    return UndefValue::get(IntegerType::get(F.getContext(), 1));
  }

  LHSVal = visitRegDDRef(LHSRef);
  RHSVal = visitRegDDRef(RHSRef);
  assert(LHSVal->getType() == RHSVal->getType() &&
         "HLIf predicate type mismatch");

  CurPred = createCmpInst(*P, LHSVal, RHSVal,
                          "hir.cmp." + std::to_string(HIf->getNumber()));

  return CurPred;
}

Value *CGVisitor::generateAllPredicates(HLIf *HIf) {

  auto FirstPred = HIf->pred_begin();
  Value *CurPred = generatePredicate(HIf, FirstPred);

  for (auto It = HIf->pred_begin() + 1, E = HIf->pred_end(); It != E; ++It) {
    // conjunctions are implicitly AND atm.
    CurPred = Builder.CreateAnd(CurPred, generatePredicate(HIf, It));
  }

  return CurPred;
}

void CGVisitor::generateBranchIfRequired(BasicBlock *ToBB) {
  auto InsertBB = Builder.GetInsertBlock();

  if (InsertBB->empty() || !(InsertBB->back().isTerminator())) {
    Builder.CreateBr(ToBB);
  }
}

Value *CGVisitor::visitIf(HLIf *HIf, Value *IVAdd, AllocaInst *IVAlloca,
                          MDNode *LoopID) {
  ScopeDbgLoc DbgLoc(*this, HIf->getDebugLoc());

  Value *CondV = generateAllPredicates(HIf);

  std::string HNumStr = std::to_string(HIf->getNumber());
  BasicBlock *MergeBB =
      BasicBlock::Create(F.getContext(), "ifmerge." + HNumStr);

  bool HasThenChildren = HIf->hasThenChildren();
  bool HasElseChildren = HIf->hasElseChildren();

  BasicBlock *ThenBB =
      HasThenChildren ? BasicBlock::Create(F.getContext(), "then." + HNumStr)
                      : MergeBB;
  BasicBlock *ElseBB =
      HasElseChildren ? BasicBlock::Create(F.getContext(), "else." + HNumStr)
                      : MergeBB;

  BranchInst *CondBr = Builder.CreateCondBr(CondV, ThenBB, ElseBB);
  if (MDNode *ProfData = HIf->getProfileData()) {
    CondBr->setMetadata(LLVMContext::MD_prof, ProfData);
  }

  if (HasThenChildren) {
    // generate then block
    F.getBasicBlockList().push_back(ThenBB);
    Builder.SetInsertPoint(ThenBB);

    if (IVAdd) {
      // Create the IV store for unknown loops inside the bottom test.
      Builder.CreateStore(IVAdd, IVAlloca);
    }

    Value *LastChildVal = nullptr;
    for (auto It = HIf->then_begin(), E = HIf->then_end(); It != E; ++It) {
      LastChildVal = visit(*It);
    }

    // If this HLIf is the bottom test of the unknown loop, the last then child
    // is the backedge. We add loop metadata to it.
    if (LoopID) {
      cast<BranchInst>(LastChildVal)->setMetadata(LLVMContext::MD_loop, LoopID);
    }

    generateBranchIfRequired(MergeBB);
  }

  if (HasElseChildren) {
    assert(!IVAdd && "Bottom test cannot have else case!");

    // generate else block
    F.getBasicBlockList().push_back(ElseBB);
    Builder.SetInsertPoint(ElseBB);
    for (auto It = HIf->else_begin(), E = HIf->else_end(); It != E; ++It) {
      visit(*It);
    }

    generateBranchIfRequired(MergeBB);
  }

  // CG resumes at merge block
  F.getBasicBlockList().push_back(MergeBB);
  Builder.SetInsertPoint(MergeBB);

  return nullptr;
}

Value *CGVisitor::visitLoop(HLLoop *Lp) {
  assert(!Lp->hasZtt() && "Ztt should have been extracted!");
  assert((!Lp->hasPreheader() && !Lp->hasPostexit()) &&
         "Preheader/Postexit should have been extracted!");

  bool IsUnknownLoop = Lp->isUnknown();
  bool IsNSW = Lp->isNSW();

  CurLoopIsNSW = IsNSW;

#if INTEL_FEATURE_CSA
  // Helper for creating intrinsic calls.
  auto genCsaIntrinCall = [&](Intrinsic::ID ID, ArrayRef<Value *> Args,
                              const Twine &Name) {
    auto *Intrin = Intrinsic::getDeclaration(F.getParent(), ID);
    return Builder.CreateCall(Intrin, Args, Name);
  };

  // Loop with parallel traits need to be annotated as parallel for CSA CG. So
  // far this is done by creating a parallel region entry/exit intrinsic calls
  // around the loop, and parallel section entry/exit calls around the loop
  // body.
  Value *CsaParRegion = nullptr;
  Value *CsaParSection = nullptr;
  bool IsCsaTarget = Triple(F.getParent()->getTargetTriple()).getArch() ==
                     Triple::ArchType::csa;

  if (IsCsaTarget && Lp->getParallelTraits()) {
    assert(!IsUnknownLoop && "unknown parallel loop");
    auto *UniqueID = Builder.getInt32(4000u + Lp->getNumber());
    CsaParRegion = genCsaIntrinCall(Intrinsic::csa_parallel_region_entry,
                                    {UniqueID}, "par.reg");
  }
#endif // INTEL_FEATURE_CSA

  // set up IV, I think we can reuse the IV allocation across
  // multiple loops of same depth
  AllocaInst *Alloca = getLoopIVAlloca(Lp);

  // Keep a stack of IV values. CanonExpr CG needs to know types
  // of loop IV itself, but this information is not available from
  // CE
  CurIVValues.push_back(Alloca);

  Value *StartVal = visitRegDDRef(Lp->getLowerDDRef());

  if (!StartVal || !Alloca)
    llvm_unreachable("Failed to CG IV");

  assert(StartVal->getType() == Lp->getIVType() &&
         "IVtype does not match start type");

  Builder.CreateStore(StartVal, Alloca);

  Value *Upper = nullptr;
  BasicBlock *LoopBB = nullptr;

  std::string LName = "loop." + std::to_string(Lp->getNumber());

  if (!IsUnknownLoop) {
    // upper is loop invariant so we can generate it outside the loop
    Upper = visitRegDDRef(Lp->getUpperDDRef());

    assert(Upper->getType() == Lp->getIVType() &&
           "IVtype does not match upper type");

    LoopBB = BasicBlock::Create(F.getContext(), LName, &F);

    // explicit fallthru to loop, terminates current bblock
    Builder.CreateBr(LoopBB);
    Builder.SetInsertPoint(LoopBB);

#if INTEL_FEATURE_CSA
    if (CsaParRegion)
      CsaParSection = genCsaIntrinCall(Intrinsic::csa_parallel_section_entry,
                                       {CsaParRegion}, "par.sec");
#endif // INTEL_FEATURE_CSA
  }

  auto LastIt =
      IsUnknownLoop ? Lp->getBottomTest()->getIterator() : Lp->child_end();

  // CG children
  for (auto It = Lp->child_begin(); It != LastIt; ++It) {
    visit(*It);
  }

  ScopeDbgLoc DbgLocBottomTest(*this, Lp->getCmpDebugLoc());

  // increment IV
  Value *CurVar = Builder.CreateLoad(Alloca);
  Value *StepVal = IsUnknownLoop ? ConstantInt::getSigned(Lp->getIVType(), 1)
                                 : visitRegDDRef(Lp->getStrideDDRef());

  assert(StepVal->getType() == Lp->getIVType() &&
         "IVtype does not match stepval type");

  // NUW flag is applicable either if loop is not unknown or we could deduce
  // NSW.
  // NOTE: We do not try to deduce NUW flag for unknown loops so we may be
  // losing info in some cases.
  Value *NextVar = Builder.CreateAdd(CurVar, StepVal, "nextiv" + LName,
                                     (IsNSW || !IsUnknownLoop), IsNSW);

  // Attach metadata to the resulting loop; Exclude opt report field
  // if there was any.
  MDNode *LoopID = LoopOptReport::eraseOptReportFromLoopID(
      Lp->getLoopMetadata(), F.getContext());

  if (LoopOptReport OptReport = Lp->getOptReport()) {
    LoopID =
        LoopOptReport::addOptReportToLoopID(LoopID, OptReport, F.getContext());
  }

  if (IsUnknownLoop) {
    // visit bottom test of unknown loop and pass in information to generate
    // IV store and attach loop metadata.
    visitIf(cast<HLIf>(&*LastIt), NextVar, Alloca, LoopID);

  } else {
    // Create store to IV.
    Builder.CreateStore(NextVar, Alloca);

#if INTEL_FEATURE_CSA
    if (CsaParSection)
      genCsaIntrinCall(Intrinsic::csa_parallel_section_exit, {CsaParSection},
                       "");
#endif // INTEL_FEATURE_CSA

    auto *ConstStepVal = dyn_cast<ConstantInt>(StepVal);

    // generate bottom test.
    Value *EndCond = nullptr;
    // For step of 1, generate canonical comparison type of '!='. These are more
    // likely to be handled by LLVM passes like loop strength reduction.
    if (ConstStepVal && ConstStepVal->isOne()) {
      // Generates: (i != upper).
      EndCond =
          Builder.CreateICmp(CmpInst::ICMP_NE, CurVar, Upper, "cond" + LName);
    } else {
      // Generates: (i+1 <= upper).
      EndCond =
          Builder.CreateICmp(IsNSW ? CmpInst::ICMP_SLE : CmpInst::ICMP_ULE,
                             NextVar, Upper, "cond" + LName);
    }

    BasicBlock *AfterBB =
        BasicBlock::Create(F.getContext(), "after" + LName, &F);

    ScopeDbgLoc DbgLocBranch(*this, Lp->getBranchDebugLoc());

    // latch
    BranchInst *Br = Builder.CreateCondBr(EndCond, LoopBB, AfterBB);
    if (MDNode *ProfData = Lp->getProfileData()) {
      Br->setMetadata(LLVMContext::MD_prof, ProfData);
    }

    if (LoopID) {
      Br->setMetadata(LLVMContext::MD_loop, LoopID);
    }

    // new code goes after loop
    Builder.SetInsertPoint(AfterBB);
  }

#if INTEL_FEATURE_CSA
  if (CsaParRegion)
    genCsaIntrinCall(Intrinsic::csa_parallel_region_exit, {CsaParRegion}, "");
#endif // INTEL_FEATURE_CSA

  CurIVValues.pop_back();

  return nullptr;
}

BasicBlock *CGVisitor::getBBlockForLabel(HLLabel *L) {
  if (InternalLabels.count(L))
    return InternalLabels[L];

  BasicBlock *LabelBB =
      BasicBlock::Create(F.getContext(), "hir.L." + Twine(L->getNumber()), &F);
  InternalLabels[L] = LabelBB;
  return LabelBB;
}

Value *CGVisitor::visitLabel(HLLabel *L) {
  // if we see label it must be internal, and it must be unique
  BasicBlock *LabelBBlock = getBBlockForLabel(L);
  assert(LabelBBlock->empty() && "label already in use");

  // create a br to L's block. ending current block
  generateBranchIfRequired(LabelBBlock);
  Builder.SetInsertPoint(LabelBBlock);
  return nullptr;
}

Value *CGVisitor::visitGoto(HLGoto *Goto) {
  ScopeDbgLoc DbgLoc(*this, Goto->getDebugLoc());

  // get basic block for G's target
  BasicBlock *TargetBBlock = Goto->getTargetBBlock();

  if (TargetBBlock) {
    addOldToNewExitBlockEntry(Goto->getSrcBBlock(), TargetBBlock,
                              Builder.GetInsertBlock());
  } else {
    TargetBBlock = getBBlockForLabel(Goto->getTargetLabel());
  }

  assert(TargetBBlock && "No bblock target for goto");
  // create a br to target, ending this block
  auto *Br = Builder.CreateBr(TargetBBlock);

  return Br;
}

Value *CGVisitor::visitSwitch(HLSwitch *S) {
  ScopeDbgLoc DbgLoc(*this, S->getDebugLoc());

  Value *CondV = visitRegDDRef(S->getConditionDDRef());
  SmallString<10> SwitchName("hir.sw." + std::to_string(S->getNumber()));

  BasicBlock *DefaultBlock =
      BasicBlock::Create(F.getContext(), SwitchName + ".default");
  BasicBlock *EndBlock =
      BasicBlock::Create(F.getContext(), SwitchName + ".end");

  SwitchInst *LLVMSwitch =
      Builder.CreateSwitch(CondV, DefaultBlock, S->getNumCases());
  if (MDNode *ProfData = S->getProfileData()) {
    LLVMSwitch->setMetadata(LLVMContext::MD_prof, ProfData);
  }

  // generate default block
  F.getBasicBlockList().push_back(DefaultBlock);
  Builder.SetInsertPoint(DefaultBlock);
  for (auto I = S->default_case_child_begin(), E = S->default_case_child_end();
       I != E; ++I) {
    visit(*I);
  }

  generateBranchIfRequired(EndBlock);

  // generate case blocks
  for (unsigned int I = 1; I <= S->getNumCases(); ++I) {
    Value *CaseV = visitRegDDRef(S->getCaseValueDDRef(I));
    // assert its a constant or rely on verifier?
    ConstantInt *CaseInt = cast<ConstantInt>(CaseV);

    BasicBlock *CaseBlock = BasicBlock::Create(
        F.getContext(), SwitchName + ".case." + std::to_string(I - 1));
    F.getBasicBlockList().push_back(CaseBlock);
    Builder.SetInsertPoint(CaseBlock);

    for (auto HNode = S->case_child_begin(I), E = S->case_child_end(I);
         HNode != E; ++HNode) {
      visit(*HNode);
    }

    generateBranchIfRequired(EndBlock);
    LLVMSwitch->addCase(CaseInt, CaseBlock);
  }

  F.getBasicBlockList().push_back(EndBlock);
  Builder.SetInsertPoint(EndBlock);
  return nullptr;
}

inline void CGVisitor::setMetadata(Value *Val, const RegDDRef *Ref) {
  if (Instruction *Instr = dyn_cast<Instruction>(Val)) {
    setMetadata(Instr, Ref);
  }
}

void CGVisitor::setMetadata(Instruction *Inst, const RegDDRef *Ref) {
  RegDDRef::MDNodesTy MDs;
  Ref->getAllMetadataOtherThanDebugLoc(MDs);
  setMetadata(Inst, MDs);
}

void CGVisitor::setMetadata(Instruction *Inst, const RegDDRef::MDNodesTy &MDs) {
  for (auto const &I : MDs) {
    Inst->setMetadata(I.first, I.second);
  }
}

void CGVisitor::generateLvalStore(const HLInst *HInst, Value *StorePtr,
                                  Value *StoreVal) {
  if (!HInst->hasLval()) {
    return;
  }

  auto LvalRef = HInst->getLvalDDRef();
  RegDDRef *MaskDDRef = const_cast<RegDDRef *>(HInst->getMaskDDRef());
  Value *MaskVal = MaskDDRef ? visitRegDDRef(MaskDDRef) : nullptr;

  ScopeDbgLoc DbgLoc(*this, HInst->getDebugLoc());

  if (LvalRef->hasGEPInfo()) {
    Instruction *ResInst;

    if (StorePtr->getType()->isVectorTy()) {
      ResInst = VPOUtils::createMaskedScatterCall(
          StorePtr, StoreVal, Builder, LvalRef->getAlignment(), MaskVal);
    } else if (MaskVal) {
      ResInst = VPOUtils::createMaskedStoreCall(
          StorePtr, StoreVal, Builder, LvalRef->getAlignment(), MaskVal);
    } else {
      ResInst = Builder.CreateAlignedStore(StoreVal, StorePtr,
                                           MaybeAlign(LvalRef->getAlignment()),
                                           LvalRef->isVolatile());
    }

    setMetadata(ResInst, LvalRef);
  } else {
    if (MaskVal) {
      // At this point we know StorePtr is an address from an alloca that
      // is safe to load. We use a blended unmasked store here which is
      // better for performance and also needed as a work around for i1
      // maskedstore issue in codegen(CQ416258).
      // As an example consider the masked HLInst:
      // %.vec = (<4 x i32>*)(%ip)[i1]; Mask = @{%Pred42_10}
      // The generated LLVM IR looks like the following. %9 contains the
      // address of (%ip)[i1], %t22. contains the mask value %Pred42_10,
      // %t24 is the address of alloca corresponding to %.vec.
      //
      // %10 = call <4 x i32> @llvm.masked.load.v4i32.p0v4i32(
      //     <4 x i32>* %9, i32 4, <4 x i1> %t22., <4 x i32> undef), !tbaa !2
      //  %t22.18 = load <4 x i1>, <4 x i1>* %t22
      //  %mload19 = load <4 x i32>, <4 x i32>* %t24
      //  %11 = select <4 x i1> %t22.18, <4 x i32> %10, <4 x i32> %mload19
      //  store <4 x i32> %11, <4 x i32>* %t24
      auto MLoad = Builder.CreateLoad(StorePtr, "mload");
      auto MSel = Builder.CreateSelect(MaskVal, StoreVal, MLoad);
      Builder.CreateStore(MSel, StorePtr);
    } else {
      Builder.CreateStore(StoreVal, StorePtr);
    }
  }
}

static void populateOperandBundles(HLInst *HInst,
                                   SmallVectorImpl<Value *> &Operands,
                                   SmallVectorImpl<OperandBundleDef> &Bundles) {
  assert(HInst->isCallInst() && "Call instruction expected!");

  unsigned NumOperandBundles = HInst->getNumOperandBundles();

  if (NumOperandBundles == 0) {
    return;
  }

  unsigned NumNonBundleOperands = HInst->getNumNonBundleOperands();
  unsigned NumPrevBundleOperands = 0;
  unsigned OpBeginIndex = NumNonBundleOperands + NumPrevBundleOperands;

  for (unsigned I = 0; I < NumOperandBundles; ++I) {
    unsigned NumCurrentBundleOperands = HInst->getNumBundleOperands(I);
    unsigned OpEndIndex = OpBeginIndex + NumCurrentBundleOperands;

    std::vector<Value *> Inputs;

    for (unsigned J = OpBeginIndex; J < OpEndIndex; ++J) {
      Inputs.push_back(Operands[J]);
    }

    Bundles.emplace_back(std::string(HInst->getOperandBundleAt(I).getTagName()),
                         Inputs);
    OpBeginIndex = OpEndIndex;
  }

  // Exclude bundle operands from Operands.
  Operands.resize(NumNonBundleOperands);
}

Value *CGVisitor::visitInst(HLInst *HInst) {
  ScopeDbgLoc DbgLoc(*this, HInst->getDebugLoc());

  // CG the operands
  SmallVector<Value *, 6> Ops;

  // See if we need to compute the mask value. Currently we need to mask
  // any generated loads if the HLInst has a mask DDRef.
  Value *MaskVal = nullptr;
  RegDDRef *MaskRef = HInst->getMaskDDRef();
  unsigned LvalTokenSymbase = InvalidSymbase;

  for (auto R = HInst->op_ddref_begin(), E = HInst->op_ddref_end(); R != E;
       ++R) {
    auto Ref = (*R);

    // We cannot load/store to token types so we will directly use the generated
    // LLVM instructions.
    if (Ref->getDestType()->isTokenTy()) {
      assert(Ref->isTerminalRef() && "Non-terminal token type ref found!");

      if (Ref->isLval()) {
        LvalTokenSymbase = Ref->getSymbase();
        // This is to keep Ops in sync with number of operands.
        Ops.push_back(nullptr);
      } else {
        Ops.push_back(getTokenVal(Ref->getSymbase()));
      }

      continue;
    }

    // We need to mask loads if we have a mask ddref - generate mask value
    // to be used for these loads.
    if (MaskRef && !MaskVal && Ref->isRval() && Ref->isMemRef()) {
      MaskVal = visitRegDDRef(MaskRef);
    }

    auto DestTy = Ref->getDestType();
    auto OpVal = visitRegDDRef(Ref, MaskVal);

    // Do a broadcast of instruction operands if needed.
    if (Ref->isRval() && DestTy->isVectorTy() &&
        !(OpVal->getType()->isVectorTy())) {
      OpVal = Builder.CreateVectorSplat(
          cast<VectorType>(DestTy)->getNumElements(), OpVal);
    }

    Ops.push_back(OpVal);
  }

  // Operands for the eventual store that needs to be generated for a lval
  // HLInst.
  Value *StorePtr = !Ops.empty() ? Ops[0] : nullptr;
  Value *StoreVal = nullptr;

  // create the inst
  auto Inst = HInst->getLLVMInstruction();

  // Any LLVM instruction which semantically has a terminal lval/rval can
  // alternatively contain a memref operand in HIR.
  // For example, add instruction can look like this- A[i] = B[i] + C[i].
  if (isa<LoadInst>(Inst) || isa<StoreInst>(Inst)) {
    StoreVal = Ops[1];

  } else if (auto BOp = dyn_cast<BinaryOperator>(Inst)) {
    StoreVal = Builder.CreateBinOp(BOp->getOpcode(), Ops[1], Ops[2], "",
                                   BOp->getMetadata(LLVMContext::MD_fpmath));

    // CreateBinOp could fold operator to constant.
    BinaryOperator *StoreBinOp = dyn_cast<BinaryOperator>(StoreVal);

    if (StoreBinOp) {
      if (isa<PossiblyExactOperator>(BOp)) {
        StoreBinOp->setIsExact(BOp->isExact());
      }

      if (isa<OverflowingBinaryOperator>(BOp)) {
        StoreBinOp->setHasNoSignedWrap(BOp->hasNoSignedWrap());
        StoreBinOp->setHasNoUnsignedWrap(BOp->hasNoUnsignedWrap());
      }
    }

  } else if (auto Call = HInst->getCallInst()) {

    SmallVector<OperandBundleDef, 4> Bundles;

    populateOperandBundles(HInst, Ops, Bundles);

    if (HInst->hasLval()) {
      // Turns Operands vector into function args vector by removing lval
      // TODO: Separate this logic from framework's implementation of putting
      // lval as the first operand.
      Ops.erase(Ops.begin());
    }

    Value *FuncVal = Call->getCalledFunction();

    if (!FuncVal) {
      // For indirect calls, function pointer is stored in last operand.
      FuncVal = Ops.pop_back_val();
    }

    CallInst *ResCall =
        Builder.CreateCall(Call->getFunctionType(), FuncVal, Ops, Bundles);

    // TODO: Copy parameter attributes as well.
    ResCall->setCallingConv(Call->getCallingConv());
    ResCall->setAttributes(Call->getAttributes());
    ResCall->setTailCallKind(Call->getTailCallKind());

    // TODO: Copy metadata from HLInst instead.
    RegDDRef::MDNodesTy MDs;
    Inst->getAllMetadata(MDs);
    setMetadata(ResCall, MDs);

    StoreVal = ResCall;

  } else if (auto Cast = dyn_cast<CastInst>(Inst)) {
    assert(Ops.size() == 2 && "invalid cast");

    StoreVal = Builder.CreateCast(Cast->getOpcode(), Ops[1],
                                  Ops[0]->getType()->getPointerElementType());

  } else if (isa<SelectInst>(Inst)) {
    Value *CmpLHS = Ops[1];
    Value *CmpRHS = Ops[2];
    Value *TVal = Ops[3];
    Value *FVal = Ops[4];

    Value *Pred =
        createCmpInst(HInst->getPredicate(), CmpLHS, CmpRHS,
                      "hir.selcmp." + std::to_string(HInst->getNumber()));
    StoreVal = Builder.CreateSelect(Pred, TVal, FVal);
    if (SelectInst *S = dyn_cast<SelectInst>(StoreVal)) {
      if (MDNode *ProfData = HInst->getProfileData()) {
        S->setMetadata(LLVMContext::MD_prof, ProfData);
      }
    }
  } else if (isa<CmpInst>(Inst)) {

    StoreVal = createCmpInst(HInst->getPredicate(), Ops[1], Ops[2],
                             "hir.cmp." + std::to_string(HInst->getNumber()));

  } else if (isa<GEPOrSubsOperator>(Inst)) {
    // Gep Instructions in LLVM may have any number of operands but the HIR
    // representation for them is always a single rhs ddref
    assert(Ops.size() == 2 && "Gep Inst have single rhs of form &val");
    StoreVal = Ops[1];

  } else if (auto *Alloca = dyn_cast<AllocaInst>(Inst)) {
    // Lval type is a pointer to type returned by alloca inst. We need to
    // dereference twice to get to element type
    Type *ElementType =
        Ops[0]->getType()->getPointerElementType()->getPointerElementType();

    auto *NewAlloca = Builder.CreateAlloca(
        ElementType, Ops[1],
        "hir.alloca." + std::to_string(HInst->getNumber()));

    NewAlloca->setAlignment(MaybeAlign(Alloca->getAlignment()));
    StoreVal = NewAlloca;

  } else if (isa<ExtractElementInst>(Inst)) {
    StoreVal = Builder.CreateExtractElement(Ops[1], Ops[2], Inst->getName());

  } else if (isa<InsertElementInst>(Inst)) {
    StoreVal =
        Builder.CreateInsertElement(Ops[1], Ops[2], Ops[3], Inst->getName());

  } else if (isa<ShuffleVectorInst>(Inst)) {
    StoreVal =
        Builder.CreateShuffleVector(Ops[1], Ops[2], Ops[3], Inst->getName());

  } else if (isa<ReturnInst>(Inst)) {
    StoreVal =
        !Ops.empty() ? Builder.CreateRet(Ops[0]) : Builder.CreateRetVoid();
  } else if (isa<UnreachableInst>(Inst)) {
    assert(Ops.empty() && "No operands expected for uneachable inst!");
    StoreVal = Builder.CreateUnreachable();

  } else if (Inst->getOpcode() == Instruction::FNeg) {
    StoreVal = Builder.CreateFNeg(Ops[1], "fneg");

  } else if (isa<FreezeInst>(Inst)) {
    StoreVal = Builder.CreateFreeze(Ops[1], "freeze");

  } else {
    llvm_unreachable("Unimpl CG for inst");
  }

  // Copy fast math flags from underlying instruction
  // Check for FPMathOperator is done on both StoreVal and Inst here because in
  // some cases non operator instructions that return float values are
  // recognized as FPMathOperator. This is potentially a bug in LLVM's
  // FPMathOperator checks. Intrinsic calls that use and return FP values are
  // valid FPMathOperator (for example, LLVM's experimental vector reduce
  // intrinsics).
  if (isa<FPMathOperator>(StoreVal) && isa<FPMathOperator>(Inst)) {
    if (auto *StoreInst = dyn_cast<Instruction>(StoreVal))
      StoreInst->copyFastMathFlags(Inst);
  }

  if (LvalTokenSymbase != InvalidSymbase) {
    addTokenEntry(LvalTokenSymbase, StoreVal);
  } else {
    generateLvalStore(HInst, StorePtr, StoreVal);
  }

  return nullptr;
}

Value *CGVisitor::sumBlobs(CanonExpr *CE) {
  if (!CE->hasBlob())
    return nullptr;

  auto CurBlobPair = CE->blob_begin();
  Value *Res = BlobPairCG(CE, CurBlobPair);
  CurBlobPair++;

  auto CEDestTy = CE->getDestType();
  for (auto E = CE->blob_end(); CurBlobPair != E; ++CurBlobPair) {
    Value *CurRes = BlobPairCG(CE, CurBlobPair);

    // We can have a CanonExpr with blobs where some of the blobs have
    // been replaced by vectorized values while others simply need a
    // broadcast of the loop invariant blob value. Handle these cases
    // here. Example CE: i1 + %N + %.vec + <i32 0, i32 1, i32 2, i32 3>
    // %N is a blob that needs a broadcast.
    if (CEDestTy->isVectorTy()) {
      if (Res->getType()->isVectorTy() && !CurRes->getType()->isVectorTy()) {
        CurRes = Builder.CreateVectorSplat(
            cast<VectorType>(CEDestTy)->getNumElements(), CurRes);
      } else if (CurRes->getType()->isVectorTy() &&
                 !Res->getType()->isVectorTy()) {
        Res = Builder.CreateVectorSplat(
            cast<VectorType>(CEDestTy)->getNumElements(), Res);
      }
    }

    Res = Builder.CreateAdd(Res, CurRes);
  }

  return Res;
}

Value *CGVisitor::sumIV(CanonExpr *CE) {
  if (!CE->hasIV())
    return nullptr;

  auto CurIVPair = CE->iv_begin();
  // start with first summation not of x*0
  for (auto E = CE->iv_end(); CurIVPair != E; ++CurIVPair) {
    if (CE->getIVConstCoeff(CurIVPair))
      break;
  }

  if (CurIVPair == CE->iv_end()) {
    llvm_unreachable("No iv in CE");
  }

  Type *CETy = CE->getSrcType();
  Type *ScalarCETy = CETy->getScalarType();
  Value *Res = IVPairCG(CE, CurIVPair, ScalarCETy);
  CurIVPair++;

  // accumulate other pairs
  for (auto E = CE->iv_end(); CurIVPair != E; ++CurIVPair) {
    if (CE->getIVConstCoeff(CurIVPair)) {
      Value *TempRes = IVPairCG(CE, CurIVPair, ScalarCETy);
      bool ResIsVec = Res->getType()->isVectorTy();
      bool TempResIsVec = TempRes->getType()->isVectorTy();

      // Do a broadcast if needed - we need a broadcast of scalar
      // value if one of Res/TempRes is a scalar and the other is a
      // vector.
      if (ResIsVec ^ TempResIsVec) {
        assert(CETy->isVectorTy() &&
               "Unexpected scalar CE type for a vector type IV pair");
        if (!ResIsVec)
          Res = Builder.CreateVectorSplat(
              cast<VectorType>(CETy)->getNumElements(), Res);
        if (!TempResIsVec)
          TempRes = Builder.CreateVectorSplat(
              cast<VectorType>(CETy)->getNumElements(), TempRes);
      }
      Res = Builder.CreateAdd(Res, TempRes);
    }
  }

  return Res;
}

Value *CGVisitor::IVPairCG(CanonExpr *CE, CanonExpr::iv_iterator IVIt,
                           Type *Ty) {

  // Load IV at given level from this loop nest
  Value *IV = Builder.CreateLoad(CurIVValues[CE->getLevel(IVIt)]);

  // If IV type and Ty(CE src type) do not match, convert as needed.
  if (IV->getType() != Ty) {
    if (Ty->getPrimitiveSizeInBits() >
        IV->getType()->getPrimitiveSizeInBits()) {
      IV = CurLoopIsNSW ? Builder.CreateSExt(IV, Ty)
                        : Builder.CreateZExt(IV, Ty);
    } else {
      IV = Builder.CreateTrunc(IV, Ty);
    }
  }

  // pairs are of form <Index, Coeff>.
  if (CE->getIVBlobCoeff(IVIt)) {
    Value *CoefV = IVCoefCG(CE, IVIt);
    Type *CoefTy = CoefV->getType();

    // If the coefficient is a vector, broadcast IV.
    if (CoefTy->isVectorTy()) {
      assert(!IV->getType()->isVectorTy() && "Non-scalar IV");
      IV = Builder.CreateVectorSplat(cast<VectorType>(CoefTy)->getNumElements(),
                                     IV);
    }
    return Builder.CreateMul(CoefV, IV);
  } else {
    return CoefCG(CE->getIVConstCoeff(IVIt), IV);
  }
}
Value *CGVisitor::CoefCG(int64_t Coeff, Value *V) {

  // do not emit 1*iv, just emit IV
  if (Coeff == 1)
    return V;
  if (Coeff == 0)
    llvm_unreachable("Dead mul in CoefCG");

  return Builder.CreateMul(
      ConstantInt::getSigned(const_cast<Type *>(V->getType()), Coeff), V);
}

void HIRCodeGen::eraseDummyInstructions(HLNodeUtils &HNU) {

  auto FirstInst = HNU.getFirstDummyInst();
  auto LastInst = HNU.getLastDummyInst();

  if (!FirstInst) {
    return;
  }

  for (auto I = BasicBlock::iterator(FirstInst),
            E = std::next(BasicBlock::iterator(LastInst));
       I != E;) {
    I = I->eraseFromParent();
  }
}

void CGVisitor::addOldToNewExitBlockEntry(BasicBlock *OldExitingBB,
                                          BasicBlock *ExitBB,
                                          BasicBlock *NewExitingBB) {

  ExitBBs.insert(ExitBB);

  for (auto &Edge : OldToNewRegionExitingBlocks) {
    if ((Edge.OldExitingBB == OldExitingBB) && (Edge.ExitBB == ExitBB)) {
      Edge.NewExitingBBs.push_back(NewExitingBB);
      return;
    }
  }

  OldToNewRegionExitingBlocks.emplace_back(OldExitingBB, ExitBB);
  OldToNewRegionExitingBlocks.back().NewExitingBBs.push_back(NewExitingBB);
}

const SmallVector<BasicBlock *, 8> *
CGVisitor::getNewExitingBlocks(BasicBlock *OldExitingBB,
                               BasicBlock *ExitBB) const {
  for (auto &Edge : OldToNewRegionExitingBlocks) {
    if ((Edge.OldExitingBB == OldExitingBB) && (Edge.ExitBB == ExitBB)) {
      return &Edge.NewExitingBBs;
    }
  }

  return nullptr;
}
