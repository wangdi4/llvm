#if INTEL_COLLAB // -*- C++ -*-
//
// INTEL CONFIDENTIAL
//
// Modifications, Copyright (C) 2021 Intel Corporation
//
// This software and the related documents are Intel copyrighted materials, and
// your use of them is governed by the express license under which they were
// provided to you ("License"). Unless the License provides otherwise, you may not
// use, modify, copy, publish, distribute, disclose or transmit this software or
// the related documents without Intel's prior written permission.
//
// This software and the related documents are provided as is, with no express
// or implied warranties, other than those that are expressly stated in the
// License.
//
//=-- VPOParoptUtils.h - Class definition for VPO Paropt utilites -*- C++ -*-=//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// Authors:
// --------
// Xinmin Tian (xinmin.tian@intel.com)
//
// Major Revisions:
// ----------------
// Nov 2015: Initial Implementation of OpenMP runtime APIs (Xinmin Tian)
//
//===----------------------------------------------------------------------===//
///
/// \file
/// This file defines the VPOParoptUtils class and provides a set of utilities
/// that are used for Paropt transformation and multi-threaded code generation.
///
//===----------------------------------------------------------------------===//

#ifndef LLVM_TRANSFORM_VPO_PAROPT_UTILS_H
#define LLVM_TRANSFORM_VPO_PAROPT_UTILS_H

#include "llvm/ADT/SmallVector.h"
#include "llvm/ADT/StringMap.h"
#include "llvm/Analysis/LoopInfo.h"
#include "llvm/IR/DebugInfo.h"
#include "llvm/IR/Dominators.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/Type.h"
#include "llvm/Support/raw_ostream.h"
#include "llvm/Support/Debug.h"
#include "llvm/Transforms/Utils/ValueMapper.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include "llvm/Analysis/VPO/WRegionInfo/WRegionUtils.h"
#include <unordered_map>
#include <utility>

#if INTEL_CUSTOMIZATION
#if INTEL_FEATURE_SW_DTRANS
#include "Intel_DTrans/Analysis/DTransTypes.h"
#endif // INTEL_FEATURE_SW_DTRANS

#endif // INTEL_CUSTOMIZATION
// Use trampoline for internal microtasks
#define KMP_IDENT_IMB              0x01

// Use c-style ident structure
#define KMP_IDENT_KMPC             0x02

// Entry point generated using Cluster OpenMP switch
#define KMP_IDENT_CLOMP            0x04

// Entry point generated by auto-parallelization
#define KMP_IDENT_AUTOPAR          0x08

// compiler generates atomic reduction option for kmpc_reduce
#define KMP_IDENT_ATOMIC_REDUCE    0x10

// replaced by KMP_IDENT_BARRIER_EXPL below
// #define KMP_IDENT_EXPLICIT_BARRIER 0x20

// Pass OpenMP version to runtime to enable non-monotonic scheduling by default
#define KMP_IDENT_OPENMP_SPEC_VERSION_5_0 0x32000000 //(major*10 + minor) << 24

//
// Implicit barriers are classified further
//
// = 0000 0010 0000 barrier directive in user's code
#define KMP_IDENT_BARRIER_EXPL           0x0020

// = 0000 0100 0000
#define KMP_IDENT_BARRIER_IMPL           0x0040

// = 0001 1100 0000 unused by cmplr
#define KMP_IDENT_BARRIER_IMPL_MASK      0x01C0

// = 0000 0100 0000
#define KMP_IDENT_BARRIER_IMPL_FOR       0x0040

// = 0000 1100 0000
#define KMP_IDENT_BARRIER_IMPL_SECTIONS  0x00C0

// = 0001 0100 0000
#define KMP_IDENT_BARRIER_IMPL_SINGLE    0x0140

// = 0001 1100 0000 workshare construct not supported by cmplr
#define KMP_IDENT_BARRIER_IMPL_WORKSHARE 0x01C0

// = 0010 0000 0000 workshare loop construct
#define KMP_IDENT_WORK_LOOP              0x0200

// = 0100 0000 0000
#define KMP_IDENT_WORK_SECTIONS          0x0400

// = 1000 0000 0000
#define KMP_IDENT_WORK_DISTRIBUTE        0x0800

namespace llvm {

class Value;
class Module;
class Function;
class Type;
class BasicBlock;
class Loop;
class LoopInfo;
class DominatorTree;
class StringRef;
class CallInst;
class IntrinsicInst;
class Constant;
class LLVMContext;
class BasicBlock;

namespace vpo {

namespace spirv {
  enum Scope {
    CrossDevice = 0,
    Device      = 1,
    Workgroup   = 2,
    Subgroup    = 3,
    Invocation  = 4
  };

  enum GroupOperations {
    GroupOperationReduce        = 0,
    GroupOperationInclusiveScan = 1,
    GroupOperationExclusiveScan = 2
  };

  enum MemorySemantics {
    None =                   0,
    Acquire =                0x2,
    Release =                0x4,
    AcquireRelease =         0x8,
    SequentiallyConsistent = 0x10,
    UniformMemory =          0x40,
    SubgroupMemory =         0x80,
    WorkgroupMemory =        0x100,
    CrossWorkgroupMemory =   0x200,
    AtomicCounterMemory =    0x400,
    ImageMemory =            0x800
  };

  static const StringRef ExecutionSchemeOptionName(
      "vpo-paropt-gpu-execution-scheme");

  enum ExecutionSchemeTy {
    // Implicit SIMD mode, which is kernel-level vectorization.
    // Maximum number of WIs per WG is controlled by
    // CL_DEVICE_MAX_WORK_GROUP_SIZE, CL_KERNEL_WORK_GROUP_SIZE,
    // OMP_THREAD_LIMIT environment variable and thread_limit() clause.
    // Maximum number of WGs controlled by CL_DEVICE_MAX_COMPUTE_UNITS and
    // num_teams() clause.
    ImplicitSIMDES = 0,

    // Implicit SIMD mode with SPMD - implicit SIMD mode, which is kernel-level
    // vectorization.
    // Maximum number of WIs per WG is controlled by
    // CL_DEVICE_MAX_WORK_GROUP_SIZE, CL_KERNEL_WORK_GROUP_SIZE,
    // OMP_THREAD_LIMIT environment variable and thread_limit() clause.
    // Maximum number of WGs is defined by the actual loop(s) trip-count(s)
    // and the maximum number of WIs per WG. This scheme will only be used
    // for combined constructs, e.g. "omp target teams distribute parallel for",
    // for which it is possible to compute the ND-range before the target
    // region. If num_teams() clause is present on the combined construct,
    // then we fall back to implicit SIMD mode (see default above).
    ImplicitSIMDSPMDES,

    // Explicit SIMD mode - loop level vectorization (not supported yet,
    // so we silently ignore this option).
    ExplicitSIMDES
  };
} // end namespace spirv

namespace intrinsics {
  /// This is an std::pair with TypeID as first member, and size as second.
  /// This can be expanded to be a Tuple in the future if needed.
  typedef std::pair<Type::TypeID, unsigned> IntrinsicOperandTy;

  /// \name Instances of IntrinsicOperandTy objects of different kinds.
  /// @{
  static const IntrinsicOperandTy I8   = { Type::IntegerTyID,  8 };
  static const IntrinsicOperandTy I16  = { Type::IntegerTyID,  16 };
  static const IntrinsicOperandTy I32  = { Type::IntegerTyID,  32 };
  static const IntrinsicOperandTy I64  = { Type::IntegerTyID,  64 };
  static const IntrinsicOperandTy P32  = { Type::PointerTyID,  32 };
  static const IntrinsicOperandTy P64  = { Type::PointerTyID,  64 };
  static const IntrinsicOperandTy F16  = { Type::HalfTyID,     16 };
  static const IntrinsicOperandTy F32  = { Type::FloatTyID,    32 };
  static const IntrinsicOperandTy F64  = { Type::DoubleTyID,   64 };
  static const IntrinsicOperandTy F80  = { Type::X86_FP80TyID, 80 };
  static const IntrinsicOperandTy F128 = { Type::FP128TyID,    128 };
  /// @}

} // end namespace intrinsics

/// Base class for offload entries. It is not supposed to be instantiated.
class OffloadEntry {
public:
  enum EntryKind : unsigned {
    RegionKind       = 0,
    VarKind          = 1u,
    IndirectFuncKind = 2u
  };

protected:
  explicit OffloadEntry(EntryKind Kind, StringRef Name, uint32_t Flags)
      : Kind(Kind), Name(Name), Flags(Flags) {}

public:
  virtual ~OffloadEntry() = default;

  EntryKind getKind() const { return Kind; }

  StringRef getName() const { return Name; }

  Constant *getAddress() const { return Addr; }

  void setAddress(Constant *NewAddr) {
    assert(!Addr && "Address has been set before!");
    Addr = NewAddr;
  }

  uint32_t getFlags() const { return Flags; }
  void setFlags(uint32_t NewFlags) { Flags = NewFlags; }

  virtual size_t getSize() const = 0;

private:
  EntryKind Kind;
  SmallString<64u> Name;
  Constant* Addr = nullptr;
  uint32_t Flags = 0;
};

/// Target region entry.
class RegionEntry final : public OffloadEntry {
public:
  enum : uint32_t {
    Region = 0x00,
    Ctor   = 0x02,
    Dtor   = 0x04
  };

public:
  RegionEntry(StringRef Name, uint32_t Flags)
      : OffloadEntry(RegionKind, Name, Flags) {}

  RegionEntry(GlobalValue *GV, uint32_t Flags)
      : OffloadEntry(RegionKind, GV->getName(), Flags) {
    setAddress(GV);
  }

  size_t getSize() const override { return 0; }

  static bool classof(const OffloadEntry *E) {
    return E->getKind() == RegionKind;
  }
};

/// Global variable entry.
class VarEntry final : public OffloadEntry {
public:
  enum : uint32_t {
    DeclareTargetTo = 0x00,
    DeclareTargetLink = 0x01
  };

public:
  explicit VarEntry(GlobalVariable *Var, StringRef Name, uint32_t Flags)
      : OffloadEntry(VarKind, Name, Flags) {
    setAddress(Var);
  }

  size_t getSize() const override {
    const auto *Var = cast<GlobalVariable>(getAddress());
    auto *VarType = Var->getValueType();
    // Global variables have pointer type always.
    // For the purpose of the size calculation, we have to
    // get the pointee's type.
    return Var->getParent()->getDataLayout().getTypeAllocSize(VarType);
  }

  bool isDeclaration() const {
    const auto *Var = cast<GlobalVariable>(getAddress());
    return Var->isDeclaration();
  }

  static bool classof(const OffloadEntry *E) {
    return E->getKind() == VarKind;
  }
};

/// Global indirect function entry.
class IndirectFunctionEntry final : public OffloadEntry {
public:
  enum : uint32_t {
    DeclareTargetFptr = 0x08
  };
  IndirectFunctionEntry(Function *Func, StringRef Name)
      : OffloadEntry(IndirectFuncKind, Name, DeclareTargetFptr) {
    setAddress(Func);
  }
  size_t getSize() const override { return 0; }
  static bool classof(const OffloadEntry *E) {
    return E->getKind() == IndirectFuncKind;
  }
};

/// This class contains a set of utility functions used by VPO Paropt
/// Transformation passes.
class VPOParoptUtils {

public:
  typedef enum SrcLocMode {
    SRC_LOC_NONE = 0,
    SRC_LOC_FUNC = 1,
    SRC_LOC_FILE = 2,
    SRC_LOC_PATH = 3
  } SrcLocMode;

  /// Get the global variable @"@tid.addr".
  /// It expects that it is already created, and asserts if it is not.
  static GlobalVariable *getTidPtrHolder(Module *M);

  /// \name Utilities for getting/creating named StructTypes.
  /// @{

  /// If module \p M has a StructType of name \p Name, and element types \p
  /// ElementTypes, return it. Assert if a struct with matching name, but
  /// different element types is found. Return `nullptr` otherwise.
  static StructType *
  getStructTypeWithNameAndElementsFromModule(Module *M, StringRef Name,
                                             ArrayRef<Type *> ElementTypes);

  /// If in the module of \p F, a StructType with name \p Name and types \p
  /// ElementTypes exists, then return it, otherwise create that struct and
  /// return it.
  static StructType *getOrCreateStructType(Function *F, StringRef Name,
                                           ArrayRef<Type *> ElementTypes);

#if INTEL_CUSTOMIZATION
#if INTEL_FEATURE_SW_DTRANS
  /// Return DTrans type describing kmpc_micro function type
  /// in the given DTransTypeManager \p TM:
  ///   typedef void(*kmpc_micro)(kmp_int32 *global_tid,
  ///                             kmp_int32 *bound_tid, ...)

  static dtransOP::DTransFunctionType *getKmpcMicroDTransType(
      dtransOP::DTransTypeManager &TM);

  /// Return DTrans type describing given OpenMP ident_t type \p IdentTy
  /// in the given DTransTypeManager \p TM:
  ///   typedef struct ident {
  ///     kmp_int32 reserved_1;
  ///     kmp_int32 flags;
  ///     kmp_int32 reserved_2;
  ///     kmp_int32 reserved_3;
  ///     char const *psource;
  ///   } ident_t;
  static dtransOP::DTransStructType *getIdentStructDTransType(
      dtransOP::DTransTypeManager &TM, StructType *IdentTy);
#endif // INTEL_FEATURE_SW_DTRANS
#endif // INTEL_CUSTOMIZATION
  /// Find any existing ident_t (LOC) struct in the module of \p F, and if
  /// found, return it. If not, create an ident_t struct and return it.
  ///
  /// The format is based on OpenMP KMP library
  ///
  /// \code
  /// typedef struct {
  ///   kmp_int32 reserved_1;   // might be used in Fortran
  ///   kmp_int32 flags;        // also f.flags; KMP_IDENT_xxx flags;
  ///                           // KMP_IDENT_KMPC identifies this union member
  ///   kmp_int32 reserved_2;   // not really used in Fortran any more
  ///   kmp_int32 reserved_3;   // source[4] in Fortran, do not use for C++
  ///   char      *psource;
  /// } ident_t;
  /// \endcode
  ///
  /// The bits that the flags field can hold are defined as KMP_IDENT_* before.
  ///
  /// Note: We need to look for exising IdentTy structs in the module before
  /// creating new ones. This is because if don't, then different Types are
  /// created for each function encountered. For example, consider a module with
  /// two functions `foo1()` and `foo2()`. When handling foo1(), a named struct
  /// type `ident_t` would be created and used for generating function
  /// declarations and calls for KMPC routines such as
  /// `__kmpc_global_thread_num(ident_t*)`. When it comes to handling `foo2()`,
  /// a new named IdentTy would be created, say `ident_t.0`, but when trying to
  /// emit a call to `__kmpc_global_thread_num`, there would be a type mismatch
  /// between the expected argument type in the declaration (ident_t *) and
  /// actual type of the argument (ident_t.0 *).
  static StructType *getIdentStructType(Function *F);

  /// @}

  /// Generate OpenMP runtime `__kmpc_begin(&loc, flags)` initialization code.
  /// The generated runtime routine call is invoked (only once) right after
  /// entering the main function.
  static CallInst *genKmpcBeginCall(Function *F, Instruction *InsertPt,
                                    StructType *IdentTy);

  /// Generate OpenMP runtime `__kmpc_end(&loc)` termination code.
  /// The generated runtime routine call is invoked (only once) right
  /// before exiting the main function.
  static CallInst *genKmpcEndCall(Function *F, Instruction *InsertPt,
                                  StructType *IdentTy);

  /// Emits a call to      __kmpc_serialized_parallel(void *loc, int tid)
  ///              or  __kmpc_end_serialized_parallel(void *loc, int tid)
  /// selected by \p IsBegin
  static CallInst *genKmpcSerializedParallelCall(WRegionNode *W,
                                                 StructType *IdentTy,
                                                 Value *TidPtr,
                                                 Instruction *InsertPt,
                                                 bool IsBegin);

  /// Check whether the call `__kmpc_global_thread_num()` exists in the given
  /// basic block \p BB, and return it.
  static CallInst *findKmpcGlobalThreadNumCall(BasicBlock *BB);

  /// Check whether the given instruction is the call
  /// `__kmpc_global_thread_num()`.
  static bool isKmpcGlobalThreadNumCall(Instruction *I);

  /// Generate OpenMP runtime `__kmpc_global_thread_num()` call.
  /// The generated runtime routine call is invoked (only once) to get
  /// runtime right after entering each function that contains OpenMP
  /// constructs.
  static CallInst *genKmpcGlobalThreadNumCall(Function *F,
                                              Instruction *InsertPt,
                                              StructType *IdentTy);

  /// Generate OpenMP runtime `__kmpc_threadprivate_cached` call.
  static CallInst *genKmpcThreadPrivateCachedCall(Function *F, Instruction *AI,
                                                  StructType *IdentTy,
                                                  Value *Tid, Value *GV,
                                                  Value *GVSize, Value *TpvGV);

  /// Generate source location information from the debug locations
  /// (Instruction::getDebugLoc()) of the first instructions in the given
  /// \p BS and \p BE. The returned value is a pointer to an aggregate
  /// variable of type \p IdentTy that is initialized with the
  /// source locations and the given \p Flags.
  static Constant *genKmpcLocfromDebugLoc(StructType *IdentTy, int Flags,
                                          BasicBlock *BS, BasicBlock *BE);

  /// Generate source location String from debug location \p Loc1 and \p Loc2.
  static GlobalVariable *genLocStrfromDebugLoc(Function *F, DILocation *Loc1,
                                               DILocation *Loc2,
                                               SrcLocMode Mode);

  /// \p Arg is a Value from a clause.  It is either a Constant or
  /// a Value of pointer type.  If it is a pointer Value, the method
  /// loads the clause's actual argument value via this pointer using
  /// \p ArgElementTy element type, otherwise the clause's actual argument
  /// value is \p Arg itself.
  /// The method sign-extends or truncates the clause's actual argument
  /// value to type \p Ty using the provided \p Builder.
  static Value *getOrLoadClauseArgValueWithSext(
      Value *Arg, Type *ArgElementTy, Type *Ty, IRBuilder<> &Builder);

  /// Generate a call to set `num_threads` for the `parallel` region and
  /// `parallel loop/sections`. Example:
  /// \code
  ///  call void @__kmpc_push_num_threads(%ident_t* %loc, i32 %tid, i32 %nths)
  /// \endcode
  static CallInst *genKmpcPushNumThreads(WRegionNode *W, StructType *IdentTy,
                                         Value *Tid, Value *NumThreads,
                                         Instruction *InsertPt);

  /// Generate a call to set `num_teams` for the `teams` region. Example:
  /// \code
  ///  call void @__kmpc_push_num_teams(%ident_t* %loc, i32 %tid, i32 %ntms,
  ///                                   i32 %nths)
  /// \endcode
  static CallInst *genKmpcPushNumTeams(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, Value *NumTeams,
                                       Type *NumTeamsTy, Value *NumThreads,
                                       Type *NumThreadsTy,
                                       Instruction *InsertPt);

  /// Generate a call to notify the runtime system that the team
  /// level static loop scheduling is started.
  /// \code
  ///   call void @__kmpc_team_static_init_4(%ident_t* %loc, i32 %tid,
  ///               i32 schedtype, i32* %islast,i32* %lb, i32* %ub, i32* %st,
  ///               i32 inc, i32 chunk)
  /// \endcode
  static CallInst *genKmpcTeamStaticInit(WRegionNode *W, StructType *IdentTy,
                                         Value *Tid, Value *IsLastVal,
                                         Value *LB, Value *UB, Value *ST,
                                         Value *Inc, Value *Chunk, int Size,
                                         bool IsUnsigned,
                                         Instruction *InsertPt);

  /// Generate a call to notify the runtime system that the static
  /// loop scheduling is started.
  /// \code
  ///   void __kmpc_dist_for_static_init_4/8[u](
  ///            ident_t *loc, kmp_int32 gtid,
  ///            kmp_int32 schedule, kmp_int32 *plastiter,
  ///            kmp_[u]int32/64 *plower, kmp_[u]int32/64 *pupper,
  ///            kmp_[u]int32/64 *pupperD,
  ///            kmp_int32/64 *pstride, kmp_int32/64 incr, kmp_int32/64 chunk)
  /// \endcode
  /// OR
  /// \code
  ///   void __kmpc_for_static_init_4/8[u](
  ///            ident_t *loc, kmp_int32 gtid,
  ///            kmp_int32 schedule, kmp_int32 *plastiter,
  ///            kmp_[u]int32/64 *plower, kmp_[u]int32/64 *pupper,
  ///            kmp_int32/64 *pstride, kmp_int32/64 incr, kmp_int32/64 chunk)
  /// \endcode
  static CallInst *genKmpcStaticInit(
      WRegionNode *W, StructType *IdentTy, Value *Tid,
      Value *IsLastVal, Value *LB, Value *UB, Value *DistUB, Value *ST,
      Value *Inc, Value *Chunk, bool IsUnsigned, IntegerType *LoopIVTy,
      Instruction *InsertPt);

  /// Generate a call to notify the runtime system that the static loop
  /// scheduling is done.
  /// \code
  ///   call void @__kmpc_for_static_fini(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcStaticFini(WRegionNode *W, StructType *IdentTy,
                                     Value *Tid, Instruction *InsertPt);

  /// Generate a call to pass all loop info to the runtime system for
  /// guided/runtime/dynamic/auto loop scheduling.
  ///
  /// \code
  ///   call void @__kmpc_for_dispatch_init_4{u}(%ident_t* %loc, i32 %tid,
  ///   i32 schedtype, i32 %lb, i32 %ub, i32 %st, i32 chunk)
  ///
  ///   call void @__kmpc_for_dispatch_init_8{u}4(%ident_t* %loc, i32 %tid,
  ///   i32 schedtype, i64 %lb, i64 %ub, i64 %st, i64 chunk)
  /// \endcode
  static CallInst *genKmpcDispatchInit(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, Value *SchedType,
                                       Value *IsLastVal, Value *LB, Value *UB,
                                       Value *ST, Value *Chunk, int Size,
                                       bool IsUnsigned, Instruction *InsertPt);

  /// Generate a call to the runtime system that performs loop partitioning for
  /// guided/runtime/dynamic/auto scheduling.
  /// \code
  ///   call void @__kmpc_for_dispatch_next_4{u}(%ident_t* %loc, i32 %tid,
  ///               i32 *isLast, i32 *%lb, i32 *%ub, i32 *%st)
  ///
  ///   call void @__kmpc_for_dispatch_next_8{u}(%ident_t* %loc, i32 %tid,
  ///               i32 *isLast, i64 *%lb, i64 *%ub, i64 *%st)
  /// \endcode
  static CallInst *genKmpcDispatchNext(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, Value *IsLastVal, Value *LB,
                                       Value *UB, Value *ST, int Size,
                                       bool IsUnsigned, Instruction *InsertPt);

  /// Generate a call to the runtime system that informs
  /// guided/runtime/dynamic/auto scheduling is done.
  ///
  /// \code
  ///   call void @__kmpc_for_dispatch_fini_4{u}(%ident_t* %loc, i32 %tid)
  ///   call void @__kmpc_for_dispatch_fini_8{u}(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcDispatchFini(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, int Size, bool IsUnsigned,
                                       Instruction *InsertPt);

  /// Update loop scheduling kind based on ordered clause and chunk size
  /// information.
  static WRNScheduleKind genScheduleKind(WRNScheduleKind Kind, bool IsOrdered,
                                         int Chunk);

  /// Query loop scheduling kind based on ordered clause and chunk size
  /// information.
  static WRNScheduleKind getLoopScheduleKind(WRegionNode *W);

  /// Set the MONOTONIC or NONMONOTONIC schedule modifier bit if specified
  static int addModifierToSchedKind(WRegionNode *W, WRNScheduleKind SchedKind);

  /// Query distribute loop scheduling kind based on schedule and chunk size
  /// information.
  static WRNScheduleKind getDistLoopScheduleKind(WRegionNode *W);

  /// int __kmpc_master_sub_group_leader();
  static CallInst *genMasterSubGroup(WRegionNode *W, Instruction *InsertPt,
                                     bool LeaderFlag);

  /// void __kmpc_get_shared_variables(void ***shareds);
  static CallInst *genGetSharingVariables(WRegionNode *W, Instruction *InsertPt,
                                          Value *Shareds);

  /// void __kmpc_begin_sharing_variables(void ***shareds, size_t num_shareds);
  static CallInst *genBeginSharingVariables(WRegionNode *W,
                                            Instruction *InsertPt,
                                            Value *Shareds, Value *NumShareds);

  /// void __kmpc_init_sharing_variables(void);
  /// void __kmpc_end_sharing_variables(void);
  static CallInst *genInitEndSharingVariables(Instruction *InsertPt, bool End);

  /// void __kmpc_spmd_kernel_init(int thread_limit, short needs_rtl,
  //                                    short needs_data_sharing);
  static CallInst *genSpmdKernelInit(WRegionNode *W, Instruction *InsertPt,
                                     Value *ThreadLimit, Value *NeedsRtl,
                                     Value *NeedsDataSharing);

  // void __kmpc_kernel_init(int thread_limit, short needs_rtl);
  static CallInst *genKernelInit(WRegionNode *W, Instruction *InsertPt,
                                 Value *ThreadLimit, Value *NeedsRtl);

  /// void __kmpc_kernel_fini(short is_rtl_initialized);
  static CallInst *genKernelFini(WRegionNode *W, Instruction *InsertPt,
                                 Value *NeedsRtl);

  /// void __kmpc_spmd_kernel_fini(short needs_rtl);
  static CallInst *genSpmdKernelFini(WRegionNode *W, Instruction *InsertPt,
                                     Value *NeedsRtl);

  /// void __kmpc_kernel_end_parallel(void);
  static CallInst *genKernelEndParallel(Instruction *InsertPt);

  /// EXTERN void __kmpc_kernel_prepare_parallel(void *work_fn,
  ///                                           short is_rtl_initialized);
  /// EXTERN bool __kmpc_kernel_parallel(void **work_fn, short
  /// is_rtl_initialized);
  static CallInst *genKernelParallel(WRegionNode *W, Instruction *InsertPt,
                                     Value *WorkFn, Value *IsRtlInitialized,
                                     bool Prepare);


  /// Generate source location information of type \p IdentTy
  /// for an explicit barrier. The source location information
  /// is taken from \p BB.
  static Constant *genKmpcLocforExplicitBarrier(StructType *IdentTy,
                                                BasicBlock *BB);

  /// Generate source location information of type \p IdentTy
  /// for an implicit barrier implied by \p W.
  /// The source location information is taken from \p BB.
  static Constant *genKmpcLocforImplicitBarrier(WRegionNode *W,
                                                StructType *IdentTy,
                                                BasicBlock *BB);

  /// Insert `kmpc_[cancel]_barrier` call (based on whether parent WRegion has
  /// a cancel construct) before \p InsertPt and return it.
  ///
  /// If the parent WRegion has a cancel construct:
  /// 1. Emit the following call:
  /// \code
  ///   %1 = call int32 @__kmpc_cancel_barrier(%ident_t* %loc, i32 %tid)
  /// \code
  /// 2. Add `%1` to the parent WRegionNode's `CancellationPoints`.
  ///
  /// Otherwise, emit the following call:
  /// \code
  ///   call void @__kmpc_barrier(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcBarrier(WRegionNode *W, Value *Tid,
                                  Instruction *InsertPt, StructType *IdentTy,
                                  bool IsExplicit, bool IsTargetSPIRV = false);

  /// Insert `kmpc_[cancel]_barrier` call (based on \p IsCancelBarrier) before
  /// \p InsertPt and return it. The CallInst inserted is:
  ///
  /// If \p CancelBarrier is \b true:
  /// \code
  ///   %1 = call int32 @__kmpc_cancel_barrier(%ident_t* %loc, i32 %tid)
  /// \endcode
  ///
  /// Otherwise:
  /// \code
  ///   call void @__kmpc_barrier(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcBarrierImpl(WRegionNode *W, Value *Tid,
                                      Instruction *InsertPt,
                                      StructType *IdentTy, bool IsExplicit,
                                      bool IsCancelBarrier,
                                      bool IsTargetSPIRV = false);

  /// Insert `__kmpc_cancel[lationpoint]` call before \p InsertPt and return
  /// it. The inserted CallInst is:
  ///
  /// If \p IsCancellationPoint is \b false:
  /// \code
  ///   %1 = call void @__kmpc_cancel(%ident_t* %loc, i32 %tid, i32
  ///   cancel_kind)
  /// \endcode
  ///
  /// If \p IsCancellationPoint is \b true:
  /// \code
  ///   %1 = call void @__kmpc_cancellation_point(%ident_t* %loc, i32 %tid,
  ///   i32 cancel_kind)
  /// \endcode
  static CallInst *genKmpcCancelOrCancellationPointCall(
      WRegionNode *W, StructType *IdentTy, Constant *TidPtr,
      Instruction *InsertPoint, WRNCancelKind CancelKind,
      bool IsCancellationPoint);

  /// Generate a critical section surrounding all the inner
  /// BasicBlocks of the WRegionNode \p W. The function works only on
  /// directives which have an Entry BasicBlock with "DIR.OMP..." intrinsic
  /// calls, and an exit BasicBlock with a corresponding "DIR.OMP.END..."
  /// calls. The middle BasicBlocks will then be surrounded by a critical
  /// section. If Hint ==0, the function emits calls to `__kmpc_critical` and
  /// `__kmpc_end_critical` in positions marked in the following diagram.
  /// if Hint !=0 the function emits calls to `__kmpc_critical_with_hint` and
  /// `__kmpc_end_critical` in positions marked in the following diagram.
  ///
  /// \code
  ///    EntryBB:
  ///      %0 = call token @llvm.directive.region.entry(...) [...]
  /// +------< begin critical >
  /// |    br label %BB1
  /// |
  /// |  BB1:
  /// |    ...
  /// |  ...
  /// |    br label %ExitBB
  /// |
  /// |  ExitBB:
  /// |    call void @llvm.directive.region.exit(%0)
  /// +------< end critical >
  ///      br label %..
  ///
  /// \endcode
  ///
  /// \param W WRegionNode for the OpenMP construct.
  /// \param IdentTy is needed to obtain the Loc struct for the KMPC calls.
  /// \param TidPtr is the AllocaInst for ThreadId, needed for the KMPC calls.
  /// \param IsTargetSPIRV is true, iff compilation is for SPIRV target.
  /// \param LockNameSuffix will be used as suffix in the name of the lock
  /// variable used for the critical section. (The prefix is determined based
  /// on the architecture and the kind of WRegionNode \p W).
  ///
  /// Example KMPC calls:
  /// \code
  ///   call void @__kmpc_critical(%ident_t* %loc.addr.11.12, i32 %my.tid,
  ///   [8 x i32]* @_kmpc_atomic_lock)
  ///
  ///   call void __kmpc_critical_with_hint((%ident_t* %loc.addr.11.12, i32
  ///   %my.tid, [8 x i32]* @_kmpc_atomic_lock, i32  2)
  ///
  ///   call void @__kmpc_end_critical(%ident_t* %loc.addr.11.122, i32 %my.tid1,
  ///   [8 x i32]* @_kmpc_atomic_lock)
  /// \endcode
  ///
  /// \returns \b true if the calls to `__kmpc_critical` and
  /// `__kmpc_end_critical` are successfully inserted, \b false otherwise.
  static bool genKmpcCriticalSection(WRegionNode *W, StructType *IdentTy,
                                     Constant *TidPtr,
                                     DominatorTree *DT,
                                     LoopInfo *LI,
                                     bool IsTargetSPIRV,
                                     const Twine &LockNameSuffix = "",
                                     uint32_t Hint = 0);

  /// Generate a critical section around Instructions \p BeginInst and \p
  /// EndInst. The function emits calls to `__kmpc_critical` or
  /// `__kmpc_critical_with_hint` \b before \p BeginInst and
  /// `__kmpc_end_critical` \b after \p EndInst.
  ///
  /// \code
  /// +------< begin critical >
  /// |    BeginInst
  /// |    ...
  /// |    ...
  /// |    EndInst
  /// +------< end critical >
  /// \endcode
  ///
  /// \param BeginInst is the Instruction \b before which the call to
  /// `__kmpc_critical` or `__kmpc_critical_with_hint` is inserted.
  /// \param EndInst is the Instruction \b after which the call to
  /// `__kmpc_end_critical` is inserted.
  /// \param IsTargetSPIRV is true, iff compilation is for SPIRV target.
  ///
  /// Note: Other Instructions, aside from the `__kmpc_critical`,
  /// `__kmpc_critical_with_hint` and
  /// `__kmpc_end_critical` calls, which are needed for the KMPC calls,
  /// are also inserted into the IR. \see genKmpcCallWithTid() for details.
  ///
  /// \see genKmpcCriticalSection(WRegionNode*, StructType*, AllocaInst*,
  /// const StringRef) for examples of the KMPC critical calls.
  ///
  /// \returns `true` if the calls to `__kmpc_critical` and
  /// `__kmpc_end_critical` or `__kmpc_critical_with_hint` and
  /// `__kmpc_end_critical` are successfully inserted, `false` otherwise.
  static bool genKmpcCriticalSection(WRegionNode *W, StructType *IdentTy,
                                     Constant *TidPtr, Instruction *BeginInst,
                                     Instruction *EndInst, DominatorTree *DT,
                                     LoopInfo *LI, bool IsTargetSPIRV,
                                     const Twine &LockNameSuffix = "",
                                     uint32_t Hint = 0);

  /// Generate tree reduce block and atomic reduce block. The tree reduce block
  /// is around Instructions \p BeginInst and \p EndInst. The function emits
  /// calls to `__kmpc_reduce` \b before \p BeginInst and `__kmpc_end_reduce`
  /// \b after \p EndInst.
  /// The atomic reduce block is around Instructions \p AtomicBeginInst and \p
  /// AtomicEndInst. atomic routine supported by runtime is generated if the
  /// data type for reduction is supported by runtime.
  ///
  /// \code
  /// EntryBB:
  ///   %1 = call i32 @__kmpc_reduce(...)
  ///   %to.tree.reduce = icmp eq i32 %1, 1
  ///   br i1 %to.tree.reduce, label %tree.reduce, label %tree.reduce.exit
  /// tree.reduce:
  ///   BeginInst
  ///   ...
  ///   ...
  ///   EndInst
  ///   call void @__kmpc_end_reduce(...)
  ///   br label %tree.reduce.exit
  /// tree.reduce.exit:
  ///   %2 = phi i1 [ false, %EntryBB ], [ true, %tree.reduce ]
  ///   %3 = icmp eq i1, %2, false
  ///   br i1 %3, label %AtomicEntryBB, label %atomic.reduce.exit
  /// AtomicEntryBB:
  ///   %to.atomic.reduce = icmp eq i32 %1, 2
  ///   br i1, %to.atomic.reduce, label %atomic.reduce,
  ///   label %atomic.reduce.exit
  /// atomic.reduce:
  ///   AtomicBeginInst
  ///   ...
  ///   ...
  ///   AtomicEndInst
  ///   call void @__kmpc_end_reduce(...)
  ///   br label %atomic.reduce.exit
  /// atomic.reduce.exit:
  /// \endcode
  ///
  /// \param BeginInst is the Instruction \b before which the call to
  /// `__kmpc_reduce` is inserted.
  /// \param EndInst is the Instruction \b after which the call to
  /// `__kmpc_end_reduce` is inserted.
  /// \param AtomicBeginInst is the Instruction \b before which the call to
  /// atomic routine is inserted.
  /// \param AtomicEndInst is the Instruction \b after which the call to
  /// atomic routine and `__kmpc_end_reduce` are inserted.
  /// \param IsTargetSPIRV is true, iff compilation is for SPIRV target.
  ///
  /// Note: Other Instructions, aside from the `__kmpc_reduce` and
  /// `__kmpc_end_reduce` calls, which are needed for the KMPC calls,
  /// are also inserted into the IR. \see genKmpcCallWithTid() for details.
  ///
  /// \returns `true` if the calls to `__kmpc_reduce`, `__kmpc_end_reduce` and
  /// atomic routines are successfully inserted, `false` otherwise.
  static bool genKmpcReduce(WRegionNode *W, StructType *IdentTy,
                            Constant *TidPtr, Value *RedVar, RDECL RedCallback,
                            Instruction *BeginInst, Instruction *EndInst,
                            Instruction *AtomicBeginInst,
                            Instruction *AtomicEndInst, DominatorTree *DT,
                            LoopInfo *LI, bool IsTargetSPIRV,
                            const StringRef LockNameSuffix);

  /// Emits a call to __kmpc_push_proc_bind(LOC, TID, i32 policy), where
  /// policy is: 2 for MASTER - 3 for CLOSE - 4 for SPREAD.
  static CallInst *genKmpcPushProcBindCall(WRegionNode *W, StructType *IdentTy,
                                           Value *TidPtr,
                                           Instruction *InsertPt);

  /// Emits a call to __kmpc_omp_taskyield(LOC, TID, i32 0)
  /// The third argument is set to 0 and used for debug tracing.
  static CallInst *genKmpcTaskyieldCall(WRegionNode *W, StructType *IdentTy,
                                        Value *TidPtr, Instruction *InsertPt);

  /// Generate a call to query if the current thread is masked thread or a
  /// call to end_masked for the team of threads. Emitted call:
  /// \code
  ///  masked = call @__kmpc_masked(%ident_t* %loc, i32 %tid, i32 %filter)
  ///      or
  ///   call void @__kmpc_end_masked(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcMaskedOrEndMaskedCall(WRegionNode *W,
                                                StructType *IdentTy, Value *Tid,
                                                Instruction *InsertPt,
                                                bool IsMaskedStart,
                                                bool IsTargetSPIRV = false);

  /// Generate a call to guard single-region is executed by one of threads in
  /// the enclosing thread team. Emitted call:
  /// \code
  ///   call single = @__kmpc_single(%ident_t* %loc, i32 %tid)
  ///      or
  ///   call void @__kmpc_end_single(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcSingleOrEndSingleCall(WRegionNode *W,
                                                StructType *IdentTy, Value *Tid,
                                                Instruction *InsertPt,
                                                bool IsSingleStart);

  /// Generate calls to guard the ordered thread execution for the ordered/end
  /// ordered region. Emitted call:
  /// \code
  ///   call void @__kmpc_ordered(%ident_t* %loc, i32 %tid)
  ///      or
  ///   call void @__kmpc_end_ordered(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcOrderedOrEndOrderedCall(WRegionNode *W,
                                                  StructType *IdentTy,
                                                  Value *Tid,
                                                  Instruction *InsertPt,
                                                  bool IsOrderedStart);

  /// \name Utilities for handling do-across loops.
  /// @{

  /// Insert a call to `kmpc_doacross_wait/post` for `#pragma omp ordered
  /// depend(source/sink)` before \p InsertPt and return it.
  ///
  /// Incoming Directive:
  /// \code
  ///   %1 = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(),
  ///        "QUAL.OMP.DEPEND.SINK"(i32 %v1, i32 %v2) ]
  /// \endcode
  ///
  /// Generated IR:
  /// \code
  ///   %dep.vec = alloca i64, i32 2
  ///   %conv1 = sext i32 %v1 to i64
  ///   %2 = getelementptr inbounds i64, i64* %dep.vec, i64 0
  ///   store i64 %conv1, i64* %2
  ///   %conv2 = sext i32 %v2 to i64
  ///   %3 = getelementptr inbounds i64, i64* %dep.vec, i64 1
  ///   store i64 %conv2, i64* %3
  ///   %4 = bitcast i64* %dep.vec to i8*
  ///   %tid = load i32, i32* %<tidptr>, align 4
  ///   call void @__kmpc_doacross_wait({ i32, i32, i32, i32, i8* }* @loc,
  ///                                   i32 %tid, i8* %4)
  /// \endcode
  ///
  /// \param DepVecValues is an ArrayRef of Values to be passed to the runtime
  /// as the dependence vector. ({%v1, $v2} in the above example).
  /// \param IsDoacrossPost If \b true, emit `kmpc_doacross_post`, otherwise
  /// emit `kmpc_doacross_wait`.
  ///
  /// A load from \p TidPtr is emitted to get `tid` for the call.
  static CallInst *
  genDoacrossWaitOrPostCall(WRNOrderedNode *W, StructType *IdentTy,
                            Value *TidPtr, Instruction *InsertPt,
                            const ArrayRef<Value *> &DepVecValues,
                            bool IsDoacrossPost);

  /// Insert a call to `kmpc_doacross_init` for `#pragma omp for ordered(n)`
  /// before \p InsertPt, and return it.
  ///
  /// Incoming Directive:
  /// \code
  ///   %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(),
  ///        "QUAL.OMP.ORDERED"(i32 2, i32 4, i32 2), ...]
  /// \endcode
  ///
  /// Generated IR:
  /// \code
  ///   %dims.vec = alloca { i64, i64, i64 }, i32 2
  ///
  ///   %3 = getelementptr inbounds { i64, i64, i64 }, %dims.vec, i32 0
  ///
  ///   %4 = getelementptr inbounds { i64, i64, i64 }, %3, i32 0, i32 0
  ///   store i64 0, i64* %4
  ///   %5 = getelementptr inbounds { i64, i64, i64 }, %3, i32 0, i32 1
  ///   store i64 4, i64* %5
  ///   %6 = getelementptr inbounds { i64, i64, i64 }, %3, i32 0, i32 2
  ///   store i64 1, i64* %6
  ///
  ///   %7 = getelementptr inbounds { i64, i64, i64 }, %dims.vec, i32 1
  ///
  ///   %8 = getelementptr inbounds { i64, i64, i64 }, %7, i32 0, i32 0
  ///   store i64 0, i64* %8
  ///   %9 = getelementptr inbounds { i64, i64, i64 }, %7, i32 0, i32 1
  ///   store i64 2, i64* %9
  ///   %10 = getelementptr inbounds { i64, i64, i64 }, %7, i32 0, i32 2
  ///   store i64 1, i64* %10
  ///
  ///   %11 = bitcast { i64, i64, i64 }* %dims.vec to i8*
  ///   %tid = load i32, i32* %<tidptr>, align 4
  ///   call void @__kmpc_doacross_init({ i32, i32, i32, i32, i8* }* @loc,
  ///                                   i32 %tid, i32 2, i8* %11)
  /// \endcode
  ///
  /// Here `%dims.vec` is a vector of structs of type {i64, i64, i64},
  /// containing a loop's lower bound, upper bound and stride respectively. The
  /// vector contains a struct for each loop in the doacross loop-nest.
  ///
  /// The call, along with the initialization of %dims.vec, is inserted before
  /// \p InsertPt.
  ///
  /// \param TripCounts contains trip counts for each loop in the nest.
  static CallInst *genKmpcDoacrossInit(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, Instruction *InsertPt,
                                       const ArrayRef<Value *> &TripCounts);

  /// Insert a call to `kmpc_doacross_fini` for `#pragma omp for ordered(n)`
  /// \b after \p InsertPt and return it.
  ///
  /// \code
  ///   call void @__kmpc_doacross_fini({ i32, i32, i32, i32, i8* }*
  ///   @.kmpc_loc, i32 %my.tid)
  /// \endcode
  ///
  static CallInst *genKmpcDoacrossFini(WRegionNode *W, StructType *IdentTy,
                                       Value *Tid, Instruction *InsertPt);

  /// @}

  /// Insert a `__kmpc_flush` call at \p InsertPt.
  /// \code
  ///   call void @__kmpc_flush(%ident_t* %loc)
  /// \endcode
  static CallInst *genKmpcFlush(WRegionNode *W, StructType *IdentTy,
                                Instruction *InsertPt);

  /// Generate KMPC runtime call to the function \p IntrinsicName
  /// with arguments Loc(obtained using \p IdentTy), Tid (Obtained using \p
  /// TidPtr), and \p Args.
  /// \param W WRegionNode for current OpenMP construct..
  /// \param TidPtr is the AllocaInst for Tid.
  /// \param IdentTy Used to obtain Loc needed by the KMPC call.
  /// \param InsertPt Used to obtain Loc needed by the KMPC call.
  /// \param IntrinsicName is the name of the function.
  /// \param ReturnTy is the return type of the function.
  /// \param Args arguments for the function call.
  /// \param Insert controls whether to insert the call (before InsertPt).
  ///        Note that the load instruction for TidPtr is always inserted,
  ///        regardless of this flag.
  /// \param EnableAtomicReduce is to set flag bit for enabling atomic reduce.
  /// Note: The function inserts a LoadInst for getting Tid, into the IR
  /// (before the \p InsertPt), and inserts the function prototype for the
  /// KMPC intrinsic \p IntrinsicName into the module symbol table. But it
  /// does not insert the KMPC call into the IR.
  ///
  /// \returns The generated CallInst.
  static CallInst *genKmpcCallWithTid(WRegionNode *W, StructType *IdentTy,
                                      Value *TidPtr, Instruction *InsertPt,
                                      StringRef IntrinsicName, Type *ReturnTy,
                                      ArrayRef<Value *> Args,
                                      bool Insert = false,
                                      bool EnableAtomicReduce = false);

  /// Inserts \p Call1 and \p Call2 at \p W's region boundary. If \p
  /// InsideRegion is \b true, \p Call1 is inserted after \p W's entry
  /// directive, and \p Call2 is inserted before \p W's exit directive.
  /// Otherwise, \p Call1 is inserted before the entry directive, and \p Call2
  /// is inserted after the exit directive. By default, \p InsideRegion is
  /// false.
  static void insertCallsAtRegionBoundary(WRegionNode *W, CallInst *Call1,
                                          CallInst *Call2,
                                          bool InsideRegion = false);

  /// \name Utilities related to __kmpc_[begin/end]_spmd_[target/parallel] calls.
  /// @{
  /// Generates an spmd codegen specific call to the function \p FnName.
  static CallInst *genKmpcSpmdCallImpl(Module *M, StringRef FnName);

  /// Returns a pair containing calls to `__kmpc_begin_spmd_parallel` and
  /// `__kmpc_end_spmd_parallel`.
  static std::pair<CallInst *, CallInst *>
  genKmpcBeginEndSpmdParallelCalls(Module *M);

  /// Returns a pair containing calls to `__kmpc_begin_spmd_target` and
  /// `__kmpc_end_spmd_target`.
  static std::pair<CallInst *, CallInst *>
  genKmpcBeginEndSpmdTargetCalls(Module *M);

  /// Delete any calls to \p CalledFnName in the function \p F, if present.
  /// \returns \b true if any such deletion happens, \b false otherwise.
  /// Asserts if a call to \p CalledFnName in \p F has any uses.
  static bool deleteCallsInFunctionTo(Function *F, StringRef CalledFnName);

  /// Delete any calls to `__kmpc_begin/end_spmd_target/parallel` in \p F.
  /// \returns \b true if any such deletion happens, \b false otherwise.
  static bool deleteKmpcBeginEndSpmdCalls(Function *F);
  /// @}

  /// \name Utilities to emit calls to ctor, dtor, cctor, and copyassign.
  /// @{
  static void genConstructorCall(Function *Ctor, Value *V,
                                 IRBuilder<> &Builder);
  static CallInst *genConstructorCall(Function *Ctor, Value *V,
                                      Value *PrivAlloca, bool IsTargetSPIRV);
  static CallInst *genDestructorCall(Function *Dtor, Value *V,
                                     Instruction *InsertBeforePt,
                                     bool IsTargetSPIRV);
  static CallInst *genCopyConstructorCall(Function *Cctor, Value *D, Value *S,
                                          Instruction *InsertBeforePt,
                                          bool IsTargetSPIRV);
  static CallInst *genCopyAssignCall(Function *Cp, Value *D, Value *S,
                                     Instruction *InsertBeforePt,
                                     bool IsTargetSPIRV);
  /// @}

  /// Generate an optionally addrspacecast'ed pointer Value for an array
  /// of Type \p ElementType, size \p NumElements, name \p VarName.
  /// \p NumElements can be null for one element.
  /// If new instructions need to be generated, they will be inserted
  /// before \p InsertPt.
  /// \p AllocaAddrSpace specifies address space in which the memory
  /// for the privatized variable needs to be allocated. If it is
  /// llvm::None, then the address space matches the default alloca's
  /// address space, as specified by DataLayout. Note that some address
  /// spaces may require allocating the private version of the variable
  /// as a GlobalVariable, not as an AllocaInst.
  /// If \p ValueAddrSpace does not match llvm::None,
  /// then the generated Value will be immediately addrspacecast'ed
  /// and the generated AddrSpaceCastInst or AddrSpaceCast constant
  /// expression will be returned as a result.
  /// If \p AllocItem is present, then the private memory will be
  /// allocated by calling omp_alloc().
  static Value *
  genPrivatizationAlloca(Type *ElementType, Value *NumElements,
                         MaybeAlign OrigAlignment, Instruction *InsertPt,
                         bool IsTargetSPIRV, const Twine &VarName = "",
                         llvm::Optional<unsigned> AllocaAddrSpace = llvm::None,
                         llvm::Optional<unsigned> ValueAddrSpace = llvm::None,
                         AllocateItem *AllocItem = nullptr);

  /// Return true if address spaces \p AS1 and \p AS2 are compatible
  /// for the current compilation target. Address spaces are compatible,
  /// if it is legal to addrspacecast from one address space to another
  /// and vice versa.
  static bool areCompatibleAddrSpaces(unsigned AS1, unsigned AS2,
                                      bool IsTargetSPIRV);

#if INTEL_CUSTOMIZATION
  /// \name Utilities specific to Fortran dope vectors.
  /// @{

  /// Emit and return a call to `data_size =_f90_dope_vector_size(DV)`, which
  /// returns the size of the array (in bytes) described by the dope vector.
  static CallInst *genF90DVSizeCall(Value *DV, Instruction *InsertBefore);

  /// Emit and return a call to `data_size = _f90_dope_vector_init(OrigDV,
  /// NewDV)`, which initializes NewDV using OrigDV, and returns the size of the
  /// array (in bytes) described by the dope vectors.
  static CallInst *genF90DVInitCall(Value *OrigDV, Value *NewDV,
                                    Instruction *InsertBefore,
                                    bool IsTargetSPIRV = false);

  /// Emit code to initialize the local copy of \p I, where \p I is an F90 dope
  /// vector. The code looks like:
  /// \code
  ///   @num_elements_gv = common thread_local global i64 0            // (1)
  ///
  ///   %size = call i64 @_f90_dope_vector_init(NewV, OrigV)
  ///   %num_elements = udiv %size, <element_size>
  ///   if (%size != 0) {                                              // (2)
  ///     %local_data = alloca <element_type>, %num_elements
  ///     store %local_data, getelementpointer(NewV, 0, 0)
  ///   }
  ///   store i64 %num_elements, i64* @num_elements_gv                 // (3)
  /// \endcode
  /// The emitted code is inserted after the alloca NewV, which is the local
  /// dope vector corresponding to \p I, and OrigV is the original. If NewV is
  /// a global variable or AllowOverrideInsertPt is false, then the code is
  /// inserted before \p InsertPt.
  /// If \p CheckOrigAllocationBeforeAllocatingNew is true, then the local data
  /// allocation is guarded by an `if (size != 0)` check (2).
  /// If \p StoreNumElementsToGlobal is true, then store `%num_elements` to
  /// a thread local global variable `@num_elements_gv` (1), (3).
  static void
  genF90DVInitCode(Item *I, Instruction *InsertPt, DominatorTree *DT,
                   LoopInfo *LI, bool IsTargetSPIRV = false,
                   bool AllowOverrideInsertPt = true,
                   bool CheckOrigAllocationBeforeAllocatingNew = true,
                   bool StoreNumElementsToGlobal = false);

  /// A variant of genF90DVInitCode which accepts source and destination dope
  /// vectors (\p SrcV and \p DstV).
  static void
  genF90DVInitCode(Item *I, Value *SrcV, Value *DstV, Instruction *InsertPt,
                   DominatorTree *DT, LoopInfo *LI, bool IsTargetSPIRV = false,
                   bool AllowOverrideInsertPt = true,
                   bool CheckOrigAllocationBeforeAllocatingNew = true,
                   bool StoreNumElementsToGlobal = false);

  /// Emits `_f90_dope_vector_init` calls to initialize dope vectors in task's
  /// privates thunk. This is done after the `__kmpc_task_alloc` call, but
  /// before `__kmpc_taskloop_5` or `__kmpc_omp_task`.
  static void
  genF90DVInitForItemsInTaskPrivatesThunk(WRegionNode *W, Value *KmpPrivatesGEP,
                                          StructType *KmpPrivatesTy,
                                          Instruction *InsertBefore);

  /// Emit a call to `_f90_firstprivate_copy(NewV, OrigV)`. The
  /// call is inserted before \p InsertBefore.
  static void genF90DVFirstprivateCopyCall(Value *NewV, Value *OrigV,
                                           Instruction *InsertBefore,
                                           bool IsTargetSPIRV = false);

  /// Emit a call to `_f90_lastprivate_copy(NewV, OrigV)`. The
  /// call is inserted before \p InsertBefore.
  static void genF90DVLastprivateCopyCall(Value *NewV, Value *OrigV,
                                          Instruction *InsertBefore,
                                          bool IsTargetSPIRV = false);

private:
  static void genF90DVFirstOrLastprivateCopyCallImpl(
      StringRef FnName, Value *NewV, Value *OrigV, Instruction *InsertBefore,
      bool IsTargetSPIRV = false);

public:
  /// Compute the destination address, number of elements and element type for
  /// reduction initialization loop for Fortran dope vectors. The function emits
  /// code to get the data array for the dope vector, which is inserted before
  /// \p InsertBefore.
  /// \code
  ///   %newv.addr0 = getelementpointer (NewV, 0, 0)
  ///   %dest.arr.begin = load <element_ty>* %newv.addr0
  /// \endcode
  /// Where NewV is the local dope vector for I.
  static void genF90DVReductionInitDstInfo(const Item *I, Value *&NewV,
                                           Value *&DestArrayBeginOut,
                                           Type *&DestElementTyOut,
                                           Value *&NumElementsOut,
                                           Instruction *InsertBefore);

  /// Compute the destination address, number of elements and element type for
  /// reduction finalization loop for Fortran dope vectors. The function emits
  /// code to get the data array for the dope vector, which is inserted before
  /// \p InsertBefore.
  /// \code
  ///   %srcv.addr0 = getelementpointer (SrcVal, 0, 0)
  ///   %src.arr.begin = load <element_ty>* %srcv.addr0
  ///   %destv.addr0 = getelementpointer (DestVal, 0, 0)
  ///   %dest.arr.begin = load <element_ty>* %destv.addr0
  /// \endcode
  /// Where SrcVal is the source dope vector (For reduction initialization, it's
  /// the original dope vector; for reduction fini, it's local dope vector).
  /// Where DestVal is the destination dope vector (For reduction
  /// initialization, it's the local dope vector; for reduction fini, it's
  /// original dope vector).
  static void genF90DVReductionSrcDstInfo(
      const Item *I, Value *&SrcVal, Value *&DestVal, Value *&SrcArrayBeginOut,
      Value *&DestArrayBeginOut, Type *&DestElementTyOut,
      Value *&NumElementsOut, Instruction *InsertBefore);
  /// @}

#endif // INTEL_CUSTOMIZATION

  /// Compute the OpenMP loop upper bound so that the loop iteration space can
  /// be closed interval.
  static CmpInst::Predicate computeOmpPredicate(CmpInst::Predicate PD);

  /// Return the predicate which includes equal for the zero trip test
  /// of the loop identified by \p Idx.
  static Value *computeOmpUpperBound(WRegionNode *W, unsigned Idx,
                                     Instruction *InsertPt,
                                     const Twine &Name = "");

  /// Update the bottom test predicate to include equal predicate
  /// for the loop identified by \p Idx.
  /// It also updates the loop upper bound.
  static void updateOmpPredicateAndUpperBound(WRegionNode *W,
                                              unsigned Idx,
                                              Value *Load,
                                              Instruction *InsertPt);

  /// Returns true, if the given \p V value may be rematerialized
  /// before the given \p W region (i.e. right before the \p W region's
  /// entry block).
  static bool mayCloneUBValueBeforeRegion(Value *V, const WRegionNode *W);

  /// Clone the instructions and insert them before \p InsertPt.
  static Value *cloneInstructions(Value *V, Instruction *InsertPt);

  /// Generate the pointer pointing to the head of the array.
  /// \param [in] ObjTy Either the array's type or the element type.
  /// \param [in] NumElements Number of elements of \p ObjTy type in the array.
  /// \param [in] BaseAddr Base address of the array.
  /// \param [in] Builder IRBuilder for any new Instructions.
  /// \returns a \b tuple of <ElementType, NumElements, BaseAddress>,
  /// where \p ElementType is the element type of the array,
  /// \p NumElements is the number of elements in the array, and
  /// \p BaseAddress is the base address of the array.
  ///
  /// \p ObjTy and \p NumElements represent the array configuration,
  /// and there are only two supported configurations:
  ///   1. \p ObjTy is an array type, and \p NumElements is either nullptr
  ///      or ConstantInt value 1.
  ///   2. \p ObjTy is a non-array type.
  static std::tuple<Type *, Value *, Value *> genArrayLength(
      Type *ObjTy, Value *NumElements, Value *BaseAddr, IRBuilder<> &Builder);

  static Value *genAddrSpaceCast(Value *Ptr, Instruction *InsertPt,
                                 unsigned AddrSpace);

  /// build the CFG for if clause.
  static void buildCFGForIfClause(Value *Cmp, Instruction *&ThenTerm,
                                  Instruction *&ElseTerm, Instruction *InsertPt,
                                  DominatorTree *DT);

  /// Generate a call to `__kmpc_omp_task_alloc`. Example:
  /// \code
  ///   i8* @__kmpc_omp_task_alloc({ i32, i32, i32, i32, i8* }*, i32, i32,
  ///   i64, i64, i32 (i32, i8*)*)
  /// \endcode
  static CallInst *
  genKmpcTaskAlloc(WRegionNode *W, StructType *IdentTy, Value *TidPtr,
                   DominatorTree *DT, Value *KmpTaskTTWithPrivatesTySz,
                   int KmpSharedTySz, PointerType *KmpRoutineEntryPtrTy,
                   Function *MicroTaskFn, Instruction *InsertPt, bool UseTbb);

  /// Generate a call to `__kmpc_omp_task_alloc` without a callback
  static CallInst *genKmpcTaskAllocWithoutCallback(WRegionNode *W,
                                                   StructType *IdentTy,
                                                   Instruction *InsertPt);

  /// Generate a call to `__kmpc_omp_task_alloc` to be used as an AsyncObj
  /// for the TARGET VARIANT DISPATCH NOWAIT construct corresponding to \p W
  static CallInst *genKmpcTaskAllocForAsyncObj(WRegionNode *W,
                                               StructType *IdentTy,
                                               int AsyncObjTySize,
                                               Instruction *InsertPt);

  /// Generate a call to `__kmpc_taskloop_5`. Example:
  /// \code
  ///   void @__kmpc_taskloop_5(
  ///          { i32, i32, i32, i32, i8* }* %loc,
  ///          i32 %tid,
  ///          i8* %thunk_temp,
  ///          i32 if_val,
  ///          i64* lb,
  ///          i64* ub,
  ///          i64 stride,
  ///          i32 nogroup,
  ///          i32 sched, // 0: no grainsize or num_tasks
  ///                     // 1: grainsize is used
  ///                     // 2: num_tasks is used
  ///          i64 grainsize,
  ///          i32 modifier,
  ///          i8* task_dup)
  /// \endcode
  static CallInst *genKmpcTaskLoop(WRegionNode *W, StructType *IdentTy,
                                   Value *TidPtr, Value *TaskAlloc, Value *Cmp,
                                   AllocaInst *LBPtr, AllocaInst *UBPtr,
                                   AllocaInst *STPtr,
                                   StructType *KmpTaskTTWithPrivatesTy,
                                   Instruction *InsertPt, bool UseTbb,
                                   Function *FnTaskDup);

  /// Generate a call to `__kmpc_task`. Example:
  /// \code
  ///   void @__kmpc_task(
  ///            { i32, i32, i32, i32, i8* }* %loc,
  ///            i32 %tid,
  ///            i8* thunk_temp)
  /// \endcode
  static CallInst *genKmpcTask(WRegionNode *W, StructType *IdentTy,
                               Value *TidPtr, Value *TaskAlloc,
                               Instruction *InsertPt);

  /// Generate a call to `__kmpc_omp_task_begin_if0`. Example:
  /// \code
  ///   void @__kmpc_omp_task_begin_if0(
  ///         { i32, i32, i32, i32, i8* }* /* &loc */,
  ///         i32 /* tid */,
  ///         i8* /*thunk_temp */)
  /// \endcode
  static CallInst *genKmpcTaskBeginIf0(WRegionNode *W, StructType *IdentTy,
                                       Value *TidPtr, Value *TaskAlloc,
                                       Instruction *InsertPt);

  /// Generate a call to `__kmpc_omp_task_complete_if0`. Example:
  /// \code
  ///   void @__kmpc_omp_task_complete_if0(
  ///         { i32, i32, i32, i32, i8* }* /* &loc */,
  ///         i32 /* tid */,
  ///         i8* /*thunk_temp */)
  /// \endcode
  static CallInst *genKmpcTaskCompleteIf0(WRegionNode *W, StructType *IdentTy,
                                          Value *TidPtr, Value *TaskAlloc,
                                          Instruction *InsertPt);

  /// Generate a call to `__kmpc_omp_task_with_deps`. Example:
  /// \code
  ///   void @__kmpc_omp_task_with_deps(
  ///          { i32, i32, i32, i32, i8* }* /* &loc */,
  ///          i32 /* tid */,
  ///          i8* /*thunk_temp */,
  ///          i32 /* depend_count */,
  ///          i8* /* &depend_record */
  ///          i32 /* 0 */,
  ///          i8* /* 0 */)
  /// \endcode
  static CallInst *genKmpcTaskWithDeps(WRegionNode *W, StructType *IdentTy,
                                       Value *TidPtr, Value *TaskAlloc,
                                       Value *Dep, Value *NumDeps,
                                       Instruction *InsertPt);

  /// Generate a call to `__kmpc_omp_wait_deps`. Example:
  /// \code
  ///   void @__kmpc_omp_wait_deps(
  ///          { i32, i32, i32, i32, i8* }* /* &loc */,
  ///          i32 /* tid */,
  ///          i32 /* depend_count */,
  ///          i8* /* &depend_record */
  ///          i32 /* 0 */,
  ///          i8* /* 0 */)
  /// \endcode
  static CallInst *genKmpcTaskWaitDeps(WRegionNode *W, StructType *IdentTy,
                                       Value *TidPtr, Value *Dep,
                                       Value *NumDeps, Instruction *InsertPt);

  /// Generic routine to generate `__kmpc_omp_task_with_deps` or
  /// `__kmpc_omp_wait_deps` calls.
  static CallInst *genKmpcTaskDepsGeneric(WRegionNode *W, StructType *IdentTy,
                                          Value *TidPtr, Value *TaskAlloc,
                                          Value *Dep, Value *NumDeps,
                                          Instruction *InsertPt,
                                          StringRef FnName);

  /// Generic function to support generation of `__kmpc_task`,
  /// `__kmpc_omp_task_begin_if0` and `__kmpc_omp_task_complete_if0` calls.
  static CallInst *genKmpcTaskGeneric(WRegionNode *W, StructType *IdentTy,
                                      Value *TidPtr, Value *TaskAlloc,
                                      Instruction *InsertPt, StringRef FnName);

  /// Generate a call to `__kmpc_omp_taskwait`. Example:
  /// \code
  ///   void @__kmpc_omp_taskwait({ i32, i32, i32, i32, i8* }*, i32)
  /// \endcode
  static CallInst *genKmpcTaskWait(WRegionNode *W, StructType *IdentTy,
                                   Value *TidPtr, Instruction *InsertPt);

  /// Generate a call to `__tgt_target_data_begin[_mapper]`. Example:
  /// \code
  ///   int32_t __tgt_target_data_begin_mapper(ident_t *loc,
  ///                                          int64_t  device_id,
  ///                                          int32_t  num_args,
  ///                                          void**   args_base,
  ///                                          void**   args,
  ///                                          int64_t* args_size,
  ///                                          int64_t* args_maptype,
  ///                                          void**   args_names,
  ///                                          void**   args_mappers)
  /// \endcode
  ///
  /// or, if -vpo-paropt-use-mapper-api=false is used:
  ///
  /// \code
  ///   int32_t __tgt_target_data_begin(int64_t  device_id,
  ///                                   int32_t  num_args,
  ///                                   void**   args_base,
  ///                                   void**   args,
  ///                                   int64_t* args_size,
  ///                                   int64_t* args_maptype)
  /// \endcode
  static CallInst *genTgtTargetDataBegin(WRegionNode *W, int NumArgs,
                                         Value *ArgsBase, Value *Args,
                                         Value *ArgsSize, Value *ArgsMaptype,
                                         Value *ArgsNames, Value *ArgsMappers,
                                         Instruction *InsertPt);

  /// Generate a call to `__tgt_target_data_end[_mapper]`. Example:
  /// \code
  ///   int32_t __tgt_target_data_end_mapper(ident_t *loc,
  ///                                        int64_t  device_id,
  ///                                        int32_t  num_args,
  ///                                        void**   args_base,
  ///                                        void**   args,
  ///                                        int64_t* args_size,
  ///                                        int64_t* args_maptype,
  ///                                        void**   args_names,
  ///                                        void**   args_mappers)
  /// \endcode
  ///
  /// or, if -vpo-paropt-use-mapper-api=false is used:
  ///
  /// \code
  ///   int32_t __tgt_target_data_end(int64_t  device_id,
  ///                                 int32_t  num_args,
  ///                                 void**   args_base,
  ///                                 void**   args,
  ///                                 int64_t* args_size,
  ///                                 int64_t* args_maptype)
  /// \endcode
  static CallInst *genTgtTargetDataEnd(WRegionNode *W, int NumArgs,
                                       Value *ArgsBase, Value *Args,
                                       Value *ArgsSize, Value *ArgsMaptype,
                                       Value *ArgsNames, Value *ArgsMappers,
                                       Instruction *InsertPt);

  /// Generate a call to `__tgt_target_data_update[_mapper]`. Example:
  /// \code
  ///   int32_t __tgt_target_data_update_mapper(ident_t *loc,
  ///                                           int64_t  device_id,
  ///                                           int32_t  num_args,
  ///                                           void**   args_base,
  ///                                           void**   args,
  ///                                           int64_t* args_size,
  ///                                           int64_t* args_maptype,
  ///                                           void**   args_names,
  ///                                           void**   args_mappers)
  /// \endcode
  ///
  /// or, if -vpo-paropt-use-mapper-api=false is used:
  ///
  /// \code
  ///   int32_t __tgt_target_data_update(int64_t  device_id,
  ///                                    int32_t  num_args,
  ///                                    void**   args_base,
  ///                                    void**   args,
  ///                                    int64_t* args_size,
  ///                                    int64_t* args_maptype)
  /// \endcode
  static CallInst *genTgtTargetDataUpdate(WRegionNode *W, int NumArgs,
                                          Value *ArgsBase, Value *Args,
                                          Value *ArgsSize, Value *ArgsMaptype,
                                          Value *ArgsNames, Value *ArgsMappers,
                                          Instruction *InsertPt);

  /// Generate a call to `__tgt_target[_mapper]`. Example:
  /// \code
  ///   int32_t __tgt_target_mapper(ident_t *loc,
  ///                               int64_t  device_id,
  ///                               void*    host_addr,
  ///                               int32_t  num_args,
  ///                               void**   args_base,
  ///                               void**   args,
  ///                               int64_t* args_size,
  ///                               int64_t* args_maptype,
  ///                               void**   args_names,
  ///                               void**   args_mappers)
  /// \endcode
  ///
  /// or, if -vpo-paropt-use-mapper-api=false is used:
  ///
  /// \code
  ///   int32_t __tgt_target(int64_t  device_id,
  ///                        void*    host_addr,
  ///                        int32_t  num_args,
  ///                        void**   args_base,
  ///                        void**   args,
  ///                        int64_t* args_size,
  ///                        int64_t* args_maptype)
  /// \endcode
  static CallInst *genTgtTarget(WRegionNode *W, Value *HostAddr, int NumArgs,
                                Value *ArgsBase, Value *Args, Value *ArgsSize,
                                Value *ArgsMaptype, Value *ArgsNames,
                                Value *ArgsMappers, Instruction *InsertPt);

  /// Generate a call to `__tgt_target_teams[_mapper]`. Example:
  /// \code
  ///   int32_t __tgt_target_teams_mapper(ident_t *loc,
  ///                               int64_t  device_id,
  ///                               void*    host_addr,
  ///                               int32_t  num_args,
  ///                               void**   args_base,
  ///                               void**   args,
  ///                               int64_t* args_size,
  ///                               int64_t* args_maptype,
  ///                               void**   args_names,
  ///                               void**   args_mappers,
  ///                               int32_t  num_teams,
  ///                               int32_t  thread_limit)
  /// \endcode
  ///
  /// or, if -vpo-paropt-use-mapper-api=false is used:
  ///
  /// \code
  ///   int32_t __tgt_target_teams(int64_t  device_id,
  ///                              void*    host_addr,
  ///                              int32_t  num_args,
  ///                              void**   args_base,
  ///                              void**   args,
  ///                              int64_t* args_size,
  ///                              int64_t* args_maptype,
  ///                              int32_t  num_teams,
  ///                              int32_t  thread_limit)
  /// \endcode
  static CallInst *genTgtTargetTeams(WRegionNode *W, Value *HostAddr,
                                     int NumArgs, Value *ArgsBase, Value *Args,
                                     Value *ArgsSize, Value *ArgsMaptype,
                                     Value *ArgsNames, Value *ArgsMappers,
                                     Instruction *InsertPt);

  /// This utility encodes the constant parameters of subdevice in
  /// \p ConstantDeviceID. \p ConstantDeviceID is both an input and an output,
  /// it encodes a new \p Param everytime this function is called.
  /// Encoding Steps:
  /// 1) Create \p Mask based on \p MaskSize:
  ///      \p Mask = (~(~(0ull) << \p MaskSize))
  /// 2) AND \p Mask and \p Param : \p Param &= \p Mask;
  /// 3) Shift the constant \p Param : \p Param <<= \p Shift;
  /// 4) encode \p param to \p ConstantDeviceID:
  ///       \p ConstantDeviceID |= \p Param;
  static void encodeSubdeviceConstants(const ConstantInt* Param,
                                      uint64_t& ConstantDeviceID,
                                      int Shift, uint64_t MaskSize);

  /// This utility inserts into the IR the code needed to encode the
  /// non-constant subdevice parameters.
  /// Generated IR :
  /// 1) % 20 = zext i32 %1 to i64 // Zero extend \p Param
  /// 2) % 21 = and i64 % 20, 255  // And Zero extended \p Param with \p Mask
  /// 3) % 22 = shl i64 % 21, 32   // Shift left Masked \p Param
  ///
  /// This utility returns the manipulated \p Param, needed for extra IR
  /// generation at the Caller.
  static Value* genEncodingSubdeviceNonConstants(Instruction* InsertPt,
                                                        Value* Param,
                                                        int Shift,
                                                        uint64_t Mask);

  /// if Subdevice clause exists, this routine encodes Subdevice info into
  /// DeviceID else, it returns DeviceID (cast to i64) without Subdevice
  /// encoding.
  /// The encoding is as follows:
  /// Device encoding (MSB=63, LSB=0)
  /// 63..63: Has subdevice
  /// 62..58: Reserved
  /// 57..56: Subdevice level
  /// 55..48: Subdevice ID start
  /// 47..40: Subdevice ID count
  /// 39..32: Subdevice ID stride
  /// 31..00: Device ID
  ///
  /// If \p W is TEAMS then subdevice clause is obtained from its parent TARGET
  /// and passed in to this function as \p SubdeviceI. For all other cases of
  /// \p W, \p SubdeviceI is null and the subdevice clause is obtained directly
  /// from \p W.
  static Value *encodeSubdevice (WRegionNode* W, Instruction* InsertPt,
                                 Value* DeviceID,
                                 SubdeviceItem* SubdeviceI = nullptr);

  /// Base routine to create `libomptarget` calls. Creates one of these calls:
  /// \code
  ///   void    __tgt_target_data_begin[_mapper]([ident_t *loc,]
  ///                                            int64_t device_id, <common>)
  ///   void    __tgt_target_data_end[_mapper]([ident_t *loc,]
  ///                                          int64_t device_id, <common>)
  ///   void    __tgt_target_data_update[_mapper]([ident_t *loc,]
  ///                                             int64_t device_id, <common>)
  ///   int32_t __tgt_target[_mapper]([ident_t *loc,]
  ///                                 int64_t device_id, void *host_addr,
  ///                                 <common>)
  ///   int32_t __tgt_target_teams[_mapper]([ident_t *loc,]
  ///                                       int64_t device_id, void *host_addr,
  ///                                       <common>, int32_t num_teams,
  ///                                       int32_t thread_limit)
  /// \endcode
  /// where `<common>` represents these 5 arguments:
  /// \code
  ///   int32_t  num_args,    // number of pointers being mapped
  ///   void**   args_base,   // array of base pointers being mapped
  ///   void**   args,        // array of section pointers (base+offset)
  ///   int64_t* args_size,   // array of sizes (bytes) of each mapped datum
  ///   int64_t* args_maptype // array of map attributes for each mapping
  ///   [void**  args_names   // array of names for each mapping]
  ///   [void**  args_mappers // array of mappers for each mapping]
  /// \endcode
  /// The mapper versions of the APIs are emitted unless
  /// -vpo-paropt-use-mapper-api=false is used.
  static CallInst *
  genTgtCall(StringRef FnName, WRegionNode *W, Value *DeviceIDPtr,
             int NumArgsCount, Value *ArgsBase, Value *Args, Value *ArgsSize,
             Value *ArgsMaptype, Value *ArgsNames, Value *ArgsMappers,
             Instruction *InsertPt, Value *HostAddr = nullptr,
             Value *NumTeamsPtr = nullptr, Type *NumTeamsTy = nullptr,
             Value *ThreadLimitPtr = nullptr, Type *ThreadLimitTy = nullptr,
             SubdeviceItem *SubdeviceI = nullptr);

  /// Generate tgt_push_code_location call which pushes source code location
  /// and the pointer to the tgt_target*() function.
  static CallInst *genTgtPushCodeLocation(Instruction *Location,
                                          CallInst *TgtTargetCall);

  /// Generate a call to `tgt_unregister_lib`. Example:
  /// \code
  ///   i32 __tgt_unregister_lib(__tgt_bin_desc *desc)
  /// \endcode
  static CallInst *genTgtUnregisterLib(Value *Desc, Instruction *InsertPt);

  /// Generate a call to `tgt_register_lib`. Example:
  /// \code
  ///   i32 __tgt_register_lib(__tgt_bin_desc *desc)`
  /// \endcode
  static CallInst *genTgtRegisterLib(Value *Desc, Instruction *InsertPt);

  /// Generic function to support the generation of `__tgt_register_lib` and
  /// `__tgt_unregister_lib` calls.
  static CallInst *genTgtRegGeneric(Value *Desc, Instruction *InsertPt,
                                    StringRef FnName);

  /// Generate a call to `_cxa_atexit`. Example:
  /// \code
  /// i32 __cxa_atexit(void (i8*)* @.omp_offloading.descriptor_unreg, i8*
  /// bitcast (%struct.__tgt_bin_desc* @.omp_offloading.descriptor to i8*),
  /// i8* @__dso_handle)
  /// \endcode
  static CallInst *genCxaAtExit(Value *TgtDescUnregFn, Value *Desc,
                                Value *Handle, Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///    i32 __tgt_is_device_available(int device_num, void *device_type)
  /// \endcode
  /// \p DeviceType is a void* argument that carries device type info. In the
  /// current implementation only device architectures in enum DeviceArch are
  /// supported, so we directly encode a bit vector representing the device
  /// architectures selected in the \p DeviceType argument. In the future when
  /// more device information needs to be represented, this argument may be a
  /// pointer to some structure containing device info.
  ///
  /// If \p DeviceNum is available, the call returns true only if \p DeviceType
  /// is null or if the device matches a device type specified in \p DeviceType.
  static CallInst *genTgtIsDeviceAvailable(Value *DeviceNum, Value *DeviceType,
                                           Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///    void *__tgt_create_buffer(int device_num, void *host_ptr)
  /// \endcode
  static CallInst *genTgtCreateBuffer(Value *DeviceNum, Value *HostPtr,
                                      Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///    int __tgt_release_buffer(int device_num, void *tgt_buffer)
  /// \endcode
  static CallInst *genTgtReleaseBuffer(Value *DeviceNum, Value *TgtBuffer,
                                       Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///    void *__tgt_create_interop_obj(int64_t device_id,
  ///                                   bool    is_async,
  ///                                   void   *async_obj)
  /// \endcode
  static CallInst *genTgtCreateInteropObj(Value *DeviceNum, bool IsAsync,
                                          Value *AsyncObj,
                                          Instruction *InsertPt);

  /// /// Generate a call to `__tgt_create_interop`.
  /// \code
  ///    void *__tgt_create_interop(int64_t device_id,
  ///                               int32_t interop_type,
  ///                               int32_t num_prefers,
  ///                               intptr_t* prefer_ids)
  /// \endcode
  static CallInst *
  genTgtCreateInterop(Value *DeviceNum, int32_t OmpInteropContext,
                      const SmallVectorImpl<unsigned> &PreferList,
                      Instruction *InsertPt);

  /// Generate a call to `__tgt_release_interop_obj`
  /// if \p EmitTgtReleaseInteropObj is true,
  /// `__tgt_release_interop` otherwise.
  static CallInst *genTgtReleaseInterop(Value *InteropObj,
                                        Instruction *InsertPt,
                                        bool EmitTgtReleaseInteropObj = false);

  /// Generate a call to
  /// \code
  ///   int __tgt_use_interop(omp_interop_t interop)
  /// \endcode
  static CallInst* genTgtUseInterop(Value* InteropObj, Instruction* InsertPt);

  /// Generate a call to
  /// \code
  ///   int omp_get_default_device()
  /// \endcode
  static CallInst *genOmpGetDefaultDevice(Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   int omp_get_num_devices()
  /// \endcode
  static CallInst *genOmpGetNumDevices(Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   int64 omp_get_interop_int(omp_interop_t interop,
  ///                             omp_get_interop_int property_id,
  ///                             int *return_code)
  /// \endcode
  static CallInst *genOmpGetInteropInt(Value *InteropObj, int PropertyID,
                                       Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   omp_get_interop_int(interop, omp_ipr_device_num, nullptr);
  /// \endcode
  /// which returns the device number of the interop obj
  static CallInst *genOmpGetInteropDeviceNum(Value *InteropObj,
                                             Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   omp_allocator_handle_t omp_get_default_allocator()
  /// \endcode
  static CallInst *genOmpGetDefaultAllocator(Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   void* omp_alloc(size_t Size, omp_allocator_handle_t Handle)
  /// \endcode
  static CallInst *genOmpAlloc(Value *Size, Value *Handle,
                               Instruction *InsertPt);

  /// Generate a call to
  /// \code
  ///   void* __kmpc_aligned_alloc(int gtid, size_t Alignment, size_t Size,
  ///                              omp_allocator_handle_t Handle);
  /// \endcode
  static CallInst *genKmpcAlignedAlloc(uint64_t Alignment, Value *Size,
                                       Value *Handle, Instruction *InsertPt);

  /// Generate a call to `__kmpc_task_reduction_get_th_data`. Prototype:
  /// \code
  ///    i8* @__kmpc_task_reduction_get_th_data(i32, i8*, i8*)
  /// \endcode
  static CallInst *genKmpcRedGetNthData(WRegionNode *W, Value *TidPtr,
                                        Value *SharedGep, Instruction *InsertPt,
                                        bool UseTbb);

  /// Generate a call to `__kmpc_taskred_init`. Prototype:
  /// \code
  ///   i8* @__kmpc_taskred_init(i32, i32, i8*)
  /// \endcode
  /// Or, if UseTbb is true:
  /// \code
  ///   i8* @__tbb_omp_task_reduction_init(i32, i32, i8*)
  /// \endcode
  static CallInst *genKmpcTaskReductionInit(WRegionNode *W, Value *TidPtr,
                                            int ParmNum, Value *RedRecord,
                                            Instruction *InsertPt, bool UseTbb);

  /// Returns min/max int of the given integer type.
  /// This can be used to initialize min/max reduction variables.
  static Constant *getMinMaxIntVal(Type *Ty, bool IsUnsigned, bool GetMax);
  // static uint64_t getMinInt(Type *IntTy, bool IsUnsigned);
  // static uint64_t getMaxInt(Type *IntTy, bool IsUnsigned);

  /// Generate a call to `__kmpc_copyprivate`. Example:
  /// \code
  ///   void __kmpc_copyprivate(
  ///     ident_t *loc, kmp_int32 global_tid, kmp_int32 cpy_size, void
  ///     *cpy_data, void(*cpy func)(void *, void *), kmp_int32 didit );
  ///
  ///   /*
  ///   loc: source location information
  ///   global_tid: global thread number
  ///   cpy_size: size of the cpy_data buffer
  ///   cpy_data: pointer to data to be copied
  ///   cpy_func: helper function to call for copying data
  ///   didit: flag variable: 1=single thread; 0=not single thread
  ///   */
  /// \endcode
  static CallInst *genKmpcCopyPrivate(WRegionNode *W, StructType *IdentTy,
                                      Value *TidPtr, unsigned Size,
                                      Value *CpyData, Function *FnCopyPriv,
                                      Value *IsSingleThread,
                                      Instruction *InsertPt);

  /// Generate a call to `__kmpc_scope`. Example:
  /// \code
  ///   void @__kmpc_scope(%ident_t* %loc.addr.11.12, i32 %my.tid,
  ///                      void* reserved)
  /// \endcode
  static CallInst *genKmpcScopeCall(WRegionNode *W, StructType *IdentTy,
                                        Value *Tid, Instruction *InsertPt);

  /// Generate a call to `__kmpc_end_scope`. Example:
  /// \code
  ///   void @__kmpc_end_scope(%ident_t* %loc.addr.11.12, i32 %my.tid,
  ///                              void* reserved)
  /// \endcode
  static CallInst *genKmpcEndScopeCall(WRegionNode *W, StructType *IdentTy,
                                           Value *Tid, Instruction *InsertPt);

  // This function generates calls for the scope region.
  //
  //   call void @__kmpc_scope(%ident_t* %loc, i32 %tid, void *reserved)
  //      or
  //   call void @__kmpc_end_scope(%ident_t* %loc, i32 %tid, void *reserved)
  static CallInst *genKmpcScopeOrEndScopeCall(WRegionNode *W,
                                              StructType *IdentTy,
                                              Value *Tid,
                                              Instruction *InsertPt,
                                              bool IsScopeStart);

  /// Generate a call to `__kmpc_taskgroup`. Example:
  /// \code
  ///   void @__kmpc_taskgroup(%ident_t* %loc.addr.11.12, i32 %my.tid)
  /// \endcode
  static CallInst *genKmpcTaskgroupCall(WRegionNode *W, StructType *IdentTy,
                                        Value *Tid, Instruction *InsertPt);

  /// Generate a call to `__kmpc_end_taskgroup`. Example:
  /// \code
  ///   void @__kmpc_end_taskgroup(%ident_t* %loc.addr.11.12, i32 %my.tid)
  /// \endcode
  static CallInst *genKmpcEndTaskgroupCall(WRegionNode *W, StructType *IdentTy,
                                           Value *Tid, Instruction *InsertPt);

  /// Generate a generic call to `get_global_id, get_local_id...`. Example:
  /// \code
  ///    %11 = call i64 @_Z14get_local_sizej(i32 0)
  ///      where the value 0 is the dimension number.
  //  \endcode
  static CallInst *genOCLGenericCall(StringRef FnName, Type *RetType,
                                     ArrayRef<Value *> FnArgs,
                                     Instruction *InsertPt);

  /// Generate SPIR-V calls OpenMP SIMD path
  ///   call spir_func i64 @_Z27__spirv_LocalInvocationId_xv()
  ///   call spir_func i64 @_Z27__spirv_LocalInvocationId_yv()
  ///   call spir_func i64 @_Z27__spirv_LocalInvocationId_zv()
  static CallInst *genSPIRVLocalIdCall(int Dim, Instruction *InsertPt);

  /// Set the calling convention for \p CI.
  /// Set SPIR_FUNC calling convention for SPIR-V targets, otherwise,
  /// set C calling convention.
  /// Since \p CI may not be inserted into any Module yet, \p M
  /// specifies a Module that is used to identify the target.
  static void setFuncCallingConv(CallInst *CI, Module *M);

  /// Add funclet operand bundle to \p CI if it lies within an EHPad
  /// or is dominated by an EHPad. The utility searches DT to check if \p CI
  /// lies within an EHPad or is dominated by one.
  static CallInst *
  addFuncletOperandBundle(CallInst *CI, DominatorTree *DT,
                          Instruction *InstToCheckFuncletRequirement = nullptr);

  /// \name Helper methods for generating calls.
  /// @{

  /// Generate KMPC runtime call to the function \p IntrinsicName
  /// with arguments Loc(obtained using \p IdentTy) and \p Args.
  /// \param IdentTy and \p InsertPt are used to obtain Loc needed by the
  /// KMPC call.
  /// \param IntrinsicName is the name of the function.
  /// \param ReturnTy is the return type of the function.
  /// \param Args arguments for the function call.
  /// \param EnableAtomicReduce is to set flag bit for enabling atomic reduce.
  /// \param Insert indicates whether to insert the call at InsertPt
  ///
  /// \returns the generated CallInst.
  static CallInst *genKmpcCall(WRegionNode *W, StructType *IdentTy,
                               Instruction *InsertPt, StringRef IntrinsicName,
                               Type *ReturnTy, ArrayRef<Value *> Args,
                               bool Insert = false,
                               bool EnableAtomicReduce = false);

  /// Generate a CallInst for the given FunctionCallee \p FnC and its argument
  /// list. \p FnC must have a non-null Callee.
  static CallInst *genCall(Module *M,
                           FunctionCallee FnC, ArrayRef<Value *> FnArgs,
                           ArrayRef<Type *> FnArgTypes, Instruction *InsertPt,
                           bool IsTail = false);

  /// Generate a call to the function \p FnName.
  /// If the function is not already declared in the module \p M, then it is
  /// declared here. Otherwise, the existing declaration is used.
  /// \param M Module for which the call is generated.
  /// \param FnName Name of the function.
  /// \param ReturnTy Return type of the function.
  /// \param FnArgs Arguments for the function call.
  /// \param FnArgTypes Types of the arguments for the function call.
  /// \param InsertPt Insertion point for the call. Default is nullptr.
  /// \param IsTail This call attribute is defaulted to false.
  /// \param IsVarArg  This call attribute is defaulted to false.
  /// \param AllowMismatchingPointerArgs If there is an existing function \p
  /// FnName in \p M, but with arguments that are either the same as \p
  /// FnTyArgs, or different pointer type, then allow using a cast and emit a
  /// call to that function.
  /// \param EmitErrorOnFnTypeMismatch Emit an error if there is an
  /// existing function \p FnName in \p M, but with a different function type.
  /// \returns the generated CallInst.
  static CallInst *genCall(Module *M, StringRef FnName, Type *ReturnTy,
                           ArrayRef<Value *> FnArgs,
                           ArrayRef<Type *> FnArgTypes,
                           Instruction *InsertPt = nullptr, bool IsTail = false,
                           bool IsVarArg = false,
                           bool AllowMismatchingPointerArgs = false,
                           bool EmitErrorOnFnTypeMismatch = false);

  // A genCall() interface where FnArgTypes is omitted; it will be computed
  // from FnArgs. **WARNING**: do not use this interface for VarArg functions,
  // as the list of FnArgTypes corresponding to the FnArgs may be longer than
  // the actual list of params in the FunctionType.
  static CallInst *genCall(Module *M, StringRef FnName, Type *ReturnTy,
                           ArrayRef<Value *> FnArgs,
                           Instruction *InsertPt = nullptr, bool IsTail = false,
                           bool IsVarArg = false);

  // A genCall() interface where the Module is omitted; it will be computed
  // from the insertion point, which is must be specified (no default).
  static CallInst *genCall(StringRef FnName, Type *ReturnTy,
                           ArrayRef<Value *> FnArgs,
                           ArrayRef<Type *> FnArgTypes, Instruction *InsertPt,
                           bool IsTail = false, bool IsVarArg = false);

  // A genCall() interface where FnArgTypes and Module are omitted;
  // FnArgTypes will be computed from FnArgs; Module will be computed
  // from InsertPt.
  // **WARNING**: do not use this interface for VarArg functions, as the list
  // of FnArgTypes corresponding to the FnArgs may be longer than the actual
  // list of params in the FunctionType.
  static CallInst *genCall(StringRef FnName, Type *ReturnTy,
                           ArrayRef<Value *> FnArgs,
                           Instruction *InsertPt, bool IsTail = false);

  /// Given a call \p BaseCall, create another call with name \p VariantName
  /// using the same arguments from \p BaseCall. Both functions are expected
  /// to have identical signatures.
  /// \p W is a TargetVariant WRN.
  /// If \p InteropPosition is 0, then the InteropObj is appended as the last
  /// argument in the argument list of the variant call. Otherwise, the
  /// InteropObj is inserted in the argument list in the position indicated
  /// by \p InteropPosition. (First argument is position 1.)
  static CallInst *genVariantCall(CallInst *BaseCall, StringRef VariantName,
                                  Value *InteropObj,
                                  llvm::Optional<uint64_t> InteropPosition,
                                  Instruction *InsertPt,
                                  WRegionNode *W = nullptr,
                                  bool IsTail = false);

  // Creates a call with no parameters.
  // If \p InsertPt is not null, insert the call before InsertPt
  static CallInst *genEmptyCall(Module *M, StringRef FnName, Type *ReturnTy,
                                Instruction *InsertPt = nullptr,
                                bool IsVarArg = false);

  /// Returns the single call site for the given function. Asserts if there
  /// are multiple callsites or no callsite. Uses such as address-taken
  /// references are OK.
  static CallInst *getSingleCallSite(Function *F);

  /// Creates new Function and outlines \p W region into it.
  /// \p DT DominatorTree is updated accordingly. If \p BBsToExtractIn is
  /// provided, only the BasicBlocks it contains are outlined, instead of the
  /// full body of \p W.
  static Function *genOutlineFunction(
      const WRegionNode &W, DominatorTree *DT, AssumptionCache *AC,
      llvm::Optional<ArrayRef<BasicBlock *>> BBsToExtractIn = llvm::None,
      std::string Suffix = "");

  // If there is a SPIRV builtin performing horizontal reduction for the given
  // reduction operation, this method will insert code with a call
  // of this builtin with \p RedDef as the reduction value.
  // \p Scope defines the SPIRV reduction scope (e.g. Group, Subgroup, etc.)
  // In some case sign/zero integer extension and truncation may be inserted
  // before and after the call.
  static Value *genSPIRVHorizontalReduction(
      ReductionItem *RedI, Type *ScalarTy, Instruction *RedDef,
      spirv::Scope Scope);

  /// Returns true, if work partitioning for the loop-kind \p W region
  /// should rely on ND-range driven parallelization.
  static bool useSPMDMode(WRegionNode *W);

  /// Returns execution scheme for loop-kind regions on SPIR targets.
  static spirv::ExecutionSchemeTy getSPIRExecutionScheme();

  /// Returns true, if OpenMP explicit SIMD code generation is enabled.
  static bool enableDeviceSimdCodeGen();

  /// Returns true, if async-offload helper thread code generation is enabled.
  static bool enableAsyncHelperThread();

  /// Returns device memory address space setting value
  ///   0: private
  ///   1: global
  ///   2: constant
  ///   3: local (default)
  ///   4: generic
  static uint32_t getDeviceMemoryKind();

  /// Returns true, if it is allowed to execute "omp target parallel for"
  /// with multiple teams/WGs. According to OpenMP specification only
  /// one team/WG is allowed, which corresponds to false return value.
  static bool getSPIRImplicitMultipleTeams();

  /// Control data prefetch API generation for GPUs
  static uint32_t dataPrefetchKind();

  /// Returns true, if the given instruction \p I represents a call
  /// to library function __kmpc_critical.
  static bool isOMPCritical(const Instruction *I, const TargetLibraryInfo &TLI);

  /// Returns the Instruction which can be used as an insertion point for
  /// any alloca which needs to be inserted before the entry directive of \p W.
  /// If \p OutsideRegion is true, then the utility looks at parent
  /// WRegions of \p W, and if it finds any that would be outlined,
  /// then it returns the first non-PHI of its first basic
  /// block. If no such parent is found, then the first non-PHI of the
  /// function \p F is returned.
  /// If \p OutsideRegion is false, and \p W will not be outlined, then
  /// the behavior is the same as with \p OutsideRegion equal to true.
  /// If \p OutsideRegion is false, and \p W will be outlined, then
  /// the utility returns \p W region's entry directive making sure
  /// that the inserted alloca will be outlined along with the region.
  static Instruction *getInsertionPtForAllocas(WRegionNode *W,
                                               Function *F,
                                               bool OutsideRegion = true);

#ifndef NDEBUG
  /// Run some Paropt related verifications to make sure IR after FE
  /// will not cause problems deep in Paropt.
  static void verifyFunctionForParopt(const Function &F, bool IsTargetSPIRV);
#endif  // NDEBUG
  /// @}

  /// Find users of \p V in function \p F and put all users of \p V into \p
  /// UserInsts, and add all ConstantExpr users of \p V in UserExprs.
  static void findUsesInFunction(Function *F, Value *V,
                                 SmallVectorImpl<Instruction *> *UserInsts,
                                 SmallPtrSetImpl<ConstantExpr *> *UserExprs);

  /// Replace users of \p Old with \p New value in the function \p F.
  static void replaceUsesInFunction(Function *F, Value *Old, Value *New);

  /// Returns true, if this is a target compilation invocation
  /// forced by dedicated compiler option.
  static bool isForcedTargetCompilation();

  /// Create a thread local global GV, insert a store of \p V to it before
  /// \p InsertBefore, and return GV.
  static GlobalVariable *storeIntToThreadLocalGlobal(Value *V,
                                                     Instruction *InsertBefore,
                                                     StringRef VarName = "");

  static void emitWarning(WRegionNode *W, const Twine &Message);

  /// This function generates calls to perform data prefetch on ATS and PVC
  /// based on data element type, data type of number of elements.
  ///
  /// \code
  /// call void __builtin_spirv_OpenCL_prefetch_p1i8_i32((const global uchar*)a, 1);
  /// call void __builtin_spirv_OpenCL_prefetch_p1i8_i64((const global uchar*)a, 1);
  /// ... ...
  /// \endcode
  static void genSPIRVPrefetchBuiltIn(WRegionNode *w, Instruction *InsertPt);

  /// This function generates calls to perform data prefetch with different API
  /// which takes base address, prefetch distance/offset, cache Hint on PVC
  ///
  /// \code
  /// void __builtin_IB_lsc_prefetch_global_uint (const __global uint  *base,
  ///                                 int elemOff, enum LSC_LDCC cacheOpt); //D32V1
  /// void __builtin_IB_lsc_prefetch_global_ulong (const __global ulong  *base,
  ///                                 int elemOff, enum LSC_LDCC cacheOpt); //D64V1
  /// ... ...
  /// \endcode
  static void genSPIRVLscPrefetchBuiltIn(WRegionNode *w, Instruction *InsertPt);

private:
  /// \name Private constructor and destructor to disable instantiation.
  /// @{

  VPOParoptUtils() = delete;
  ~VPOParoptUtils() = delete;

  /// @}

  /// \name Helper methods for generating a Critical Section.
  /// @{

  /// Creates a prefix for the name of the lock var to be used in KMPC
  /// critical calls based on the kind of WRegionNode \p W.
  ///
  /// \returns A string prefix for the KMPC Lock object for \p W.
  static SmallString<64> getKmpcCriticalLockNamePrefix(WRegionNode *W);

  /// Returns a GlobalVariable that can be used as the Lock object for
  /// `__kmpc_critical` and __kmpc_end_critical` calls.
  /// \p W is used to determine the name of the Lock object generated.
  /// \p LockNameSuffix will be used as suffix in the name of the lock
  /// variable used for the critical section. (The prefix is determined based
  /// on the architecture and the kind of WRegionNode \W).
  /// \p IsTargetSPIRV is true, iff compilation is for SPIRV target.
  ///
  /// \returns The lock variable for the critical section to be generated.
  static GlobalVariable *
  genKmpcCriticalLockVar(WRegionNode *W, const Twine &LockNameSuffix,
                         bool IsTargetSPIRV);

  /// Handles generation of a critical section around \p BeginInst and \p
  /// EndInst. The function needs a lock variable \p LockVar, which is
  /// generated by genKmpcCriticalLockVar().
  /// \p IsTargetSPIRV is true, iff compilation is for SPIRV target.
  ///
  /// \see genKmpcCriticalSection() functions for more details. They are the
  /// public functions which invokes this private helper.
  ///
  /// \returns `true` if the calls to `__kmpc_critical` or
  /// `__kmpc_critical_with_hint`and `__kmpc_end_critical` are successfully
  /// inserted, `false` otherwise.
  static bool genKmpcCriticalSectionImpl(WRegionNode *W, StructType *IdentTy,
                                         Constant *TidPtr,
                                         Instruction *BeginInst,
                                         Instruction *EndInst,
                                         GlobalVariable *LockVar,
                                         DominatorTree *DT,
                                         LoopInfo *LI,
                                         bool IsTargetSPIRV,
                                         uint32_t Hint);

  /// Handles generation of tree reduce block around \p BeginInst and \p
  /// EndInst, atomic reduce block around \p AtomicBeginInst and \p
  /// AtomicEndInst. The function needs a lock variable \p LockVar, which is
  /// generated by genKmpcCriticalLockVar().
  /// \p IsTargetSPIRV is true, iff compilation is for SPIRV target.
  ///
  /// \see genKmpcReduce() functions for more details. They are the public
  /// functions which invokes this private helper.
  ///
  /// \returns `true` if the calls to `__kmpc_reduce` and
  /// `__kmpc_end_reduce` are successfully inserted, `false` otherwise.
  static bool
  genKmpcReduceImpl(WRegionNode *W, StructType *IdentTy, Constant *TidPtr,
                    Value *RedVar, RDECL RedCallback, Instruction *BeginInst,
                    Instruction *EndInst, Instruction *AtomicBeginInst,
                    Instruction *AtomicEndInst, GlobalVariable *LockVar,
                    DominatorTree *DT, LoopInfo *LI, bool IsTargetSPIRV);


  /// Wrap lane-by-lane loop around \p BeginInst and \p EndInst:
  ///   for (int id = 0; id < get_sub_group_size(); ++id) {
  ///     if (id != get_sub_group_local_id())
  ///       continue;
  ///     <BeginInst>
  ///     ...
  ///   }
  ///   <EndInst>
  /// This is a helper method for generating the serialization loop
  /// for OpenMP critical execution. \p DT and \p LI are updated
  /// accordingly.
  static bool genCriticalLoopForSPIRHelper(Instruction *BeginInst,
                                           Instruction *EndInst,
                                           DominatorTree *DT,
                                           LoopInfo *LI);

  /// Generates lane-by-lane execution loop for code inside a critical
  /// section (guarded with __kmpc_critical/__kmpc_end_critical calls).
  /// \p BeginCritical and \p EndCritical identify a piece of code that needs
  /// to be wrapped into the loop. \p BeginCritical must dominate
  /// \p EndCritical, and \p EndCritical must post-dominate \p BeginCritical.
  /// \p DT and \p LI are updated accordingly.
  /// Exit blocks of region \p W may be modified due to blocks splitting.
  ///
  /// The generated loop looks like this:
  ///   <BeginCritical>
  ///   for (int id = 0; id < get_sub_group_size(); ++id) {
  ///     if (id != get_sub_group_local_id())
  ///       continue;
  ///     <LoopBeginInst>
  ///     ...
  ///     <LoopEndInst>
  ///   }
  ///   <EndCritical>
  ///
  /// Legend:
  ///   LoopBeginInst is the next instruction after \p BeginCritical.
  ///   LoopEndInst is the previous instruction before \p EndCritical.
  ///
  /// Under some controls this method also generates two identical
  /// branches for even and odd sub-groups:
  ///   if (get_sub_group_id() & 1) {
  ///     <BeginCritical>
  ///     for (int id = 0; id < get_sub_group_size(); ++id) {
  ///       if (id != get_sub_group_local_id())
  ///         continue;
  ///       <LoopBeginInst>
  ///       ...
  ///       <LoopEndInst>
  ///     }
  ///     <EndCritical>
  ///   } else {
  ///     <BeginCritical>
  ///     for (int id = 0; id < get_sub_group_size(); ++id) {
  ///       if (id != get_sub_group_local_id())
  ///         continue;
  ///       <LoopBeginInst>
  ///       ...
  ///       <LoopEndInst>
  ///     }
  ///     <EndCritical>
  ///   }
  ///
  /// This helps resolve dead-lock in case two sub-groups are executed
  /// in lock step mode, e.g. on devices with EU fusion.
  /// __kmpc_critical lock setup will dead-lock on such devices, unless
  /// we explicitly execute the lock-unlock sequence of blocks for each
  /// fused sub-group separately.
  static bool genCriticalLoopForSPIR(WRegionNode *W,
                                     CallInst *BeginCritical,
                                     CallInst *EndCritical,
                                     DominatorTree *DT,
                                     LoopInfo *LI);

  /// Generate a call to `__kmpc_[end_]taskgroup`.
  /// \code
  ///   call void @__kmpc_taskgroup(%ident_t* %loc, i32 %tid)
  ///      or
  ///   call void @__kmpc_end_taskgroup(%ident_t* %loc, i32 %tid)
  /// \endcode
  static CallInst *genKmpcTaskgroupOrEndTaskgroupCall(WRegionNode *W,
                                                      StructType *IdentTy,
                                                      Value *Tid,
                                                      Instruction *InsertPt,
                                                      bool IsTaskGroupStart);
  /// @}

public:
  /// Utility to go through all Items in the given clause, if clause is not
  /// nullptr and execute the passed lambda function with each item as a
  /// parameter.
  /// Example Usage: Note: A Lambda can be templated in C++20
  /// onwards. Therefore, currently we can use Item base class as an argument
  /// and use the APIs provided in the base class. For using specific Item Type
  /// APIs, pass a specially created lambda.
  ///
  /// auto AddItem = [&](Item* I){
  ///   ValueSet.insert(I->getOrig());
  /// };
  /// executeForEachItemInClause(WRegionNodePtr->getSharedIfSupported(),
  ///                            AddItem);
  /// executeForEachItemInClause(WRegionNodePtr->getPrivIfSupported(),
  ///                            AddItem);
  ///
  /// auto GenDestCallLpriv = [&](LastprivateItem *LprivI){
  ///   VPOParoptUtils::genDestructorCall(
  //                      LprivI->getDestructor(),
  ///                     LprivI->getNew(),
  ///                     WRegionNodePtr->getExitBBlock()->getTerminator());
  /// };
  ///
  /// executeForEachItemInClause(WRegionNodePtr->getLprivIfSupported(),
  ///                            GenDestCallLpriv);

  template <typename T, class UnaryFunction>
  static void executeForEachItemInClause(T *Clause, UnaryFunction Code) {
    if (Clause != nullptr)
      std::for_each(Clause->items().begin(), Clause->items().end(), Code);
  }

  /// Utility to search all Items in the given clause for a specific condition.
  /// Returns the first "Item* I" in \p Clause for which "Condition(I)" returns
  /// true. Returns \b nullptr if no such Item exists.

  template <typename T, class UnaryFunction>
  static auto findItemInClauseForWhich(T *Clause, UnaryFunction Condition) ->
      typename std::decay<decltype(*Clause->items().begin())>::type {
    if (!Clause)
      return nullptr;
    auto ResultIt =
        std::find_if(Clause->items().begin(), Clause->items().end(), Condition);
    if (ResultIt == Clause->items().end())
      return nullptr;
    return *ResultIt;
  }

  /// Extract the type and size of local Alloca to be created to privatize
  /// \p OrigValue.
  /// \param [in] OrigValue Input Value
  /// \param [in] OrigValueElemType Input Value's Element Type
  /// \param [out] ElementType Type of one element
  /// \param [out] NumElements Number of elements, in case \p OrigValue is
  /// an array, \b nullptr otherwise.
  /// \param [out] AddrSpace Address space of the input value object.
  static void getItemInfoFromValue(Value *OrigValue, Type *OrigValueElemType,
                                   Type *&ElementType, Value *&NumElements,
                                   unsigned &AddrSpace);

  /// Extract the type and size of local Alloca to be created to privatize
  /// \p I.
  /// \returns a \b tuple of <ElementType, NumElements, AddrSpace>. where
  /// NumElements is the number of elements, in case I's Orig is an array, \b
  /// nullptr otherwise. AddrSpace is the address space of the input item
  /// object.
  static std::tuple<Type *, Value *, unsigned> getItemInfo(const Item *I);
#if INTEL_CUSTOMIZATION

  /// For an F90_DV Item \p I, return the DV struct's type and the pointee
  /// data's element type. \returns a \b tuple of <DVType, PointeeElementType>.
  static std::tuple<Type *, Type *> getF90DVItemInfo(const Item *I);
#endif // INTEL_CUSTOMIZATION

  /// Load offload metadata from the module and create offload entries that
  /// need to be emitted after lowering all target constructs.
  /// The entries are returned in the result vector.
  static SmallVector<OffloadEntry *, 8> loadOffloadMetadata(const Module &M);

  /// Erase "omp_offload.info" metadata from the module \p M.
  static bool eraseOffloadMetadata(Module &M);

  /// Given \p W pointing to an instance of WRNTargetNode and \p OffloadEntries
  /// array of offload entries read by VPOParoptUtils::loadOffloadMetadata()
  /// the function returns OffloadEntry data structure corresponding
  /// to the given target region.
  static OffloadEntry *getTargetRegionOffloadEntry(
      const WRegionNode *W,
      const SmallVectorImpl<OffloadEntry *> &OffloadEntries);

  /// Check if reduction item \p RedI is supported by atomic-free reduction.
  /// It is necessary to correctly match global buffers created at prepare
  /// pass when making actual codegen at transform pass.
  /// \param [in] RedI Reduction Item to Check
  static bool supportsAtomicFreeReduction(const ReductionItem *RedI);
};

} // namespace vpo
} // namespace llvm

#endif // LLVM_TRANSFORMS_VPO_PAROPT_UTILS_H
#endif // INTEL_COLLAB
