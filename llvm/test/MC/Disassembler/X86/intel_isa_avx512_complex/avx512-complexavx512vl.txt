# REQUIRES: intel_feature_isa_avx512_complex
# RUN: llvm-mc --disassemble %s -triple=i386 | FileCheck %s --check-prefixes=ATT
# RUN: llvm-mc --disassemble %s -triple=i386 -x86-asm-syntax=intel --output-asm-variant=1 | FileCheck %s --check-prefixes=INTEL

# ATT:   vaddsubpd %ymm4, %ymm3, %ymm2
# INTEL: vaddsubpd ymm2, ymm3, ymm4
0xc5,0xe5,0xd0,0xd4

# ATT:   vaddsubpd %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vaddsubpd ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0xe5,0x2f,0xd0,0xd4

# ATT:   vaddsubpd %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubpd ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0xe5,0xaf,0xd0,0xd4

# ATT:   vaddsubpd %xmm4, %xmm3, %xmm2
# INTEL: vaddsubpd xmm2, xmm3, xmm4
0xc5,0xe1,0xd0,0xd4

# ATT:   vaddsubpd %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vaddsubpd xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0xe5,0x0f,0xd0,0xd4

# ATT:   vaddsubpd %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubpd xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0xe5,0x8f,0xd0,0xd4

# ATT:   vaddsubpd  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vaddsubpd ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0xc5,0xe5,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubpd  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vaddsubpd ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0xe5,0x2f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubpd  (%eax){1to4}, %ymm3, %ymm2
# INTEL: vaddsubpd ymm2, ymm3, qword ptr [eax]{1to4}
0x62,0xf6,0xe5,0x38,0xd0,0x10

# ATT:   vaddsubpd  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vaddsubpd ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0xc5,0xe5,0xd0,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vaddsubpd  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubpd ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0xe5,0xaf,0xd0,0x51,0x7f

# ATT:   vaddsubpd  -1024(%edx){1to4}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubpd ymm2 {k7} {z}, ymm3, qword ptr [edx - 1024]{1to4}
0x62,0xf6,0xe5,0xbf,0xd0,0x52,0x80

# ATT:   vaddsubpd  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vaddsubpd xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0xc5,0xe1,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubpd  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vaddsubpd xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0xe5,0x0f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubpd  (%eax){1to2}, %xmm3, %xmm2
# INTEL: vaddsubpd xmm2, xmm3, qword ptr [eax]{1to2}
0x62,0xf6,0xe5,0x18,0xd0,0x10

# ATT:   vaddsubpd  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vaddsubpd xmm2, xmm3, xmmword ptr [2*ebp - 512]
0xc5,0xe1,0xd0,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vaddsubpd  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubpd xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0xe5,0x8f,0xd0,0x51,0x7f

# ATT:   vaddsubpd  -1024(%edx){1to2}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubpd xmm2 {k7} {z}, xmm3, qword ptr [edx - 1024]{1to2}
0x62,0xf6,0xe5,0x9f,0xd0,0x52,0x80

# ATT:   vaddsubph %ymm4, %ymm3, %ymm2
# INTEL: vaddsubph ymm2, ymm3, ymm4
0x62,0xf6,0x64,0x28,0xd0,0xd4

# ATT:   vaddsubph %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vaddsubph ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0x64,0x2f,0xd0,0xd4

# ATT:   vaddsubph %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubph ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0x64,0xaf,0xd0,0xd4

# ATT:   vaddsubph %xmm4, %xmm3, %xmm2
# INTEL: vaddsubph xmm2, xmm3, xmm4
0x62,0xf6,0x64,0x08,0xd0,0xd4

# ATT:   vaddsubph %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vaddsubph xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0x64,0x0f,0xd0,0xd4

# ATT:   vaddsubph %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubph xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0x64,0x8f,0xd0,0xd4

# ATT:   vaddsubph  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vaddsubph ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x64,0x28,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubph  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vaddsubph ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x64,0x2f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubph  (%eax){1to16}, %ymm3, %ymm2
# INTEL: vaddsubph ymm2, ymm3, word ptr [eax]{1to16}
0x62,0xf6,0x64,0x38,0xd0,0x10

# ATT:   vaddsubph  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vaddsubph ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0x62,0xf6,0x64,0x28,0xd0,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vaddsubph  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubph ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0x64,0xaf,0xd0,0x51,0x7f

# ATT:   vaddsubph  -256(%edx){1to16}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubph ymm2 {k7} {z}, ymm3, word ptr [edx - 256]{1to16}
0x62,0xf6,0x64,0xbf,0xd0,0x52,0x80

# ATT:   vaddsubph  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vaddsubph xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x64,0x08,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubph  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vaddsubph xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x64,0x0f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubph  (%eax){1to8}, %xmm3, %xmm2
# INTEL: vaddsubph xmm2, xmm3, word ptr [eax]{1to8}
0x62,0xf6,0x64,0x18,0xd0,0x10

# ATT:   vaddsubph  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vaddsubph xmm2, xmm3, xmmword ptr [2*ebp - 512]
0x62,0xf6,0x64,0x08,0xd0,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vaddsubph  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubph xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0x64,0x8f,0xd0,0x51,0x7f

# ATT:   vaddsubph  -256(%edx){1to8}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubph xmm2 {k7} {z}, xmm3, word ptr [edx - 256]{1to8}
0x62,0xf6,0x64,0x9f,0xd0,0x52,0x80

# ATT:   vaddsubps %ymm4, %ymm3, %ymm2
# INTEL: vaddsubps ymm2, ymm3, ymm4
0xc5,0xe7,0xd0,0xd4

# ATT:   vaddsubps %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vaddsubps ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0x65,0x2f,0xd0,0xd4

# ATT:   vaddsubps %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubps ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0x65,0xaf,0xd0,0xd4

# ATT:   vaddsubps %xmm4, %xmm3, %xmm2
# INTEL: vaddsubps xmm2, xmm3, xmm4
0xc5,0xe3,0xd0,0xd4

# ATT:   vaddsubps %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vaddsubps xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0x65,0x0f,0xd0,0xd4

# ATT:   vaddsubps %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubps xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0x65,0x8f,0xd0,0xd4

# ATT:   vaddsubps  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vaddsubps ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0xc5,0xe7,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubps  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vaddsubps ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x65,0x2f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubps  (%eax){1to8}, %ymm3, %ymm2
# INTEL: vaddsubps ymm2, ymm3, dword ptr [eax]{1to8}
0x62,0xf6,0x65,0x38,0xd0,0x10

# ATT:   vaddsubps  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vaddsubps ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0xc5,0xe7,0xd0,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vaddsubps  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubps ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0x65,0xaf,0xd0,0x51,0x7f

# ATT:   vaddsubps  -512(%edx){1to8}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vaddsubps ymm2 {k7} {z}, ymm3, dword ptr [edx - 512]{1to8}
0x62,0xf6,0x65,0xbf,0xd0,0x52,0x80

# ATT:   vaddsubps  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vaddsubps xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0xc5,0xe3,0xd0,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vaddsubps  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vaddsubps xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x65,0x0f,0xd0,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vaddsubps  (%eax){1to4}, %xmm3, %xmm2
# INTEL: vaddsubps xmm2, xmm3, dword ptr [eax]{1to4}
0x62,0xf6,0x65,0x18,0xd0,0x10

# ATT:   vaddsubps  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vaddsubps xmm2, xmm3, xmmword ptr [2*ebp - 512]
0xc5,0xe3,0xd0,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vaddsubps  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubps xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0x65,0x8f,0xd0,0x51,0x7f

# ATT:   vaddsubps  -512(%edx){1to4}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vaddsubps xmm2 {k7} {z}, xmm3, dword ptr [edx - 512]{1to4}
0x62,0xf6,0x65,0x9f,0xd0,0x52,0x80

# ATT:   vmovdhdup %xmm3, %xmm2
# INTEL: vmovdhdup xmm2, xmm3
0x62,0xf1,0xff,0x08,0x16,0xd3

# ATT:   vmovdhdup %xmm3, %xmm2 {%k7}
# INTEL: vmovdhdup xmm2 {k7}, xmm3
0x62,0xf1,0xff,0x0f,0x16,0xd3

# ATT:   vmovdhdup %xmm3, %xmm2 {%k7} {z}
# INTEL: vmovdhdup xmm2 {k7} {z}, xmm3
0x62,0xf1,0xff,0x8f,0x16,0xd3

# ATT:   vmovdhdup %ymm3, %ymm2
# INTEL: vmovdhdup ymm2, ymm3
0x62,0xf1,0xff,0x28,0x16,0xd3

# ATT:   vmovdhdup %ymm3, %ymm2 {%k7}
# INTEL: vmovdhdup ymm2 {k7}, ymm3
0x62,0xf1,0xff,0x2f,0x16,0xd3

# ATT:   vmovdhdup %ymm3, %ymm2 {%k7} {z}
# INTEL: vmovdhdup ymm2 {k7} {z}, ymm3
0x62,0xf1,0xff,0xaf,0x16,0xd3

# ATT:   vmovdhdup  268435456(%esp,%esi,8), %xmm2
# INTEL: vmovdhdup xmm2, xmmword ptr [esp + 8*esi + 268435456]
0x62,0xf1,0xff,0x08,0x16,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vmovdhdup  291(%edi,%eax,4), %xmm2 {%k7}
# INTEL: vmovdhdup xmm2 {k7}, xmmword ptr [edi + 4*eax + 291]
0x62,0xf1,0xff,0x0f,0x16,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vmovdhdup  (%eax), %xmm2
# INTEL: vmovdhdup xmm2, xmmword ptr [eax]
0x62,0xf1,0xff,0x08,0x16,0x10

# ATT:   vmovdhdup  -512(,%ebp,2), %xmm2
# INTEL: vmovdhdup xmm2, xmmword ptr [2*ebp - 512]
0x62,0xf1,0xff,0x08,0x16,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vmovdhdup  1016(%ecx), %xmm2 {%k7} {z}
# INTEL: vmovdhdup xmm2 {k7} {z}, xmmword ptr [ecx + 1016]
0x62,0xf1,0xff,0x8f,0x16,0x51,0x7f

# ATT:   vmovdhdup  -1024(%edx), %xmm2 {%k7} {z}
# INTEL: vmovdhdup xmm2 {k7} {z}, xmmword ptr [edx - 1024]
0x62,0xf1,0xff,0x8f,0x16,0x52,0x80

# ATT:   vmovdhdup  268435456(%esp,%esi,8), %ymm2
# INTEL: vmovdhdup ymm2, ymmword ptr [esp + 8*esi + 268435456]
0x62,0xf1,0xff,0x28,0x16,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vmovdhdup  291(%edi,%eax,4), %ymm2 {%k7}
# INTEL: vmovdhdup ymm2 {k7}, ymmword ptr [edi + 4*eax + 291]
0x62,0xf1,0xff,0x2f,0x16,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vmovdhdup  (%eax), %ymm2
# INTEL: vmovdhdup ymm2, ymmword ptr [eax]
0x62,0xf1,0xff,0x28,0x16,0x10

# ATT:   vmovdhdup  -1024(,%ebp,2), %ymm2
# INTEL: vmovdhdup ymm2, ymmword ptr [2*ebp - 1024]
0x62,0xf1,0xff,0x28,0x16,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vmovdhdup  4064(%ecx), %ymm2 {%k7} {z}
# INTEL: vmovdhdup ymm2 {k7} {z}, ymmword ptr [ecx + 4064]
0x62,0xf1,0xff,0xaf,0x16,0x51,0x7f

# ATT:   vmovdhdup  -4096(%edx), %ymm2 {%k7} {z}
# INTEL: vmovdhdup ymm2 {k7} {z}, ymmword ptr [edx - 4096]
0x62,0xf1,0xff,0xaf,0x16,0x52,0x80

# ATT:   vsubaddpd %ymm4, %ymm3, %ymm2
# INTEL: vsubaddpd ymm2, ymm3, ymm4
0x62,0xf6,0xe5,0x28,0xd1,0xd4

# ATT:   vsubaddpd %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vsubaddpd ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0xe5,0x2f,0xd1,0xd4

# ATT:   vsubaddpd %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddpd ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0xe5,0xaf,0xd1,0xd4

# ATT:   vsubaddpd %xmm4, %xmm3, %xmm2
# INTEL: vsubaddpd xmm2, xmm3, xmm4
0x62,0xf6,0xe5,0x08,0xd1,0xd4

# ATT:   vsubaddpd %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vsubaddpd xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0xe5,0x0f,0xd1,0xd4

# ATT:   vsubaddpd %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddpd xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0xe5,0x8f,0xd1,0xd4

# ATT:   vsubaddpd  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vsubaddpd ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0xe5,0x28,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddpd  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vsubaddpd ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0xe5,0x2f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddpd  (%eax){1to4}, %ymm3, %ymm2
# INTEL: vsubaddpd ymm2, ymm3, qword ptr [eax]{1to4}
0x62,0xf6,0xe5,0x38,0xd1,0x10

# ATT:   vsubaddpd  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vsubaddpd ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0x62,0xf6,0xe5,0x28,0xd1,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vsubaddpd  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddpd ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0xe5,0xaf,0xd1,0x51,0x7f

# ATT:   vsubaddpd  -1024(%edx){1to4}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddpd ymm2 {k7} {z}, ymm3, qword ptr [edx - 1024]{1to4}
0x62,0xf6,0xe5,0xbf,0xd1,0x52,0x80

# ATT:   vsubaddpd  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vsubaddpd xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0xe5,0x08,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddpd  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vsubaddpd xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0xe5,0x0f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddpd  (%eax){1to2}, %xmm3, %xmm2
# INTEL: vsubaddpd xmm2, xmm3, qword ptr [eax]{1to2}
0x62,0xf6,0xe5,0x18,0xd1,0x10

# ATT:   vsubaddpd  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vsubaddpd xmm2, xmm3, xmmword ptr [2*ebp - 512]
0x62,0xf6,0xe5,0x08,0xd1,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vsubaddpd  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddpd xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0xe5,0x8f,0xd1,0x51,0x7f

# ATT:   vsubaddpd  -1024(%edx){1to2}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddpd xmm2 {k7} {z}, xmm3, qword ptr [edx - 1024]{1to2}
0x62,0xf6,0xe5,0x9f,0xd1,0x52,0x80

# ATT:   vsubaddph %ymm4, %ymm3, %ymm2
# INTEL: vsubaddph ymm2, ymm3, ymm4
0x62,0xf6,0x64,0x28,0xd1,0xd4

# ATT:   vsubaddph %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vsubaddph ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0x64,0x2f,0xd1,0xd4

# ATT:   vsubaddph %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddph ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0x64,0xaf,0xd1,0xd4

# ATT:   vsubaddph %xmm4, %xmm3, %xmm2
# INTEL: vsubaddph xmm2, xmm3, xmm4
0x62,0xf6,0x64,0x08,0xd1,0xd4

# ATT:   vsubaddph %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vsubaddph xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0x64,0x0f,0xd1,0xd4

# ATT:   vsubaddph %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddph xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0x64,0x8f,0xd1,0xd4

# ATT:   vsubaddph  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vsubaddph ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x64,0x28,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddph  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vsubaddph ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x64,0x2f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddph  (%eax){1to16}, %ymm3, %ymm2
# INTEL: vsubaddph ymm2, ymm3, word ptr [eax]{1to16}
0x62,0xf6,0x64,0x38,0xd1,0x10

# ATT:   vsubaddph  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vsubaddph ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0x62,0xf6,0x64,0x28,0xd1,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vsubaddph  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddph ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0x64,0xaf,0xd1,0x51,0x7f

# ATT:   vsubaddph  -256(%edx){1to16}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddph ymm2 {k7} {z}, ymm3, word ptr [edx - 256]{1to16}
0x62,0xf6,0x64,0xbf,0xd1,0x52,0x80

# ATT:   vsubaddph  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vsubaddph xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x64,0x08,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddph  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vsubaddph xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x64,0x0f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddph  (%eax){1to8}, %xmm3, %xmm2
# INTEL: vsubaddph xmm2, xmm3, word ptr [eax]{1to8}
0x62,0xf6,0x64,0x18,0xd1,0x10

# ATT:   vsubaddph  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vsubaddph xmm2, xmm3, xmmword ptr [2*ebp - 512]
0x62,0xf6,0x64,0x08,0xd1,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vsubaddph  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddph xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0x64,0x8f,0xd1,0x51,0x7f

# ATT:   vsubaddph  -256(%edx){1to8}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddph xmm2 {k7} {z}, xmm3, word ptr [edx - 256]{1to8}
0x62,0xf6,0x64,0x9f,0xd1,0x52,0x80

# ATT:   vsubaddps %ymm4, %ymm3, %ymm2
# INTEL: vsubaddps ymm2, ymm3, ymm4
0x62,0xf6,0x65,0x28,0xd1,0xd4

# ATT:   vsubaddps %ymm4, %ymm3, %ymm2 {%k7}
# INTEL: vsubaddps ymm2 {k7}, ymm3, ymm4
0x62,0xf6,0x65,0x2f,0xd1,0xd4

# ATT:   vsubaddps %ymm4, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddps ymm2 {k7} {z}, ymm3, ymm4
0x62,0xf6,0x65,0xaf,0xd1,0xd4

# ATT:   vsubaddps %xmm4, %xmm3, %xmm2
# INTEL: vsubaddps xmm2, xmm3, xmm4
0x62,0xf6,0x65,0x08,0xd1,0xd4

# ATT:   vsubaddps %xmm4, %xmm3, %xmm2 {%k7}
# INTEL: vsubaddps xmm2 {k7}, xmm3, xmm4
0x62,0xf6,0x65,0x0f,0xd1,0xd4

# ATT:   vsubaddps %xmm4, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddps xmm2 {k7} {z}, xmm3, xmm4
0x62,0xf6,0x65,0x8f,0xd1,0xd4

# ATT:   vsubaddps  268435456(%esp,%esi,8), %ymm3, %ymm2
# INTEL: vsubaddps ymm2, ymm3, ymmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x65,0x28,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddps  291(%edi,%eax,4), %ymm3, %ymm2 {%k7}
# INTEL: vsubaddps ymm2 {k7}, ymm3, ymmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x65,0x2f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddps  (%eax){1to8}, %ymm3, %ymm2
# INTEL: vsubaddps ymm2, ymm3, dword ptr [eax]{1to8}
0x62,0xf6,0x65,0x38,0xd1,0x10

# ATT:   vsubaddps  -1024(,%ebp,2), %ymm3, %ymm2
# INTEL: vsubaddps ymm2, ymm3, ymmword ptr [2*ebp - 1024]
0x62,0xf6,0x65,0x28,0xd1,0x14,0x6d,0x00,0xfc,0xff,0xff

# ATT:   vsubaddps  4064(%ecx), %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddps ymm2 {k7} {z}, ymm3, ymmword ptr [ecx + 4064]
0x62,0xf6,0x65,0xaf,0xd1,0x51,0x7f

# ATT:   vsubaddps  -512(%edx){1to8}, %ymm3, %ymm2 {%k7} {z}
# INTEL: vsubaddps ymm2 {k7} {z}, ymm3, dword ptr [edx - 512]{1to8}
0x62,0xf6,0x65,0xbf,0xd1,0x52,0x80

# ATT:   vsubaddps  268435456(%esp,%esi,8), %xmm3, %xmm2
# INTEL: vsubaddps xmm2, xmm3, xmmword ptr [esp + 8*esi + 268435456]
0x62,0xf6,0x65,0x08,0xd1,0x94,0xf4,0x00,0x00,0x00,0x10

# ATT:   vsubaddps  291(%edi,%eax,4), %xmm3, %xmm2 {%k7}
# INTEL: vsubaddps xmm2 {k7}, xmm3, xmmword ptr [edi + 4*eax + 291]
0x62,0xf6,0x65,0x0f,0xd1,0x94,0x87,0x23,0x01,0x00,0x00

# ATT:   vsubaddps  (%eax){1to4}, %xmm3, %xmm2
# INTEL: vsubaddps xmm2, xmm3, dword ptr [eax]{1to4}
0x62,0xf6,0x65,0x18,0xd1,0x10

# ATT:   vsubaddps  -512(,%ebp,2), %xmm3, %xmm2
# INTEL: vsubaddps xmm2, xmm3, xmmword ptr [2*ebp - 512]
0x62,0xf6,0x65,0x08,0xd1,0x14,0x6d,0x00,0xfe,0xff,0xff

# ATT:   vsubaddps  2032(%ecx), %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddps xmm2 {k7} {z}, xmm3, xmmword ptr [ecx + 2032]
0x62,0xf6,0x65,0x8f,0xd1,0x51,0x7f

# ATT:   vsubaddps  -512(%edx){1to4}, %xmm3, %xmm2 {%k7} {z}
# INTEL: vsubaddps xmm2 {k7} {z}, xmm3, dword ptr [edx - 512]{1to4}
0x62,0xf6,0x65,0x9f,0xd1,0x52,0x80

