; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_movrs
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avxmovrs -verify-machineinstrs --show-mc-encoding | FileCheck %s --check-prefixes=CHECK

declare <4 x i32> @llvm.x86.avx2.vmovadvisew.load.128(i8* %a, i8 %i) nounwind readonly
declare void @llvm.x86.avx2.vmovadvisew.store.128(i8*, <4 x i32>, i8 %i) nounwind
declare <8 x i32> @llvm.x86.avx2.vmovadvisew.load.256(i8* %a, i8 %i) nounwind readonly
declare void @llvm.x86.avx2.vmovadvisew.store.256(i8*, <8 x i32>, i8 %i) nounwind
declare void @llvm.x86.avx2.vmemadvise.128(i8* %a, i8 %i) nounwind
declare void @llvm.x86.avx2.vmemadvise.256(i8* %a, i8 %i) nounwind

define <4 x i32> @test_x86_avx2_vmovadvisew_load_128(i8* %a0) {
; CHECK-LABEL: test_x86_avx2_vmovadvisew_load_128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmovadvisew $16, (%rdi), %xmm0 # encoding: [0xc4,0xe3,0x7a,0x10,0x07,0x10]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <4 x i32> @llvm.x86.avx2.vmovadvisew.load.128(i8* %a0, i8 16)
  ret <4 x i32> %res
}

define void @test_x86_avx2_vmovadvisew_store_128(i8* %a0, <4 x i32> %a1) {
; CHECK-LABEL: test_x86_avx2_vmovadvisew_store_128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmovadvisew $16, %xmm0, (%rdi) # encoding: [0xc4,0xe3,0x7a,0x11,0x07,0x10]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.vmovadvisew.store.128(i8* %a0, <4 x i32> %a1, i8 16)
  ret void
}

define <8 x i32> @test_x86_avx2_vmovadvisew_load_256(i8* %a0) {
; CHECK-LABEL: test_x86_avx2_vmovadvisew_load_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmovadvisew $16, (%rdi), %ymm0 # encoding: [0xc4,0xe3,0x7e,0x10,0x07,0x10]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <8 x i32> @llvm.x86.avx2.vmovadvisew.load.256(i8* %a0, i8 16)
  ret <8 x i32> %res
}

define void @test_x86_avx2_vmovadvisew_store_256(i8* %a0, <8 x i32> %a1) {
; CHECK-LABEL: test_x86_avx2_vmovadvisew_store_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmovadvisew $16, %ymm0, (%rdi) # encoding: [0xc4,0xe3,0x7e,0x11,0x07,0x10]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.vmovadvisew.store.256(i8* %a0, <8 x i32> %a1, i8 16)
  ret void
}

define void @test_x86_avx2_vmemadvise_128(i8* %a0) {
; CHECK-LABEL: test_x86_avx2_vmemadvise_128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmemadvisex $123, (%rdi) # encoding: [0xc5,0xfb,0x71,0x07,0x7b]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.vmemadvise.128(i8* %a0, i8 123)
  ret void
}

define void @test_x86_avx2_vmemadvise_256(i8* %a0) {
; CHECK-LABEL: test_x86_avx2_vmemadvise_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    {vex} vmemadvisey $12, (%rdi) # encoding: [0xc5,0xff,0x71,0x07,0x0c]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.vmemadvise.256(i8* %a0, i8 12)
  ret void
}
