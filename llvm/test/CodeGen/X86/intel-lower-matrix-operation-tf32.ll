; INTEL_FEATURE_ISA_AMX_TF32
; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; REQUIRES: intel_feature_isa_amx_tf32
; RUN: opt -x86-lower-matrix-intrinsics -mtriple=x86_64-unknown-unknown -mcpu=diamondrapids %s -S | FileCheck %s

define void @load_mad_store_tf32(ptr addrspace(4) %ptrA, ptr addrspace(4) %ptrB, ptr addrspace(4) %ptrC, ptr addrspace(4) %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_tf32
; CHECK-SAME: (ptr addrspace(4) [[PTRA:%.*]], ptr addrspace(4) [[PTRB:%.*]], ptr addrspace(4) [[PTRC:%.*]], ptr addrspace(4) [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTRA]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEA]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTRB]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEB]], 4
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 16, i16 64, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <256 x float> @llvm.x86.cast.tile.to.vector.v256f32(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTRC]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f32(<256 x float> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tmmultf32ps.internal(i16 8, i16 64, i16 64, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(4) [[PTRD]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4(ptr addrspace(4) %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %B = call <256 x float> @llvm.experimental.matrix.load.v256f32.p4(ptr addrspace(4) %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4(ptr addrspace(4) %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float> %A, metadata !"matrix.rowmajor", <256 x float> %B, metadata !"matrix.rowmajor", <128 x float> %C, metadata !"matrix.dynamic", i32 8, i32 16, i32 16, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v128f32.p4(<128 x float> %D, ptr addrspace(4) %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_tf32_with_use(ptr addrspace(4) %ptrA, ptr addrspace(4) %ptrB, ptr addrspace(4) %ptrC, ptr addrspace(4) %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_tf32_with_use
; CHECK-SAME: (ptr addrspace(4) [[PTRA:%.*]], ptr addrspace(4) [[PTRB:%.*]], ptr addrspace(4) [[PTRC:%.*]], ptr addrspace(4) [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTRA]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEA]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTRB]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEB]], 4
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 16, i16 64, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <256 x float> @llvm.x86.cast.tile.to.vector.v256f32(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTRC]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f32(<256 x float> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tmmultf32ps.internal(i16 8, i16 64, i16 64, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(4) [[PTRD]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4(ptr addrspace(4) %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  %B = call <256 x float> @llvm.experimental.matrix.load.v256f32.p4(ptr addrspace(4) %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.b")
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4(ptr addrspace(4) %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float> %A, metadata !"matrix.rowmajor", <256 x float> %B, metadata !"matrix.rowmajor", <128 x float> %C, metadata !"matrix.dynamic", i32 8, i32 16, i32 16, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v128f32.p4(<128 x float> %D, ptr addrspace(4) %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret void
}

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float>, metadata, <256 x float>, metadata, <128 x float>, metadata, i32, i32, i32, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <128 x float> @llvm.experimental.matrix.load.v128f32.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <256 x float> @llvm.experimental.matrix.load.v256f32.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v128f32.p4(<128 x float>, ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

attributes #0 = { nocallback nofree nosync nounwind willreturn memory(none) }
attributes #1 = { nocallback nofree nosync nounwind willreturn memory(read) }
attributes #2 = { nocallback nofree nosync nounwind willreturn memory(write) }
; end INTEL_FEATURE_ISA_AMX_TF32
