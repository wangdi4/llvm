; INTEL_FEATURE_ISA_AMX_TF32
; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; REQUIRES: intel_feature_isa_amx_tf32
; RUN: opt -opaque-pointers=0 -x86-lower-matrix-intrinsics -mtriple=x86_64-unknown-unknown -mcpu=diamondrapids %s -S | FileCheck %s

define void @load_mad_store_tf32(float addrspace(4)* %ptrA, float addrspace(4)* %ptrB, float addrspace(4)* %ptrC, float addrspace(4)* %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_tf32
; CHECK-SAME: (float addrspace(4)* [[PTRA:%.*]], float addrspace(4)* [[PTRB:%.*]], float addrspace(4)* [[PTRC:%.*]], float addrspace(4)* [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast float addrspace(4)* [[PTRA]] to i8*
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEA]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, i8* [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast float addrspace(4)* [[PTRB]] to i8*
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEB]], 4
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 16, i16 64, i8* [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <256 x float> @llvm.x86.cast.tile.to.vector.v256f32(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast float addrspace(4)* [[PTRC]] to i8*
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, i8* [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f32(<256 x float> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tmmultf32ps.internal(i16 8, i16 64, i16 64, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast float addrspace(4)* [[PTRD]] to i8*
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, i8* [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4f32(float addrspace(4)* %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %B = call <256 x float> @llvm.experimental.matrix.load.v256f32.p4f32(float addrspace(4)* %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup",metadata !"matrix.use.unnecessary")
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4f32(float addrspace(4)* %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float> %A, metadata !"matrix.rowmajor", <256 x float> %B, metadata !"matrix.rowmajor", <128 x float> %C, metadata !"matrix.dynamic", i32 8, i32 16, i32 16, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v128f32.p4f32(<128 x float> %D, float addrspace(4)* %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_tf32_with_use(float addrspace(4)* %ptrA, float addrspace(4)* %ptrB, float addrspace(4)* %ptrC, float addrspace(4)* %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_tf32_with_use
; CHECK-SAME: (float addrspace(4)* [[PTRA:%.*]], float addrspace(4)* [[PTRB:%.*]], float addrspace(4)* [[PTRC:%.*]], float addrspace(4)* [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast float addrspace(4)* [[PTRA]] to i8*
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEA]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, i8* [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast float addrspace(4)* [[PTRB]] to i8*
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEB]], 4
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 16, i16 64, i8* [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <256 x float> @llvm.x86.cast.tile.to.vector.v256f32(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast float addrspace(4)* [[PTRC]] to i8*
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, i8* [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f32(<256 x float> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tmmultf32ps.internal(i16 8, i16 64, i16 64, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast float addrspace(4)* [[PTRD]] to i8*
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, i8* [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4f32(float addrspace(4)* %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  %B = call <256 x float> @llvm.experimental.matrix.load.v256f32.p4f32(float addrspace(4)* %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.b")
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p4f32(float addrspace(4)* %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float> %A, metadata !"matrix.rowmajor", <256 x float> %B, metadata !"matrix.rowmajor", <128 x float> %C, metadata !"matrix.dynamic", i32 8, i32 16, i32 16, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v128f32.p4f32(<128 x float> %D, float addrspace(4)* %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret void
}

declare <128 x float> @llvm.experimental.matrix.load.v128f32.p4f32(float addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata, metadata)
declare <256 x float> @llvm.experimental.matrix.load.v256f32.p4f32(float addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata, metadata)
declare <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f32.v256f32(<128 x float>, metadata, <256 x float>, metadata, <128 x float>, metadata, i32, i32, i32, metadata)
declare void @llvm.experimental.matrix.store.v128f32.p4f32(<128 x float>, float addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata, metadata)


; end INTEL_FEATURE_ISA_AMX_TF32
