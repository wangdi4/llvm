; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --no_x86_scrub_sp
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -vector-library=none -mattr=avx512f | FileCheck --check-prefix=X64 %s
; RUN: llc < %s -mtriple=i386-unknown-linux-gnu -vector-library=none -mattr=avx512f | FileCheck --check-prefix=X86 %s

define <8 x double> @svml_cc_v8i1(<8 x double> %x, <8 x i64> %mask) {
; X64-LABEL: svml_cc_v8i1:
; X64:       # %bb.0:
; X64-NEXT:    pushq %rax
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    vptestnmq %zmm1, %zmm1, %k1
; X64-NEXT:    callq *mysvml_v8f64_v8i1@GOTPCREL(%rip)
; X64-NEXT:    popq %rax
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    retq
;
; X86-LABEL: svml_cc_v8i1:
; X86:       # %bb.0:
; X86-NEXT:    subl $12, %esp
; X86-NEXT:    .cfi_def_cfa_offset 16
; X86-NEXT:    vptestnmq %zmm1, %zmm1, %k1
; X86-NEXT:    calll mysvml_v8f64_v8i1@PLT
; X86-NEXT:    addl $12, %esp
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
  %m = icmp eq <8 x i64> %mask, zeroinitializer
  %res = call svml_unified_cc_512 <8 x double> @mysvml_v8f64_v8i1(<8 x double> %x, <8 x i1> %m)
  ret <8 x double> %res
}

define <16 x float> @svml_cc_v16i1(<16 x float> %x, <16 x i32> %mask) {
; X64-LABEL: svml_cc_v16i1:
; X64:       # %bb.0:
; X64-NEXT:    pushq %rax
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    vptestnmd %zmm1, %zmm1, %k1
; X64-NEXT:    callq *mysvml_v16f32_v16i1@GOTPCREL(%rip)
; X64-NEXT:    popq %rax
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    retq
;
; X86-LABEL: svml_cc_v16i1:
; X86:       # %bb.0:
; X86-NEXT:    subl $12, %esp
; X86-NEXT:    .cfi_def_cfa_offset 16
; X86-NEXT:    vptestnmd %zmm1, %zmm1, %k1
; X86-NEXT:    calll mysvml_v16f32_v16i1@PLT
; X86-NEXT:    addl $12, %esp
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
  %m = icmp eq <16 x i32> %mask, zeroinitializer
  %res = call svml_unified_cc_512 <16 x float> @mysvml_v16f32_v16i1(<16 x float> %x, <16 x i1> %m)
  ret <16 x float> %res
}

%struct.__m512x2_t = type { <16 x float>, <16 x float> }

define dso_local svml_unified_cc_512 %struct.__m512x2_t @__svml_cdivf16_mask_l0(<16 x float> %r.0, <16 x float> %r.1, <16 x i1> %mask.coerce, <16 x float> %a.0, <16 x float> %a.1, <16 x float> %b.0, <16 x float> %b.1) {
; X64-LABEL: __svml_cdivf16_mask_l0:
; X64:       # %bb.0: # %entry
; X64-NEXT:    pushq %rsi
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    pushq %rdi
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    subq $1448, %rsp # imm = 0x5A8
; X64-NEXT:    kmovw %k7, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; X64-NEXT:    kmovw %k6, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; X64-NEXT:    kmovw %k5, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; X64-NEXT:    kmovw %k4, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; X64-NEXT:    vmovups %zmm31, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm30, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm29, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm28, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm27, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm26, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm25, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm24, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm23, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm22, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm21, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm20, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm19, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm18, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm17, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm16, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    .cfi_def_cfa_offset 1472
; X64-NEXT:    .cfi_offset %rdi, -24
; X64-NEXT:    .cfi_offset %rsi, -16
; X64-NEXT:    .cfi_offset %xmm16, -1088
; X64-NEXT:    .cfi_offset %xmm17, -1024
; X64-NEXT:    .cfi_offset %xmm18, -960
; X64-NEXT:    .cfi_offset %xmm19, -896
; X64-NEXT:    .cfi_offset %xmm20, -832
; X64-NEXT:    .cfi_offset %xmm21, -768
; X64-NEXT:    .cfi_offset %xmm22, -704
; X64-NEXT:    .cfi_offset %xmm23, -640
; X64-NEXT:    .cfi_offset %xmm24, -576
; X64-NEXT:    .cfi_offset %xmm25, -512
; X64-NEXT:    .cfi_offset %xmm26, -448
; X64-NEXT:    .cfi_offset %xmm27, -384
; X64-NEXT:    .cfi_offset %xmm28, -320
; X64-NEXT:    .cfi_offset %xmm29, -256
; X64-NEXT:    .cfi_offset %xmm30, -192
; X64-NEXT:    .cfi_offset %xmm31, -128
; X64-NEXT:    .cfi_offset %k4, -32
; X64-NEXT:    .cfi_offset %k5, -30
; X64-NEXT:    .cfi_offset %k6, -28
; X64-NEXT:    .cfi_offset %k7, -26
; X64-NEXT:    vmovups %zmm5, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    vmovups %zmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; X64-NEXT:    vmovups %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq use_m512
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    callq use_m512
; X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k0 # 2-byte Reload
; X64-NEXT:    kmovw %k0, %edi
; X64-NEXT:    vzeroupper
; X64-NEXT:    callq use_int
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    callq use_m512
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    callq use_m512
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    callq use_m512
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    callq use_m512
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovaps %zmm0, %zmm1
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm16 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm17 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm18 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm19 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm20 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm21 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm22 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm23 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm24 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm25 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm26 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm27 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm28 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm29 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm30 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm31 # 64-byte Reload
; X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k4 # 2-byte Reload
; X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k5 # 2-byte Reload
; X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k6 # 2-byte Reload
; X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k7 # 2-byte Reload
; X64-NEXT:    addq $1448, %rsp # imm = 0x5A8
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    popq %rdi
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    popq %rsi
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    retq
;
; X86-LABEL: __svml_cdivf16_mask_l0:
; X86:       # %bb.0: # %entry
; X86-NEXT:    subl $348, %esp # imm = 0x15C
; X86-NEXT:    kmovw %k7, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; X86-NEXT:    kmovw %k6, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; X86-NEXT:    kmovw %k5, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; X86-NEXT:    kmovw %k4, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; X86-NEXT:    .cfi_def_cfa_offset 352
; X86-NEXT:    .cfi_offset %k4, -12
; X86-NEXT:    .cfi_offset %k5, -10
; X86-NEXT:    .cfi_offset %k6, -8
; X86-NEXT:    .cfi_offset %k7, -6
; X86-NEXT:    vmovups %zmm5, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    vmovups %zmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    vmovups %zmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    vmovups %zmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; X86-NEXT:    vmovups %zmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll use_m512
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    calll use_m512
; X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k0 # 2-byte Reload
; X86-NEXT:    kmovw %k0, %eax
; X86-NEXT:    movl %eax, (%esp)
; X86-NEXT:    vzeroupper
; X86-NEXT:    calll use_int
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    calll use_m512
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    calll use_m512
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    calll use_m512
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    calll use_m512
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovaps %zmm0, %zmm1
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm0 # 64-byte Reload
; X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k4 # 2-byte Reload
; X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k5 # 2-byte Reload
; X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k6 # 2-byte Reload
; X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k7 # 2-byte Reload
; X86-NEXT:    addl $348, %esp # imm = 0x15C
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
entry:
  %0 = bitcast <16 x i1> %mask.coerce to i16
  tail call void @use_m512(<16 x float> %r.0)
  tail call void @use_m512(<16 x float> %r.1)
  %conv = zext i16 %0 to i32
  tail call void @use_int(i32 %conv)
  tail call void @use_m512(<16 x float> %a.0)
  tail call void @use_m512(<16 x float> %a.1)
  tail call void @use_m512(<16 x float> %b.0)
  tail call void @use_m512(<16 x float> %b.1)
  %call = tail call fast <16 x float> @get_m512()
  %call16 = tail call fast <16 x float> @get_m512()
  %.fca.0.insert = insertvalue %struct.__m512x2_t undef, <16 x float> %call, 0
  %.fca.1.insert = insertvalue %struct.__m512x2_t %.fca.0.insert, <16 x float> %call16, 1
  ret %struct.__m512x2_t %.fca.1.insert
}

define dso_local void @svml_cc_call_cdivf16() {
; X64-LABEL: svml_cc_call_cdivf16:
; X64:       # %bb.0: # %entry
; X64-NEXT:    subq $328, %rsp # imm = 0x148
; X64-NEXT:    .cfi_def_cfa_offset 336
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, (%rsp) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    callq get_m512
; X64-NEXT:    vmovaps %zmm0, %zmm5
; X64-NEXT:    movw $26213, %ax # imm = 0x6665
; X64-NEXT:    kmovw %eax, %k1
; X64-NEXT:    vmovups (%rsp), %zmm0 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm1 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm2 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm3 # 64-byte Reload
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm4 # 64-byte Reload
; X64-NEXT:    callq *__svml_cdivf16_mask_l0@GOTPCREL(%rip)
; X64-NEXT:    vmovups %zmm1, (%rsp) # 64-byte Spill
; X64-NEXT:    callq use_m512
; X64-NEXT:    vmovups (%rsp), %zmm0 # 64-byte Reload
; X64-NEXT:    addq $328, %rsp # imm = 0x148
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    jmp use_m512 # TAILCALL
;
; X86-LABEL: svml_cc_call_cdivf16:
; X86:       # %bb.0: # %entry
; X86-NEXT:    subl $332, %esp # imm = 0x14C
; X86-NEXT:    .cfi_def_cfa_offset 336
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, (%esp) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 64-byte Spill
; X86-NEXT:    calll get_m512
; X86-NEXT:    vmovaps %zmm0, %zmm5
; X86-NEXT:    movw $26213, %ax # imm = 0x6665
; X86-NEXT:    kmovw %eax, %k1
; X86-NEXT:    vmovups (%esp), %zmm0 # 64-byte Reload
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm1 # 64-byte Reload
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm2 # 64-byte Reload
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm3 # 64-byte Reload
; X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %zmm4 # 64-byte Reload
; X86-NEXT:    calll __svml_cdivf16_mask_l0
; X86-NEXT:    vmovups %zmm1, (%esp) # 64-byte Spill
; X86-NEXT:    calll use_m512
; X86-NEXT:    vmovups (%esp), %zmm0 # 64-byte Reload
; X86-NEXT:    addl $332, %esp # imm = 0x14C
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    jmp use_m512 # TAILCALL
entry:
  %call = tail call fast <16 x float> @get_m512()
  %call1 = tail call fast <16 x float> @get_m512()
  %call3 = tail call fast <16 x float> @get_m512()
  %call5 = tail call fast <16 x float> @get_m512()
  %call7 = tail call fast <16 x float> @get_m512()
  %call9 = tail call fast <16 x float> @get_m512()
  %call16 = tail call svml_unified_cc_512 %struct.__m512x2_t @__svml_cdivf16_mask_l0(<16 x float> %call, <16 x float> %call1, <16 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 true, i1 true, i1 false, i1 false, i1 true, i1 true, i1 false, i1 false, i1 true, i1 true, i1 false>, <16 x float> %call3, <16 x float> %call5, <16 x float> %call7, <16 x float> %call9)
  %0 = extractvalue %struct.__m512x2_t %call16, 0
  %1 = extractvalue %struct.__m512x2_t %call16, 1
  tail call void @use_m512(<16 x float> %0)
  tail call void @use_m512(<16 x float> %1)
  ret void
}

declare <8 x double> @mysvml_v8f64_v8i1(<8 x double>, <8 x i1>)
declare <16 x float> @mysvml_v16f32_v16i1(<16 x float>, <16 x i1>)
declare dso_local void @use_m512(<16 x float>)
declare dso_local void @use_int(i32)
declare dso_local <16 x float> @get_m512()
