; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_rao_fp
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512raofp,+avx512fp16 | FileCheck %s --check-prefixes=X64,X64-AVX512
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512raofp,+avx512fp16 | FileCheck %s --check-prefixes=X86,X86-AVX512
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512raofp,+avx512fp16,+avxraofp | FileCheck %s --check-prefixes=X64,X64-AVX
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512raofp,+avx512fp16,+avxraofp | FileCheck %s --check-prefixes=X86,X86-AVX

define void @test_int_x86_vaaddpbf16128(i8* %A, <8 x i16> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddpbf16128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddpbf16 %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7d,0x08,0x94,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddpbf16128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddpbf16 %xmm0, (%eax) # encoding: [0x62,0xf2,0x7d,0x08,0x94,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddpbf16128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddpbf16 %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x94,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddpbf16128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddpbf16 %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x94,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddpbf16128(i8* %A, <8 x i16> %B)
  ret  void
}
declare void @llvm.x86.vaaddpbf16128(i8* %A, <8 x i16> %B)

define void @test_int_x86_mask_vaaddpbf16128(i8* %A, <8 x i16> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddpbf16128:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddpbf16 %xmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0x94,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddpbf16128:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddpbf16 %xmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0x94,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddpbf16128(i8* %A, <8 x i16> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddpbf16128(i8* %A, <8 x i16> %B, i8 %C)

define void @test_int_x86_vaaddpbf16256(i8* %A, <16 x i16> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddpbf16256:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddpbf16 %ymm0, (%rdi) # encoding: [0x62,0xf2,0x7d,0x28,0x94,0x07]
; X64-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddpbf16256:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddpbf16 %ymm0, (%eax) # encoding: [0x62,0xf2,0x7d,0x28,0x94,0x00]
; X86-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddpbf16256:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddpbf16 %ymm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7d,0x94,0x07]
; X64-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddpbf16256:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddpbf16 %ymm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7d,0x94,0x00]
; X86-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddpbf16256(i8* %A, <16 x i16> %B)
  ret  void
}
declare void @llvm.x86.vaaddpbf16256(i8* %A, <16 x i16> %B)

define void @test_int_x86_mask_vaaddpbf16256(i8* %A, <16 x i16> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vaaddpbf16256:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddpbf16 %ymm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0x94,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddpbf16256:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddpbf16 %ymm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0x94,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddpbf16256(i8* %A, <16 x i16> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddpbf16256(i8* %A, <16 x i16> %B, i16 %C)

define void @test_int_x86_vaaddpd128(i8* %A, <2 x double> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddpd128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddpd %xmm0, (%rdi) # encoding: [0x62,0xf2,0xfd,0x08,0x84,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddpd128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddpd %xmm0, (%eax) # encoding: [0x62,0xf2,0xfd,0x08,0x84,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddpd128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddpd %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xf9,0x84,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddpd128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddpd %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xf9,0x84,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddpd128(i8* %A, <2 x double> %B)
  ret  void
}
declare void @llvm.x86.vaaddpd128(i8* %A, <2 x double> %B)

define void @test_int_x86_mask_vaaddpd128(i8* %A, <2 x double> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddpd128:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddpd %xmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xfd,0x09,0x84,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddpd128:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddpd %xmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xfd,0x09,0x84,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddpd128(i8* %A, <2 x double> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddpd128(i8* %A, <2 x double> %B, i8 %C)

define void @test_int_x86_vaaddpd256(i8* %A, <4 x double> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddpd256:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddpd %ymm0, (%rdi) # encoding: [0x62,0xf2,0xfd,0x28,0x84,0x07]
; X64-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddpd256:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddpd %ymm0, (%eax) # encoding: [0x62,0xf2,0xfd,0x28,0x84,0x00]
; X86-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddpd256:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddpd %ymm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xfd,0x84,0x07]
; X64-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddpd256:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddpd %ymm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xfd,0x84,0x00]
; X86-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddpd256(i8* %A, <4 x double> %B)
  ret  void
}
declare void @llvm.x86.vaaddpd256(i8* %A, <4 x double> %B)

define void @test_int_x86_mask_vaaddpd256(i8* %A, <4 x double> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddpd256:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddpd %ymm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xfd,0x29,0x84,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddpd256:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddpd %ymm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xfd,0x29,0x84,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddpd256(i8* %A, <4 x double> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddpd256(i8* %A, <4 x double> %B, i8 %C)

define void @test_int_x86_vaaddph128(i8* %A, <8 x half> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddph128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddph %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7c,0x08,0x94,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddph128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddph %xmm0, (%eax) # encoding: [0x62,0xf2,0x7c,0x08,0x94,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddph128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddph %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x78,0x94,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddph128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddph %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x78,0x94,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddph128(i8* %A, <8 x half> %B)
  ret  void
}
declare void @llvm.x86.vaaddph128(i8* %A, <8 x half> %B)

define void @test_int_x86_mask_vaaddph128(i8* %A, <8 x half> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddph128:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddph %xmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x94,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddph128:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddph %xmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x94,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddph128(i8* %A, <8 x half> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddph128(i8* %A, <8 x half> %B, i8 %C)

define void @test_int_x86_vaaddph256(i8* %A, <16 x half> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddph256:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddph %ymm0, (%rdi) # encoding: [0x62,0xf2,0x7c,0x28,0x94,0x07]
; X64-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddph256:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddph %ymm0, (%eax) # encoding: [0x62,0xf2,0x7c,0x28,0x94,0x00]
; X86-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddph256:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddph %ymm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7c,0x94,0x07]
; X64-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddph256:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddph %ymm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7c,0x94,0x00]
; X86-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddph256(i8* %A, <16 x half> %B)
  ret  void
}
declare void @llvm.x86.vaaddph256(i8* %A, <16 x half> %B)

define void @test_int_x86_mask_vaaddph256(i8* %A, <16 x half> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vaaddph256:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddph %ymm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x94,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddph256:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddph %ymm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x94,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddph256(i8* %A, <16 x half> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddph256(i8* %A, <16 x half> %B, i16 %C)

define void @test_int_x86_vaaddps128(i8* %A, <4 x float> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddps128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddps %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7c,0x08,0x84,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddps128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddps %xmm0, (%eax) # encoding: [0x62,0xf2,0x7c,0x08,0x84,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddps128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddps %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x78,0x84,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddps128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddps %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x78,0x84,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddps128(i8* %A, <4 x float> %B)
  ret  void
}
declare void @llvm.x86.vaaddps128(i8* %A, <4 x float> %B)

define void @test_int_x86_mask_vaaddps128(i8* %A, <4 x float> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddps %xmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x84,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddps128:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddps %xmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x84,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddps128(i8* %A, <4 x float> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddps128(i8* %A, <4 x float> %B, i8 %C)

define void @test_int_x86_vaaddps256(i8* %A, <8 x float> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddps256:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddps %ymm0, (%rdi) # encoding: [0x62,0xf2,0x7c,0x28,0x84,0x07]
; X64-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddps256:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddps %ymm0, (%eax) # encoding: [0x62,0xf2,0x7c,0x28,0x84,0x00]
; X86-AVX512-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddps256:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddps %ymm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7c,0x84,0x07]
; X64-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddps256:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddps %ymm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7c,0x84,0x00]
; X86-AVX-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddps256(i8* %A, <8 x float> %B)
  ret  void
}
declare void @llvm.x86.vaaddps256(i8* %A, <8 x float> %B)

define void @test_int_x86_mask_vaaddps256(i8* %A, <8 x float> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vaaddps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vaaddps %ymm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x84,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vaaddps256:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vaaddps %ymm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x84,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vaaddps256(i8* %A, <8 x float> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vaaddps256(i8* %A, <8 x float> %B, i8 %C)

define void @test_int_x86_vaaddsbf16128(i8* %A, <8 x i16> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddsbf16128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddsbf16 %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7f,0x08,0x94,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddsbf16128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddsbf16 %xmm0, (%eax) # encoding: [0x62,0xf2,0x7f,0x08,0x94,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddsbf16128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddsbf16 %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7b,0x94,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddsbf16128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddsbf16 %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7b,0x94,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddsbf16128(i8* %A, <8 x i16> %B)
  ret  void
}
declare void @llvm.x86.vaaddsbf16128(i8* %A, <8 x i16> %B)

define void @test_int_x86_vaaddsd128(i8* %A, <2 x double> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddsd128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddsd %xmm0, (%rdi) # encoding: [0x62,0xf2,0xff,0x08,0x84,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddsd128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddsd %xmm0, (%eax) # encoding: [0x62,0xf2,0xff,0x08,0x84,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddsd128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddsd %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xfb,0x84,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddsd128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddsd %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0xfb,0x84,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddsd128(i8* %A, <2 x double> %B)
  ret  void
}
declare void @llvm.x86.vaaddsd128(i8* %A, <2 x double> %B)

define void @test_int_x86_vaaddsh128(i8* %A, <8 x half> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddsh128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddsh %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7e,0x08,0x94,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddsh128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddsh %xmm0, (%eax) # encoding: [0x62,0xf2,0x7e,0x08,0x94,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddsh128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddsh %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7a,0x94,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddsh128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddsh %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7a,0x94,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddsh128(i8* %A, <8 x half> %B)
  ret  void
}
declare void @llvm.x86.vaaddsh128(i8* %A, <8 x half> %B)

define void @test_int_x86_vaaddss128(i8* %A, <4 x float> %B) {
; X64-AVX512-LABEL: test_int_x86_vaaddss128:
; X64-AVX512:       # %bb.0:
; X64-AVX512-NEXT:    vaaddss %xmm0, (%rdi) # encoding: [0x62,0xf2,0x7e,0x08,0x84,0x07]
; X64-AVX512-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX512-LABEL: test_int_x86_vaaddss128:
; X86-AVX512:       # %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX512-NEXT:    vaaddss %xmm0, (%eax) # encoding: [0x62,0xf2,0x7e,0x08,0x84,0x00]
; X86-AVX512-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX-LABEL: test_int_x86_vaaddss128:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vaaddss %xmm0, (%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7a,0x84,0x07]
; X64-AVX-NEXT:    retq # encoding: [0xc3]
;
; X86-AVX-LABEL: test_int_x86_vaaddss128:
; X86-AVX:       # %bb.0:
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-AVX-NEXT:    vaaddss %xmm0, (%eax) # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7a,0x84,0x00]
; X86-AVX-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vaaddss128(i8* %A, <4 x float> %B)
  ret  void
}
declare void @llvm.x86.vaaddss128(i8* %A, <4 x float> %B)
