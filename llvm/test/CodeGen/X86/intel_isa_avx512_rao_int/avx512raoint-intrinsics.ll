; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_rao_int
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512raoint | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512raoint | FileCheck %s --check-prefixes=X86

define void @test_int_x86_vpaaddd512(i8* %A, <16 x i32> %B) {
; X64-LABEL: test_int_x86_vpaaddd512:
; X64:       # %bb.0:
; X64-NEXT:    vpaaddd %zmm0, (%rdi) # encoding: [0x62,0xf2,0x7c,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaaddd512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaaddd %zmm0, (%eax) # encoding: [0x62,0xf2,0x7c,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaaddd512(i8* %A, <16 x i32> %B)
  ret  void
}
declare void @llvm.x86.vpaaddd512(i8* %A, <16 x i32> %B)

define void @test_int_x86_mask_vpaaddd512(i8* %A, <16 x i32> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vpaaddd512:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaaddd %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaaddd512:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaaddd %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaaddd512(i8* %A, <16 x i32> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaaddd512(i8* %A, <16 x i32> %B, i16 %C)

define void @test_int_x86_vpaaddq512(i8* %A, <8 x i64> %B) {
; X64-LABEL: test_int_x86_vpaaddq512:
; X64:       # %bb.0:
; X64-NEXT:    vpaaddq %zmm0, (%rdi) # encoding: [0x62,0xf2,0xfc,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaaddq512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaaddq %zmm0, (%eax) # encoding: [0x62,0xf2,0xfc,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaaddq512(i8* %A, <8 x i64> %B)
  ret  void
}
declare void @llvm.x86.vpaaddq512(i8* %A, <8 x i64> %B)

define void @test_int_x86_mask_vpaaddq512(i8* %A, <8 x i64> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vpaaddq512:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaaddq %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xfc,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaaddq512:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaaddq %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xfc,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaaddq512(i8* %A, <8 x i64> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaaddq512(i8* %A, <8 x i64> %B, i8 %C)

define void @test_int_x86_vpaandd512(i8* %A, <16 x i32> %B) {
; X64-LABEL: test_int_x86_vpaandd512:
; X64:       # %bb.0:
; X64-NEXT:    vpaandd %zmm0, (%rdi) # encoding: [0x62,0xf2,0x7d,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaandd512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaandd %zmm0, (%eax) # encoding: [0x62,0xf2,0x7d,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaandd512(i8* %A, <16 x i32> %B)
  ret  void
}
declare void @llvm.x86.vpaandd512(i8* %A, <16 x i32> %B)

define void @test_int_x86_mask_vpaandd512(i8* %A, <16 x i32> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vpaandd512:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaandd %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaandd512:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaandd %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaandd512(i8* %A, <16 x i32> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaandd512(i8* %A, <16 x i32> %B, i16 %C)

define void @test_int_x86_vpaandq512(i8* %A, <8 x i64> %B) {
; X64-LABEL: test_int_x86_vpaandq512:
; X64:       # %bb.0:
; X64-NEXT:    vpaandq %zmm0, (%rdi) # encoding: [0x62,0xf2,0xfd,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaandq512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaandq %zmm0, (%eax) # encoding: [0x62,0xf2,0xfd,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaandq512(i8* %A, <8 x i64> %B)
  ret  void
}
declare void @llvm.x86.vpaandq512(i8* %A, <8 x i64> %B)

define void @test_int_x86_mask_vpaandq512(i8* %A, <8 x i64> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vpaandq512:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaandq %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xfd,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaandq512:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaandq %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xfd,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaandq512(i8* %A, <8 x i64> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaandq512(i8* %A, <8 x i64> %B, i8 %C)

define void @test_int_x86_vpaord512(i8* %A, <16 x i32> %B) {
; X64-LABEL: test_int_x86_vpaord512:
; X64:       # %bb.0:
; X64-NEXT:    vpaord %zmm0, (%rdi) # encoding: [0x62,0xf2,0x7f,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaord512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaord %zmm0, (%eax) # encoding: [0x62,0xf2,0x7f,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaord512(i8* %A, <16 x i32> %B)
  ret  void
}
declare void @llvm.x86.vpaord512(i8* %A, <16 x i32> %B)

define void @test_int_x86_mask_vpaord512(i8* %A, <16 x i32> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vpaord512:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaord %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaord512:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaord %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaord512(i8* %A, <16 x i32> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaord512(i8* %A, <16 x i32> %B, i16 %C)

define void @test_int_x86_vpaorq512(i8* %A, <8 x i64> %B) {
; X64-LABEL: test_int_x86_vpaorq512:
; X64:       # %bb.0:
; X64-NEXT:    vpaorq %zmm0, (%rdi) # encoding: [0x62,0xf2,0xff,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaorq512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaorq %zmm0, (%eax) # encoding: [0x62,0xf2,0xff,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaorq512(i8* %A, <8 x i64> %B)
  ret  void
}
declare void @llvm.x86.vpaorq512(i8* %A, <8 x i64> %B)

define void @test_int_x86_mask_vpaorq512(i8* %A, <8 x i64> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vpaorq512:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaorq %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xff,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaorq512:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaorq %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xff,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaorq512(i8* %A, <8 x i64> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaorq512(i8* %A, <8 x i64> %B, i8 %C)

define void @test_int_x86_vpaxord512(i8* %A, <16 x i32> %B) {
; X64-LABEL: test_int_x86_vpaxord512:
; X64:       # %bb.0:
; X64-NEXT:    vpaxord %zmm0, (%rdi) # encoding: [0x62,0xf2,0x7e,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaxord512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaxord %zmm0, (%eax) # encoding: [0x62,0xf2,0x7e,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaxord512(i8* %A, <16 x i32> %B)
  ret  void
}
declare void @llvm.x86.vpaxord512(i8* %A, <16 x i32> %B)

define void @test_int_x86_mask_vpaxord512(i8* %A, <16 x i32> %B, i16 %C) {
; X64-LABEL: test_int_x86_mask_vpaxord512:
; X64:       # %bb.0:
; X64-NEXT:    movw %si, %cx # encoding: [0x66,0x89,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaxord %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaxord512:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaxord %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaxord512(i8* %A, <16 x i32> %B, i16 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaxord512(i8* %A, <16 x i32> %B, i16 %C)

define void @test_int_x86_vpaxorq512(i8* %A, <8 x i64> %B) {
; X64-LABEL: test_int_x86_vpaxorq512:
; X64:       # %bb.0:
; X64-NEXT:    vpaxorq %zmm0, (%rdi) # encoding: [0x62,0xf2,0xfe,0x48,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpaxorq512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vpaxorq %zmm0, (%eax) # encoding: [0x62,0xf2,0xfe,0x48,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.vpaxorq512(i8* %A, <8 x i64> %B)
  ret  void
}
declare void @llvm.x86.vpaxorq512(i8* %A, <8 x i64> %B)

define void @test_int_x86_mask_vpaxorq512(i8* %A, <8 x i64> %B, i8 %C) {
; X64-LABEL: test_int_x86_mask_vpaxorq512:
; X64:       # %bb.0:
; X64-NEXT:    movb %sil, %cl # encoding: [0x40,0x88,0xf1]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovw %eax, %k1 # encoding: [0xc5,0xf8,0x92,0xc8]
; X64-NEXT:    vpaxorq %zmm0, (%rdi) {%k1} # encoding: [0x62,0xf2,0xfe,0x49,0xfc,0x07]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpaxorq512:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x08]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovw %ecx, %k1 # encoding: [0xc5,0xf8,0x92,0xc9]
; X86-NEXT:    vpaxorq %zmm0, (%eax) {%k1} # encoding: [0x62,0xf2,0xfe,0x49,0xfc,0x00]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  call void @llvm.x86.mask.vpaxorq512(i8* %A, <8 x i64> %B, i8 %C)
  ret  void
}
declare void @llvm.x86.mask.vpaxorq512(i8* %A, <8 x i64> %B, i8 %C)

