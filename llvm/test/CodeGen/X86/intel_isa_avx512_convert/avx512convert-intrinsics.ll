; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_convert
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+avx512convert,+avx512fp16 --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512convert,+avx512fp16 --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X64

declare <32 x half> @llvm.x86.avx512.mask.vcvt2ps2ph.512(<16 x float>, <16 x float>, <32 x half>, i32, i32)

define <32 x half>@test_int_x86_avx512_mask_vcvt2ps2ph_512(<16 x float> %x0, <16 x float> %x1, <32 x half> %x2, i32 %x3) {
; X86-LABEL: test_int_x86_avx512_mask_vcvt2ps2ph_512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvt2ps2ph {ru-sae}, %zmm1, %zmm0, %zmm2 {%k1} # encoding: [0x62,0xf2,0x7d,0x59,0x67,0xd1]
; X86-NEXT:    vcvt2ps2ph {rn-sae}, %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x18,0x67,0xc1]
; X86-NEXT:    vaddph %zmm0, %zmm2, %zmm0 # encoding: [0x62,0xf5,0x6c,0x48,0x58,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvt2ps2ph_512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvt2ps2ph {ru-sae}, %zmm1, %zmm0, %zmm2 {%k1} # encoding: [0x62,0xf2,0x7d,0x59,0x67,0xd1]
; X64-NEXT:    vcvt2ps2ph {rn-sae}, %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x18,0x67,0xc1]
; X64-NEXT:    vaddph %zmm0, %zmm2, %zmm0 # encoding: [0x62,0xf5,0x6c,0x48,0x58,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <32 x half> @llvm.x86.avx512.mask.vcvt2ps2ph.512(<16 x float> %x0, <16 x float> %x1, <32 x half> %x2, i32 %x3, i32 10)
  %res1 = call <32 x half> @llvm.x86.avx512.mask.vcvt2ps2ph.512(<16 x float> %x0, <16 x float> %x1, <32 x half> %x2, i32 -1, i32 8)
  %res2 = fadd <32 x half> %res, %res1
  ret <32 x half> %res2
}

declare <32 x half> @llvm.x86.avx512.mask.vcvtbf162ph.512(<32 x i16>,  <32 x half>, i32, i32)

define <32 x half>@test_int_x86_avx512_mask_vcvtbf162ph_512(<32 x i16> %x0, <32 x half> %x1, i32 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbf162ph {ru-sae}, %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x59,0x67,0xc8]
; X86-NEXT:    vcvtbf162ph {rn-sae}, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7e,0x18,0x67,0xc0]
; X86-NEXT:    vaddph %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf5,0x74,0x48,0x58,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbf162ph {ru-sae}, %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x59,0x67,0xc8]
; X64-NEXT:    vcvtbf162ph {rn-sae}, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7e,0x18,0x67,0xc0]
; X64-NEXT:    vaddph %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf5,0x74,0x48,0x58,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <32 x half> @llvm.x86.avx512.mask.vcvtbf162ph.512(<32 x i16> %x0, <32 x half> %x1, i32 %x2, i32 10)
  %res1 = call <32 x half> @llvm.x86.avx512.mask.vcvtbf162ph.512(<32 x i16> %x0, <32 x half> %x1, i32 -1, i32 8)
  %res2 = fadd <32 x half> %res, %res1
  ret <32 x half> %res2
}

declare <32 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.512(<32 x half>, <32 x i16>, i32)

define <32 x i16>@test_int_x86_avx512_mask_vcvtneph2bf16_512(<32 x half> %x0, <32 x i16> %x1, i32 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf16 %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0x67,0xc8]
; X86-NEXT:    vcvtneph2bf16 %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0x67,0xc0]
; X86-NEXT:    vpaddw %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf1,0x75,0x48,0xfd,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf16 %zmm0, %zmm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0x67,0xc8]
; X64-NEXT:    vcvtneph2bf16 %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0x67,0xc0]
; X64-NEXT:    vpaddw %zmm0, %zmm1, %zmm0 # encoding: [0x62,0xf1,0x75,0x48,0xfd,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <32 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.512(<32 x half> %x0, <32 x i16> %x1, i32 %x2)
  %res1 = call <32 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.512(<32 x half> %x0, <32 x i16> %x1, i32 -1)
  %res2 = add <32 x i16> %res, %res1
  ret <32 x i16> %res2
}
