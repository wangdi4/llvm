; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_convert
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+avx512vl,+avx512convert,+avx512fp16 --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512vl,+avx512convert,+avx512fp16 --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X64

declare <8 x half> @llvm.x86.avx512.mask.vcvtbf162ph.128(<8 x i16>,  <8 x half>, i8)

define <8 x half>@test_int_x86_avx512_vcvtbf162ph_128(<8 x i16> %x0) {
; CHECK-LABEL: test_int_x86_avx512_vcvtbf162ph_128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbf162ph %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7a,0x67,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %res = call <8 x half> @llvm.x86.avx512.mask.vcvtbf162ph.128(<8 x i16> %x0, <8 x half> undef, i8 -1)
  ret <8 x half> %res
}

define <8 x half>@test_int_x86_avx512_mask_vcvtbf162ph_128(<8 x i16> %x0, <8 x half> %x1, i8 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbf162ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0x67,0xc8]
; X86-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbf162ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0x67,0xc8]
; X64-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <8 x half> @llvm.x86.avx512.mask.vcvtbf162ph.128(<8 x i16> %x0, <8 x half> %x1, i8 %x2)
  ret <8 x half> %res
}

define <8 x half>@test_int_x86_avx512_maskz_vcvtbf162ph_128(<8 x i16> %x0, i8 %x2) {
; X86-LABEL: test_int_x86_avx512_maskz_vcvtbf162ph_128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbf162ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0x89,0x67,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_maskz_vcvtbf162ph_128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbf162ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0x89,0x67,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <8 x half> @llvm.x86.avx512.mask.vcvtbf162ph.128(<8 x i16> %x0, <8 x half> zeroinitializer, i8 %x2)
  ret <8 x half> %res
}

declare <16 x half> @llvm.x86.avx512.mask.vcvtbf162ph.256(<16 x i16>,  <16 x half>, i16)

define <16 x half>@test_int_x86_avx512_vcvtbf162ph_256(<16 x i16> %x0) {
; CHECK-LABEL: test_int_x86_avx512_vcvtbf162ph_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbf162ph %ymm0, %ymm0 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7e,0x67,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %res = call <16 x half> @llvm.x86.avx512.mask.vcvtbf162ph.256(<16 x i16> %x0, <16 x half> undef, i16 -1)
  ret <16 x half> %res
}

define <16 x half>@test_int_x86_avx512_mask_vcvtbf162ph_256(<16 x i16> %x0, <16 x half> %x1, i16 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbf162ph %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0x67,0xc8]
; X86-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtbf162ph_256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbf162ph %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0x67,0xc8]
; X64-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <16 x half> @llvm.x86.avx512.mask.vcvtbf162ph.256(<16 x i16> %x0, <16 x half> %x1, i16 %x2)
  ret <16 x half> %res
}

define <16 x half>@test_int_x86_avx512_maskz_vcvtbf162ph_256(<16 x i16> %x0, i16 %x2) {
; X86-LABEL: test_int_x86_avx512_maskz_vcvtbf162ph_256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbf162ph %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0xa9,0x67,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_maskz_vcvtbf162ph_256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbf162ph %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0xa9,0x67,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <16 x half> @llvm.x86.avx512.mask.vcvtbf162ph.256(<16 x i16> %x0, <16 x half> zeroinitializer, i16 %x2)
  ret <16 x half> %res
}

declare <8 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.128(<8 x half>, <8 x i16>, i8)

define <8 x i16>@test_int_x86_avx512_vcvtneph2bf16_128(<8 x half> %x0) {
; CHECK-LABEL: test_int_x86_avx512_vcvtneph2bf16_128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf16 %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7b,0x67,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %res = call <8 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.128(<8 x half> %x0, <8 x i16> undef, i8 -1)
  ret <8 x i16> %res
}

define <8 x i16>@test_int_x86_avx512_mask_vcvtneph2bf16_128(<8 x half> %x0, <8 x i16> %x1, i8 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0x67,0xc8]
; X86-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf16 %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0x67,0xc8]
; X64-NEXT:    vmovaps %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <8 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.128(<8 x half> %x0, <8 x i16> %x1, i8 %x2)
  ret <8 x i16> %res
}

define <8 x i16>@test_int_x86_avx512_maskz_vcvtneph2bf16_128(<8 x half> %x0, i8 %x2) {
; X86-LABEL: test_int_x86_avx512_maskz_vcvtneph2bf16_128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7f,0x89,0x67,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_maskz_vcvtneph2bf16_128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf16 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7f,0x89,0x67,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <8 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.128(<8 x half> %x0, <8 x i16> zeroinitializer, i8 %x2)
  ret <8 x i16> %res
}

declare <16 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.256(<16 x half>, <16 x i16>, i16)

define <16 x i16>@test_int_x86_avx512_vcvtneph2bf16_256(<16 x half> %x0) {
; CHECK-LABEL: test_int_x86_avx512_vcvtneph2bf16_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf16 %ymm0, %ymm0 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7f,0x67,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %res = call <16 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.256(<16 x half> %x0, <16 x i16> undef, i16 -1)
  ret <16 x i16> %res
}

define <16 x i16>@test_int_x86_avx512_mask_vcvtneph2bf16_256(<16 x half> %x0, <16 x i16> %x1, i16 %x2) {
; X86-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0x67,0xc8]
; X86-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_mask_vcvtneph2bf16_256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf16 %ymm0, %ymm1 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0x67,0xc8]
; X64-NEXT:    vmovaps %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <16 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.256(<16 x half> %x0, <16 x i16> %x1, i16 %x2)
  ret <16 x i16> %res
}

define <16 x i16>@test_int_x86_avx512_maskz_vcvtneph2bf16_256(<16 x half> %x0, i16 %x2) {
; X86-LABEL: test_int_x86_avx512_maskz_vcvtneph2bf16_256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7f,0xa9,0x67,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: test_int_x86_avx512_maskz_vcvtneph2bf16_256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf16 %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7f,0xa9,0x67,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
  %res =  call <16 x i16> @llvm.x86.avx512.mask.vcvtneph2bf16.256(<16 x half> %x0, <16 x i16> zeroinitializer, i16 %x2)
  ret <16 x i16> %res
}
