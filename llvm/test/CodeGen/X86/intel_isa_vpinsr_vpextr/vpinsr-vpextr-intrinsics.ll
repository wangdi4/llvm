; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_vpinsr_vpextr
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+vpinsr-vpextr | FileCheck %s --check-prefixes=X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+vpinsr-vpextr | FileCheck %s --check-prefixes=X64

define i8 @extract_v32i8(<32 x i8> %x, i8* %dst) {
; X86-LABEL: extract_v32i8:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrb $17, %ymm0, %eax
; X86-NEXT:    vpextrb $16, %ymm0, (%ecx)
; X86-NEXT:    # kill: def $al killed $al killed $eax
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v32i8:
; X64:       # %bb.0:
; X64-NEXT:    vpextrb $17, %ymm0, %eax
; X64-NEXT:    vpextrb $16, %ymm0, (%rdi)
; X64-NEXT:    # kill: def $al killed $al killed $eax
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <32 x i8> %x, i32 17
  %r2 = extractelement <32 x i8> %x, i32 16
  store i8 %r2, i8* %dst, align 1
  ret i8 %r1
}

define i8 @extract_v64i8(<64 x i8> %x, i8* %dst) {
; X86-LABEL: extract_v64i8:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrb $60, %zmm0, %eax
; X86-NEXT:    vpextrb $61, %zmm0, (%ecx)
; X86-NEXT:    # kill: def $al killed $al killed $eax
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v64i8:
; X64:       # %bb.0:
; X64-NEXT:    vpextrb $60, %zmm0, %eax
; X64-NEXT:    vpextrb $61, %zmm0, (%rdi)
; X64-NEXT:    # kill: def $al killed $al killed $eax
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <64 x i8> %x, i32 60
  %r2 = extractelement <64 x i8> %x, i32 61
  store i8 %r2, i8* %dst, align 1
  ret i8 %r1
}

define i16 @extract_v16i16(<16 x i16> %x, i16* %dst) {
; X86-LABEL: extract_v16i16:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrw $15, %ymm0, %eax
; X86-NEXT:    vpextrw $14, %ymm0, (%ecx)
; X86-NEXT:    # kill: def $ax killed $ax killed $eax
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v16i16:
; X64:       # %bb.0:
; X64-NEXT:    vpextrw $15, %ymm0, %eax
; X64-NEXT:    vpextrw $14, %ymm0, (%rdi)
; X64-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <16 x i16> %x, i32 15
  %r2 = extractelement <16 x i16> %x, i32 14
  store i16 %r2, i16* %dst, align 1
  ret i16 %r1
}

define i16 @extract_v32i16(<32 x i16> %x, i16* %dst) {
; X86-LABEL: extract_v32i16:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrw $30, %zmm0, %eax
; X86-NEXT:    vpextrw $31, %zmm0, (%ecx)
; X86-NEXT:    # kill: def $ax killed $ax killed $eax
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v32i16:
; X64:       # %bb.0:
; X64-NEXT:    vpextrw $30, %zmm0, %eax
; X64-NEXT:    vpextrw $31, %zmm0, (%rdi)
; X64-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <32 x i16> %x, i32 30
  %r2 = extractelement <32 x i16> %x, i32 31
  store i16 %r2, i16* %dst, align 1
  ret i16 %r1
}

define i32 @extract_v8i32(<8 x i32> %x, i32* %dst) {
; X86-LABEL: extract_v8i32:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrd $5, %ymm0, %eax
; X86-NEXT:    vpextrd $7, %ymm0, (%ecx)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v8i32:
; X64:       # %bb.0:
; X64-NEXT:    vpextrd $5, %ymm0, %eax
; X64-NEXT:    vpextrd $7, %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <8 x i32> %x, i32 5
  %r2 = extractelement <8 x i32> %x, i32 7
  store i32 %r2, i32* %dst, align 1
  ret i32 %r1
}

define i32 @extract_v16i32(<16 x i32> %x, i32* %dst) {
; X86-LABEL: extract_v16i32:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrd $10, %zmm0, %eax
; X86-NEXT:    vpextrd $15, %zmm0, (%ecx)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v16i32:
; X64:       # %bb.0:
; X64-NEXT:    vpextrd $10, %zmm0, %eax
; X64-NEXT:    vpextrd $15, %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <16 x i32> %x, i32 10
  %r2 = extractelement <16 x i32> %x, i32 15
  store i32 %r2, i32* %dst, align 1
  ret i32 %r1
}

define i64 @extract_v4i64(<4 x i64> %x, i64* %dst) {
; X86-LABEL: extract_v4i64:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrd $6, %ymm0, %eax
; X86-NEXT:    vpextrd $7, %ymm0, %edx
; X86-NEXT:    vextracti128 $1, %ymm0, %xmm0
; X86-NEXT:    vmovq %xmm0, (%ecx)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v4i64:
; X64:       # %bb.0:
; X64-NEXT:    vpextrq $3, %ymm0, %rax
; X64-NEXT:    vpextrq $2, %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <4 x i64> %x, i32 3
  %r2 = extractelement <4 x i64> %x, i32 2
  store i64 %r2, i64* %dst, align 1
  ret i64 %r1
}

define i64 @extract_v8i64(<8 x i64> %x, i64* %dst) {
; X86-LABEL: extract_v8i64:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    vpextrd $10, %zmm0, %eax
; X86-NEXT:    vpextrd $11, %zmm0, %edx
; X86-NEXT:    vextracti32x4 $3, %zmm0, %xmm0
; X86-NEXT:    vpshufd {{.*#+}} xmm0 = xmm0[2,3,0,1]
; X86-NEXT:    vmovq %xmm0, (%ecx)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
;
; X64-LABEL: extract_v8i64:
; X64:       # %bb.0:
; X64-NEXT:    vpextrq $5, %zmm0, %rax
; X64-NEXT:    vpextrq $7, %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
  %r1 = extractelement <8 x i64> %x, i32 5
  %r2 = extractelement <8 x i64> %x, i32 7
  store i64 %r2, i64* %dst, align 1
  ret i64 %r1
}

define <32 x i8> @insert_v32i8(<32 x i8> %x, i8 %y, i8* %ptr) {
; X86-LABEL: insert_v32i8:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrb $30, {{[0-9]+}}(%esp), %ymm0, %ymm0
; X86-NEXT:    vpinsrb $31, (%eax), %ymm0, %ymm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v32i8:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrb $30, %edi, %ymm0, %ymm0
; X64-NEXT:    vpinsrb $31, (%rsi), %ymm0, %ymm0
; X64-NEXT:    retq
  %val = load i8, i8* %ptr
  %r0 = insertelement <32 x i8> %x, i8 %val, i32 31
  %r1 = insertelement <32 x i8> %r0, i8 %y, i32 30
  ret <32 x i8> %r1
}

define <64 x i8> @insert_v64i8(<64 x i8> %x, i8 %y, i8* %ptr) {
; X86-LABEL: insert_v64i8:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrb $60, {{[0-9]+}}(%esp), %zmm0, %zmm0
; X86-NEXT:    vpinsrb $61, (%eax), %zmm0, %zmm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v64i8:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrb $60, %edi, %zmm0, %zmm0
; X64-NEXT:    vpinsrb $61, (%rsi), %zmm0, %zmm0
; X64-NEXT:    retq
  %val = load i8, i8* %ptr
  %r0 = insertelement <64 x i8> %x, i8 %val, i32 61
  %r1 = insertelement <64 x i8> %r0, i8 %y, i32 60
  ret <64 x i8> %r1
}

define <16 x i16> @insert_v16i16(<16 x i16> %x, i16 %y, i16* %ptr) {
; X86-LABEL: insert_v16i16:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrw $10, {{[0-9]+}}(%esp), %ymm0, %ymm0
; X86-NEXT:    vpinsrw $12, (%eax), %ymm0, %ymm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v16i16:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrw $10, %edi, %ymm0, %ymm0
; X64-NEXT:    vpinsrw $12, (%rsi), %ymm0, %ymm0
; X64-NEXT:    retq
  %val = load i16, i16* %ptr
  %r0 = insertelement <16 x i16> %x, i16 %val, i32 12
  %r1 = insertelement <16 x i16> %r0, i16 %y, i32 10
  ret <16 x i16> %r1
}

define <32 x i16> @insert_v32i16(<32 x i16> %x, i16 %y, i16* %ptr) {
; X86-LABEL: insert_v32i16:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrw $30, {{[0-9]+}}(%esp), %zmm0, %zmm0
; X86-NEXT:    vpinsrw $31, (%eax), %zmm0, %zmm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v32i16:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrw $30, %edi, %zmm0, %zmm0
; X64-NEXT:    vpinsrw $31, (%rsi), %zmm0, %zmm0
; X64-NEXT:    retq
  %val = load i16, i16* %ptr
  %r0 = insertelement <32 x i16> %x, i16 %val, i32 31
  %r1 = insertelement <32 x i16> %r0, i16 %y, i32 30
  ret <32 x i16> %r1
}

define <8 x i32> @insert_v8i32(<8 x i32> %x, i32 %y, i32* %ptr) {
; X86-LABEL: insert_v8i32:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrd $3, {{[0-9]+}}(%esp), %ymm0, %ymm0
; X86-NEXT:    vpinsrd $7, (%eax), %ymm0, %ymm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v8i32:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrd $3, %edi, %ymm0, %ymm0
; X64-NEXT:    vpinsrd $7, (%rsi), %ymm0, %ymm0
; X64-NEXT:    retq
  %val = load i32, i32* %ptr
  %r0 = insertelement <8 x i32> %x, i32 %val, i32 7
  %r1 = insertelement <8 x i32> %r0, i32 %y, i32 3
  ret <8 x i32> %r1
}

define <16 x i32> @insert_v16i32(<16 x i32> %x, i32 %y, i32* %ptr) {
; X86-LABEL: insert_v16i32:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrd $10, {{[0-9]+}}(%esp), %zmm0, %zmm0
; X86-NEXT:    vpinsrd $15, (%eax), %zmm0, %zmm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v16i32:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrd $10, %edi, %zmm0, %zmm0
; X64-NEXT:    vpinsrd $15, (%rsi), %zmm0, %zmm0
; X64-NEXT:    retq
  %val = load i32, i32* %ptr
  %r0 = insertelement <16 x i32> %x, i32 %val, i32 15
  %r1 = insertelement <16 x i32> %r0, i32 %y, i32 10
  ret <16 x i32> %r1
}

define <4 x i64> @insert_v4i64(<4 x i64> %x, i64 %y, i64* %ptr) {
; X86-LABEL: insert_v4i64:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrd $4, (%eax), %ymm0, %ymm0
; X86-NEXT:    vpinsrd $5, 4(%eax), %ymm0, %ymm0
; X86-NEXT:    vpinsrd $6, {{[0-9]+}}(%esp), %ymm0, %ymm0
; X86-NEXT:    vpinsrd $7, {{[0-9]+}}(%esp), %ymm0, %ymm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v4i64:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrq $2, (%rsi), %ymm0, %ymm0
; X64-NEXT:    vpinsrq $3, %rdi, %ymm0, %ymm0
; X64-NEXT:    retq
  %val = load i64, i64* %ptr
  %r0 = insertelement <4 x i64> %x, i64 %val, i32 2
  %r1 = insertelement <4 x i64> %r0, i64 %y, i32 3
  ret <4 x i64> %r1
}

define <8 x i64> @insert_v8i64(<8 x i64> %x, i64 %y, i64* %ptr) {
; X86-LABEL: insert_v8i64:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vpinsrd $12, {{[0-9]+}}(%esp), %zmm0, %zmm0
; X86-NEXT:    vpinsrd $13, {{[0-9]+}}(%esp), %zmm0, %zmm0
; X86-NEXT:    vpinsrd $14, (%eax), %zmm0, %zmm0
; X86-NEXT:    vpinsrd $15, 4(%eax), %zmm0, %zmm0
; X86-NEXT:    retl
;
; X64-LABEL: insert_v8i64:
; X64:       # %bb.0:
; X64-NEXT:    vpinsrq $6, %rdi, %zmm0, %zmm0
; X64-NEXT:    vpinsrq $7, (%rsi), %zmm0, %zmm0
; X64-NEXT:    retq
  %val = load i64, i64* %ptr
  %r0 = insertelement <8 x i64> %x, i64 %val, i32 7
  %r1 = insertelement <8 x i64> %r0, i64 %y, i32 6
  ret <8 x i64> %r1
}
