; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; This test uses a mix of middle end and codegen passes. It must be run with
; the legacy pass manager for now. Eventually, we should split the run line
; so that middle end passes use the new pass manager and the codegen pass
; (hetero-arch-opt) uses the legacy pass manager.
; RUN: opt < %s -enable-new-pm=0 -mtriple=x86_64-- -mcpu=alderlake --loop-simplify --lcssa --hetero-arch-opt --verify -S | FileCheck %s

define double @test_clone_loop_with_outside_def(double *%src, i32 *%srcidx, double %ext) #0 {
; CHECK-LABEL: @test_clone_loop_with_outside_def(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = call i8 @llvm.x86.intel.fast.cpuid.coretype()
; CHECK-NEXT:    [[TMP1:%.*]] = icmp eq i8 [[TMP0]], 64
; CHECK-NEXT:    br i1 [[TMP1]], label [[LOOP:%.*]], label [[LOOP_CLONE:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[SUM:%.*]] = phi double [ 0.000000e+00, [[ENTRY:%.*]] ], [ [[TMP8:%.*]], [[BODY2:%.*]] ]
; CHECK-NEXT:    [[I:%.*]] = phi i32 [ 0, [[ENTRY]] ], [ [[NEXTI:%.*]], [[BODY2]] ]
; CHECK-NEXT:    [[I64:%.*]] = zext i32 [[I]] to i64
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i32, i32* [[SRCIDX:%.*]], i64 [[I64]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32* [[TMP2]] to <16 x i32>*
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i32>, <16 x i32>* [[TMP3]], align 4
; CHECK-NEXT:    [[TMP5:%.*]] = zext <16 x i32> [[TMP4]] to <16 x i64>
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[SRC:%.*]], <16 x i64> [[TMP5]]
; CHECK-NEXT:    [[TMP7:%.*]] = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> [[TMP6]], i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef)
; CHECK-NEXT:    [[TMP8]] = call fast double @llvm.vector.reduce.fadd.v16f64(double [[SUM]], <16 x double> [[TMP7]])
; CHECK-NEXT:    [[COND1:%.*]] = fcmp olt double [[TMP8]], 1.234000e+03
; CHECK-NEXT:    br i1 [[COND1]], label [[BODY2]], label [[EXIT:%.*]]
; CHECK:       body2:
; CHECK-NEXT:    [[NEXTI]] = add nuw nsw i32 [[I]], 16
; CHECK-NEXT:    [[COND2:%.*]] = icmp ult i32 [[NEXTI]], 4096
; CHECK-NEXT:    br i1 [[COND2]], label [[LOOP]], label [[EXIT]]
; CHECK:       loop.clone:
; CHECK-NEXT:    [[SUM_CLONE:%.*]] = phi double [ 0.000000e+00, [[ENTRY]] ], [ [[TMP15:%.*]], [[BODY2_CLONE:%.*]] ]
; CHECK-NEXT:    [[I_CLONE:%.*]] = phi i32 [ 0, [[ENTRY]] ], [ [[NEXTI_CLONE:%.*]], [[BODY2_CLONE]] ]
; CHECK-NEXT:    [[I64_CLONE:%.*]] = zext i32 [[I_CLONE]] to i64
; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i32, i32* [[SRCIDX]], i64 [[I64_CLONE]]
; CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32* [[TMP9]] to <16 x i32>*
; CHECK-NEXT:    [[TMP11:%.*]] = load <16 x i32>, <16 x i32>* [[TMP10]], align 4
; CHECK-NEXT:    [[TMP12:%.*]] = zext <16 x i32> [[TMP11]] to <16 x i64>
; CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds double, double* [[SRC]], <16 x i64> [[TMP12]]
; CHECK-NEXT:    [[TMP14:%.*]] = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> [[TMP13]], i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef), !hetero.arch.opt.disable.gather !0
; CHECK-NEXT:    [[TMP15]] = call fast double @llvm.vector.reduce.fadd.v16f64(double [[SUM_CLONE]], <16 x double> [[TMP14]])
; CHECK-NEXT:    [[COND1_CLONE:%.*]] = fcmp olt double [[TMP15]], 1.234000e+03
; CHECK-NEXT:    br i1 [[COND1_CLONE]], label [[BODY2_CLONE]], label [[EXIT]]
; CHECK:       body2.clone:
; CHECK-NEXT:    [[NEXTI_CLONE]] = add nuw nsw i32 [[I_CLONE]], 16
; CHECK-NEXT:    [[COND2_CLONE:%.*]] = icmp ult i32 [[NEXTI_CLONE]], 4096
; CHECK-NEXT:    br i1 [[COND2_CLONE]], label [[LOOP_CLONE]], label [[EXIT]]
; CHECK:       exit:
; CHECK-NEXT:    [[RESULT:%.*]] = phi double [ [[EXT:%.*]], [[LOOP]] ], [ [[SUM]], [[BODY2]] ], [ [[EXT]], [[LOOP_CLONE]] ], [ [[SUM_CLONE]], [[BODY2_CLONE]] ]
; CHECK-NEXT:    ret double [[RESULT]]
;
entry:
  br label %loop

loop:
  %sum = phi double [ zeroinitializer, %entry ], [ %6, %body2 ]
  %i = phi i32 [ 0, %entry ], [ %nexti, %body2 ]
  %i64 = zext i32 %i to i64
  %0 = getelementptr inbounds i32, i32* %srcidx, i64 %i64
  %1 = bitcast i32* %0 to <16 x i32>*
  %2 = load <16 x i32>, <16 x i32>* %1, align 4
  %3 = zext <16 x i32> %2 to <16 x i64>
  %4 = getelementptr inbounds double, double* %src, <16 x i64> %3
  %5 = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> %4, i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef)
  %6 = call fast double @llvm.vector.reduce.fadd.v16f64(double %sum, <16 x double> %5)
  %cond1 = fcmp olt double %6, 1234.0
  br i1 %cond1, label %body2, label %exit

body2:
  %nexti = add nuw nsw i32 %i, 16
  %cond2 = icmp ult i32 %nexti, 4096
  br i1 %cond2, label %loop, label %exit

exit:
  %result = phi double [ %ext, %loop ], [ %sum, %body2 ]
  ret double %result
}

declare double @llvm.vector.reduce.fadd.v16f64(double, <16 x double>)
declare <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*>, i32 , <16 x i1>, <16 x double>)

attributes #0 = { "target-cpu"="alderlake" }
