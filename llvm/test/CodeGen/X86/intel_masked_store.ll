; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse2    | FileCheck %s --check-prefixes=SSE2
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse4.2  | FileCheck %s --check-prefixes=SSE4
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx2    | FileCheck %s --check-prefixes=AVX2

define void @mstore_constmask_v12i8_v12i8(<12 x i8>* %addr, <12 x i8> %val) {
; SSE2-LABEL: mstore_constmask_v12i8_v12i8:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movq %xmm0, (%rdi)
; SSE2-NEXT:    movdqa %xmm0, -{{[0-9]+}}(%rsp)
; SSE2-NEXT:    movzbl -{{[0-9]+}}(%rsp), %eax
; SSE2-NEXT:    movb %al, 10(%rdi)
; SSE2-NEXT:    pextrw $4, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 8(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: mstore_constmask_v12i8_v12i8:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pextrb $10, %xmm0, 10(%rdi)
; SSE4-NEXT:    pextrw $4, %xmm0, 8(%rdi)
; SSE4-NEXT:    movq %xmm0, (%rdi)
; SSE4-NEXT:    retq
;
; AVX2-LABEL: mstore_constmask_v12i8_v12i8:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpextrb $10, %xmm0, 10(%rdi)
; AVX2-NEXT:    vpextrw $4, %xmm0, 8(%rdi)
; AVX2-NEXT:    vmovq %xmm0, (%rdi)
; AVX2-NEXT:    retq
  call void @llvm.masked.store.v12i8.p0v12i8(<12 x i8> %val, <12 x i8>* %addr, i32 4, <12 x i1> <i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 0>)
  ret void
}

define void @mstore_constmask_v16f64_v16f64(<16 x double>* %addr, <16 x double> %val) {
; SSE2-LABEL: mstore_constmask_v16f64_v16f64:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movups %xmm5, 80(%rdi)
; SSE2-NEXT:    movups %xmm4, 64(%rdi)
; SSE2-NEXT:    movups %xmm3, 48(%rdi)
; SSE2-NEXT:    movups %xmm2, 32(%rdi)
; SSE2-NEXT:    movups %xmm1, 16(%rdi)
; SSE2-NEXT:    movups %xmm0, (%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: mstore_constmask_v16f64_v16f64:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movups %xmm5, 80(%rdi)
; SSE4-NEXT:    movups %xmm4, 64(%rdi)
; SSE4-NEXT:    movups %xmm3, 48(%rdi)
; SSE4-NEXT:    movups %xmm2, 32(%rdi)
; SSE4-NEXT:    movups %xmm1, 16(%rdi)
; SSE4-NEXT:    movups %xmm0, (%rdi)
; SSE4-NEXT:    retq
;
; AVX2-LABEL: mstore_constmask_v16f64_v16f64:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovups %ymm1, 32(%rdi)
; AVX2-NEXT:    vmovups %ymm0, (%rdi)
; AVX2-NEXT:    vmovups %ymm2, 64(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  call void @llvm.masked.store.v16f64.p0v16f64(<16 x double> %val, <16 x double>* %addr, i32 1, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

declare void @llvm.masked.store.v12i8.p0v12i8(<12 x i8>, <12 x i8>*, i32, <12 x i1>)
declare void @llvm.masked.store.v16f64.p0v16f64(<16 x double>, <16 x double>*, i32, <16 x i1>)
