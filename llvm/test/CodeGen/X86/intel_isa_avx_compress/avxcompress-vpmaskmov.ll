; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx_compress
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avxcompress -verify-machineinstrs --show-mc-encoding | FileCheck %s --check-prefixes=CHECK

declare <16 x i8> @llvm.x86.avx2.maskload.b(i8*, <16 x i8>) nounwind readonly
declare <32 x i8> @llvm.x86.avx2.maskload.b.256(i8*, <32 x i8>) nounwind readonly
declare <8 x i16> @llvm.x86.avx2.maskload.w(i8*, <8 x i16>) nounwind readonly
declare <16 x i16> @llvm.x86.avx2.maskload.w.256(i8*, <16 x i16>) nounwind readonly
declare void @llvm.x86.avx2.maskstore.b(i8*, <16 x i8>, <16 x i8>) nounwind
declare void @llvm.x86.avx2.maskstore.b.256(i8*, <32 x i8>, <32 x i8>) nounwind
declare void @llvm.x86.avx2.maskstore.w(i8*, <8 x i16>, <8 x i16>) nounwind
declare void @llvm.x86.avx2.maskstore.w.256(i8*, <16 x i16>, <16 x i16>) nounwind

define <16 x i8> @test_x86_avx2_maskload_b(i8* %a0, <16 x i8> %a1) {
; CHECK-LABEL: test_x86_avx2_maskload_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovb (%rdi), %xmm0, %xmm0 # encoding: [0xc4,0xe2,0x78,0x8c,0x07]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <16 x i8> @llvm.x86.avx2.maskload.b(i8* %a0, <16 x i8> %a1) ; <<2 x i64>> [#uses=1]
  ret <16 x i8> %res
}

define <32 x i8> @test_x86_avx2_maskload_b_256(i8* %a0, <32 x i8> %a1) {
; CHECK-LABEL: test_x86_avx2_maskload_b_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovb (%rdi), %ymm0, %ymm0 # encoding: [0xc4,0xe2,0x7c,0x8c,0x07]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <32 x i8> @llvm.x86.avx2.maskload.b.256(i8* %a0, <32 x i8> %a1) ; <<2 x i64>> [#uses=1]
  ret <32 x i8> %res
}

define <8 x i16> @test_x86_avx2_maskload_w(i8* %a0, <8 x i16> %a1) {
; CHECK-LABEL: test_x86_avx2_maskload_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovw (%rdi), %xmm0, %xmm0 # encoding: [0xc4,0xe2,0xf8,0x8c,0x07]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <8 x i16> @llvm.x86.avx2.maskload.w(i8* %a0, <8 x i16> %a1) ; <<2 x i64>> [#uses=1]
  ret <8 x i16> %res
}


define <16 x i16> @test_x86_avx2_maskload_w_256(i8* %a0, <16 x i16> %a1) {
; CHECK-LABEL: test_x86_avx2_maskload_w_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovw (%rdi), %ymm0, %ymm0 # encoding: [0xc4,0xe2,0xfc,0x8c,0x07]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %res = call <16 x i16> @llvm.x86.avx2.maskload.w.256(i8* %a0, <16 x i16> %a1) ; <<2 x i64>> [#uses=1]
  ret <16 x i16> %res
}


define void @test_x86_avx2_maskstore_b(i8* %a0, <16 x i8> %a1, <16 x i8> %a2) {
; CHECK-LABEL: test_x86_avx2_maskstore_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovb %xmm1, %xmm0, (%rdi) # encoding: [0xc4,0xe2,0x78,0x8e,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.maskstore.b(i8* %a0, <16 x i8> %a1, <16 x i8> %a2)
  ret void
}


define void @test_x86_avx2_maskstore_b_256(i8* %a0, <32 x i8> %a1, <32 x i8> %a2) {
; CHECK-LABEL: test_x86_avx2_maskstore_b_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovb %ymm1, %ymm0, (%rdi) # encoding: [0xc4,0xe2,0x7c,0x8e,0x0f]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.maskstore.b.256(i8* %a0, <32 x i8> %a1, <32 x i8> %a2)
  ret void
}


define void @test_x86_avx2_maskstore_w(i8* %a0, <8 x i16> %a1, <8 x i16> %a2) {
; CHECK-LABEL: test_x86_avx2_maskstore_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovw %xmm1, %xmm0, (%rdi) # encoding: [0xc4,0xe2,0xf8,0x8e,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.maskstore.w(i8* %a0, <8 x i16> %a1, <8 x i16> %a2)
  ret void
}


define void @test_x86_avx2_maskstore_w_256(i8* %a0, <16 x i16> %a1, <16 x i16> %a2) {
; CHECK-LABEL: test_x86_avx2_maskstore_w_256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmaskmovw %ymm1, %ymm0, (%rdi) # encoding: [0xc4,0xe2,0xfc,0x8e,0x0f]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    retq # encoding: [0xc3]
  call void @llvm.x86.avx2.maskstore.w.256(i8* %a0, <16 x i16> %a1, <16 x i16> %a2)
  ret void
}

