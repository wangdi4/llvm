# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -o - %s -run-pass=machine-scheduler | FileCheck %s --check-prefix=X64
# RUN: llc -o - %s -run-pass=machine-scheduler -misched-regpressure=false | FileCheck %s --check-prefix=X64_DISRP
--- |
  ; ModuleID = '<stdin>'
  source_filename = "<stdin>"
  target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
  target triple = "x86_64-unknown-linux-gnu"

  ; Function Attrs: nofree
  define void @tilt([8 x i64]* nocapture %p) #0 {
  entry:
    br label %loop.93

  loop.93:                                          ; preds = %loop.93, %entry
    %lsr.iv = phi i64 [ %lsr.iv.next, %loop.93 ], [ 0, %entry ]
    %0 = bitcast [8 x i64]* %p to i8*
    %uglygep45 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep4546 = bitcast i8* %uglygep45 to <8 x i64>*
    %gepload = load <8 x i64>, <8 x i64>* %uglygep4546, align 8
    %uglygep42 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep4243 = bitcast i8* %uglygep42 to <8 x i64>*
    %scevgep44 = getelementptr <8 x i64>, <8 x i64>* %uglygep4243, i64 8
    %gepload347 = load <8 x i64>, <8 x i64>* %scevgep44, align 8
    %uglygep39 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep3940 = bitcast i8* %uglygep39 to <8 x i64>*
    %scevgep41 = getelementptr <8 x i64>, <8 x i64>* %uglygep3940, i64 16
    %gepload349 = load <8 x i64>, <8 x i64>* %scevgep41, align 8
    %uglygep36 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep3637 = bitcast i8* %uglygep36 to <8 x i64>*
    %scevgep38 = getelementptr <8 x i64>, <8 x i64>* %uglygep3637, i64 24
    %gepload351 = load <8 x i64>, <8 x i64>* %scevgep38, align 8
    %uglygep32 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep3233 = bitcast i8* %uglygep32 to <8 x i64>*
    %scevgep35 = getelementptr <8 x i64>, <8 x i64>* %uglygep3233, i64 32
    %gepload353 = load <8 x i64>, <8 x i64>* %scevgep35, align 8
    %uglygep29 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep2930 = bitcast i8* %uglygep29 to <8 x i64>*
    %scevgep31 = getelementptr <8 x i64>, <8 x i64>* %uglygep2930, i64 40
    %gepload355 = load <8 x i64>, <8 x i64>* %scevgep31, align 8
    %uglygep26 = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep2627 = bitcast i8* %uglygep26 to <8 x i64>*
    %scevgep28 = getelementptr <8 x i64>, <8 x i64>* %uglygep2627, i64 48
    %gepload357 = load <8 x i64>, <8 x i64>* %scevgep28, align 8
    %uglygep = getelementptr i8, i8* %0, i64 %lsr.iv
    %uglygep24 = bitcast i8* %uglygep to <8 x i64>*
    %scevgep25 = getelementptr <8 x i64>, <8 x i64>* %uglygep24, i64 56
    %gepload359 = load <8 x i64>, <8 x i64>* %scevgep25, align 8
    %1 = lshr <8 x i64> %gepload347, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %2 = xor <8 x i64> %1, %gepload
    %3 = and <8 x i64> %2, <i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695>
    %4 = xor <8 x i64> %3, %gepload
    %5 = shl nuw <8 x i64> %3, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %6 = xor <8 x i64> %5, %gepload347
    %7 = lshr <8 x i64> %gepload351, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %8 = xor <8 x i64> %7, %gepload349
    %9 = and <8 x i64> %8, <i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695>
    %10 = xor <8 x i64> %9, %gepload349
    %11 = shl nuw <8 x i64> %9, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %12 = xor <8 x i64> %11, %gepload351
    %13 = lshr <8 x i64> %gepload355, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %14 = xor <8 x i64> %13, %gepload353
    %15 = and <8 x i64> %14, <i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695>
    %16 = xor <8 x i64> %15, %gepload353
    %17 = shl nuw <8 x i64> %15, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %18 = xor <8 x i64> %17, %gepload355
    %19 = lshr <8 x i64> %gepload359, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %20 = xor <8 x i64> %19, %gepload357
    %21 = and <8 x i64> %20, <i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695, i64 71777214294589695>
    %22 = xor <8 x i64> %21, %gepload357
    %23 = shl nuw <8 x i64> %21, <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
    %24 = xor <8 x i64> %23, %gepload359
    %scevgep21 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep2122 = bitcast i64* %scevgep21 to <8 x i64>*
    store <8 x i64> %4, <8 x i64>* %scevgep2122, align 8
    %scevgep18 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep1819 = bitcast i64* %scevgep18 to <8 x i64>*
    %scevgep20 = getelementptr <8 x i64>, <8 x i64>* %scevgep1819, i64 1
    store <8 x i64> %6, <8 x i64>* %scevgep20, align 8
    %scevgep15 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep1516 = bitcast i64* %scevgep15 to <8 x i64>*
    %scevgep17 = getelementptr <8 x i64>, <8 x i64>* %scevgep1516, i64 2
    store <8 x i64> %10, <8 x i64>* %scevgep17, align 8
    %scevgep12 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep1213 = bitcast i64* %scevgep12 to <8 x i64>*
    %scevgep14 = getelementptr <8 x i64>, <8 x i64>* %scevgep1213, i64 3
    store <8 x i64> %12, <8 x i64>* %scevgep14, align 8
    %scevgep9 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep910 = bitcast i64* %scevgep9 to <8 x i64>*
    %scevgep11 = getelementptr <8 x i64>, <8 x i64>* %scevgep910, i64 4
    store <8 x i64> %16, <8 x i64>* %scevgep11, align 8
    %scevgep6 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep67 = bitcast i64* %scevgep6 to <8 x i64>*
    %scevgep8 = getelementptr <8 x i64>, <8 x i64>* %scevgep67, i64 5
    store <8 x i64> %18, <8 x i64>* %scevgep8, align 8
    %scevgep3 = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep34 = bitcast i64* %scevgep3 to <8 x i64>*
    %scevgep5 = getelementptr <8 x i64>, <8 x i64>* %scevgep34, i64 6
    store <8 x i64> %22, <8 x i64>* %scevgep5, align 8
    %scevgep = getelementptr [8 x i64], [8 x i64]* %p, i64 0, i64 %lsr.iv
    %scevgep1 = bitcast i64* %scevgep to <8 x i64>*
    %scevgep2 = getelementptr <8 x i64>, <8 x i64>* %scevgep1, i64 7
    store <8 x i64> %24, <8 x i64>* %scevgep2, align 8
    %lsr.iv.next = add nuw nsw i64 %lsr.iv, 64
    %condloop.89 = icmp eq i64 %lsr.iv.next, 512
    br i1 %condloop.89, label %afterloop.89, label %loop.93

  afterloop.89:                                     ; preds = %loop.93
    ret void
  }

  attributes #0 = { nofree "stack-protector-buffer-size"="8" "target-cpu"="skylake-avx512" }

...
---
name:            tilt
alignment:       16
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
registers:
  - { id: 0, class: gr64_nosp, preferred-register: '' }
  - { id: 1, class: gr64_nosp, preferred-register: '' }
  - { id: 2, class: gr64, preferred-register: '' }
  - { id: 3, class: gr64_with_sub_8bit, preferred-register: '' }
  - { id: 4, class: gr32, preferred-register: '' }
  - { id: 5, class: vr512, preferred-register: '' }
  - { id: 6, class: vr512, preferred-register: '' }
  - { id: 7, class: vr512, preferred-register: '' }
  - { id: 8, class: vr512, preferred-register: '' }
  - { id: 9, class: vr512, preferred-register: '' }
  - { id: 10, class: vr512, preferred-register: '' }
  - { id: 11, class: vr512, preferred-register: '' }
  - { id: 12, class: vr512, preferred-register: '' }
  - { id: 13, class: vr512, preferred-register: '' }
  - { id: 14, class: vr512, preferred-register: '' }
  - { id: 15, class: vr512, preferred-register: '' }
  - { id: 16, class: vr512, preferred-register: '' }
  - { id: 17, class: vr512, preferred-register: '' }
  - { id: 18, class: vr512, preferred-register: '' }
  - { id: 19, class: vr512, preferred-register: '' }
  - { id: 20, class: vr512, preferred-register: '' }
  - { id: 21, class: vr512, preferred-register: '' }
  - { id: 22, class: vr512, preferred-register: '' }
  - { id: 23, class: vr512, preferred-register: '' }
  - { id: 24, class: vr512, preferred-register: '' }
  - { id: 25, class: vr512, preferred-register: '' }
  - { id: 26, class: vr512, preferred-register: '' }
  - { id: 27, class: vr512, preferred-register: '' }
  - { id: 28, class: vr512, preferred-register: '' }
  - { id: 29, class: vr512, preferred-register: '' }
  - { id: 30, class: vr512, preferred-register: '' }
  - { id: 31, class: vr512, preferred-register: '' }
  - { id: 32, class: vr512, preferred-register: '' }
  - { id: 33, class: vr512, preferred-register: '' }
  - { id: 34, class: gr64, preferred-register: '' }
  - { id: 35, class: gr64_nosp, preferred-register: '' }
liveins:
  - { reg: '$rdi', virtual-reg: '%2' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    false
  hasCalls:        false
  stackProtector:  ''
  maxCallFrameSize: 4294967295
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:
  - id:              0
    value:           i16 255
    alignment:       2
    isTargetSpecific: false
machineFunctionInfo: {}
body:             |
  ; X64-LABEL: name: tilt
  ; X64: bb.0.entry:
  ; X64:   successors: %bb.1(0x80000000)
  ; X64:   liveins: $rdi
  ; X64:   [[COPY:%[0-9]+]]:gr64 = COPY $rdi
  ; X64:   undef %35.sub_32bit:gr64_nosp = MOV32r0 implicit-def dead $eflags
  ; X64:   [[VPBROADCASTWZrm:%[0-9]+]]:vr512 = VPBROADCASTWZrm $rip, 1, $noreg, %const.0, $noreg :: (load 2 from constant-pool)
  ; X64: bb.1.loop.93:
  ; X64:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  ; X64:   [[VMOVDQU64Zrm:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 512, $noreg :: (load 64 from %ir.scevgep44, align 8)
  ; X64:   [[VMOVDQU64Zrm1:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 1536, $noreg :: (load 64 from %ir.scevgep38, align 8)
  ; X64:   [[VMOVDQU64Zrm2:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 2560, $noreg :: (load 64 from %ir.scevgep31, align 8)
  ; X64:   [[VMOVDQU64Zrm3:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 3584, $noreg :: (load 64 from %ir.scevgep25, align 8)
  ; X64:   [[VMOVDQU64Zrm4:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 0, $noreg :: (load 64 from %ir.uglygep4546, align 8)
  ; X64:   [[VMOVDQU64Zrm5:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 1024, $noreg :: (load 64 from %ir.scevgep41, align 8)
  ; X64:   [[VMOVDQU64Zrm6:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 2048, $noreg :: (load 64 from %ir.scevgep35, align 8)
  ; X64:   [[VMOVDQU64Zrm7:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 3072, $noreg :: (load 64 from %ir.scevgep28, align 8)
  ; X64:   [[VPSRLQZri:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm]], 8
  ; X64:   [[VPSRLQZri1:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm1]], 8
  ; X64:   [[VPSRLQZri2:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm2]], 8
  ; X64:   [[VPSRLQZri3:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm3]], 8
  ; X64:   [[VPXORQZrr:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri]], [[VMOVDQU64Zrm4]]
  ; X64:   [[VPTERNLOGQZrri:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm4]], 226
  ; X64:   [[VPXORQZrr1:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri1]], [[VMOVDQU64Zrm5]]
  ; X64:   [[VPTERNLOGQZrri1:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri1]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm5]], 226
  ; X64:   [[VPXORQZrr2:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri2]], [[VMOVDQU64Zrm6]]
  ; X64:   [[VPTERNLOGQZrri2:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri2]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm6]], 226
  ; X64:   [[VPXORQZrr3:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri3]], [[VMOVDQU64Zrm7]]
  ; X64:   [[VPTERNLOGQZrri3:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri3]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm7]], 226
  ; X64:   [[VPSLLWZri:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr]], 8
  ; X64:   [[VPXORQZrr4:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri]], [[VMOVDQU64Zrm]]
  ; X64:   [[VPSLLWZri1:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr1]], 8
  ; X64:   [[VPXORQZrr5:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri1]], [[VMOVDQU64Zrm1]]
  ; X64:   [[VPSLLWZri2:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr2]], 8
  ; X64:   [[VPSLLWZri3:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr3]], 8
  ; X64:   [[VPXORQZrr6:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri2]], [[VMOVDQU64Zrm2]]
  ; X64:   [[VPXORQZrr7:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri3]], [[VMOVDQU64Zrm3]]
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 0, $noreg, [[VPTERNLOGQZrri]] :: (store 64 into %ir.scevgep2122, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 64, $noreg, [[VPXORQZrr4]] :: (store 64 into %ir.scevgep20, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 128, $noreg, [[VPTERNLOGQZrri1]] :: (store 64 into %ir.scevgep17, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 192, $noreg, [[VPXORQZrr5]] :: (store 64 into %ir.scevgep14, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 256, $noreg, [[VPTERNLOGQZrri2]] :: (store 64 into %ir.scevgep11, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 320, $noreg, [[VPXORQZrr6]] :: (store 64 into %ir.scevgep8, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 384, $noreg, [[VPTERNLOGQZrri3]] :: (store 64 into %ir.scevgep5, align 8)
  ; X64:   VMOVDQU64Zmr [[COPY]], 8, %35, 448, $noreg, [[VPXORQZrr7]] :: (store 64 into %ir.scevgep2, align 8)
  ; X64:   [[ADD64ri8_:%[0-9]+]]:gr64_nosp = nuw nsw ADD64ri8 [[ADD64ri8_]], 64, implicit-def dead $eflags
  ; X64:   CMP64ri32 [[ADD64ri8_]], 512, implicit-def $eflags
  ; X64:   JCC_1 %bb.1, 5, implicit killed $eflags
  ; X64:   JMP_1 %bb.2
  ; X64: bb.2.afterloop.89:
  ; X64:   RET 0
  ; X64_DISRP-LABEL: name: tilt
  ; X64_DISRP: bb.0.entry:
  ; X64_DISRP:   successors: %bb.1(0x80000000)
  ; X64_DISRP:   liveins: $rdi
  ; X64_DISRP:   [[COPY:%[0-9]+]]:gr64 = COPY $rdi
  ; X64_DISRP:   undef %35.sub_32bit:gr64_nosp = MOV32r0 implicit-def dead $eflags
  ; X64_DISRP:   [[VPBROADCASTWZrm:%[0-9]+]]:vr512 = VPBROADCASTWZrm $rip, 1, $noreg, %const.0, $noreg :: (load 2 from constant-pool)
  ; X64_DISRP: bb.1.loop.93:
  ; X64_DISRP:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  ; X64_DISRP:   [[VMOVDQU64Zrm:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 512, $noreg :: (load 64 from %ir.scevgep44, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm1:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 1536, $noreg :: (load 64 from %ir.scevgep38, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm2:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 2560, $noreg :: (load 64 from %ir.scevgep31, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm3:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 3584, $noreg :: (load 64 from %ir.scevgep25, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm4:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 0, $noreg :: (load 64 from %ir.uglygep4546, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm5:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 1024, $noreg :: (load 64 from %ir.scevgep41, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm6:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 2048, $noreg :: (load 64 from %ir.scevgep35, align 8)
  ; X64_DISRP:   [[VMOVDQU64Zrm7:%[0-9]+]]:vr512 = VMOVDQU64Zrm [[COPY]], 1, %35, 3072, $noreg :: (load 64 from %ir.scevgep28, align 8)
  ; X64_DISRP:   [[VPSRLQZri:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm]], 8
  ; X64_DISRP:   [[VPSRLQZri1:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm1]], 8
  ; X64_DISRP:   [[VPSRLQZri2:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm2]], 8
  ; X64_DISRP:   [[VPSRLQZri3:%[0-9]+]]:vr512 = VPSRLQZri [[VMOVDQU64Zrm3]], 8
  ; X64_DISRP:   [[VPXORQZrr:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri]], [[VMOVDQU64Zrm4]]
  ; X64_DISRP:   [[VPXORQZrr1:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri1]], [[VMOVDQU64Zrm5]]
  ; X64_DISRP:   [[VPXORQZrr2:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri2]], [[VMOVDQU64Zrm6]]
  ; X64_DISRP:   [[VPXORQZrr3:%[0-9]+]]:vr512 = VPXORQZrr [[VPSRLQZri3]], [[VMOVDQU64Zrm7]]
  ; X64_DISRP:   [[VPTERNLOGQZrri:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm4]], 226
  ; X64_DISRP:   [[VPTERNLOGQZrri1:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri1]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm5]], 226
  ; X64_DISRP:   [[VPSLLWZri:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr]], 8
  ; X64_DISRP:   [[VPSLLWZri1:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr1]], 8
  ; X64_DISRP:   [[VPSLLWZri2:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr2]], 8
  ; X64_DISRP:   [[VPSLLWZri3:%[0-9]+]]:vr512 = VPSLLWZri [[VPXORQZrr3]], 8
  ; X64_DISRP:   [[VPTERNLOGQZrri2:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri2]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm6]], 226
  ; X64_DISRP:   [[VPTERNLOGQZrri3:%[0-9]+]]:vr512 = VPTERNLOGQZrri [[VPTERNLOGQZrri3]], [[VPBROADCASTWZrm]], [[VMOVDQU64Zrm7]], 226
  ; X64_DISRP:   [[VPXORQZrr4:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri]], [[VMOVDQU64Zrm]]
  ; X64_DISRP:   [[VPXORQZrr5:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri1]], [[VMOVDQU64Zrm1]]
  ; X64_DISRP:   [[VPXORQZrr6:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri2]], [[VMOVDQU64Zrm2]]
  ; X64_DISRP:   [[VPXORQZrr7:%[0-9]+]]:vr512 = VPXORQZrr [[VPSLLWZri3]], [[VMOVDQU64Zrm3]]
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 0, $noreg, [[VPTERNLOGQZrri]] :: (store 64 into %ir.scevgep2122, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 64, $noreg, [[VPXORQZrr4]] :: (store 64 into %ir.scevgep20, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 128, $noreg, [[VPTERNLOGQZrri1]] :: (store 64 into %ir.scevgep17, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 192, $noreg, [[VPXORQZrr5]] :: (store 64 into %ir.scevgep14, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 256, $noreg, [[VPTERNLOGQZrri2]] :: (store 64 into %ir.scevgep11, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 320, $noreg, [[VPXORQZrr6]] :: (store 64 into %ir.scevgep8, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 384, $noreg, [[VPTERNLOGQZrri3]] :: (store 64 into %ir.scevgep5, align 8)
  ; X64_DISRP:   VMOVDQU64Zmr [[COPY]], 8, %35, 448, $noreg, [[VPXORQZrr7]] :: (store 64 into %ir.scevgep2, align 8)
  ; X64_DISRP:   [[ADD64ri8_:%[0-9]+]]:gr64_nosp = nuw nsw ADD64ri8 [[ADD64ri8_]], 64, implicit-def dead $eflags
  ; X64_DISRP:   CMP64ri32 [[ADD64ri8_]], 512, implicit-def $eflags
  ; X64_DISRP:   JCC_1 %bb.1, 5, implicit killed $eflags
  ; X64_DISRP:   JMP_1 %bb.2
  ; X64_DISRP: bb.2.afterloop.89:
  ; X64_DISRP:   RET 0
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $rdi

    %2:gr64 = COPY $rdi
    undef %35.sub_32bit:gr64_nosp = MOV32r0 implicit-def dead $eflags
    %15:vr512 = VPBROADCASTWZrm $rip, 1, $noreg, %const.0, $noreg :: (load 2 from constant-pool)

  bb.1.loop.93:
    successors: %bb.2(0x04000000), %bb.1(0x7c000000)

    %5:vr512 = VMOVDQU64Zrm %2, 1, %35, 0, $noreg :: (load 64 from %ir.uglygep4546, align 8)
    %6:vr512 = VMOVDQU64Zrm %2, 1, %35, 512, $noreg :: (load 64 from %ir.scevgep44, align 8)
    %7:vr512 = VMOVDQU64Zrm %2, 1, %35, 1024, $noreg :: (load 64 from %ir.scevgep41, align 8)
    %8:vr512 = VMOVDQU64Zrm %2, 1, %35, 1536, $noreg :: (load 64 from %ir.scevgep38, align 8)
    %9:vr512 = VMOVDQU64Zrm %2, 1, %35, 2048, $noreg :: (load 64 from %ir.scevgep35, align 8)
    %10:vr512 = VMOVDQU64Zrm %2, 1, %35, 2560, $noreg :: (load 64 from %ir.scevgep31, align 8)
    %11:vr512 = VMOVDQU64Zrm %2, 1, %35, 3072, $noreg :: (load 64 from %ir.scevgep28, align 8)
    %12:vr512 = VMOVDQU64Zrm %2, 1, %35, 3584, $noreg :: (load 64 from %ir.scevgep25, align 8)
    %16:vr512 = VPSRLQZri %6, 8
    %14:vr512 = VPXORQZrr %16, %5
    %16:vr512 = VPTERNLOGQZrri %16, %15, %5, 226
    %17:vr512 = VPSLLWZri %14, 8
    %18:vr512 = VPXORQZrr %17, %6
    %21:vr512 = VPSRLQZri %8, 8
    %20:vr512 = VPXORQZrr %21, %7
    %21:vr512 = VPTERNLOGQZrri %21, %15, %7, 226
    %22:vr512 = VPSLLWZri %20, 8
    %23:vr512 = VPXORQZrr %22, %8
    %26:vr512 = VPSRLQZri %10, 8
    %25:vr512 = VPXORQZrr %26, %9
    %26:vr512 = VPTERNLOGQZrri %26, %15, %9, 226
    %27:vr512 = VPSLLWZri %25, 8
    %28:vr512 = VPXORQZrr %27, %10
    %31:vr512 = VPSRLQZri %12, 8
    %30:vr512 = VPXORQZrr %31, %11
    %31:vr512 = VPTERNLOGQZrri %31, %15, %11, 226
    %32:vr512 = VPSLLWZri %30, 8
    %33:vr512 = VPXORQZrr %32, %12
    VMOVDQU64Zmr %2, 8, %35, 0, $noreg, %16 :: (store 64 into %ir.scevgep2122, align 8)
    VMOVDQU64Zmr %2, 8, %35, 64, $noreg, %18 :: (store 64 into %ir.scevgep20, align 8)
    VMOVDQU64Zmr %2, 8, %35, 128, $noreg, %21 :: (store 64 into %ir.scevgep17, align 8)
    VMOVDQU64Zmr %2, 8, %35, 192, $noreg, %23 :: (store 64 into %ir.scevgep14, align 8)
    VMOVDQU64Zmr %2, 8, %35, 256, $noreg, %26 :: (store 64 into %ir.scevgep11, align 8)
    VMOVDQU64Zmr %2, 8, %35, 320, $noreg, %28 :: (store 64 into %ir.scevgep8, align 8)
    VMOVDQU64Zmr %2, 8, %35, 384, $noreg, %31 :: (store 64 into %ir.scevgep5, align 8)
    VMOVDQU64Zmr %2, 8, %35, 448, $noreg, %33 :: (store 64 into %ir.scevgep2, align 8)
    %35:gr64_nosp = nuw nsw ADD64ri8 %35, 64, implicit-def dead $eflags
    CMP64ri32 %35, 512, implicit-def $eflags
    JCC_1 %bb.1, 5, implicit killed $eflags
    JMP_1 %bb.2

  bb.2.afterloop.89:
    RET 0

...
