; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_amx_transpose
; RUN: llc < %s -O0 -mtriple=x86_64-unknown-unknown -mattr=+amx-tile,+amx-bf16,+avx512f, \
; RUN: -mattr=+amx-transpose,+amx-avx512 \
; RUN: -verify-machineinstrs | FileCheck %s
; ModuleID = 'intel_amx_transpose_api2.c'

; This test is desigend to check AMX fast register allocation works well for tile-pair.

; This test comes from buiding clang/test/CodeGen/Intel_AMX_TRANSPOSE/intel_amx_transpose_transpose_t2.c with
; clang -cc1 -O0 -flax-vector-conversions=none -ffreestanding -triple=x86_64-unknown-unknown
; -target-feature +avx512f  -target-feature +amx-bf16 -target-feature +amx-transpose
; -target-feature +amx-avx512 -target-feature +amx-int8 -emit-llvm intel_amx_transpose_api2.c

%struct.__tile1024i_str = type <{ i16, i16, [60 x i8], <256 x i32> }>

@buf = global [2048 x i8] zeroinitializer, align 16
@buf2 = global [2048 x i8] zeroinitializer, align 16

; Function Attrs: noinline nounwind optnone
define dso_local void @test_tile_t2rpntlvwz0(i16 noundef signext %row, i16 noundef signext %col0, i16 noundef signext %col1) #0 {
; CHECK-LABEL: test_tile_t2rpntlvwz0:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    movq %rsp, %rbp
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    andq $-1024, %rsp # imm = 0xFC00
; CHECK-NEXT:    subq $25600, %rsp # imm = 0x6400
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movb $1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, %ax
; CHECK-NEXT:    movw %si, %cx
; CHECK-NEXT:    movw %di, %dx
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    xorl %esi, %esi
; CHECK-NEXT:    movl %esi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $1088, %edx # imm = 0x440
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rsi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq buf@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw (%r9), %bx
; CHECK-NEXT:    movw 2(%r9), %r11w
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw 2(%r8), %r10w
; CHECK-NEXT:    addq $64, %r9
; CHECK-NEXT:    addq $64, %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw %bx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r11w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rdi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %rbx
; CHECK-NEXT:    movw %bx, %r10w
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw %r9w, %di
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; CHECK-NEXT:    t2rpntlvwz0 (%r14,%r15), %tmm2
; CHECK-NEXT:    tilestored %tmm2, (%r11,%rbx)
; CHECK-NEXT:    tilestored %tmm3, (%r8,%r9)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw (%rcx), %ax
; CHECK-NEXT:    movswq 2(%rcx), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    tilezero %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    # kill: def $rcx killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    vmovdqa64 64(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 128(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 192(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 256(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 320(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 384(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 448(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 512(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 576(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 640(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 704(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 768(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 832(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 896(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 960(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 1024(%rax), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movl $1024, %edx # imm = 0x400
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %di # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %cx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $r8 killed $rax
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %ax # 2-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movw %r11w, %di
; CHECK-NEXT:    movl %r11d, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    movzwl %r9w, %r9d
; CHECK-NEXT:    shrl $2, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $r9b
; CHECK-NEXT:    movb %r9b, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm0
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r11), %tmm1
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm2
; CHECK-NEXT:    tdpbssd %tmm2, %tmm1, %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movq buf2@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %si # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %dx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %si, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movswq %cx, %r8
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    tileloadd (%rdi,%r8), %tmm0
; CHECK-NEXT:    tilestored %tmm0, (%rdx,%rsi)
; CHECK-NEXT:    leaq -24(%rbp), %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    tilerelease
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %m.addr.i33 = alloca i16, align 2
  %n.addr.i34 = alloca i16, align 2
  %base.addr.i35 = alloca i8*, align 8
  %stride.addr.i36 = alloca i64, align 8
  %tile.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp.i2032 = alloca <256 x i32>, align 1024
  %m.addr.i = alloca i16, align 2
  %n.addr.i = alloca i16, align 2
  %k.addr.i = alloca i16, align 2
  %dst.addr.i31 = alloca <256 x i32>, align 64
  %src1.addr.i = alloca <256 x i32>, align 64
  %src2.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp5.i30 = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i29 = alloca <256 x i32>, align 1024
  %indirect-arg-temp.i28 = alloca <256 x i32>, align 1024
  %row.addr.i = alloca i16, align 2
  %col0.addr.i = alloca i16, align 2
  %col1.addr.i = alloca i16, align 2
  %dst0.addr.i24 = alloca <256 x i32>*, align 8
  %dst1.addr.i25 = alloca <256 x i32>*, align 8
  %base.addr.i26 = alloca i8*, align 8
  %stride.addr.i27 = alloca i64, align 8
  %base.addr.i18 = alloca i8*, align 8
  %stride.addr.i19 = alloca i64, align 8
  %indirect-arg-temp.i20 = alloca <256 x i32>, align 1024
  %dst17 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i11 = alloca %struct.__tile1024i_str*, align 8
  %indirect-arg-temp.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp5.i = alloca <256 x i32>, align 1024
  %tile110 = alloca %struct.__tile1024i_str, align 64
  %tile09 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i = alloca %struct.__tile1024i_str*, align 8
  %dst0.addr.i = alloca %struct.__tile1024i_str*, align 8
  %dst1.addr.i = alloca %struct.__tile1024i_str*, align 8
  %base.addr.i = alloca i8*, align 8
  %stride.addr.i = alloca i64, align 8
  %row.addr = alloca i16, align 2
  %col0.addr = alloca i16, align 2
  %col1.addr = alloca i16, align 2
  %tile0 = alloca %struct.__tile1024i_str, align 64
  %tile1 = alloca %struct.__tile1024i_str, align 64
  %dst = alloca %struct.__tile1024i_str, align 64
  store i16 %row, i16* %row.addr, align 2
  store i16 %col0, i16* %col0.addr, align 2
  store i16 %col1, i16* %col1.addr, align 2
  %0 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %0, i8 0, i64 1088, i1 false)
  %row1 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 0
  %1 = load i16, i16* %row.addr, align 2
  store i16 %1, i16* %row1, align 64
  %col = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 1
  %2 = load i16, i16* %col0.addr, align 2
  store i16 %2, i16* %col, align 2
  %3 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %3, i8 0, i64 1088, i1 false)
  %row2 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 0
  %4 = load i16, i16* %row.addr, align 2
  store i16 %4, i16* %row2, align 64
  %col3 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 1
  %5 = load i16, i16* %col1.addr, align 2
  store i16 %5, i16* %col3, align 2
  %6 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %6, i8 0, i64 1088, i1 false)
  %row4 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 0
  %7 = load i16, i16* %row.addr, align 2
  store i16 %7, i16* %row4, align 64
  %col5 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 1
  %8 = load i16, i16* %col0.addr, align 2
  store i16 %8, i16* %col5, align 2
  store %struct.__tile1024i_str* %tile0, %struct.__tile1024i_str** %dst0.addr.i, align 8
  store %struct.__tile1024i_str* %tile1, %struct.__tile1024i_str** %dst1.addr.i, align 8
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf, i64 0, i64 0), i8** %base.addr.i, align 8
  store i64 32, i64* %stride.addr.i, align 8
  %9 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %row.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %9, i32 0, i32 0
  %10 = load i16, i16* %row.i, align 64
  %11 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %col.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %11, i32 0, i32 1
  %12 = load i16, i16* %col.i, align 2
  %13 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %col1.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %13, i32 0, i32 1
  %14 = load i16, i16* %col1.i, align 2
  %15 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %tile.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %15, i32 0, i32 3
  %16 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %tile2.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %16, i32 0, i32 3
  %17 = load i8*, i8** %base.addr.i, align 8
  %18 = load i64, i64* %stride.addr.i, align 8
  store i16 %10, i16* %row.addr.i, align 2
  store i16 %12, i16* %col0.addr.i, align 2
  store i16 %14, i16* %col1.addr.i, align 2
  store <256 x i32>* %tile.i, <256 x i32>** %dst0.addr.i24, align 8
  store <256 x i32>* %tile2.i, <256 x i32>** %dst1.addr.i25, align 8
  store i8* %17, i8** %base.addr.i26, align 8
  store i64 %18, i64* %stride.addr.i27, align 8
  %19 = load i16, i16* %row.addr.i, align 2
  %20 = load i16, i16* %col0.addr.i, align 2
  %21 = load i16, i16* %col1.addr.i, align 2
  %22 = load <256 x i32>*, <256 x i32>** %dst0.addr.i24, align 8
  %23 = load <256 x i32>*, <256 x i32>** %dst1.addr.i25, align 8
  %24 = load i8*, i8** %base.addr.i26, align 8
  %25 = load i64, i64* %stride.addr.i27, align 8
  %26 = call { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz0.internal(i16 %19, i16 %20, i16 %21, i8* %24, i64 %25) #4
  %27 = extractvalue { x86_amx, x86_amx } %26, 0
  %28 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %27) #4
  store <256 x i32> %28, <256 x i32>* %22, align 1024
  %29 = extractvalue { x86_amx, x86_amx } %26, 1
  %30 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %29) #4
  store <256 x i32> %30, <256 x i32>* %23, align 1024
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i, align 8
  %31 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %row.i6 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %31, i32 0, i32 0
  %32 = load i16, i16* %row.i6, align 64
  %33 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %col.i7 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %33, i32 0, i32 1
  %34 = load i16, i16* %col.i7, align 2
  %35 = call x86_amx @llvm.x86.tilezero.internal(i16 %32, i16 %34) #4
  %36 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %35) #4
  %37 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %tile.i8 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %37, i32 0, i32 3
  store <256 x i32> %36, <256 x i32>* %tile.i8, align 64
  %38 = bitcast %struct.__tile1024i_str* %tile110 to i8*
  %39 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %38, i8* align 1 %39, i64 1088, i1 false) #4
  %40 = bitcast %struct.__tile1024i_str* %tile09 to i8*
  %41 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %40, i8* align 1 %41, i64 1088, i1 false) #4
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i11, align 8
  %row.i12 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile09, i32 0, i32 0
  %42 = load i16, i16* %row.i12, align 64
  %col.i13 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile110, i32 0, i32 1
  %43 = load i16, i16* %col.i13, align 2
  %col1.i14 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile09, i32 0, i32 1
  %44 = load i16, i16* %col1.i14, align 2
  %45 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i11, align 8
  %tile.i15 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %45, i32 0, i32 3
  %46 = load <256 x i32>, <256 x i32>* %tile.i15, align 64
  %tile2.i16 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile09, i32 0, i32 3
  %47 = load <256 x i32>, <256 x i32>* %tile2.i16, align 64
  %tile3.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile110, i32 0, i32 3
  %48 = load <256 x i32>, <256 x i32>* %tile3.i, align 64
  store <256 x i32> %46, <256 x i32>* %indirect-arg-temp.i, align 1024
  store <256 x i32> %47, <256 x i32>* %indirect-arg-temp4.i, align 1024
  store <256 x i32> %48, <256 x i32>* %indirect-arg-temp5.i, align 1024
  %49 = bitcast <256 x i32>* %indirect-arg-temp5.i30 to i8*
  %50 = bitcast <256 x i32>* %indirect-arg-temp5.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %49, i8* align 1 %50, i64 1024, i1 false) #4
  %51 = bitcast <256 x i32>* %indirect-arg-temp4.i29 to i8*
  %52 = bitcast <256 x i32>* %indirect-arg-temp4.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %51, i8* align 1 %52, i64 1024, i1 false) #4
  %53 = bitcast <256 x i32>* %indirect-arg-temp.i28 to i8*
  %54 = bitcast <256 x i32>* %indirect-arg-temp.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %53, i8* align 1 %54, i64 1024, i1 false) #4
  %dst.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i28, align 1024
  %src1.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp4.i29, align 1024
  %src2.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp5.i30, align 1024
  store i16 %42, i16* %m.addr.i, align 2
  store i16 %43, i16* %n.addr.i, align 2
  store i16 %44, i16* %k.addr.i, align 2
  store <256 x i32> %dst.i, <256 x i32>* %dst.addr.i31, align 64
  store <256 x i32> %src1.i, <256 x i32>* %src1.addr.i, align 64
  store <256 x i32> %src2.i, <256 x i32>* %src2.addr.i, align 64
  %55 = load i16, i16* %m.addr.i, align 2
  %56 = load i16, i16* %n.addr.i, align 2
  %57 = load i16, i16* %k.addr.i, align 2
  %58 = load <256 x i32>, <256 x i32>* %dst.addr.i31, align 64
  %59 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %58) #4
  %60 = load <256 x i32>, <256 x i32>* %src1.addr.i, align 64
  %61 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %60) #4
  %62 = load <256 x i32>, <256 x i32>* %src2.addr.i, align 64
  %63 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %62) #4
  %64 = call x86_amx @llvm.x86.tdpbssd.internal(i16 %55, i16 %56, i16 %57, x86_amx %59, x86_amx %61, x86_amx %63) #4
  %65 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %64) #4
  %66 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i11, align 8
  %tile6.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %66, i32 0, i32 3
  store <256 x i32> %65, <256 x i32>* %tile6.i, align 64
  %67 = bitcast %struct.__tile1024i_str* %dst17 to i8*
  %68 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %67, i8* align 1 %68, i64 1088, i1 false) #4
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf2, i64 0, i64 0), i8** %base.addr.i18, align 8
  store i64 32, i64* %stride.addr.i19, align 8
  %row.i21 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst17, i32 0, i32 0
  %69 = load i16, i16* %row.i21, align 64
  %col.i22 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst17, i32 0, i32 1
  %70 = load i16, i16* %col.i22, align 2
  %71 = load i8*, i8** %base.addr.i18, align 8
  %72 = load i64, i64* %stride.addr.i19, align 8
  %tile.i23 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst17, i32 0, i32 3
  %73 = load <256 x i32>, <256 x i32>* %tile.i23, align 64
  store <256 x i32> %73, <256 x i32>* %indirect-arg-temp.i20, align 1024
  %74 = bitcast <256 x i32>* %indirect-arg-temp.i2032 to i8*
  %75 = bitcast <256 x i32>* %indirect-arg-temp.i20 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %74, i8* align 1 %75, i64 1024, i1 false) #4
  %tile.i37 = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i2032, align 1024
  store i16 %69, i16* %m.addr.i33, align 2
  store i16 %70, i16* %n.addr.i34, align 2
  store i8* %71, i8** %base.addr.i35, align 8
  store i64 %72, i64* %stride.addr.i36, align 8
  store <256 x i32> %tile.i37, <256 x i32>* %tile.addr.i, align 64
  %76 = load i16, i16* %m.addr.i33, align 2
  %77 = load i16, i16* %n.addr.i34, align 2
  %78 = load i8*, i8** %base.addr.i35, align 8
  %79 = load i64, i64* %stride.addr.i36, align 8
  %80 = load <256 x i32>, <256 x i32>* %tile.addr.i, align 64
  %81 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %80) #4
  call void @llvm.x86.tilestored64.internal(i16 %76, i16 %77, i8* %78, i64 %79, x86_amx %81) #4
  ret void
}

; Function Attrs: argmemonly nofree nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #1

; Function Attrs: noinline nounwind optnone
define dso_local void @test_tile_t2rpntlvwz0t1(i16 noundef signext %row, i16 noundef signext %col0, i16 noundef signext %col1) #0 {
; CHECK-LABEL: test_tile_t2rpntlvwz0t1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    movq %rsp, %rbp
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    andq $-1024, %rsp # imm = 0xFC00
; CHECK-NEXT:    subq $25600, %rsp # imm = 0x6400
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movb $1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, %ax
; CHECK-NEXT:    movw %si, %cx
; CHECK-NEXT:    movw %di, %dx
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    xorl %esi, %esi
; CHECK-NEXT:    movl %esi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $1088, %edx # imm = 0x440
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rsi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq buf@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw (%r9), %bx
; CHECK-NEXT:    movw 2(%r9), %r11w
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw 2(%r8), %r10w
; CHECK-NEXT:    addq $64, %r9
; CHECK-NEXT:    addq $64, %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw %bx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r11w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rdi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %rbx
; CHECK-NEXT:    movw %bx, %r10w
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw %r9w, %di
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; CHECK-NEXT:    t2rpntlvwz0t1 (%r14,%r15), %tmm2
; CHECK-NEXT:    tilestored %tmm2, (%r11,%rbx)
; CHECK-NEXT:    tilestored %tmm3, (%r8,%r9)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw (%rcx), %ax
; CHECK-NEXT:    movswq 2(%rcx), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    tilezero %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    # kill: def $rcx killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    vmovdqa64 64(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 128(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 192(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 256(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 320(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 384(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 448(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 512(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 576(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 640(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 704(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 768(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 832(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 896(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 960(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 1024(%rax), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movl $1024, %edx # imm = 0x400
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %di # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %cx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $r8 killed $rax
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %ax # 2-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movw %r11w, %di
; CHECK-NEXT:    movl %r11d, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    movzwl %r9w, %r9d
; CHECK-NEXT:    shrl $2, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $r9b
; CHECK-NEXT:    movb %r9b, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm0
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r11), %tmm1
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm2
; CHECK-NEXT:    tdpbssd %tmm2, %tmm1, %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movq buf2@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %si # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %dx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %si, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movswq %cx, %r8
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    tileloadd (%rdi,%r8), %tmm0
; CHECK-NEXT:    tilestored %tmm0, (%rdx,%rsi)
; CHECK-NEXT:    leaq -24(%rbp), %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    tilerelease
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %row.addr.i = alloca i16, align 2
  %col0.addr.i = alloca i16, align 2
  %col1.addr.i = alloca i16, align 2
  %dst0.addr.i34 = alloca <256 x i32>*, align 8
  %dst1.addr.i35 = alloca <256 x i32>*, align 8
  %base.addr.i36 = alloca i8*, align 8
  %stride.addr.i37 = alloca i64, align 8
  %m.addr.i29 = alloca i16, align 2
  %n.addr.i30 = alloca i16, align 2
  %base.addr.i31 = alloca i8*, align 8
  %stride.addr.i32 = alloca i64, align 8
  %tile.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp.i1328 = alloca <256 x i32>, align 1024
  %m.addr.i = alloca i16, align 2
  %n.addr.i = alloca i16, align 2
  %k.addr.i = alloca i16, align 2
  %dst.addr.i27 = alloca <256 x i32>, align 64
  %src1.addr.i = alloca <256 x i32>, align 64
  %src2.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp5.i26 = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i25 = alloca <256 x i32>, align 1024
  %indirect-arg-temp.i24 = alloca <256 x i32>, align 1024
  %dst0.addr.i = alloca %struct.__tile1024i_str*, align 8
  %dst1.addr.i = alloca %struct.__tile1024i_str*, align 8
  %base.addr.i17 = alloca i8*, align 8
  %stride.addr.i18 = alloca i64, align 8
  %base.addr.i = alloca i8*, align 8
  %stride.addr.i = alloca i64, align 8
  %indirect-arg-temp.i13 = alloca <256 x i32>, align 1024
  %dst12 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i8 = alloca %struct.__tile1024i_str*, align 8
  %indirect-arg-temp.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp5.i = alloca <256 x i32>, align 1024
  %tile17 = alloca %struct.__tile1024i_str, align 64
  %tile06 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i = alloca %struct.__tile1024i_str*, align 8
  %row.addr = alloca i16, align 2
  %col0.addr = alloca i16, align 2
  %col1.addr = alloca i16, align 2
  %tile0 = alloca %struct.__tile1024i_str, align 64
  %tile1 = alloca %struct.__tile1024i_str, align 64
  %dst = alloca %struct.__tile1024i_str, align 64
  store i16 %row, i16* %row.addr, align 2
  store i16 %col0, i16* %col0.addr, align 2
  store i16 %col1, i16* %col1.addr, align 2
  %0 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %0, i8 0, i64 1088, i1 false)
  %row1 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 0
  %1 = load i16, i16* %row.addr, align 2
  store i16 %1, i16* %row1, align 64
  %col = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 1
  %2 = load i16, i16* %col0.addr, align 2
  store i16 %2, i16* %col, align 2
  %3 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %3, i8 0, i64 1088, i1 false)
  %row2 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 0
  %4 = load i16, i16* %row.addr, align 2
  store i16 %4, i16* %row2, align 64
  %col3 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 1
  %5 = load i16, i16* %col1.addr, align 2
  store i16 %5, i16* %col3, align 2
  %6 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %6, i8 0, i64 1088, i1 false)
  %row4 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 0
  %7 = load i16, i16* %row.addr, align 2
  store i16 %7, i16* %row4, align 64
  %col5 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 1
  %8 = load i16, i16* %col0.addr, align 2
  store i16 %8, i16* %col5, align 2
  store %struct.__tile1024i_str* %tile0, %struct.__tile1024i_str** %dst0.addr.i, align 8
  store %struct.__tile1024i_str* %tile1, %struct.__tile1024i_str** %dst1.addr.i, align 8
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf, i64 0, i64 0), i8** %base.addr.i17, align 8
  store i64 32, i64* %stride.addr.i18, align 8
  %9 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %row.i19 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %9, i32 0, i32 0
  %10 = load i16, i16* %row.i19, align 64
  %11 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %col.i20 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %11, i32 0, i32 1
  %12 = load i16, i16* %col.i20, align 2
  %13 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %col1.i21 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %13, i32 0, i32 1
  %14 = load i16, i16* %col1.i21, align 2
  %15 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %tile.i22 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %15, i32 0, i32 3
  %16 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %tile2.i23 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %16, i32 0, i32 3
  %17 = load i8*, i8** %base.addr.i17, align 8
  %18 = load i64, i64* %stride.addr.i18, align 8
  store i16 %10, i16* %row.addr.i, align 2
  store i16 %12, i16* %col0.addr.i, align 2
  store i16 %14, i16* %col1.addr.i, align 2
  store <256 x i32>* %tile.i22, <256 x i32>** %dst0.addr.i34, align 8
  store <256 x i32>* %tile2.i23, <256 x i32>** %dst1.addr.i35, align 8
  store i8* %17, i8** %base.addr.i36, align 8
  store i64 %18, i64* %stride.addr.i37, align 8
  %19 = load i16, i16* %row.addr.i, align 2
  %20 = load i16, i16* %col0.addr.i, align 2
  %21 = load i16, i16* %col1.addr.i, align 2
  %22 = load <256 x i32>*, <256 x i32>** %dst0.addr.i34, align 8
  %23 = load <256 x i32>*, <256 x i32>** %dst1.addr.i35, align 8
  %24 = load i8*, i8** %base.addr.i36, align 8
  %25 = load i64, i64* %stride.addr.i37, align 8
  %26 = call { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz0t1.internal(i16 %19, i16 %20, i16 %21, i8* %24, i64 %25) #4
  %27 = extractvalue { x86_amx, x86_amx } %26, 0
  %28 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %27) #4
  store <256 x i32> %28, <256 x i32>* %22, align 1024
  %29 = extractvalue { x86_amx, x86_amx } %26, 1
  %30 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %29) #4
  store <256 x i32> %30, <256 x i32>* %23, align 1024
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i, align 8
  %31 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %row.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %31, i32 0, i32 0
  %32 = load i16, i16* %row.i, align 64
  %33 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %col.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %33, i32 0, i32 1
  %34 = load i16, i16* %col.i, align 2
  %35 = call x86_amx @llvm.x86.tilezero.internal(i16 %32, i16 %34) #4
  %36 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %35) #4
  %37 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %tile.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %37, i32 0, i32 3
  store <256 x i32> %36, <256 x i32>* %tile.i, align 64
  %38 = bitcast %struct.__tile1024i_str* %tile17 to i8*
  %39 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %38, i8* align 1 %39, i64 1088, i1 false) #4
  %40 = bitcast %struct.__tile1024i_str* %tile06 to i8*
  %41 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %40, i8* align 1 %41, i64 1088, i1 false) #4
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %row.i9 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 0
  %42 = load i16, i16* %row.i9, align 64
  %col.i10 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 1
  %43 = load i16, i16* %col.i10, align 2
  %col1.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 1
  %44 = load i16, i16* %col1.i, align 2
  %45 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile.i11 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %45, i32 0, i32 3
  %46 = load <256 x i32>, <256 x i32>* %tile.i11, align 64
  %tile2.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 3
  %47 = load <256 x i32>, <256 x i32>* %tile2.i, align 64
  %tile3.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 3
  %48 = load <256 x i32>, <256 x i32>* %tile3.i, align 64
  store <256 x i32> %46, <256 x i32>* %indirect-arg-temp.i, align 1024
  store <256 x i32> %47, <256 x i32>* %indirect-arg-temp4.i, align 1024
  store <256 x i32> %48, <256 x i32>* %indirect-arg-temp5.i, align 1024
  %49 = bitcast <256 x i32>* %indirect-arg-temp5.i26 to i8*
  %50 = bitcast <256 x i32>* %indirect-arg-temp5.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %49, i8* align 1 %50, i64 1024, i1 false) #4
  %51 = bitcast <256 x i32>* %indirect-arg-temp4.i25 to i8*
  %52 = bitcast <256 x i32>* %indirect-arg-temp4.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %51, i8* align 1 %52, i64 1024, i1 false) #4
  %53 = bitcast <256 x i32>* %indirect-arg-temp.i24 to i8*
  %54 = bitcast <256 x i32>* %indirect-arg-temp.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %53, i8* align 1 %54, i64 1024, i1 false) #4
  %dst.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i24, align 1024
  %src1.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp4.i25, align 1024
  %src2.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp5.i26, align 1024
  store i16 %42, i16* %m.addr.i, align 2
  store i16 %43, i16* %n.addr.i, align 2
  store i16 %44, i16* %k.addr.i, align 2
  store <256 x i32> %dst.i, <256 x i32>* %dst.addr.i27, align 64
  store <256 x i32> %src1.i, <256 x i32>* %src1.addr.i, align 64
  store <256 x i32> %src2.i, <256 x i32>* %src2.addr.i, align 64
  %55 = load i16, i16* %m.addr.i, align 2
  %56 = load i16, i16* %n.addr.i, align 2
  %57 = load i16, i16* %k.addr.i, align 2
  %58 = load <256 x i32>, <256 x i32>* %dst.addr.i27, align 64
  %59 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %58) #4
  %60 = load <256 x i32>, <256 x i32>* %src1.addr.i, align 64
  %61 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %60) #4
  %62 = load <256 x i32>, <256 x i32>* %src2.addr.i, align 64
  %63 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %62) #4
  %64 = call x86_amx @llvm.x86.tdpbssd.internal(i16 %55, i16 %56, i16 %57, x86_amx %59, x86_amx %61, x86_amx %63) #4
  %65 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %64) #4
  %66 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile6.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %66, i32 0, i32 3
  store <256 x i32> %65, <256 x i32>* %tile6.i, align 64
  %67 = bitcast %struct.__tile1024i_str* %dst12 to i8*
  %68 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %67, i8* align 1 %68, i64 1088, i1 false) #4
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf2, i64 0, i64 0), i8** %base.addr.i, align 8
  store i64 32, i64* %stride.addr.i, align 8
  %row.i14 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 0
  %69 = load i16, i16* %row.i14, align 64
  %col.i15 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 1
  %70 = load i16, i16* %col.i15, align 2
  %71 = load i8*, i8** %base.addr.i, align 8
  %72 = load i64, i64* %stride.addr.i, align 8
  %tile.i16 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 3
  %73 = load <256 x i32>, <256 x i32>* %tile.i16, align 64
  store <256 x i32> %73, <256 x i32>* %indirect-arg-temp.i13, align 1024
  %74 = bitcast <256 x i32>* %indirect-arg-temp.i1328 to i8*
  %75 = bitcast <256 x i32>* %indirect-arg-temp.i13 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %74, i8* align 1 %75, i64 1024, i1 false) #4
  %tile.i33 = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i1328, align 1024
  store i16 %69, i16* %m.addr.i29, align 2
  store i16 %70, i16* %n.addr.i30, align 2
  store i8* %71, i8** %base.addr.i31, align 8
  store i64 %72, i64* %stride.addr.i32, align 8
  store <256 x i32> %tile.i33, <256 x i32>* %tile.addr.i, align 64
  %76 = load i16, i16* %m.addr.i29, align 2
  %77 = load i16, i16* %n.addr.i30, align 2
  %78 = load i8*, i8** %base.addr.i31, align 8
  %79 = load i64, i64* %stride.addr.i32, align 8
  %80 = load <256 x i32>, <256 x i32>* %tile.addr.i, align 64
  %81 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %80) #4
  call void @llvm.x86.tilestored64.internal(i16 %76, i16 %77, i8* %78, i64 %79, x86_amx %81) #4
  ret void
}

; Function Attrs: noinline nounwind optnone
define dso_local void @test_tile_t2rpntlvwz1(i16 noundef signext %row, i16 noundef signext %col0, i16 noundef signext %col1) #0 {
; CHECK-LABEL: test_tile_t2rpntlvwz1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    movq %rsp, %rbp
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    andq $-1024, %rsp # imm = 0xFC00
; CHECK-NEXT:    subq $25600, %rsp # imm = 0x6400
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movb $1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, %ax
; CHECK-NEXT:    movw %si, %cx
; CHECK-NEXT:    movw %di, %dx
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    xorl %esi, %esi
; CHECK-NEXT:    movl %esi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $1088, %edx # imm = 0x440
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rsi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq buf@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw (%r9), %bx
; CHECK-NEXT:    movw 2(%r9), %r11w
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw 2(%r8), %r10w
; CHECK-NEXT:    addq $64, %r9
; CHECK-NEXT:    addq $64, %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw %bx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r11w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rdi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %rbx
; CHECK-NEXT:    movw %bx, %r10w
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw %r9w, %di
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; CHECK-NEXT:    t2rpntlvwz1 (%r14,%r15), %tmm2
; CHECK-NEXT:    tilestored %tmm2, (%r11,%rbx)
; CHECK-NEXT:    tilestored %tmm3, (%r8,%r9)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw (%rcx), %ax
; CHECK-NEXT:    movswq 2(%rcx), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    tilezero %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    # kill: def $rcx killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    vmovdqa64 64(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 128(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 192(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 256(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 320(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 384(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 448(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 512(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 576(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 640(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 704(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 768(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 832(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 896(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 960(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 1024(%rax), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movl $1024, %edx # imm = 0x400
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %di # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %cx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $r8 killed $rax
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %ax # 2-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movw %r11w, %di
; CHECK-NEXT:    movl %r11d, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    movzwl %r9w, %r9d
; CHECK-NEXT:    shrl $2, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $r9b
; CHECK-NEXT:    movb %r9b, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm0
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r11), %tmm1
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm2
; CHECK-NEXT:    tdpbssd %tmm2, %tmm1, %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movq buf2@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %si # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %dx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %si, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movswq %cx, %r8
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    tileloadd (%rdi,%r8), %tmm0
; CHECK-NEXT:    tilestored %tmm0, (%rdx,%rsi)
; CHECK-NEXT:    leaq -24(%rbp), %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    tilerelease
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %row.addr.i = alloca i16, align 2
  %col0.addr.i = alloca i16, align 2
  %col1.addr.i = alloca i16, align 2
  %dst0.addr.i34 = alloca <256 x i32>*, align 8
  %dst1.addr.i35 = alloca <256 x i32>*, align 8
  %base.addr.i36 = alloca i8*, align 8
  %stride.addr.i37 = alloca i64, align 8
  %m.addr.i29 = alloca i16, align 2
  %n.addr.i30 = alloca i16, align 2
  %base.addr.i31 = alloca i8*, align 8
  %stride.addr.i32 = alloca i64, align 8
  %tile.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp.i1328 = alloca <256 x i32>, align 1024
  %m.addr.i = alloca i16, align 2
  %n.addr.i = alloca i16, align 2
  %k.addr.i = alloca i16, align 2
  %dst.addr.i27 = alloca <256 x i32>, align 64
  %src1.addr.i = alloca <256 x i32>, align 64
  %src2.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp5.i26 = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i25 = alloca <256 x i32>, align 1024
  %indirect-arg-temp.i24 = alloca <256 x i32>, align 1024
  %dst0.addr.i = alloca %struct.__tile1024i_str*, align 8
  %dst1.addr.i = alloca %struct.__tile1024i_str*, align 8
  %base.addr.i17 = alloca i8*, align 8
  %stride.addr.i18 = alloca i64, align 8
  %base.addr.i = alloca i8*, align 8
  %stride.addr.i = alloca i64, align 8
  %indirect-arg-temp.i13 = alloca <256 x i32>, align 1024
  %dst12 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i8 = alloca %struct.__tile1024i_str*, align 8
  %indirect-arg-temp.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp5.i = alloca <256 x i32>, align 1024
  %tile17 = alloca %struct.__tile1024i_str, align 64
  %tile06 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i = alloca %struct.__tile1024i_str*, align 8
  %row.addr = alloca i16, align 2
  %col0.addr = alloca i16, align 2
  %col1.addr = alloca i16, align 2
  %tile0 = alloca %struct.__tile1024i_str, align 64
  %tile1 = alloca %struct.__tile1024i_str, align 64
  %dst = alloca %struct.__tile1024i_str, align 64
  store i16 %row, i16* %row.addr, align 2
  store i16 %col0, i16* %col0.addr, align 2
  store i16 %col1, i16* %col1.addr, align 2
  %0 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %0, i8 0, i64 1088, i1 false)
  %row1 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 0
  %1 = load i16, i16* %row.addr, align 2
  store i16 %1, i16* %row1, align 64
  %col = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 1
  %2 = load i16, i16* %col0.addr, align 2
  store i16 %2, i16* %col, align 2
  %3 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %3, i8 0, i64 1088, i1 false)
  %row2 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 0
  %4 = load i16, i16* %row.addr, align 2
  store i16 %4, i16* %row2, align 64
  %col3 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 1
  %5 = load i16, i16* %col1.addr, align 2
  store i16 %5, i16* %col3, align 2
  %6 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %6, i8 0, i64 1088, i1 false)
  %row4 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 0
  %7 = load i16, i16* %row.addr, align 2
  store i16 %7, i16* %row4, align 64
  %col5 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 1
  %8 = load i16, i16* %col0.addr, align 2
  store i16 %8, i16* %col5, align 2
  store %struct.__tile1024i_str* %tile0, %struct.__tile1024i_str** %dst0.addr.i, align 8
  store %struct.__tile1024i_str* %tile1, %struct.__tile1024i_str** %dst1.addr.i, align 8
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf, i64 0, i64 0), i8** %base.addr.i17, align 8
  store i64 32, i64* %stride.addr.i18, align 8
  %9 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %row.i19 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %9, i32 0, i32 0
  %10 = load i16, i16* %row.i19, align 64
  %11 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %col.i20 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %11, i32 0, i32 1
  %12 = load i16, i16* %col.i20, align 2
  %13 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %col1.i21 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %13, i32 0, i32 1
  %14 = load i16, i16* %col1.i21, align 2
  %15 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %tile.i22 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %15, i32 0, i32 3
  %16 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %tile2.i23 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %16, i32 0, i32 3
  %17 = load i8*, i8** %base.addr.i17, align 8
  %18 = load i64, i64* %stride.addr.i18, align 8
  store i16 %10, i16* %row.addr.i, align 2
  store i16 %12, i16* %col0.addr.i, align 2
  store i16 %14, i16* %col1.addr.i, align 2
  store <256 x i32>* %tile.i22, <256 x i32>** %dst0.addr.i34, align 8
  store <256 x i32>* %tile2.i23, <256 x i32>** %dst1.addr.i35, align 8
  store i8* %17, i8** %base.addr.i36, align 8
  store i64 %18, i64* %stride.addr.i37, align 8
  %19 = load i16, i16* %row.addr.i, align 2
  %20 = load i16, i16* %col0.addr.i, align 2
  %21 = load i16, i16* %col1.addr.i, align 2
  %22 = load <256 x i32>*, <256 x i32>** %dst0.addr.i34, align 8
  %23 = load <256 x i32>*, <256 x i32>** %dst1.addr.i35, align 8
  %24 = load i8*, i8** %base.addr.i36, align 8
  %25 = load i64, i64* %stride.addr.i37, align 8
  %26 = call { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz1.internal(i16 %19, i16 %20, i16 %21, i8* %24, i64 %25) #4
  %27 = extractvalue { x86_amx, x86_amx } %26, 0
  %28 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %27) #4
  store <256 x i32> %28, <256 x i32>* %22, align 1024
  %29 = extractvalue { x86_amx, x86_amx } %26, 1
  %30 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %29) #4
  store <256 x i32> %30, <256 x i32>* %23, align 1024
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i, align 8
  %31 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %row.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %31, i32 0, i32 0
  %32 = load i16, i16* %row.i, align 64
  %33 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %col.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %33, i32 0, i32 1
  %34 = load i16, i16* %col.i, align 2
  %35 = call x86_amx @llvm.x86.tilezero.internal(i16 %32, i16 %34) #4
  %36 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %35) #4
  %37 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %tile.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %37, i32 0, i32 3
  store <256 x i32> %36, <256 x i32>* %tile.i, align 64
  %38 = bitcast %struct.__tile1024i_str* %tile17 to i8*
  %39 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %38, i8* align 1 %39, i64 1088, i1 false) #4
  %40 = bitcast %struct.__tile1024i_str* %tile06 to i8*
  %41 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %40, i8* align 1 %41, i64 1088, i1 false) #4
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %row.i9 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 0
  %42 = load i16, i16* %row.i9, align 64
  %col.i10 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 1
  %43 = load i16, i16* %col.i10, align 2
  %col1.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 1
  %44 = load i16, i16* %col1.i, align 2
  %45 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile.i11 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %45, i32 0, i32 3
  %46 = load <256 x i32>, <256 x i32>* %tile.i11, align 64
  %tile2.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 3
  %47 = load <256 x i32>, <256 x i32>* %tile2.i, align 64
  %tile3.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 3
  %48 = load <256 x i32>, <256 x i32>* %tile3.i, align 64
  store <256 x i32> %46, <256 x i32>* %indirect-arg-temp.i, align 1024
  store <256 x i32> %47, <256 x i32>* %indirect-arg-temp4.i, align 1024
  store <256 x i32> %48, <256 x i32>* %indirect-arg-temp5.i, align 1024
  %49 = bitcast <256 x i32>* %indirect-arg-temp5.i26 to i8*
  %50 = bitcast <256 x i32>* %indirect-arg-temp5.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %49, i8* align 1 %50, i64 1024, i1 false) #4
  %51 = bitcast <256 x i32>* %indirect-arg-temp4.i25 to i8*
  %52 = bitcast <256 x i32>* %indirect-arg-temp4.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %51, i8* align 1 %52, i64 1024, i1 false) #4
  %53 = bitcast <256 x i32>* %indirect-arg-temp.i24 to i8*
  %54 = bitcast <256 x i32>* %indirect-arg-temp.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %53, i8* align 1 %54, i64 1024, i1 false) #4
  %dst.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i24, align 1024
  %src1.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp4.i25, align 1024
  %src2.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp5.i26, align 1024
  store i16 %42, i16* %m.addr.i, align 2
  store i16 %43, i16* %n.addr.i, align 2
  store i16 %44, i16* %k.addr.i, align 2
  store <256 x i32> %dst.i, <256 x i32>* %dst.addr.i27, align 64
  store <256 x i32> %src1.i, <256 x i32>* %src1.addr.i, align 64
  store <256 x i32> %src2.i, <256 x i32>* %src2.addr.i, align 64
  %55 = load i16, i16* %m.addr.i, align 2
  %56 = load i16, i16* %n.addr.i, align 2
  %57 = load i16, i16* %k.addr.i, align 2
  %58 = load <256 x i32>, <256 x i32>* %dst.addr.i27, align 64
  %59 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %58) #4
  %60 = load <256 x i32>, <256 x i32>* %src1.addr.i, align 64
  %61 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %60) #4
  %62 = load <256 x i32>, <256 x i32>* %src2.addr.i, align 64
  %63 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %62) #4
  %64 = call x86_amx @llvm.x86.tdpbssd.internal(i16 %55, i16 %56, i16 %57, x86_amx %59, x86_amx %61, x86_amx %63) #4
  %65 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %64) #4
  %66 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile6.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %66, i32 0, i32 3
  store <256 x i32> %65, <256 x i32>* %tile6.i, align 64
  %67 = bitcast %struct.__tile1024i_str* %dst12 to i8*
  %68 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %67, i8* align 1 %68, i64 1088, i1 false) #4
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf2, i64 0, i64 0), i8** %base.addr.i, align 8
  store i64 32, i64* %stride.addr.i, align 8
  %row.i14 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 0
  %69 = load i16, i16* %row.i14, align 64
  %col.i15 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 1
  %70 = load i16, i16* %col.i15, align 2
  %71 = load i8*, i8** %base.addr.i, align 8
  %72 = load i64, i64* %stride.addr.i, align 8
  %tile.i16 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 3
  %73 = load <256 x i32>, <256 x i32>* %tile.i16, align 64
  store <256 x i32> %73, <256 x i32>* %indirect-arg-temp.i13, align 1024
  %74 = bitcast <256 x i32>* %indirect-arg-temp.i1328 to i8*
  %75 = bitcast <256 x i32>* %indirect-arg-temp.i13 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %74, i8* align 1 %75, i64 1024, i1 false) #4
  %tile.i33 = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i1328, align 1024
  store i16 %69, i16* %m.addr.i29, align 2
  store i16 %70, i16* %n.addr.i30, align 2
  store i8* %71, i8** %base.addr.i31, align 8
  store i64 %72, i64* %stride.addr.i32, align 8
  store <256 x i32> %tile.i33, <256 x i32>* %tile.addr.i, align 64
  %76 = load i16, i16* %m.addr.i29, align 2
  %77 = load i16, i16* %n.addr.i30, align 2
  %78 = load i8*, i8** %base.addr.i31, align 8
  %79 = load i64, i64* %stride.addr.i32, align 8
  %80 = load <256 x i32>, <256 x i32>* %tile.addr.i, align 64
  %81 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %80) #4
  call void @llvm.x86.tilestored64.internal(i16 %76, i16 %77, i8* %78, i64 %79, x86_amx %81) #4
  ret void
}

; Function Attrs: noinline nounwind optnone
define dso_local void @test_tile_t2rpntlvwz1t1(i16 noundef signext %row, i16 noundef signext %col0, i16 noundef signext %col1) #0 {
; CHECK-LABEL: test_tile_t2rpntlvwz1t1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    movq %rsp, %rbp
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    andq $-1024, %rsp # imm = 0xFC00
; CHECK-NEXT:    subq $25600, %rsp # imm = 0x6400
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movb $1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, %ax
; CHECK-NEXT:    movw %si, %cx
; CHECK-NEXT:    movw %di, %dx
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    xorl %esi, %esi
; CHECK-NEXT:    movl %esi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $1088, %edx # imm = 0x440
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %esi # 4-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    callq memset@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %di
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rsi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq buf@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw (%r9), %bx
; CHECK-NEXT:    movw 2(%r9), %r11w
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw 2(%r8), %r10w
; CHECK-NEXT:    addq $64, %r9
; CHECK-NEXT:    addq $64, %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw %bx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r11w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %r8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rdi, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %rbx
; CHECK-NEXT:    movw %bx, %r10w
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    movw %r9w, %di
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %r10w, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $cl
; CHECK-NEXT:    movb %cl, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r14
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r15
; CHECK-NEXT:    t2rpntlvwz1t1 (%r14,%r15), %tmm2
; CHECK-NEXT:    tilestored %tmm2, (%r11,%rbx)
; CHECK-NEXT:    tilestored %tmm3, (%r8,%r9)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movw (%rcx), %ax
; CHECK-NEXT:    movswq 2(%rcx), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    tilezero %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    # kill: def $rcx killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    vmovdqa64 64(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 128(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 192(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 256(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 320(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 384(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 448(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 512(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 576(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 640(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 704(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 768(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 832(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 896(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 960(%rax), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 1024(%rax), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movl $1024, %edx # imm = 0x400
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %di # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %cx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    # kill: def $r8 killed $rax
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %ax # 2-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovaps %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm16
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm17
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm18
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm19
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm20
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm21
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm22
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm23
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm24
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm25
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm26
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm27
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm28
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm29
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm30
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm31
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovaps %zmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %ax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; CHECK-NEXT:    vmovdqa64 %zmm31, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm30, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm29, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm28, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm27, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm26, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm25, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm24, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm23, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm22, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm21, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm20, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm19, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm18, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm17, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm16, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    movw %r8w, %cx
; CHECK-NEXT:    movswq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    movw %r11w, %di
; CHECK-NEXT:    movl %r11d, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    movzwl %r9w, %r9d
; CHECK-NEXT:    shrl $2, %r9d
; CHECK-NEXT:    # kill: def $r9w killed $r9w killed $r9d
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $r9b
; CHECK-NEXT:    movb %r9b, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %di, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm0
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r11), %tmm1
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    tileloadd (%r10,%r8), %tmm2
; CHECK-NEXT:    tdpbssd %tmm2, %tmm1, %tmm0
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    addq $64, %rdi
; CHECK-NEXT:    tilestored %tmm0, (%rdi,%r8)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; CHECK-NEXT:    movq buf2@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $32, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw %ax, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    callq memcpy@PLT
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %si # 2-byte Reload
; CHECK-NEXT:    movw {{[-0-9]+}}(%r{{[sb]}}p), %dx # 2-byte Reload
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; CHECK-NEXT:    # kill: def $rdi killed $rax
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm0
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm1
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm2
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm3
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm4
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm5
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm6
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm7
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm8
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm9
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm10
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm11
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm12
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm13
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm14
; CHECK-NEXT:    vmovdqa64 {{[0-9]+}}(%rsp), %zmm15
; CHECK-NEXT:    movw %si, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %dx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm15, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm14, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm13, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm12, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm11, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm10, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm9, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm8, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm7, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm6, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm5, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm4, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm3, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm2, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    vmovdqa64 %zmm0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %ax
; CHECK-NEXT:    movw {{[0-9]+}}(%rsp), %cx
; CHECK-NEXT:    # implicit-def: $al
; CHECK-NEXT:    movb %al, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movw %cx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    ldtilecfg {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    movswq %cx, %r8
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    tileloadd (%rdi,%r8), %tmm0
; CHECK-NEXT:    tilestored %tmm0, (%rdx,%rsi)
; CHECK-NEXT:    leaq -24(%rbp), %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    tilerelease
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %row.addr.i = alloca i16, align 2
  %col0.addr.i = alloca i16, align 2
  %col1.addr.i = alloca i16, align 2
  %dst0.addr.i34 = alloca <256 x i32>*, align 8
  %dst1.addr.i35 = alloca <256 x i32>*, align 8
  %base.addr.i36 = alloca i8*, align 8
  %stride.addr.i37 = alloca i64, align 8
  %m.addr.i29 = alloca i16, align 2
  %n.addr.i30 = alloca i16, align 2
  %base.addr.i31 = alloca i8*, align 8
  %stride.addr.i32 = alloca i64, align 8
  %tile.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp.i1328 = alloca <256 x i32>, align 1024
  %m.addr.i = alloca i16, align 2
  %n.addr.i = alloca i16, align 2
  %k.addr.i = alloca i16, align 2
  %dst.addr.i27 = alloca <256 x i32>, align 64
  %src1.addr.i = alloca <256 x i32>, align 64
  %src2.addr.i = alloca <256 x i32>, align 64
  %indirect-arg-temp5.i26 = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i25 = alloca <256 x i32>, align 1024
  %indirect-arg-temp.i24 = alloca <256 x i32>, align 1024
  %dst0.addr.i = alloca %struct.__tile1024i_str*, align 8
  %dst1.addr.i = alloca %struct.__tile1024i_str*, align 8
  %base.addr.i17 = alloca i8*, align 8
  %stride.addr.i18 = alloca i64, align 8
  %base.addr.i = alloca i8*, align 8
  %stride.addr.i = alloca i64, align 8
  %indirect-arg-temp.i13 = alloca <256 x i32>, align 1024
  %dst12 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i8 = alloca %struct.__tile1024i_str*, align 8
  %indirect-arg-temp.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp4.i = alloca <256 x i32>, align 1024
  %indirect-arg-temp5.i = alloca <256 x i32>, align 1024
  %tile17 = alloca %struct.__tile1024i_str, align 64
  %tile06 = alloca %struct.__tile1024i_str, align 64
  %dst.addr.i = alloca %struct.__tile1024i_str*, align 8
  %row.addr = alloca i16, align 2
  %col0.addr = alloca i16, align 2
  %col1.addr = alloca i16, align 2
  %tile0 = alloca %struct.__tile1024i_str, align 64
  %tile1 = alloca %struct.__tile1024i_str, align 64
  %dst = alloca %struct.__tile1024i_str, align 64
  store i16 %row, i16* %row.addr, align 2
  store i16 %col0, i16* %col0.addr, align 2
  store i16 %col1, i16* %col1.addr, align 2
  %0 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %0, i8 0, i64 1088, i1 false)
  %row1 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 0
  %1 = load i16, i16* %row.addr, align 2
  store i16 %1, i16* %row1, align 64
  %col = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile0, i32 0, i32 1
  %2 = load i16, i16* %col0.addr, align 2
  store i16 %2, i16* %col, align 2
  %3 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %3, i8 0, i64 1088, i1 false)
  %row2 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 0
  %4 = load i16, i16* %row.addr, align 2
  store i16 %4, i16* %row2, align 64
  %col3 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile1, i32 0, i32 1
  %5 = load i16, i16* %col1.addr, align 2
  store i16 %5, i16* %col3, align 2
  %6 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memset.p0i8.i64(i8* align 64 %6, i8 0, i64 1088, i1 false)
  %row4 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 0
  %7 = load i16, i16* %row.addr, align 2
  store i16 %7, i16* %row4, align 64
  %col5 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst, i32 0, i32 1
  %8 = load i16, i16* %col0.addr, align 2
  store i16 %8, i16* %col5, align 2
  store %struct.__tile1024i_str* %tile0, %struct.__tile1024i_str** %dst0.addr.i, align 8
  store %struct.__tile1024i_str* %tile1, %struct.__tile1024i_str** %dst1.addr.i, align 8
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf, i64 0, i64 0), i8** %base.addr.i17, align 8
  store i64 32, i64* %stride.addr.i18, align 8
  %9 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %row.i19 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %9, i32 0, i32 0
  %10 = load i16, i16* %row.i19, align 64
  %11 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %col.i20 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %11, i32 0, i32 1
  %12 = load i16, i16* %col.i20, align 2
  %13 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %col1.i21 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %13, i32 0, i32 1
  %14 = load i16, i16* %col1.i21, align 2
  %15 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst0.addr.i, align 8
  %tile.i22 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %15, i32 0, i32 3
  %16 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst1.addr.i, align 8
  %tile2.i23 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %16, i32 0, i32 3
  %17 = load i8*, i8** %base.addr.i17, align 8
  %18 = load i64, i64* %stride.addr.i18, align 8
  store i16 %10, i16* %row.addr.i, align 2
  store i16 %12, i16* %col0.addr.i, align 2
  store i16 %14, i16* %col1.addr.i, align 2
  store <256 x i32>* %tile.i22, <256 x i32>** %dst0.addr.i34, align 8
  store <256 x i32>* %tile2.i23, <256 x i32>** %dst1.addr.i35, align 8
  store i8* %17, i8** %base.addr.i36, align 8
  store i64 %18, i64* %stride.addr.i37, align 8
  %19 = load i16, i16* %row.addr.i, align 2
  %20 = load i16, i16* %col0.addr.i, align 2
  %21 = load i16, i16* %col1.addr.i, align 2
  %22 = load <256 x i32>*, <256 x i32>** %dst0.addr.i34, align 8
  %23 = load <256 x i32>*, <256 x i32>** %dst1.addr.i35, align 8
  %24 = load i8*, i8** %base.addr.i36, align 8
  %25 = load i64, i64* %stride.addr.i37, align 8
  %26 = call { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz1t1.internal(i16 %19, i16 %20, i16 %21, i8* %24, i64 %25) #4
  %27 = extractvalue { x86_amx, x86_amx } %26, 0
  %28 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %27) #4
  store <256 x i32> %28, <256 x i32>* %22, align 1024
  %29 = extractvalue { x86_amx, x86_amx } %26, 1
  %30 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %29) #4
  store <256 x i32> %30, <256 x i32>* %23, align 1024
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i, align 8
  %31 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %row.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %31, i32 0, i32 0
  %32 = load i16, i16* %row.i, align 64
  %33 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %col.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %33, i32 0, i32 1
  %34 = load i16, i16* %col.i, align 2
  %35 = call x86_amx @llvm.x86.tilezero.internal(i16 %32, i16 %34) #4
  %36 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %35) #4
  %37 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i, align 8
  %tile.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %37, i32 0, i32 3
  store <256 x i32> %36, <256 x i32>* %tile.i, align 64
  %38 = bitcast %struct.__tile1024i_str* %tile17 to i8*
  %39 = bitcast %struct.__tile1024i_str* %tile1 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %38, i8* align 1 %39, i64 1088, i1 false) #4
  %40 = bitcast %struct.__tile1024i_str* %tile06 to i8*
  %41 = bitcast %struct.__tile1024i_str* %tile0 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %40, i8* align 1 %41, i64 1088, i1 false) #4
  store %struct.__tile1024i_str* %dst, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %row.i9 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 0
  %42 = load i16, i16* %row.i9, align 64
  %col.i10 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 1
  %43 = load i16, i16* %col.i10, align 2
  %col1.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 1
  %44 = load i16, i16* %col1.i, align 2
  %45 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile.i11 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %45, i32 0, i32 3
  %46 = load <256 x i32>, <256 x i32>* %tile.i11, align 64
  %tile2.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile06, i32 0, i32 3
  %47 = load <256 x i32>, <256 x i32>* %tile2.i, align 64
  %tile3.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %tile17, i32 0, i32 3
  %48 = load <256 x i32>, <256 x i32>* %tile3.i, align 64
  store <256 x i32> %46, <256 x i32>* %indirect-arg-temp.i, align 1024
  store <256 x i32> %47, <256 x i32>* %indirect-arg-temp4.i, align 1024
  store <256 x i32> %48, <256 x i32>* %indirect-arg-temp5.i, align 1024
  %49 = bitcast <256 x i32>* %indirect-arg-temp5.i26 to i8*
  %50 = bitcast <256 x i32>* %indirect-arg-temp5.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %49, i8* align 1 %50, i64 1024, i1 false) #4
  %51 = bitcast <256 x i32>* %indirect-arg-temp4.i25 to i8*
  %52 = bitcast <256 x i32>* %indirect-arg-temp4.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %51, i8* align 1 %52, i64 1024, i1 false) #4
  %53 = bitcast <256 x i32>* %indirect-arg-temp.i24 to i8*
  %54 = bitcast <256 x i32>* %indirect-arg-temp.i to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %53, i8* align 1 %54, i64 1024, i1 false) #4
  %dst.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i24, align 1024
  %src1.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp4.i25, align 1024
  %src2.i = load <256 x i32>, <256 x i32>* %indirect-arg-temp5.i26, align 1024
  store i16 %42, i16* %m.addr.i, align 2
  store i16 %43, i16* %n.addr.i, align 2
  store i16 %44, i16* %k.addr.i, align 2
  store <256 x i32> %dst.i, <256 x i32>* %dst.addr.i27, align 64
  store <256 x i32> %src1.i, <256 x i32>* %src1.addr.i, align 64
  store <256 x i32> %src2.i, <256 x i32>* %src2.addr.i, align 64
  %55 = load i16, i16* %m.addr.i, align 2
  %56 = load i16, i16* %n.addr.i, align 2
  %57 = load i16, i16* %k.addr.i, align 2
  %58 = load <256 x i32>, <256 x i32>* %dst.addr.i27, align 64
  %59 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %58) #4
  %60 = load <256 x i32>, <256 x i32>* %src1.addr.i, align 64
  %61 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %60) #4
  %62 = load <256 x i32>, <256 x i32>* %src2.addr.i, align 64
  %63 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %62) #4
  %64 = call x86_amx @llvm.x86.tdpbssd.internal(i16 %55, i16 %56, i16 %57, x86_amx %59, x86_amx %61, x86_amx %63) #4
  %65 = call <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx %64) #4
  %66 = load %struct.__tile1024i_str*, %struct.__tile1024i_str** %dst.addr.i8, align 8
  %tile6.i = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %66, i32 0, i32 3
  store <256 x i32> %65, <256 x i32>* %tile6.i, align 64
  %67 = bitcast %struct.__tile1024i_str* %dst12 to i8*
  %68 = bitcast %struct.__tile1024i_str* %dst to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %67, i8* align 1 %68, i64 1088, i1 false) #4
  store i8* getelementptr inbounds ([2048 x i8], [2048 x i8]* @buf2, i64 0, i64 0), i8** %base.addr.i, align 8
  store i64 32, i64* %stride.addr.i, align 8
  %row.i14 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 0
  %69 = load i16, i16* %row.i14, align 64
  %col.i15 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 1
  %70 = load i16, i16* %col.i15, align 2
  %71 = load i8*, i8** %base.addr.i, align 8
  %72 = load i64, i64* %stride.addr.i, align 8
  %tile.i16 = getelementptr inbounds %struct.__tile1024i_str, %struct.__tile1024i_str* %dst12, i32 0, i32 3
  %73 = load <256 x i32>, <256 x i32>* %tile.i16, align 64
  store <256 x i32> %73, <256 x i32>* %indirect-arg-temp.i13, align 1024
  %74 = bitcast <256 x i32>* %indirect-arg-temp.i1328 to i8*
  %75 = bitcast <256 x i32>* %indirect-arg-temp.i13 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %74, i8* align 1 %75, i64 1024, i1 false) #4
  %tile.i33 = load <256 x i32>, <256 x i32>* %indirect-arg-temp.i1328, align 1024
  store i16 %69, i16* %m.addr.i29, align 2
  store i16 %70, i16* %n.addr.i30, align 2
  store i8* %71, i8** %base.addr.i31, align 8
  store i64 %72, i64* %stride.addr.i32, align 8
  store <256 x i32> %tile.i33, <256 x i32>* %tile.addr.i, align 64
  %76 = load i16, i16* %m.addr.i29, align 2
  %77 = load i16, i16* %n.addr.i30, align 2
  %78 = load i8*, i8** %base.addr.i31, align 8
  %79 = load i64, i64* %stride.addr.i32, align 8
  %80 = load <256 x i32>, <256 x i32>* %tile.addr.i, align 64
  %81 = call x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32> %80) #4
  call void @llvm.x86.tilestored64.internal(i16 %76, i16 %77, i8* %78, i64 %79, x86_amx %81) #4
  ret void
}

; Function Attrs: argmemonly nounwind readonly
declare { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz0.internal(i16, i16, i16, i8*, i64) #2

; Function Attrs: nounwind readnone
declare <256 x i32> @llvm.x86.cast.tile.to.vector.v256i32(x86_amx) #3

; Function Attrs: nounwind
declare x86_amx @llvm.x86.tilezero.internal(i16, i16) #4

; Function Attrs: nounwind
declare x86_amx @llvm.x86.tdpbssd.internal(i16, i16, i16, x86_amx, x86_amx, x86_amx) #4

; Function Attrs: nounwind readnone
declare x86_amx @llvm.x86.cast.vector.to.tile.v256i32(<256 x i32>) #3

; Function Attrs: argmemonly nounwind writeonly
declare void @llvm.x86.tilestored64.internal(i16, i16, i8*, i64, x86_amx) #5

; Function Attrs: argmemonly nounwind
declare { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz0t1.internal(i16, i16, i16, i8*, i64) #6

; Function Attrs: argmemonly nounwind
declare { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz1.internal(i16, i16, i16, i8*, i64) #6

; Function Attrs: argmemonly nounwind
declare { x86_amx, x86_amx } @llvm.x86.t2rpntlvwz1t1.internal(i16, i16, i16, i8*, i64) #6

; Function Attrs: argmemonly nofree nounwind willreturn
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* noalias nocapture writeonly, i8* noalias nocapture readonly, i64, i1 immarg) #7

attributes #0 = { noinline nounwind optnone "frame-pointer"="none" "min-legal-vector-width"="8192" "no-builtins" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+amx-avx512,+amx-bf16,+amx-int8,+amx-tile,+amx-transpose,+avx,+avx2,+avx512f,+crc32,+cx8,+f16c,+fma,+mmx,+popcnt,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave" }
attributes #1 = { argmemonly nofree nounwind willreturn writeonly }
attributes #2 = { argmemonly nounwind readonly }
attributes #3 = { nounwind readnone }
attributes #4 = { nounwind }
attributes #5 = { argmemonly nounwind writeonly }
attributes #6 = { argmemonly nounwind }
attributes #7 = { argmemonly nofree nounwind willreturn }

!llvm.module.flags = !{!0}
!llvm.ident = !{!1}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2023.0.0 (2023.x.0.YYYYMMDD)"}
