; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_dotprod_int8
; RUN: llc -O3 -disable-peephole -mtriple=x86_64-unknown-unknown -mattr=+avx512vl,+avx512dotprodint8 < %s | FileCheck %s --check-prefixes=CHECK,AVX512DOTPROD
; RUN: llc -O3 -disable-peephole -mtriple=x86_64-unknown-unknown -mattr=+avxdotprodint8 < %s | FileCheck %s --check-prefixes=CHECK,AVXDOTPROD

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-unknown"

declare <4 x i32> @llvm.x86.avx2.vpdpbssd.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbssd(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssd:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssd.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbssd_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssd_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssd.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbssd_mask_commuted(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssd_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssd_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbssd.128(<4 x i32> %2, <4 x i32> %a2, <4 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbssd_maskz_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssd_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssd_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbssd %xmm1, %xmm2, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssd.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <4 x i32> @llvm.x86.avx2.vpdpbssds.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbssds(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssds:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssds.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbssds_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssds_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssds.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbssds_mask_commuted(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssds_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssds_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbssds.128(<4 x i32> %2, <4 x i32> %a2, <4 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbssds_maskz_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssds_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssds_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbssds %xmm1, %xmm2, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbssds.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <4 x i32> @llvm.x86.avx2.vpdpbuud.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbuud(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuud:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuud.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbuud_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuud_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuud.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbuud_mask_commuted(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuud_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuud_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbuud.128(<4 x i32> %2, <4 x i32> %a2, <4 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbuud_maskz_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuud_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuud_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbuud %xmm1, %xmm2, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuud.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <4 x i32> @llvm.x86.avx2.vpdpbuuds.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbuuds(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuuds:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuuds.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbuuds_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuuds_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuuds.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbuuds_mask_commuted(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuuds_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuuds_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbuuds.128(<4 x i32> %2, <4 x i32> %a2, <4 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbuuds_maskz_commuted(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuuds_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuuds_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbuuds %xmm1, %xmm2, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbuuds.128(<4 x i32> %a0, <4 x i32> %a2, <4 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <4 x i32> @llvm.x86.avx2.vpdpbsud.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbsud(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbsud:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbsud.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbsud_mask(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsud_mask:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsud_mask:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbsud.128(<4 x i32> %2, <4 x i32> %a1, <4 x i32> %a2)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbsud_maskz(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsud_maskz:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsud_maskz:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbsud %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbsud.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <4 x i32> @llvm.x86.avx2.vpdpbsuds.128(<4 x i32>, <4 x i32>, <4 x i32>)

define <4 x i32> @stack_fold_vpdpbsuds(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbsuds:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 # 16-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbsuds.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  ret <4 x i32> %2
}

define <4 x i32> @stack_fold_vpdpbsuds_mask(<4 x i32>* %a0, <4 x i32> %a1, <4 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsuds_mask:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %xmm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 {%k1} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, %xmm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsuds_mask:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %xmm1
; AVXDOTPROD-NEXT:    vmovaps %xmm1, %xmm2
; AVXDOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm2 # 16-byte Folded Reload
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    movl %esi, %ecx
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vmovd %ecx, %xmm0
; AVXDOTPROD-NEXT:    vpinsrb $2, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    movl %esi, %eax
; AVXDOTPROD-NEXT:    shrb $2, %al
; AVXDOTPROD-NEXT:    andb $1, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $4, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    shrb $3, %sil
; AVXDOTPROD-NEXT:    movzbl %sil, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm0, %xmm0
; AVXDOTPROD-NEXT:    vblendvps %xmm0, %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <4 x i32>, <4 x i32>* %a0
  %3 = call <4 x i32> @llvm.x86.avx2.vpdpbsuds.128(<4 x i32> %2, <4 x i32> %a1, <4 x i32> %a2)
  %4 = bitcast i8 %mask to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %3, <4 x i32> %2
  ret <4 x i32> %5
}

define <4 x i32> @stack_fold_vpdpbsuds_maskz(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsuds_maskz:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm0 {%k1} {z} # 16-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsuds_maskz:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vpdpbsuds %xmm2, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    movb (%rdi), %al
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    movl %eax, %edx
; AVXDOTPROD-NEXT:    andb $1, %dl
; AVXDOTPROD-NEXT:    movzbl %dl, %edx
; AVXDOTPROD-NEXT:    vmovd %edx, %xmm1
; AVXDOTPROD-NEXT:    vpinsrb $2, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    movl %eax, %ecx
; AVXDOTPROD-NEXT:    shrb $2, %cl
; AVXDOTPROD-NEXT:    andb $1, %cl
; AVXDOTPROD-NEXT:    movzbl %cl, %ecx
; AVXDOTPROD-NEXT:    vpinsrb $4, %ecx, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    shrb $3, %al
; AVXDOTPROD-NEXT:    movzbl %al, %eax
; AVXDOTPROD-NEXT:    vpinsrb $6, %eax, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpmovzxwd {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVXDOTPROD-NEXT:    vpslld $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpsrad $31, %xmm1, %xmm1
; AVXDOTPROD-NEXT:    vpand %xmm0, %xmm1, %xmm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <4 x i32> @llvm.x86.avx2.vpdpbsuds.128(<4 x i32> %a0, <4 x i32> %a1, <4 x i32> %a2)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %extract = shufflevector <8 x i1> %4, <8 x i1> %4, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %5 = select <4 x i1> %extract, <4 x i32> %2, <4 x i32> zeroinitializer
  ret <4 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbssd.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbssd256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssd256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssd.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbssd256_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssd256_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssd.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbssd256__mask_commuted(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssd256__mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssd256__mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbssd.256(<8 x i32> %2, <8 x i32> %a2, <8 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbssd256_maskz_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssd256_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssd256_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbssd {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssd.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbssds.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbssds256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssds256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssds.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbssds256_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbssds256_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssds.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbssds256_mask_commuted(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssds256_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssds256_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbssds.256(<8 x i32> %2, <8 x i32> %a2, <8 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbssds256_maskz_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbssds256_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbssds256_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbssds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbssds.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbuud.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbuud256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuud256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuud.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbuud256_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuud256_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuud.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbuud256_mask_commuted(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuud256_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuud256_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbuud.256(<8 x i32> %2, <8 x i32> %a2, <8 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbuud256_maskz_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuud256_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuud256_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbuud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuud.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbuuds.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbuuds256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuuds256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuuds.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbuuds256_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbuuds256_commuted:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuuds.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbuuds256_mask_commuted(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuuds256_mask_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuuds256_mask_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbuuds.256(<8 x i32> %2, <8 x i32> %a2, <8 x i32> %a1)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbuuds256_maskz_commuted(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbuuds256_maskz_commuted:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbuuds256_maskz_commuted:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbuuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbuuds.256(<8 x i32> %a0, <8 x i32> %a2, <8 x i32> %a1)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbsud.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbsud256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbsud256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbsud.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbsud256_mask(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsud256_mask:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsud256_mask:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbsud.256(<8 x i32> %2, <8 x i32> %a1, <8 x i32> %a2)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbsud256_maskz(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsud256_maskz:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsud256_maskz:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbsud {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbsud.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}

declare <8 x i32> @llvm.x86.avx2.vpdpbsuds.256(<8 x i32>, <8 x i32>, <8 x i32>)

define <8 x i32> @stack_fold_vpdpbsuds256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2) {
; CHECK-LABEL: stack_fold_vpdpbsuds256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    #APP
; CHECK-NEXT:    nop
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; CHECK-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbsuds.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  ret <8 x i32> %2
}

define <8 x i32> @stack_fold_vpdpbsuds256_mask(<8 x i32>* %a0, <8 x i32> %a1, <8 x i32> %a2, i8 %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsuds256_mask:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    vmovaps (%rdi), %ymm2
; AVX512DOTPROD-NEXT:    kmovw %esi, %k1
; AVX512DOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    vmovaps %ymm2, %ymm0
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsuds256_mask:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vmovaps (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovaps %ymm1, %ymm2
; AVXDOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vmovd %esi, %xmm0
; AVXDOTPROD-NEXT:    vpbroadcastb %xmm0, %ymm0
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm3 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm3, %ymm0, %ymm0
; AVXDOTPROD-NEXT:    vblendvps %ymm0, %ymm2, %ymm1, %ymm0
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = load <8 x i32>, <8 x i32>* %a0
  %3 = call <8 x i32> @llvm.x86.avx2.vpdpbsuds.256(<8 x i32> %2, <8 x i32> %a1, <8 x i32> %a2)
  %4 = bitcast i8 %mask to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %3, <8 x i32> %2
  ret <8 x i32> %5
}

define <8 x i32> @stack_fold_vpdpbsuds256_maskz(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2, i8* %mask) {
; AVX512DOTPROD-LABEL: stack_fold_vpdpbsuds256_maskz:
; AVX512DOTPROD:       # %bb.0:
; AVX512DOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVX512DOTPROD-NEXT:    #APP
; AVX512DOTPROD-NEXT:    nop
; AVX512DOTPROD-NEXT:    #NO_APP
; AVX512DOTPROD-NEXT:    movzbl (%rdi), %eax
; AVX512DOTPROD-NEXT:    kmovw %eax, %k1
; AVX512DOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; AVX512DOTPROD-NEXT:    retq
;
; AVXDOTPROD-LABEL: stack_fold_vpdpbsuds256_maskz:
; AVXDOTPROD:       # %bb.0:
; AVXDOTPROD-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; AVXDOTPROD-NEXT:    #APP
; AVXDOTPROD-NEXT:    nop
; AVXDOTPROD-NEXT:    #NO_APP
; AVXDOTPROD-NEXT:    vpdpbsuds {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; AVXDOTPROD-NEXT:    vpbroadcastb (%rdi), %ymm1
; AVXDOTPROD-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVXDOTPROD-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVXDOTPROD-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
<<<<<<< HEAD
; AVXDOTPROD-NEXT:    vpand %ymm0, %ymm1, %ymm0
=======
; AVXDOTPROD-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVXDOTPROD-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
>>>>>>> a74dcdaa5c8db9d7a935f862d27dbbe92ae44d0a
; AVXDOTPROD-NEXT:    retq
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = call <8 x i32> @llvm.x86.avx2.vpdpbsuds.256(<8 x i32> %a0, <8 x i32> %a1, <8 x i32> %a2)
  %3 = load i8, i8* %mask
  %4 = bitcast i8 %3 to <8 x i1>
  %5 = select <8 x i1> %4, <8 x i32> %2, <8 x i32> zeroinitializer
  ret <8 x i32> %5
}
