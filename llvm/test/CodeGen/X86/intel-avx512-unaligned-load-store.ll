; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -x86-enable-unaligned-vector-move -mattr=avx512f | FileCheck %s -check-prefix=X64
; RUN: llc < %s -mtriple=i686-unknown-unknown -x86-enable-unaligned-vector-move -mattr=avx512f | FileCheck %s -check-prefix=X86

define <16 x i32> @test17(i8 * %addr) {
; X64-LABEL: test17:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test17:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x i32>*
  %res = load <16 x i32>, <16 x i32>* %vaddr, align 64
  ret <16 x i32>%res
}

define void @test18(i8 * %addr, <8 x i64> %data) {
; X64-LABEL: test18:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test18:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i64>*
  store <8 x i64>%data, <8 x i64>* %vaddr, align 64
  ret void
}

define void @test19(i8 * %addr, <16 x i32> %data) {
; X64-LABEL: test19:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test19:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x i32>*
  store <16 x i32>%data, <16 x i32>* %vaddr, align 1
  ret void
}

define void @test20(i8 * %addr, <16 x i32> %data) {
; X64-LABEL: test20:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test20:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x i32>*
  store <16 x i32>%data, <16 x i32>* %vaddr, align 64
  ret void
}

define  <8 x i64> @test21(i8 * %addr) {
; X64-LABEL: test21:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test21:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %res = load <8 x i64>, <8 x i64>* %vaddr, align 64
  ret <8 x i64>%res
}

define void @test22(i8 * %addr, <8 x i64> %data) {
; X64-LABEL: test22:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test22:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i64>*
  store <8 x i64>%data, <8 x i64>* %vaddr, align 1
  ret void
}

define <8 x i64> @test23(i8 * %addr) {
; X64-LABEL: test23:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0
; X64-NEXT:    retq
;
; X86-LABEL: test23:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %res = load <8 x i64>, <8 x i64>* %vaddr, align 1
  ret <8 x i64>%res
}

define void @test24(i8 * %addr, <8 x double> %data) {
; X64-LABEL: test24:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test24:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x double>*
  store <8 x double>%data, <8 x double>* %vaddr, align 64
  ret void
}

define <8 x double> @test25(i8 * %addr) {
; X64-LABEL: test25:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test25:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x double>*
  %res = load <8 x double>, <8 x double>* %vaddr, align 64
  ret <8 x double>%res
}

define void @test26(i8 * %addr, <16 x float> %data) {
; X64-LABEL: test26:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test26:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x float>*
  store <16 x float>%data, <16 x float>* %vaddr, align 64
  ret void
}

define <16 x float> @test27(i8 * %addr) {
; X64-LABEL: test27:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test27:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x float>*
  %res = load <16 x float>, <16 x float>* %vaddr, align 64
  ret <16 x float>%res
}

define void @test28(i8 * %addr, <8 x double> %data) {
; X64-LABEL: test28:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test28:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x double>*
  store <8 x double>%data, <8 x double>* %vaddr, align 1
  ret void
}

define <8 x double> @test29(i8 * %addr) {
; X64-LABEL: test29:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0
; X64-NEXT:    retq
;
; X86-LABEL: test29:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x double>*
  %res = load <8 x double>, <8 x double>* %vaddr, align 1
  ret <8 x double>%res
}

define void @test30(i8 * %addr, <16 x float> %data) {
; X64-LABEL: test30:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test30:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %zmm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x float>*
  store <16 x float>%data, <16 x float>* %vaddr, align 1
  ret void
}

define <16 x float> @test31(i8 * %addr) {
; X64-LABEL: test31:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %zmm0
; X64-NEXT:    retq
;
; X86-LABEL: test31:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %zmm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <16 x float>*
  %res = load <16 x float>, <16 x float>* %vaddr, align 1
  ret <16 x float>%res
}

define <16 x i32> @test32(i8 * %addr, <16 x i32> %old, <16 x i32> %mask1) {
; X64-LABEL: test32:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %zmm1, %zmm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test32:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %zmm1, %zmm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <16 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x i32>*
  %r = load <16 x i32>, <16 x i32>* %vaddr, align 64
  %res = select <16 x i1> %mask, <16 x i32> %r, <16 x i32> %old
  ret <16 x i32>%res
}

define <16 x i32> @test33(i8 * %addr, <16 x i32> %old, <16 x i32> %mask1) {
; X64-LABEL: test33:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %zmm1, %zmm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %zmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test33:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %zmm1, %zmm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %zmm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <16 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x i32>*
  %r = load <16 x i32>, <16 x i32>* %vaddr, align 1
  %res = select <16 x i1> %mask, <16 x i32> %r, <16 x i32> %old
  ret <16 x i32>%res
}

define <16 x i32> @test34(i8 * %addr, <16 x i32> %mask1) {
; X64-LABEL: test34:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %zmm0, %zmm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test34:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %zmm0, %zmm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <16 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x i32>*
  %r = load <16 x i32>, <16 x i32>* %vaddr, align 64
  %res = select <16 x i1> %mask, <16 x i32> %r, <16 x i32> zeroinitializer
  ret <16 x i32>%res
}

define <16 x i32> @test35(i8 * %addr, <16 x i32> %mask1) {
; X64-LABEL: test35:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %zmm0, %zmm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %zmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test35:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %zmm0, %zmm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %zmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <16 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x i32>*
  %r = load <16 x i32>, <16 x i32>* %vaddr, align 1
  %res = select <16 x i1> %mask, <16 x i32> %r, <16 x i32> zeroinitializer
  ret <16 x i32>%res
}

define <8 x i64> @test36(i8 * %addr, <8 x i64> %old, <8 x i64> %mask1) {
; X64-LABEL: test36:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %zmm1, %zmm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test36:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %zmm1, %zmm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <8 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %r = load <8 x i64>, <8 x i64>* %vaddr, align 64
  %res = select <8 x i1> %mask, <8 x i64> %r, <8 x i64> %old
  ret <8 x i64>%res
}

define <8 x i64> @test37(i8 * %addr, <8 x i64> %old, <8 x i64> %mask1) {
; X64-LABEL: test37:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %zmm1, %zmm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %zmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test37:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %zmm1, %zmm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %zmm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <8 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %r = load <8 x i64>, <8 x i64>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x i64> %r, <8 x i64> %old
  ret <8 x i64>%res
}

define <8 x i64> @test38(i8 * %addr, <8 x i64> %mask1) {
; X64-LABEL: test38:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %zmm0, %zmm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test38:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %zmm0, %zmm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <8 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %r = load <8 x i64>, <8 x i64>* %vaddr, align 64
  %res = select <8 x i1> %mask, <8 x i64> %r, <8 x i64> zeroinitializer
  ret <8 x i64>%res
}

define <8 x i64> @test39(i8 * %addr, <8 x i64> %mask1) {
; X64-LABEL: test39:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %zmm0, %zmm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %zmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test39:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %zmm0, %zmm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %zmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <8 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i64>*
  %r = load <8 x i64>, <8 x i64>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x i64> %r, <8 x i64> zeroinitializer
  ret <8 x i64>%res
}

define <16 x float> @test40(i8 * %addr, <16 x float> %old, <16 x float> %mask1) {
; X64-LABEL: test40:
; X64:       # %bb.0:
; X64-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X64-NEXT:    vcmpneq_oqps %zmm2, %zmm1, %k1
; X64-NEXT:    vmovups (%rdi), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test40:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X86-NEXT:    vcmpneq_oqps %zmm2, %zmm1, %k1
; X86-NEXT:    vmovups (%eax), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = fcmp one <16 x float> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x float>*
  %r = load <16 x float>, <16 x float>* %vaddr, align 64
  %res = select <16 x i1> %mask, <16 x float> %r, <16 x float> %old
  ret <16 x float>%res
}

define <16 x float> @test41(i8 * %addr, <16 x float> %old, <16 x float> %mask1) {
; X64-LABEL: test41:
; X64:       # %bb.0:
; X64-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X64-NEXT:    vcmpneq_oqps %zmm2, %zmm1, %k1
; X64-NEXT:    vmovups (%rdi), %zmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test41:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X86-NEXT:    vcmpneq_oqps %zmm2, %zmm1, %k1
; X86-NEXT:    vmovups (%eax), %zmm0 {%k1}
; X86-NEXT:    retl
  %mask = fcmp one <16 x float> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x float>*
  %r = load <16 x float>, <16 x float>* %vaddr, align 1
  %res = select <16 x i1> %mask, <16 x float> %r, <16 x float> %old
  ret <16 x float>%res
}

define <16 x float> @test42(i8 * %addr, <16 x float> %mask1) {
; X64-LABEL: test42:
; X64:       # %bb.0:
; X64-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; X64-NEXT:    vcmpneq_oqps %zmm1, %zmm0, %k1
; X64-NEXT:    vmovups (%rdi), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test42:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; X86-NEXT:    vcmpneq_oqps %zmm1, %zmm0, %k1
; X86-NEXT:    vmovups (%eax), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = fcmp one <16 x float> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x float>*
  %r = load <16 x float>, <16 x float>* %vaddr, align 64
  %res = select <16 x i1> %mask, <16 x float> %r, <16 x float> zeroinitializer
  ret <16 x float>%res
}

define <16 x float> @test43(i8 * %addr, <16 x float> %mask1) {
; X64-LABEL: test43:
; X64:       # %bb.0:
; X64-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; X64-NEXT:    vcmpneq_oqps %zmm1, %zmm0, %k1
; X64-NEXT:    vmovups (%rdi), %zmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test43:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; X86-NEXT:    vcmpneq_oqps %zmm1, %zmm0, %k1
; X86-NEXT:    vmovups (%eax), %zmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = fcmp one <16 x float> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <16 x float>*
  %r = load <16 x float>, <16 x float>* %vaddr, align 1
  %res = select <16 x i1> %mask, <16 x float> %r, <16 x float> zeroinitializer
  ret <16 x float>%res
}

define <8 x double> @test44(i8 * %addr, <8 x double> %old, <8 x double> %mask1) {
; X64-LABEL: test44:
; X64:       # %bb.0:
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vcmpneq_oqpd %zmm2, %zmm1, %k1
; X64-NEXT:    vmovupd (%rdi), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test44:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vcmpneq_oqpd %zmm2, %zmm1, %k1
; X86-NEXT:    vmovupd (%eax), %zmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = fcmp one <8 x double> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x double>*
  %r = load <8 x double>, <8 x double>* %vaddr, align 64
  %res = select <8 x i1> %mask, <8 x double> %r, <8 x double> %old
  ret <8 x double>%res
}

define <8 x double> @test45(i8 * %addr, <8 x double> %old, <8 x double> %mask1) {
; X64-LABEL: test45:
; X64:       # %bb.0:
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vcmpneq_oqpd %zmm2, %zmm1, %k1
; X64-NEXT:    vmovupd (%rdi), %zmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test45:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vcmpneq_oqpd %zmm2, %zmm1, %k1
; X86-NEXT:    vmovupd (%eax), %zmm0 {%k1}
; X86-NEXT:    retl
  %mask = fcmp one <8 x double> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x double>*
  %r = load <8 x double>, <8 x double>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x double> %r, <8 x double> %old
  ret <8 x double>%res
}

define <8 x double> @test46(i8 * %addr, <8 x double> %mask1) {
; X64-LABEL: test46:
; X64:       # %bb.0:
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vcmpneq_oqpd %zmm1, %zmm0, %k1
; X64-NEXT:    vmovupd (%rdi), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test46:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vcmpneq_oqpd %zmm1, %zmm0, %k1
; X86-NEXT:    vmovupd (%eax), %zmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = fcmp one <8 x double> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x double>*
  %r = load <8 x double>, <8 x double>* %vaddr, align 64
  %res = select <8 x i1> %mask, <8 x double> %r, <8 x double> zeroinitializer
  ret <8 x double>%res
}

define <8 x double> @test47(i8 * %addr, <8 x double> %mask1) {
; X64-LABEL: test47:
; X64:       # %bb.0:
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vcmpneq_oqpd %zmm1, %zmm0, %k1
; X64-NEXT:    vmovupd (%rdi), %zmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test47:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vcmpneq_oqpd %zmm1, %zmm0, %k1
; X86-NEXT:    vmovupd (%eax), %zmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = fcmp one <8 x double> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x double>*
  %r = load <8 x double>, <8 x double>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x double> %r, <8 x double> zeroinitializer
  ret <8 x double>%res
}
