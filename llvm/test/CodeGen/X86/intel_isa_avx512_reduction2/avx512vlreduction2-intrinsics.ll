; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_reduction2
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512reduction2,+avx512bw | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512reduction2,+avx512bw | FileCheck %s --check-prefixes=X86

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddbd128(<4 x i32> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddbd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddbd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x43,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddbd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddbd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x43,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd128(<4 x i32> %A, <16 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd128(<4 x i32> %A, <16 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddbd128(<4 x i32> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddbd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x43,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddbd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x43,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd128(<4 x i32> %A, <16 x i8> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd128(<4 x i32> %A, <16 x i8> %B, i16 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddbd256(<4 x i32> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddbd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddbd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddbd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddbd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd256(<4 x i32> %A, <32 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd256(<4 x i32> %A, <32 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddbd256(<4 x i32> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddbd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddbd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd256(<4 x i32> %A, <32 x i8> %B, i32 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd256(<4 x i32> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddwd128(<4 x i32> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddwd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddwd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x43,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddwd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddwd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x43,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd128(<4 x i32> %A, <8 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd128(<4 x i32> %A, <8 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddwd128(<4 x i32> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddwd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x43,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraaddwd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x43,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd128(<4 x i32> %A, <8 x i16> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd128(<4 x i32> %A, <8 x i16> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddwd256(<4 x i32> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddwd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddwd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddwd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddwd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd256(<4 x i32> %A, <16 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd256(<4 x i32> %A, <16 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddwd256(<4 x i32> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddwd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddwd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd256(<4 x i32> %A, <16 x i16> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd256(<4 x i32> %A, <16 x i16> %B, i16 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddsbd128(<4 x i32> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddsbd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddsbd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x44,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddsbd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddsbd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x44,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd128(<4 x i32> %A, <16 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd128(<4 x i32> %A, <16 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddsbd128(<4 x i32> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddsbd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x44,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddsbd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x44,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd128(<4 x i32> %A, <16 x i8> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd128(<4 x i32> %A, <16 x i8> %B, i16 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddsbd256(<4 x i32> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddsbd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddsbd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddsbd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddsbd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd256(<4 x i32> %A, <32 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd256(<4 x i32> %A, <32 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddsbd256(<4 x i32> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddsbd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddsbd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd256(<4 x i32> %A, <32 x i8> %B, i32 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd256(<4 x i32> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddswd128(<4 x i32> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddswd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddswd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x44,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddswd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddswd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x44,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd128(<4 x i32> %A, <8 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd128(<4 x i32> %A, <8 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddswd128(<4 x i32> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddswd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x44,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraaddswd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x44,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd128(<4 x i32> %A, <8 x i16> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd128(<4 x i32> %A, <8 x i16> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddswd256(<4 x i32> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddswd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddswd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddswd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddswd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd256(<4 x i32> %A, <16 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd256(<4 x i32> %A, <16 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddswd256(<4 x i32> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddswd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddswd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd256(<4 x i32> %A, <16 x i16> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd256(<4 x i32> %A, <16 x i16> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraandb128(<16 x i8> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandb128:
; X64:       # %bb.0:
; X64-NEXT:    vphraandb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandb128:
; X86:       # %bb.0:
; X86-NEXT:    vphraandb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraandb128(<16 x i8> %A, <16 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraandb128(<16 x i8> %A, <16 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraandb128(<16 x i8> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraandb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb128(<16 x i8> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb128(<16 x i8> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraandb256(<16 x i8> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandb256:
; X64:       # %bb.0:
; X64-NEXT:    vphraandb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandb256:
; X86:       # %bb.0:
; X86-NEXT:    vphraandb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraandb256(<16 x i8> %A, <32 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraandb256(<16 x i8> %A, <32 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraandb256(<16 x i8> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraandb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb256(<16 x i8> %A, <32 x i8> %B, i32 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb256(<16 x i8> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraandd128(<4 x i32> %A, <4 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraandd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraandd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraandd128(<4 x i32> %A, <4 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraandd128(<4 x i32> %A, <4 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraandd128(<4 x i32> %A, <4 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd128(<4 x i32> %A, <4 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd128(<4 x i32> %A, <4 x i32> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraandd256(<4 x i32> %A, <8 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraandd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraandd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraandd256(<4 x i32> %A, <8 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraandd256(<4 x i32> %A, <8 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraandd256(<4 x i32> %A, <8 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd256(<4 x i32> %A, <8 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd256(<4 x i32> %A, <8 x i32> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraandq128(<2 x i64> %A, <2 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandq128:
; X64:       # %bb.0:
; X64-NEXT:    vphraandq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandq128:
; X86:       # %bb.0:
; X86-NEXT:    vphraandq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraandq128(<2 x i64> %A, <2 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraandq128(<2 x i64> %A, <2 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraandq128(<2 x i64> %A, <2 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq128(<2 x i64> %A, <2 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq128(<2 x i64> %A, <2 x i64> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraandq256(<2 x i64> %A, <4 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandq256:
; X64:       # %bb.0:
; X64-NEXT:    vphraandq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandq256:
; X86:       # %bb.0:
; X86-NEXT:    vphraandq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraandq256(<2 x i64> %A, <4 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraandq256(<2 x i64> %A, <4 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraandq256(<2 x i64> %A, <4 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq256(<2 x i64> %A, <4 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq256(<2 x i64> %A, <4 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraandw128(<8 x i16> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandw128:
; X64:       # %bb.0:
; X64-NEXT:    vphraandw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandw128:
; X86:       # %bb.0:
; X86-NEXT:    vphraandw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraandw128(<8 x i16> %A, <8 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraandw128(<8 x i16> %A, <8 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraandw128(<8 x i16> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x4d,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x4d,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw128(<8 x i16> %A, <8 x i16> %B, i8 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw128(<8 x i16> %A, <8 x i16> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraandw256(<8 x i16> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandw256:
; X64:       # %bb.0:
; X64-NEXT:    vphraandw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandw256:
; X86:       # %bb.0:
; X86-NEXT:    vphraandw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraandw256(<8 x i16> %A, <16 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraandw256(<8 x i16> %A, <16 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraandw256(<8 x i16> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraandw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw256(<8 x i16> %A, <16 x i16> %B, i16 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw256(<8 x i16> %A, <16 x i16> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphramaxsb128(<16 x i8> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsb128:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsb128:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb128(<16 x i8> %A, <16 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb128(<16 x i8> %A, <16 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphramaxsb128(<16 x i8> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramaxsb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb128(<16 x i8> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb128(<16 x i8> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphramaxsb256(<16 x i8> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsb256:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsb256:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb256(<16 x i8> %A, <32 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb256(<16 x i8> %A, <32 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphramaxsb256(<16 x i8> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramaxsb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb256(<16 x i8> %A, <32 x i8> %B, i32 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb256(<16 x i8> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramaxsd128(<4 x i32> %A, <4 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsd128:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsd128:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd128(<4 x i32> %A, <4 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd128(<4 x i32> %A, <4 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramaxsd128(<4 x i32> %A, <4 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd128(<4 x i32> %A, <4 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd128(<4 x i32> %A, <4 x i32> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramaxsd256(<4 x i32> %A, <8 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsd256:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsd256:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd256(<4 x i32> %A, <8 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd256(<4 x i32> %A, <8 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramaxsd256(<4 x i32> %A, <8 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd256(<4 x i32> %A, <8 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd256(<4 x i32> %A, <8 x i32> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphramaxsq128(<2 x i64> %A, <2 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsq128:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsq128:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq128(<2 x i64> %A, <2 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq128(<2 x i64> %A, <2 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphramaxsq128(<2 x i64> %A, <2 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq128(<2 x i64> %A, <2 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq128(<2 x i64> %A, <2 x i64> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphramaxsq256(<2 x i64> %A, <4 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsq256:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsq256:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq256(<2 x i64> %A, <4 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq256(<2 x i64> %A, <4 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphramaxsq256(<2 x i64> %A, <4 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq256(<2 x i64> %A, <4 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq256(<2 x i64> %A, <4 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphramaxsw128(<8 x i16> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsw128:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsw128:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw128(<8 x i16> %A, <8 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw128(<8 x i16> %A, <8 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphramaxsw128(<8 x i16> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x4b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x4b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw128(<8 x i16> %A, <8 x i16> %B, i8 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw128(<8 x i16> %A, <8 x i16> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphramaxsw256(<8 x i16> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsw256:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsw256:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw256(<8 x i16> %A, <16 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw256(<8 x i16> %A, <16 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphramaxsw256(<8 x i16> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramaxsw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw256(<8 x i16> %A, <16 x i16> %B, i16 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw256(<8 x i16> %A, <16 x i16> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminb128(<16 x i8> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminb128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminb128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminb128(<16 x i8> %A, <16 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminb128(<16 x i8> %A, <16 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminb128(<16 x i8> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb128(<16 x i8> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb128(<16 x i8> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminb256(<16 x i8> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminb256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminb256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminb256(<16 x i8> %A, <32 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminb256(<16 x i8> %A, <32 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminb256(<16 x i8> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb256(<16 x i8> %A, <32 x i8> %B, i32 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb256(<16 x i8> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramind128(<4 x i32> %A, <4 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramind128:
; X64:       # %bb.0:
; X64-NEXT:    vphramind %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramind128:
; X86:       # %bb.0:
; X86-NEXT:    vphramind %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramind128(<4 x i32> %A, <4 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramind128(<4 x i32> %A, <4 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramind128(<4 x i32> %A, <4 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramind128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramind %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramind128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramind %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind128(<4 x i32> %A, <4 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind128(<4 x i32> %A, <4 x i32> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramind256(<4 x i32> %A, <8 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramind256:
; X64:       # %bb.0:
; X64-NEXT:    vphramind %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramind256:
; X86:       # %bb.0:
; X86-NEXT:    vphramind %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramind256(<4 x i32> %A, <8 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramind256(<4 x i32> %A, <8 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramind256(<4 x i32> %A, <8 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramind256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramind %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramind256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramind %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind256(<4 x i32> %A, <8 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind256(<4 x i32> %A, <8 x i32> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminq128(<2 x i64> %A, <2 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminq128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminq128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminq128(<2 x i64> %A, <2 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminq128(<2 x i64> %A, <2 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminq128(<2 x i64> %A, <2 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq128(<2 x i64> %A, <2 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq128(<2 x i64> %A, <2 x i64> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminq256(<2 x i64> %A, <4 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminq256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminq256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminq256(<2 x i64> %A, <4 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminq256(<2 x i64> %A, <4 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminq256(<2 x i64> %A, <4 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq256(<2 x i64> %A, <4 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq256(<2 x i64> %A, <4 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminw128(<8 x i16> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminw128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminw128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminw128(<8 x i16> %A, <8 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminw128(<8 x i16> %A, <8 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminw128(<8 x i16> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x48,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x48,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw128(<8 x i16> %A, <8 x i16> %B, i8 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw128(<8 x i16> %A, <8 x i16> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminw256(<8 x i16> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminw256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminw256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminw256(<8 x i16> %A, <16 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminw256(<8 x i16> %A, <16 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminw256(<8 x i16> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw256(<8 x i16> %A, <16 x i16> %B, i16 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw256(<8 x i16> %A, <16 x i16> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminsb128(<16 x i8> %A, <16 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsb128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsb128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsb %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminsb128(<16 x i8> %A, <16 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminsb128(<16 x i8> %A, <16 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminsb128(<16 x i8> %A, <16 x i8> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminsb %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb128(<16 x i8> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb128(<16 x i8> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminsb256(<16 x i8> %A, <32 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsb256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsb256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsb %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminsb256(<16 x i8> %A, <32 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminsb256(<16 x i8> %A, <32 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminsb256(<16 x i8> %A, <32 x i8> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminsb %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb256(<16 x i8> %A, <32 x i8> %B, i32 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb256(<16 x i8> %A, <32 x i8> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraminsd128(<4 x i32> %A, <4 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsd %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraminsd128(<4 x i32> %A, <4 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraminsd128(<4 x i32> %A, <4 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraminsd128(<4 x i32> %A, <4 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsd %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd128(<4 x i32> %A, <4 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd128(<4 x i32> %A, <4 x i32> %B, i8 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraminsd256(<4 x i32> %A, <8 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsd %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x28,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraminsd256(<4 x i32> %A, <8 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraminsd256(<4 x i32> %A, <8 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraminsd256(<4 x i32> %A, <8 x i32> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsd %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd256(<4 x i32> %A, <8 x i32> %B, i8 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd256(<4 x i32> %A, <8 x i32> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminsq128(<2 x i64> %A, <2 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsq128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsq128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsq %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x08,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminsq128(<2 x i64> %A, <2 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminsq128(<2 x i64> %A, <2 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminsq128(<2 x i64> %A, <2 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsq %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x09,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq128(<2 x i64> %A, <2 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq128(<2 x i64> %A, <2 x i64> %B, i8 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminsq256(<2 x i64> %A, <4 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsq256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsq256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsq %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x28,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminsq256(<2 x i64> %A, <4 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminsq256(<2 x i64> %A, <4 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminsq256(<2 x i64> %A, <4 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsq %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x29,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq256(<2 x i64> %A, <4 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq256(<2 x i64> %A, <4 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminsw128(<8 x i16> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsw128:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsw128:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsw %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x08,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminsw128(<8 x i16> %A, <8 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminsw128(<8 x i16> %A, <8 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminsw128(<8 x i16> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x49,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsw %xmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x09,0x49,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw128(<8 x i16> %A, <8 x i16> %B, i8 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw128(<8 x i16> %A, <8 x i16> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminsw256(<8 x i16> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsw256:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsw256:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsw %ymm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x28,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminsw256(<8 x i16> %A, <16 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminsw256(<8 x i16> %A, <16 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminsw256(<8 x i16> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminsw %ymm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x29,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw256(<8 x i16> %A, <16 x i16> %B, i16 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw256(<8 x i16> %A, <16 x i16> %B, i16 %C)

