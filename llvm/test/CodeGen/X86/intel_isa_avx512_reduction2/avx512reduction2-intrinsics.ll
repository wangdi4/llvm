; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_reduction2
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512reduction2,+avx512bw,+avx512vl | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512reduction2,+avx512bw,+avx512vl | FileCheck %s --check-prefixes=X86

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddbd512(<4 x i32> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddbd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddbd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddbd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddbd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd512(<4 x i32> %A, <64 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddbd512(<4 x i32> %A, <64 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphraaddbd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddbd512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphraaddbd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddwd512(<4 x i32> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddwd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddwd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddwd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddwd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd512(<4 x i32> %A, <32 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddwd512(<4 x i32> %A, <32 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddwd512(<4 x i32> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddwd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x43,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddwd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddwd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x43,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd512(<4 x i32> %A, <32 x i16> %B, i32 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddwd512(<4 x i32> %A, <32 x i16> %B, i32 %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddsbd512(<4 x i32> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddsbd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddsbd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddsbd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddsbd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd512(<4 x i32> %A, <64 x i8> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddsbd512(<4 x i32> %A, <64 x i8> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddsbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphraaddsbd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddsbd512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphraaddsbd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddsbd512(<4 x i32> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraaddswd512(<4 x i32> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraaddswd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraaddswd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraaddswd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraaddswd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd512(<4 x i32> %A, <32 x i16> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraaddswd512(<4 x i32> %A, <32 x i16> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraaddswd512(<4 x i32> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraaddswd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x44,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraaddswd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraaddswd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x44,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd512(<4 x i32> %A, <32 x i16> %B, i32 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraaddswd512(<4 x i32> %A, <32 x i16> %B, i32 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraandb512(<16 x i8> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandb512:
; X64:       # %bb.0:
; X64-NEXT:    vphraandb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandb512:
; X86:       # %bb.0:
; X86-NEXT:    vphraandb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraandb512(<16 x i8> %A, <64 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraandb512(<16 x i8> %A, <64 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraandb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphraandb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphraandb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraandb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraandd512(<4 x i32> %A, <16 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraandd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraandd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraandd512(<4 x i32> %A, <16 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraandd512(<4 x i32> %A, <16 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraandd512(<4 x i32> %A, <16 x i32> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraandd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd512(<4 x i32> %A, <16 x i32> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraandd512(<4 x i32> %A, <16 x i32> %B, i16 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraandq512(<2 x i64> %A, <8 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandq512:
; X64:       # %bb.0:
; X64-NEXT:    vphraandq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandq512:
; X86:       # %bb.0:
; X86-NEXT:    vphraandq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraandq512(<2 x i64> %A, <8 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraandq512(<2 x i64> %A, <8 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraandq512(<2 x i64> %A, <8 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraandq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq512(<2 x i64> %A, <8 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraandq512(<2 x i64> %A, <8 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraandw512(<8 x i16> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraandw512:
; X64:       # %bb.0:
; X64-NEXT:    vphraandw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraandw512:
; X86:       # %bb.0:
; X86-NEXT:    vphraandw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraandw512(<8 x i16> %A, <32 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraandw512(<8 x i16> %A, <32 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraandw512(<8 x i16> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraandw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraandw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x4d,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraandw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraandw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x4d,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw512(<8 x i16> %A, <32 x i16> %B, i32 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraandw512(<8 x i16> %A, <32 x i16> %B, i32 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphramaxsb512(<16 x i8> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsb512:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsb512:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb512(<16 x i8> %A, <64 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphramaxsb512(<16 x i8> %A, <64 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphramaxsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphramaxsb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphramaxsb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphramaxsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramaxsd512(<4 x i32> %A, <16 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsd512:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsd512:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd512(<4 x i32> %A, <16 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramaxsd512(<4 x i32> %A, <16 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramaxsd512(<4 x i32> %A, <16 x i32> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramaxsd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd512(<4 x i32> %A, <16 x i32> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramaxsd512(<4 x i32> %A, <16 x i32> %B, i16 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphramaxsq512(<2 x i64> %A, <8 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsq512:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsq512:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq512(<2 x i64> %A, <8 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphramaxsq512(<2 x i64> %A, <8 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphramaxsq512(<2 x i64> %A, <8 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphramaxsq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq512(<2 x i64> %A, <8 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphramaxsq512(<2 x i64> %A, <8 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphramaxsw512(<8 x i16> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramaxsw512:
; X64:       # %bb.0:
; X64-NEXT:    vphramaxsw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramaxsw512:
; X86:       # %bb.0:
; X86-NEXT:    vphramaxsw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw512(<8 x i16> %A, <32 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphramaxsw512(<8 x i16> %A, <32 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphramaxsw512(<8 x i16> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramaxsw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x4b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramaxsw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramaxsw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x4b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw512(<8 x i16> %A, <32 x i16> %B, i32 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphramaxsw512(<8 x i16> %A, <32 x i16> %B, i32 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminb512(<16 x i8> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminb512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminb512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminb512(<16 x i8> %A, <64 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminb512(<16 x i8> %A, <64 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphraminb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphraminb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphramind512(<4 x i32> %A, <16 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphramind512:
; X64:       # %bb.0:
; X64-NEXT:    vphramind %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphramind512:
; X86:       # %bb.0:
; X86-NEXT:    vphramind %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphramind512(<4 x i32> %A, <16 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphramind512(<4 x i32> %A, <16 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphramind512(<4 x i32> %A, <16 x i32> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphramind512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphramind %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphramind512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphramind %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind512(<4 x i32> %A, <16 x i32> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphramind512(<4 x i32> %A, <16 x i32> %B, i16 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminq512(<2 x i64> %A, <8 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminq512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminq512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminq512(<2 x i64> %A, <8 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminq512(<2 x i64> %A, <8 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminq512(<2 x i64> %A, <8 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq512(<2 x i64> %A, <8 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminq512(<2 x i64> %A, <8 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminw512(<8 x i16> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminw512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminw512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminw512(<8 x i16> %A, <32 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminw512(<8 x i16> %A, <32 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminw512(<8 x i16> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x48,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x48,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw512(<8 x i16> %A, <32 x i16> %B, i32 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminw512(<8 x i16> %A, <32 x i16> %B, i32 %C)

define <16 x i8> @test_int_x86_avx512reduction2_vphraminsb512(<16 x i8> %A, <64 x i8> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsb512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsb512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsb %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x48,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.vphraminsb512(<16 x i8> %A, <64 x i8> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.vphraminsb512(<16 x i8> %A, <64 x i8> %B)

define <16 x i8> @test_int_x86_avx512reduction2_mask_vphraminsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X64-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X64-NEXT:    vphraminsb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm2, %zmm2 # encoding: [0x62,0xf1,0x6d,0x48,0x71,0xf2,0x07]
; X86-NEXT:    vpmovb2m %zmm2, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xca]
; X86-NEXT:    vphraminsb %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x49,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction2.mask.vphraminsb512(<16 x i8> %A, <64 x i8> %B, <64 x i1> %C)

define <4 x i32> @test_int_x86_avx512reduction2_vphraminsd512(<4 x i32> %A, <16 x i32> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsd %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x48,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.vphraminsd512(<4 x i32> %A, <16 x i32> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.vphraminsd512(<4 x i32> %A, <16 x i32> %B)

define <4 x i32> @test_int_x86_avx512reduction2_mask_vphraminsd512(<4 x i32> %A, <16 x i32> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminsd %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7f,0x49,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd512(<4 x i32> %A, <16 x i32> %B, i16 %C)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction2.mask.vphraminsd512(<4 x i32> %A, <16 x i32> %B, i16 %C)

define <2 x i64> @test_int_x86_avx512reduction2_vphraminsq512(<2 x i64> %A, <8 x i64> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsq512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsq512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsq %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xff,0x48,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.vphraminsq512(<2 x i64> %A, <8 x i64> %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.vphraminsq512(<2 x i64> %A, <8 x i64> %B)

define <2 x i64> @test_int_x86_avx512reduction2_mask_vphraminsq512(<2 x i64> %A, <8 x i64> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraminsq %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xff,0x49,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq512(<2 x i64> %A, <8 x i64> %B, i8 %C)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction2.mask.vphraminsq512(<2 x i64> %A, <8 x i64> %B, i8 %C)

define <8 x i16> @test_int_x86_avx512reduction2_vphraminsw512(<8 x i16> %A, <32 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_vphraminsw512:
; X64:       # %bb.0:
; X64-NEXT:    vphraminsw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_vphraminsw512:
; X86:       # %bb.0:
; X86-NEXT:    vphraminsw %zmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfe,0x48,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.vphraminsw512(<8 x i16> %A, <32 x i16> %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.vphraminsw512(<8 x i16> %A, <32 x i16> %B)

define <8 x i16> @test_int_x86_avx512reduction2_mask_vphraminsw512(<8 x i16> %A, <32 x i16> %B, i32 %C) nounwind {
; X64-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraminsw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x49,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction2_mask_vphraminsw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraminsw %zmm1, %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfe,0x49,0x49,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw512(<8 x i16> %A, <32 x i16> %B, i32 %C)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction2.mask.vphraminsw512(<8 x i16> %A, <32 x i16> %B, i32 %C)

