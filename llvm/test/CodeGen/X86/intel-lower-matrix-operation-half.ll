; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 2
; RUN: opt -x86-lower-matrix-intrinsics -mtriple=x86_64-unknown-unknown -mcpu=diamondrapids %s -S | FileCheck %s

define void @load_mad_store_half_with(ptr addrspace(1) %ptrA, ptr addrspace(1) %ptrB, ptr addrspace(1) %ptrC, ptr addrspace(1) %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_half_with
; CHECK-SAME: (ptr addrspace(1) [[PTRA:%.*]], ptr addrspace(1) [[PTRB:%.*]], ptr addrspace(1) [[PTRC:%.*]], ptr addrspace(1) [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(1) [[PTRC]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(1) [[PTRA]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEA]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 32, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <128 x half> @llvm.x86.cast.tile.to.vector.v128f16(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(1) [[PTRB]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEB]], 2
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <256 x half> @llvm.x86.cast.tile.to.vector.v256f16(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f16(<128 x half> [[TMP8]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f16(<256 x half> [[TMP12]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpfp16ps.internal(i16 8, i16 64, i16 32, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(1) [[PTRD]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p1(ptr addrspace(1) %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %A = call <128 x half> @llvm.experimental.matrix.load.v128f16.p1(ptr addrspace(1) %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %B = call <256 x half> @llvm.experimental.matrix.load.v256f16.p1(ptr addrspace(1) %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f16.v256f16(<128 x half> %A, <256 x half> %B, <128 x float> %C, i32 8, i32 16, i32 16, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v128f32.p1(<128 x float> %D, ptr addrspace(1) %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_half_with_use(ptr addrspace(1) %ptrA, ptr addrspace(1) %ptrB, ptr addrspace(1) %ptrC, ptr addrspace(1) %ptrD, i64 %StrideA, i64 %StrideB, i64 %StrideC, i64 %StrideD) {
; CHECK-LABEL: define void @load_mad_store_half_with_use
; CHECK-SAME: (ptr addrspace(1) [[PTRA:%.*]], ptr addrspace(1) [[PTRB:%.*]], ptr addrspace(1) [[PTRC:%.*]], ptr addrspace(1) [[PTRD:%.*]], i64 [[STRIDEA:%.*]], i64 [[STRIDEB:%.*]], i64 [[STRIDEC:%.*]], i64 [[STRIDED:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(1) [[PTRC]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDEC]], 4
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(1) [[PTRA]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDEA]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 32, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <128 x half> @llvm.x86.cast.tile.to.vector.v128f16(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(1) [[PTRB]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDEB]], 2
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 64, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <256 x half> @llvm.x86.cast.tile.to.vector.v256f16(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP4]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f16(<128 x half> [[TMP8]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v256f16(<256 x half> [[TMP12]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpfp16ps.internal(i16 8, i16 64, i16 32, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <128 x float> @llvm.x86.cast.tile.to.vector.v128f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(1) [[PTRD]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDED]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v128f32(<128 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 64, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %C = call <128 x float> @llvm.experimental.matrix.load.v128f32.p1(ptr addrspace(1) %ptrC, i64 %StrideC, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  %A = call <128 x half> @llvm.experimental.matrix.load.v128f16.p1(ptr addrspace(1) %ptrA, i64 %StrideA, i1 false, i32 8, i32 16, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  %B = call <256 x half> @llvm.experimental.matrix.load.v256f16.p1(ptr addrspace(1) %ptrB, i64 %StrideB, i1 false, i32 16, i32 16, metadata !"matrix.packed", metadata !"matrix.packed", metadata !"scope.subgroup", metadata !"matrix.use.b")
  %D = call <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f16.v256f16(<128 x half> %A, <256 x half> %B, <128 x float> %C, i32 8, i32 16, i32 16, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v128f32.p1(<128 x float> %D, ptr addrspace(1) %ptrD, i64 %StrideD, i1 false, i32 8, i32 16, metadata !"matrix.dynamic", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret void
}

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <128 x float> @llvm.experimental.matrix.mad.v128f32.v128f16.v256f16(<128 x half>, <256 x half>, <128 x float>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <128 x float> @llvm.experimental.matrix.load.v128f32.p1(ptr addrspace(1), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <128 x half> @llvm.experimental.matrix.load.v128f16.p1(ptr addrspace(1), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <256 x half> @llvm.experimental.matrix.load.v256f16.p1(ptr addrspace(1), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v128f32.p1(<128 x float>, ptr addrspace(1), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

attributes #0 = { nocallback nofree nosync nounwind willreturn memory(none) }
attributes #1 = { nocallback nofree nosync nounwind willreturn memory(read) }
attributes #2 = { nocallback nofree nosync nounwind willreturn memory(write) }
