; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_ne_convert_fp8
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512neconvertfp8 | FileCheck %s --check-prefixes=CHECK,X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512neconvertfp8 | FileCheck %s --check-prefixes=CHECK,X86

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2bf8 %xmm2, %xmm1, %xmm0 # encoding: [0x62,0xf2,0x75,0x08,0x74,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2bf8 %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf2,0x75,0x28,0x74,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2bf8s %xmm2, %xmm1, %xmm0 # encoding: [0x62,0xf5,0x75,0x08,0x74,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2bf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2bf8s %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf5,0x75,0x28,0x74,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2bf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2hf8 %xmm2, %xmm1, %xmm0 # encoding: [0x62,0xf5,0x75,0x08,0x18,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2hf8 %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf5,0x75,0x28,0x18,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2hf8s %xmm2, %xmm1, %xmm0 # encoding: [0x62,0xf5,0x75,0x08,0x1b,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8s128(<16 x i8> %A, <8 x half> %B, <8 x half> %C)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbias2ph2hf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbias2ph2hf8s %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf5,0x75,0x28,0x1b,0xc2]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtbias2ph2hf8s256(<32 x i8> %A, <16 x half> %B, <16 x half> %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2bf8 %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7c,0x08,0x74,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x74,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0x74,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7c,0x89,0x74,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7c,0x89,0x74,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2bf8 %ymm0, %xmm0 # encoding: [0x62,0xf2,0x7c,0x28,0x74,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x74,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0x74,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7c,0xa9,0x74,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7c,0xa9,0x74,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8s128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2bf8s %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x74,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x74,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x74,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x74,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x74,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8s256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2bf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2bf8s %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x74,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x74,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2bf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x74,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2bf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x74,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2bf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2bf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x74,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2bf8s256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2hf8 %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x18,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x18,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x18,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x18,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x18,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2hf8 %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x18,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x18,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x18,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x18,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x18,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8s128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2hf8s %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x1b,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x1b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x1b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x1b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0x89,0x1b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8s256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtbiasph2hf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtbiasph2hf8s %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x1b,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x1b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtbiasph2hf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x1b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtbiasph2hf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x1b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtbiasph2hf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtbiasph2hf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7c,0xa9,0x1b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtbiasph2hf8s256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8128(<8 x half> %A, <8 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2bf8 %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7f,0x08,0x74,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8128(<8 x half> %A, <8 x half> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8128(<8 x half> %A, <8 x half> %B)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8256(<16 x half> %A, <16 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2bf8 %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf2,0x7f,0x28,0x74,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8256(<16 x half> %A, <16 x half> %B)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8256(<16 x half> %A, <16 x half> %B)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8s128(<8 x half> %A, <8 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2bf8s %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x74,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8s128(<8 x half> %A, <8 x half> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8s128(<8 x half> %A, <8 x half> %B)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8s256(<16 x half> %A, <16 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2bf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2bf8s %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf5,0x7f,0x28,0x74,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8s256(<16 x half> %A, <16 x half> %B)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2bf8s256(<16 x half> %A, <16 x half> %B)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8128(<8 x half> %A, <8 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2hf8 %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x18,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8128(<8 x half> %A, <8 x half> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8128(<8 x half> %A, <8 x half> %B)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8256(<16 x half> %A, <16 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2hf8 %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf5,0x7f,0x28,0x18,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8256(<16 x half> %A, <16 x half> %B)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8256(<16 x half> %A, <16 x half> %B)

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8s128(<8 x half> %A, <8 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2hf8s %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x1b,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8s128(<8 x half> %A, <8 x half> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8s128(<8 x half> %A, <8 x half> %B)

define <32 x i8> @test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8s256(<16 x half> %A, <16 x half> %B) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtne2ph2hf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtne2ph2hf8s %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf5,0x7f,0x28,0x1b,0xc1]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8s256(<16 x half> %A, <16 x half> %B)
  ret <32 x i8> %ret
}

declare <32 x i8> @llvm.x86.avx512neconvertfp8.vcvtne2ph2hf8s256(<16 x half> %A, <16 x half> %B)

define <8 x half> @test_int_x86_avx512neconvertfp8_vcvtnebf82ph128(<16 x i8> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtnebf82ph128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtnebf82ph %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x1e,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph128(<16 x i8> %A, <8 x half> undef, i8 -1)
  ret <8 x half> %ret
}

define <8 x half> @test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnebf82ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x1e,0xc8]
; X64-NEXT:    vmovdqa %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnebf82ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x1e,0xc8]
; X86-NEXT:    vmovdqa %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C)
  ret <8 x half> %ret
}

declare <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C)

define <8 x half> @test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph128(<16 x i8> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnebf82ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x1e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnebf82ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x1e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph128(<16 x i8> %A, <8 x half> zeroinitializer, i8 %B)
  ret <8 x half> %ret
}

define <16 x half> @test_int_x86_avx512neconvertfp8_vcvtnebf82ph256(<16 x i8> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtnebf82ph256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtnebf82ph %xmm0, %ymm0 # encoding: [0x62,0xf5,0x7e,0x28,0x1e,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph256(<16 x i8> %A, <16 x half> undef, i16 -1)
  ret <16 x half> %ret
}

define <16 x half> @test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnebf82ph %xmm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x1e,0xc8]
; X64-NEXT:    vmovdqa %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnebf82ph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnebf82ph %xmm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x1e,0xc8]
; X86-NEXT:    vmovdqa %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C)
  ret <16 x half> %ret
}

declare <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C)

define <16 x half> @test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph256(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnebf82ph %xmm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x1e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnebf82ph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnebf82ph %xmm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x1e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnebf82ph256(<16 x i8> %A, <16 x half> zeroinitializer, i16 %B)
  ret <16 x half> %ret
}

define <8 x half> @test_int_x86_avx512neconvertfp8_vcvtnehf82ph128(<16 x i8> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtnehf82ph128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtnehf82ph %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7f,0x08,0x1e,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph128(<16 x i8> %A, <8 x half> undef, i8 -1)
  ret <8 x half> %ret
}

define <8 x half> @test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnehf82ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x1e,0xc8]
; X64-NEXT:    vmovdqa %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnehf82ph %xmm0, %xmm1 {%k1} # encoding: [0x62,0xf5,0x7f,0x09,0x1e,0xc8]
; X86-NEXT:    vmovdqa %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C)
  ret <8 x half> %ret
}

declare <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph128(<16 x i8> %A, <8 x half> %B, i8 %C)

define <8 x half> @test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph128(<16 x i8> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnehf82ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7f,0x89,0x1e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnehf82ph %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7f,0x89,0x1e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph128(<16 x i8> %A, <8 x half> zeroinitializer, i8 %B)
  ret <8 x half> %ret
}

define <16 x half> @test_int_x86_avx512neconvertfp8_vcvtnehf82ph256(<16 x i8> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtnehf82ph256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtnehf82ph %xmm0, %ymm0 # encoding: [0x62,0xf5,0x7f,0x28,0x1e,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph256(<16 x i8> %A, <16 x half> undef, i16 -1)
  ret <16 x half> %ret
}

define <16 x half> @test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnehf82ph %xmm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x1e,0xc8]
; X64-NEXT:    vmovdqa %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtnehf82ph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnehf82ph %xmm0, %ymm1 {%k1} # encoding: [0x62,0xf5,0x7f,0x29,0x1e,0xc8]
; X86-NEXT:    vmovdqa %ymm1, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C)
  ret <16 x half> %ret
}

declare <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph256(<16 x i8> %A, <16 x half> %B, i16 %C)

define <16 x half> @test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph256(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtnehf82ph %xmm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7f,0xa9,0x1e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtnehf82ph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtnehf82ph %xmm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf5,0x7f,0xa9,0x1e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512neconvertfp8.mask.vcvtnehf82ph256(<16 x i8> %A, <16 x half> zeroinitializer, i16 %B)
  ret <16 x half> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2bf8128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2bf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf8 %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x74,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0x74,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0x74,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0x89,0x74,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0x89,0x74,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2bf8256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2bf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf8 %ymm0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x28,0x74,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0x74,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0x74,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0xa9,0x74,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7e,0xa9,0x74,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2bf8s128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2bf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf8s %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x74,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x74,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x74,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x74,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x74,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2bf8s256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2bf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2bf8s %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x74,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x74,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2bf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x74,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2bf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x74,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2bf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2bf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x74,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2bf8s256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2hf8128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2hf8128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2hf8 %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x18,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x18,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8 %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x18,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x18,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8 %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x18,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2hf8256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2hf8256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2hf8 %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x18,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x18,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8 %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x18,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x18,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8 %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x18,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2hf8s128(<8 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2hf8s128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2hf8s %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x08,0x1b,0xc0]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s128(<8 x half> %A, <16 x i8> undef, i8 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s128(<16 x i8> %B, <8 x half> %A, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x1b,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8s %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x09,0x1b,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s128(<8 x half> %A, <16 x i8> %B, i8 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s128(<8 x half> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x1b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8s %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0x89,0x1b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s128(<8 x half> %A, <16 x i8> zeroinitializer, i8 %B)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_vcvtneph2hf8s256(<16 x half> %A) nounwind {
; CHECK-LABEL: test_int_x86_avx512neconvertfp8_vcvtneph2hf8s256:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vcvtneph2hf8s %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7e,0x28,0x1b,0xc0]
; CHECK-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; CHECK-NEXT:    ret{{[l|q]}} # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s256(<16 x half> %A, <16 x i8> undef, i16 -1)
  ret <16 x i8> %ret
}

define <16 x i8> @test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s256(<16 x i8> %B, <16 x half> %A, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x1b,0xc1]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_mask_vcvtneph2hf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8s %ymm1, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7e,0x29,0x1b,0xc1]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s256(<16 x half> %A, <16 x i8> %B, i16 %C)

define <16 x i8> @test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s256(<16 x half> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneph2hf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x1b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512neconvertfp8_maskz_vcvtneph2hf8s256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneph2hf8s %ymm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf5,0x7e,0xa9,0x1b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512neconvertfp8.mask.vcvtneph2hf8s256(<16 x half> %A, <16 x i8> zeroinitializer, i16 %B)
  ret <16 x i8> %ret
}
