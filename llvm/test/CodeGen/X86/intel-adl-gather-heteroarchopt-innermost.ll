; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mcpu=alderlake -enable-intel-advanced-opts -O3 | FileCheck %s --check-prefix=X64
; RUN: llc < %s -mtriple=i386-unknown-unknown  -mcpu=alderlake -enable-intel-advanced-opts -O3 | FileCheck %s --check-prefix=X86

; Function Attrs: nofree norecurse nosync nounwind readonly uwtable
define dso_local double @foo(double* noalias nocapture readonly %dst, double* noalias nocapture readonly %luval, i32** nocapture readonly %rowstart, double*** noalias nocapture readonly %src1, double*** noalias nocapture readonly %src2, i32* nocapture readnone %first_after_diagonal, i32 %N) local_unnamed_addr #0 {
; X64-LABEL: foo:
; X64:       # %bb.0: # %entry
; X64-NEXT:    pushq %rbp
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    pushq %r15
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    pushq %r14
; X64-NEXT:    .cfi_def_cfa_offset 32
; X64-NEXT:    pushq %r13
; X64-NEXT:    .cfi_def_cfa_offset 40
; X64-NEXT:    pushq %r12
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    pushq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 56
; X64-NEXT:    .cfi_offset %rbx, -56
; X64-NEXT:    .cfi_offset %r12, -48
; X64-NEXT:    .cfi_offset %r13, -40
; X64-NEXT:    .cfi_offset %r14, -32
; X64-NEXT:    .cfi_offset %r15, -24
; X64-NEXT:    .cfi_offset %rbp, -16
; X64-NEXT:    movl {{[0-9]+}}(%rsp), %r9d
; X64-NEXT:    testl %r9d, %r9d
; X64-NEXT:    jle .LBB0_1
; X64-NEXT:  # %bb.3: # %for.body.preheader
; X64-NEXT:    movl %r9d, %r10d
; X64-NEXT:    decq %r10
; X64-NEXT:    addq $8, %rsi
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:    xorl %r11d, %r11d
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_4: # %loop.73
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_5 Depth 2
; X64-NEXT:    # Child Loop BB0_7 Depth 2
; X64-NEXT:    # Child Loop BB0_8 Depth 3
; X64-NEXT:    movl %r11d, %r14d
; X64-NEXT:    notl %r14d
; X64-NEXT:    addl %r9d, %r14d
; X64-NEXT:    movq (%rdx,%r14,8), %rbx
; X64-NEXT:    xorl %eax, %eax
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X64-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_5: # %loop.90
; X64-NEXT:    # Parent Loop BB0_4 Depth=1
; X64-NEXT:    # => This Inner Loop Header: Depth=2
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vxorpd %xmm6, %xmm6, %xmm6
; X64-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X64-NEXT:    vgatherqpd %ymm7, (%rdi,%ymm5,8), %ymm6
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vxorpd %xmm7, %xmm7, %xmm7
; X64-NEXT:    vpcmpeqd %ymm8, %ymm8, %ymm8
; X64-NEXT:    vgatherqpd %ymm8, (%rdi,%ymm5,8), %ymm7
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vxorpd %xmm8, %xmm8, %xmm8
; X64-NEXT:    vpcmpeqd %ymm9, %ymm9, %ymm9
; X64-NEXT:    vgatherqpd %ymm9, (%rdi,%ymm5,8), %ymm8
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vxorpd %xmm9, %xmm9, %xmm9
; X64-NEXT:    vpcmpeqd %ymm10, %ymm10, %ymm10
; X64-NEXT:    vgatherqpd %ymm10, (%rdi,%ymm5,8), %ymm9
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm9 * mem) + ymm3
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm8 * mem) + ymm2
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm7 * mem) + ymm1
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm6 * mem) + ymm4
; X64-NEXT:    addq $16, %rax
; X64-NEXT:    leal -16(%rax), %ebp
; X64-NEXT:    cmpl $4080, %ebp # imm = 0xFF0
; X64-NEXT:    jb .LBB0_5
; X64-NEXT:  # %bb.6: # %afterloop.90
; X64-NEXT:    # in Loop: Header=BB0_4 Depth=1
; X64-NEXT:    vaddpd %ymm3, %ymm1, %ymm1
; X64-NEXT:    vaddpd %ymm4, %ymm2, %ymm2
; X64-NEXT:    vaddpd %ymm2, %ymm1, %ymm1
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vpermilpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    movq (%rcx,%r14,8), %r15
; X64-NEXT:    movq (%r8,%r14,8), %r14
; X64-NEXT:    xorl %r12d, %r12d
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_7: # %loop.75
; X64-NEXT:    # Parent Loop BB0_4 Depth=1
; X64-NEXT:    # => This Loop Header: Depth=2
; X64-NEXT:    # Child Loop BB0_8 Depth 3
; X64-NEXT:    movq (%r15,%r12,8), %r13
; X64-NEXT:    movq (%r14,%r12,8), %rax
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    movq $-4, %rbx
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_8: # %loop.107
; X64-NEXT:    # Parent Loop BB0_4 Depth=1
; X64-NEXT:    # Parent Loop BB0_7 Depth=2
; X64-NEXT:    # => This Inner Loop Header: Depth=3
; X64-NEXT:    vaddpd 32(%r13,%rbx,8), %ymm1, %ymm1
; X64-NEXT:    vaddpd 32(%rax,%rbx,8), %ymm1, %ymm1
; X64-NEXT:    addq $4, %rbx
; X64-NEXT:    cmpq $4092, %rbx # imm = 0xFFC
; X64-NEXT:    jb .LBB0_8
; X64-NEXT:  # %bb.9: # %afterloop.107
; X64-NEXT:    # in Loop: Header=BB0_7 Depth=2
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vpermilpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    cmpq $4095, %r12 # imm = 0xFFF
; X64-NEXT:    leaq 1(%r12), %r12
; X64-NEXT:    jne .LBB0_7
; X64-NEXT:  # %bb.10: # %afterloop.75
; X64-NEXT:    # in Loop: Header=BB0_4 Depth=1
; X64-NEXT:    addq $32768, %rsi # imm = 0x8000
; X64-NEXT:    cmpq %r10, %r11
; X64-NEXT:    leaq 1(%r11), %r11
; X64-NEXT:    jne .LBB0_4
; X64-NEXT:    jmp .LBB0_2
; X64-NEXT:  .LBB0_1: # %entry
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:  .LBB0_2: # %for.cond.cleanup
; X64-NEXT:    popq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    popq %r12
; X64-NEXT:    .cfi_def_cfa_offset 40
; X64-NEXT:    popq %r13
; X64-NEXT:    .cfi_def_cfa_offset 32
; X64-NEXT:    popq %r14
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    popq %r15
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    popq %rbp
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: foo:
; X86:       # %bb.0: # %entry
; X86-NEXT:    pushl %ebp
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %ebp, -8
; X86-NEXT:    movl %esp, %ebp
; X86-NEXT:    .cfi_def_cfa_register %ebp
; X86-NEXT:    pushl %ebx
; X86-NEXT:    pushl %edi
; X86-NEXT:    pushl %esi
; X86-NEXT:    andl $-32, %esp
; X86-NEXT:    subl $160, %esp
; X86-NEXT:    .cfi_offset %esi, -20
; X86-NEXT:    .cfi_offset %edi, -16
; X86-NEXT:    .cfi_offset %ebx, -12
; X86-NEXT:    movl 32(%ebp), %eax
; X86-NEXT:    testl %eax, %eax
; X86-NEXT:    jle .LBB0_1
; X86-NEXT:  # %bb.3: # %for.body.preheader
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    movl %eax, %ecx
; X86-NEXT:    addl $-1, %ecx
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl $0, %ecx
; X86-NEXT:    adcl $-1, %ecx
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    xorl %esi, %esi
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_4: # %loop.73
; X86-NEXT:    # =>This Loop Header: Depth=1
; X86-NEXT:    # Child Loop BB0_5 Depth 2
; X86-NEXT:    # Child Loop BB0_7 Depth 2
; X86-NEXT:    # Child Loop BB0_8 Depth 3
; X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vmovsd %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 8-byte Spill
; X86-NEXT:    movl %edx, %edi
; X86-NEXT:    notl %edi
; X86-NEXT:    addl %eax, %edi
; X86-NEXT:    movl 16(%ebp), %eax
; X86-NEXT:    movl (%eax,%edi,4), %esi
; X86-NEXT:    movl $-16, %ebx
; X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    shll $12, %eax
; X86-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X86-NEXT:    movl 12(%ebp), %edx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_5: # %loop.90
; X86-NEXT:    # Parent Loop BB0_4 Depth=1
; X86-NEXT:    # => This Inner Loop Header: Depth=2
; X86-NEXT:    vmovapd %ymm2, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovaps %ymm0, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm5, %xmm5, %xmm5
; X86-NEXT:    vgatherqpd %ymm7, (%ecx,%ymm0,8), %ymm5
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm1 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    vgatherqpd %ymm7, (%ecx,%ymm1,8), %ymm0
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm2 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vgatherqpd %ymm7, (%ecx,%ymm2,8), %ymm1
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm2, %ymm2, %ymm2
; X86-NEXT:    vxorpd %xmm6, %xmm6, %xmm6
; X86-NEXT:    vgatherqpd %ymm2, (%ecx,%ymm7,8), %ymm6
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm2 # 32-byte Reload
; X86-NEXT:    leal 17(%eax,%ebx), %ecx
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm6 * mem) + ymm3
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm1 * mem) + ymm2
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm1 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm0 * mem) + ymm1
; X86-NEXT:    vmovapd %ymm1, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm0 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm5 * mem) + ymm4
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    addl $16, %ebx
; X86-NEXT:    cmpl $4080, %ebx # imm = 0xFF0
; X86-NEXT:    jb .LBB0_5
; X86-NEXT:  # %bb.6: # %afterloop.90
; X86-NEXT:    # in Loop: Header=BB0_4 Depth=1
; X86-NEXT:    vaddpd %ymm3, %ymm0, %ymm0
; X86-NEXT:    vaddpd %ymm4, %ymm2, %ymm1
; X86-NEXT:    vaddpd %ymm1, %ymm0, %ymm0
; X86-NEXT:    vextractf128 $1, %ymm0, %xmm1
; X86-NEXT:    vaddpd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vpermilpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vmovsd {{[-0-9]+}}(%e{{[sb]}}p), %xmm2 # 8-byte Reload
; X86-NEXT:    # xmm2 = mem[0],zero
; X86-NEXT:    vaddsd %xmm0, %xmm2, %xmm2
; X86-NEXT:    movl 20(%ebp), %eax
; X86-NEXT:    movl (%eax,%edi,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl 24(%ebp), %eax
; X86-NEXT:    movl (%eax,%edi,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    xorl %esi, %esi
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_7: # %loop.75
; X86-NEXT:    # Parent Loop BB0_4 Depth=1
; X86-NEXT:    # => This Loop Header: Depth=2
; X86-NEXT:    # Child Loop BB0_8 Depth 3
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl (%eax,%esi,4), %edx
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl (%eax,%esi,4), %ecx
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    xorl %esi, %esi
; X86-NEXT:    xorl %edi, %edi
; X86-NEXT:    xorl %ebx, %ebx
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_8: # %loop.107
; X86-NEXT:    # Parent Loop BB0_4 Depth=1
; X86-NEXT:    # Parent Loop BB0_7 Depth=2
; X86-NEXT:    # => This Inner Loop Header: Depth=3
; X86-NEXT:    vaddpd (%edx,%esi,8), %ymm1, %ymm0
; X86-NEXT:    vaddpd (%ecx,%esi,8), %ymm0, %ymm1
; X86-NEXT:    addl $4, %ebx
; X86-NEXT:    adcl $0, %eax
; X86-NEXT:    cmpl $4092, %esi # imm = 0xFFC
; X86-NEXT:    sbbl $0, %edi
; X86-NEXT:    movl %ebx, %esi
; X86-NEXT:    movl %eax, %edi
; X86-NEXT:    jb .LBB0_8
; X86-NEXT:  # %bb.9: # %afterloop.107
; X86-NEXT:    # in Loop: Header=BB0_7 Depth=2
; X86-NEXT:    vextractf128 $1, %ymm1, %xmm0
; X86-NEXT:    vaddpd %xmm0, %xmm1, %xmm0
; X86-NEXT:    vpermilpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vaddsd %xmm0, %xmm2, %xmm2
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl $4095, %edx # imm = 0xFFF
; X86-NEXT:    orl %esi, %edx
; X86-NEXT:    movl %eax, %esi
; X86-NEXT:    jne .LBB0_7
; X86-NEXT:  # %bb.10: # %afterloop.75
; X86-NEXT:    # in Loop: Header=BB0_4 Depth=1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Folded Reload
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Folded Reload
; X86-NEXT:    orl %edx, %esi
; X86-NEXT:    movl %eax, %edx
; X86-NEXT:    movl %ecx, %esi
; X86-NEXT:    movl 32(%ebp), %eax
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    jne .LBB0_4
; X86-NEXT:    jmp .LBB0_2
; X86-NEXT:  .LBB0_1: # %entry
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:  .LBB0_2: # %for.cond.cleanup
; X86-NEXT:    vmovsd %xmm2, {{[0-9]+}}(%esp)
; X86-NEXT:    fldl {{[0-9]+}}(%esp)
; X86-NEXT:    leal -12(%ebp), %esp
; X86-NEXT:    popl %esi
; X86-NEXT:    popl %edi
; X86-NEXT:    popl %ebx
; X86-NEXT:    popl %ebp
; X86-NEXT:    .cfi_def_cfa %esp, 4
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
entry:
  %cmp70 = icmp sgt i32 %N, 0
  br i1 %cmp70, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %0 = zext i32 %N to i64
  %1 = add nsw i64 %0, -1
  br label %loop.73

for.cond.cleanup:                                 ; preds = %afterloop.75, %entry
  %dst_row.0.lcssa = phi double [ 0.000000e+00, %entry ], [ %31, %afterloop.75 ]
  ret double %dst_row.0.lcssa

loop.73:                                          ; preds = %afterloop.75, %for.body.preheader
  %i1.i64.0 = phi i64 [ 0, %for.body.preheader ], [ %nextivloop.73, %afterloop.75 ]
  %t3.0 = phi double [ 0.000000e+00, %for.body.preheader ], [ %31, %afterloop.75 ]
  %2 = trunc i64 %i1.i64.0 to i32
  %3 = xor i32 %2, -1
  %4 = add i32 %3, %N
  %5 = zext i32 %4 to i64
  %6 = getelementptr inbounds i32*, i32** %rowstart, i64 %5
  %gepload = load i32*, i32** %6, align 8, !tbaa !3
  br label %loop.90

loop.90:                                          ; preds = %loop.90, %loop.73
  %i2.i32.0 = phi i32 [ 0, %loop.73 ], [ %nextivloop.90, %loop.90 ]
  %t63.0 = phi <16 x double> [ zeroinitializer, %loop.73 ], [ %19, %loop.90 ]
  %7 = or i32 %i2.i32.0, 1
  %8 = zext i32 %7 to i64
  %9 = getelementptr inbounds i32, i32* %gepload, i64 %8
  %10 = bitcast i32* %9 to <16 x i32>*
  %gepload100 = load <16 x i32>, <16 x i32>* %10, align 4, !tbaa !7
  %11 = zext <16 x i32> %gepload100 to <16 x i64>
  %12 = getelementptr inbounds double, double* %dst, <16 x i64> %11
  %13 = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> %12, i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef), !tbaa !9
  %14 = shl i64 %i1.i64.0, 12
  %15 = add i64 %14, %8
  %16 = getelementptr inbounds double, double* %luval, i64 %15
  %17 = bitcast double* %16 to <16 x double>*
  %gepload101 = load <16 x double>, <16 x double>* %17, align 8, !tbaa !9
  %18 = fmul fast <16 x double> %13, %gepload101
  %19 = fadd fast <16 x double> %18, %t63.0
  %nextivloop.90 = add nuw nsw i32 %i2.i32.0, 16
  %condloop.90 = icmp ult i32 %i2.i32.0, 4080
  br i1 %condloop.90, label %loop.90, label %afterloop.90, !llvm.loop !11

afterloop.90:                                     ; preds = %loop.90
  %20 = call fast double @llvm.vector.reduce.fadd.v16f64(double %t3.0, <16 x double> %19)
  %21 = getelementptr inbounds double**, double*** %src1, i64 %5
  %gepload103 = load double**, double*** %21, align 8, !tbaa !16
  %22 = getelementptr inbounds double**, double*** %src2, i64 %5
  %gepload104 = load double**, double*** %22, align 8, !tbaa !16
  br label %loop.75

loop.75:                                          ; preds = %afterloop.107, %afterloop.90
  %i2.i64.0 = phi i64 [ 0, %afterloop.90 ], [ %nextivloop.75, %afterloop.107 ]
  %t3.1 = phi double [ %20, %afterloop.90 ], [ %31, %afterloop.107 ]
  %23 = getelementptr inbounds double*, double** %gepload103, i64 %i2.i64.0
  %gepload105 = load double*, double** %23, align 8, !tbaa !18
  %24 = getelementptr inbounds double*, double** %gepload104, i64 %i2.i64.0
  %gepload106 = load double*, double** %24, align 8, !tbaa !18
  br label %loop.107

loop.107:                                         ; preds = %loop.107, %loop.75
  %t75.0 = phi <4 x double> [ zeroinitializer, %loop.75 ], [ %30, %loop.107 ]
  %i3.i64.0 = phi i64 [ 0, %loop.75 ], [ %nextivloop.107, %loop.107 ]
  %25 = getelementptr inbounds double, double* %gepload105, i64 %i3.i64.0
  %26 = bitcast double* %25 to <4 x double>*
  %gepload107 = load <4 x double>, <4 x double>* %26, align 8, !tbaa !9
  %27 = fadd fast <4 x double> %gepload107, %t75.0
  %28 = getelementptr inbounds double, double* %gepload106, i64 %i3.i64.0
  %29 = bitcast double* %28 to <4 x double>*
  %gepload108 = load <4 x double>, <4 x double>* %29, align 8, !tbaa !9
  %30 = fadd fast <4 x double> %27, %gepload108
  %nextivloop.107 = add nuw nsw i64 %i3.i64.0, 4
  %condloop.107 = icmp ult i64 %i3.i64.0, 4092
  br i1 %condloop.107, label %loop.107, label %afterloop.107, !llvm.loop !20

afterloop.107:                                    ; preds = %loop.107
  %31 = call fast double @llvm.vector.reduce.fadd.v4f64(double %t3.1, <4 x double> %30)
  %nextivloop.75 = add nuw nsw i64 %i2.i64.0, 1
  %condloop.75.not = icmp eq i64 %i2.i64.0, 4095
  br i1 %condloop.75.not, label %afterloop.75, label %loop.75, !llvm.loop !21

afterloop.75:                                     ; preds = %afterloop.107
  %nextivloop.73 = add nuw nsw i64 %i1.i64.0, 1
  %condloop.73.not = icmp eq i64 %i1.i64.0, %1
  br i1 %condloop.73.not, label %for.cond.cleanup, label %loop.73, !llvm.loop !22
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare double @llvm.vector.reduce.fadd.v16f64(double, <16 x double>) #1

; Function Attrs: nofree nosync nounwind readnone willreturn
declare double @llvm.vector.reduce.fadd.v4f64(double, <4 x double>) #1

; Function Attrs: nofree nosync nounwind readonly willreturn
declare <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*>, i32 immarg, <16 x i1>, <16 x double>) #2

attributes #0 = { nofree norecurse nosync nounwind readonly uwtable "approx-func-fp-math"="true" "denormal-fp-math"="preserve-sign,preserve-sign" "denormal-fp-math-f32"="ieee,ieee" "frame-pointer"="none" "loopopt-pipeline"="full" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="alderlake" "target-features"="+adx,+aes,+avx,+avx2,+avxvnni,+bmi,+bmi2,+cldemote,+clflushopt,+clwb,+crc32,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+gfni,+hreset,+invpcid,+kl,+lzcnt,+mmx,+movbe,+movdir64b,+movdiri,+pclmul,+pconfig,+pku,+popcnt,+prfchw,+ptwrite,+rdpid,+rdrnd,+rdseed,+sahf,+serialize,+sgx,+sha,+shstk,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+vaes,+vpclmulqdq,+waitpkg,+widekl,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "unsafe-fp-math"="true" }
attributes #1 = { nofree nosync nounwind readnone willreturn }
attributes #2 = { nofree nosync nounwind readonly willreturn }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.x.0.YYYYMMDD)"}
!3 = !{!4, !4, i64 0}
!4 = !{!"pointer@_ZTSPj", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C/C++ TBAA"}
!7 = !{!8, !8, i64 0}
!8 = !{!"int", !5, i64 0}
!9 = !{!10, !10, i64 0}
!10 = !{!"double", !5, i64 0}
!11 = distinct !{!11, !12, !13, !14, !15}
!12 = !{!"llvm.loop.mustprogress"}
!13 = !{!"llvm.loop.unroll.disable"}
!14 = !{!"llvm.loop.vectorize.width", i32 1}
!15 = !{!"llvm.loop.interleave.count", i32 1}
!16 = !{!17, !17, i64 0}
!17 = !{!"pointer@_ZTSPPd", !5, i64 0}
!18 = !{!19, !19, i64 0}
!19 = !{!"pointer@_ZTSPd", !5, i64 0}
!20 = distinct !{!20, !12, !13, !14, !15}
!21 = distinct !{!21, !12, !13}
!22 = distinct !{!22, !12, !13}
