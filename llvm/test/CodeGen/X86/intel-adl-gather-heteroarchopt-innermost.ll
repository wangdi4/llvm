; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -intel-libirc-allowed -mcpu=alderlake -O3 | FileCheck %s --check-prefix=X64
; RUN: llc < %s -mtriple=i386-unknown-unknown -intel-libirc-allowed -mcpu=alderlake -O3 | FileCheck %s --check-prefix=X86

; Function Attrs: nofree norecurse nosync nounwind readonly uwtable
define dso_local double @foo(double* noalias nocapture readonly %dst, double* noalias nocapture readonly %luval, i32** nocapture readonly %rowstart, double*** noalias nocapture readonly %src1, double*** noalias nocapture readonly %src2, i32* nocapture readnone %first_after_diagonal, i32 %N) local_unnamed_addr #0 {
; X64-LABEL: foo:
; X64:       # %bb.0: # %entry
; X64-NEXT:    pushq %rbp
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    pushq %r15
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    pushq %r14
; X64-NEXT:    .cfi_def_cfa_offset 32
; X64-NEXT:    pushq %r13
; X64-NEXT:    .cfi_def_cfa_offset 40
; X64-NEXT:    pushq %r12
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    pushq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 56
; X64-NEXT:    pushq %rax
; X64-NEXT:    .cfi_def_cfa_offset 64
; X64-NEXT:    .cfi_offset %rbx, -56
; X64-NEXT:    .cfi_offset %r12, -48
; X64-NEXT:    .cfi_offset %r13, -40
; X64-NEXT:    .cfi_offset %r14, -32
; X64-NEXT:    .cfi_offset %r15, -24
; X64-NEXT:    .cfi_offset %rbp, -16
; X64-NEXT:    movl {{[0-9]+}}(%rsp), %ebx
; X64-NEXT:    testl %ebx, %ebx
; X64-NEXT:    jle .LBB0_1
; X64-NEXT:  # %bb.2: # %for.body.preheader
; X64-NEXT:    movl %ebx, %r14d
; X64-NEXT:    #APP
; X64-NEXT:    rdpid %rax
; X64-NEXT:    #NO_APP
; X64-NEXT:    andl $1023, %eax # imm = 0x3FF
; X64-NEXT:    movq __cpu_core_type@GOTPCREL(%rip), %r9
; X64-NEXT:    movzbl (%r9,%rax), %eax
; X64-NEXT:    testb %al, %al
; X64-NEXT:    je .LBB0_20
; X64-NEXT:  .LBB0_3:
; X64-NEXT:    decq %r14
; X64-NEXT:    addq $8, %rsi
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:    xorl %r9d, %r9d
; X64-NEXT:    cmpb $32, %al
; X64-NEXT:    je .LBB0_4
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_8: # %loop.73
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_9 Depth 2
; X64-NEXT:    # Child Loop BB0_11 Depth 2
; X64-NEXT:    # Child Loop BB0_12 Depth 3
; X64-NEXT:    movl %r9d, %eax
; X64-NEXT:    notl %eax
; X64-NEXT:    addl %ebx, %eax
; X64-NEXT:    movq (%rdx,%rax,8), %r10
; X64-NEXT:    xorl %r11d, %r11d
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X64-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_9: # %loop.90
; X64-NEXT:    # Parent Loop BB0_8 Depth=1
; X64-NEXT:    # => This Inner Loop Header: Depth=2
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm7, %xmm7, %xmm7
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm7
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm8, %xmm8, %xmm8
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm8
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm9, %xmm9, %xmm9
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm9
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm10, %xmm10, %xmm10
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm10
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm10 * mem) + ymm3
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm9 * mem) + ymm2
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm8 * mem) + ymm1
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm7 * mem) + ymm4
; X64-NEXT:    addq $16, %r11
; X64-NEXT:    leal -16(%r11), %ebp
; X64-NEXT:    cmpl $4080, %ebp # imm = 0xFF0
; X64-NEXT:    jb .LBB0_9
; X64-NEXT:  # %bb.10: # %afterloop.90
; X64-NEXT:    # in Loop: Header=BB0_8 Depth=1
; X64-NEXT:    vaddpd %ymm3, %ymm1, %ymm1
; X64-NEXT:    vaddpd %ymm4, %ymm2, %ymm2
; X64-NEXT:    vaddpd %ymm2, %ymm1, %ymm1
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vshufpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    movq (%rcx,%rax,8), %r10
; X64-NEXT:    movq (%r8,%rax,8), %rax
; X64-NEXT:    xorl %r11d, %r11d
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_11: # %loop.75
; X64-NEXT:    # Parent Loop BB0_8 Depth=1
; X64-NEXT:    # => This Loop Header: Depth=2
; X64-NEXT:    # Child Loop BB0_12 Depth 3
; X64-NEXT:    movq (%r10,%r11,8), %r15
; X64-NEXT:    movq (%rax,%r11,8), %r12
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    movq $-4, %r13
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_12: # %loop.107
; X64-NEXT:    # Parent Loop BB0_8 Depth=1
; X64-NEXT:    # Parent Loop BB0_11 Depth=2
; X64-NEXT:    # => This Inner Loop Header: Depth=3
; X64-NEXT:    vaddpd 32(%r15,%r13,8), %ymm1, %ymm1
; X64-NEXT:    vaddpd 32(%r12,%r13,8), %ymm1, %ymm1
; X64-NEXT:    addq $4, %r13
; X64-NEXT:    cmpq $4092, %r13 # imm = 0xFFC
; X64-NEXT:    jb .LBB0_12
; X64-NEXT:  # %bb.13: # %afterloop.107
; X64-NEXT:    # in Loop: Header=BB0_11 Depth=2
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vshufpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    leaq 1(%r11), %r15
; X64-NEXT:    cmpq $4095, %r11 # imm = 0xFFF
; X64-NEXT:    movq %r15, %r11
; X64-NEXT:    jne .LBB0_11
; X64-NEXT:  # %bb.14: # %afterloop.75
; X64-NEXT:    # in Loop: Header=BB0_8 Depth=1
; X64-NEXT:    leaq 1(%r9), %rax
; X64-NEXT:    addq $32768, %rsi # imm = 0x8000
; X64-NEXT:    cmpq %r14, %r9
; X64-NEXT:    movq %rax, %r9
; X64-NEXT:    jne .LBB0_8
; X64-NEXT:    jmp .LBB0_16
; X64-NEXT:  .LBB0_1:
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:    jmp .LBB0_16
; X64-NEXT:  .LBB0_4:
; X64-NEXT:    vmovq %rdi, %xmm1
; X64-NEXT:    vpbroadcastq %xmm1, %ymm1
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_5: # %loop.73.clone
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_6 Depth 2
; X64-NEXT:    # Child Loop BB0_19 Depth 2
; X64-NEXT:    # Child Loop BB0_17 Depth 3
; X64-NEXT:    movl %r9d, %eax
; X64-NEXT:    notl %eax
; X64-NEXT:    addl %ebx, %eax
; X64-NEXT:    movq (%rdx,%rax,8), %rdi
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    xorl %r10d, %r10d
; X64-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X64-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X64-NEXT:    vxorpd %xmm5, %xmm5, %xmm5
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_6: # %loop.90.clone
; X64-NEXT:    # Parent Loop BB0_5 Depth=1
; X64-NEXT:    # => This Inner Loop Header: Depth=2
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm7 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm9 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm8 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpsllq $3, %ymm6, %ymm6
; X64-NEXT:    vpaddq %ymm6, %ymm1, %ymm6
; X64-NEXT:    vpsllq $3, %ymm8, %ymm8
; X64-NEXT:    vpaddq %ymm1, %ymm8, %ymm8
; X64-NEXT:    vpsllq $3, %ymm9, %ymm9
; X64-NEXT:    vpaddq %ymm1, %ymm9, %ymm10
; X64-NEXT:    vpsllq $3, %ymm7, %ymm7
; X64-NEXT:    vpaddq %ymm7, %ymm1, %ymm7
; X64-NEXT:    vextracti128 $1, %ymm7, %xmm9
; X64-NEXT:    vmovq %xmm9, %r11
; X64-NEXT:    vpextrq $1, %xmm9, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm9 = mem[0],zero
; X64-NEXT:    vmovq %xmm10, %r11
; X64-NEXT:    vextracti128 $1, %ymm10, %xmm11
; X64-NEXT:    vmovhpd {{.*#+}} xmm9 = xmm9[0],mem[0]
; X64-NEXT:    vmovq %xmm11, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm12 = mem[0],zero
; X64-NEXT:    vpextrq $1, %xmm11, %r15
; X64-NEXT:    vmovhpd {{.*#+}} xmm11 = xmm12[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm10, %r15
; X64-NEXT:    vextracti128 $1, %ymm8, %xmm10
; X64-NEXT:    vmovsd {{.*#+}} xmm12 = mem[0],zero
; X64-NEXT:    vmovq %xmm10, %r11
; X64-NEXT:    vmovhpd {{.*#+}} xmm12 = xmm12[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm10, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm10 = mem[0],zero
; X64-NEXT:    vmovq %xmm8, %r11
; X64-NEXT:    vmovhpd {{.*#+}} xmm10 = xmm10[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm8, %r15
; X64-NEXT:    vextracti128 $1, %ymm6, %xmm8
; X64-NEXT:    vmovsd {{.*#+}} xmm13 = mem[0],zero
; X64-NEXT:    vmovq %xmm8, %r11
; X64-NEXT:    vmovhpd {{.*#+}} xmm13 = xmm13[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm8, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm8 = mem[0],zero
; X64-NEXT:    vmovq %xmm6, %r11
; X64-NEXT:    vmovhpd {{.*#+}} xmm8 = xmm8[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm6, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm6 = mem[0],zero
; X64-NEXT:    vmovq %xmm7, %r11
; X64-NEXT:    vmovhpd {{.*#+}} xmm6 = xmm6[0],mem[0]
; X64-NEXT:    vpextrq $1, %xmm7, %r15
; X64-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X64-NEXT:    vinsertf128 $1, %xmm11, %ymm12, %ymm11
; X64-NEXT:    vinsertf128 $1, %xmm10, %ymm13, %ymm10
; X64-NEXT:    vinsertf128 $1, %xmm8, %ymm6, %ymm6
; X64-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm5 = (ymm6 * mem) + ymm5
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm10 * mem) + ymm4
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm11 * mem) + ymm3
; X64-NEXT:    vinsertf128 $1, %xmm9, %ymm7, %ymm6
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm6 * mem) + ymm2
; X64-NEXT:    addq $16, %r10
; X64-NEXT:    leal -16(%r10), %r11d
; X64-NEXT:    cmpl $4080, %r11d # imm = 0xFF0
; X64-NEXT:    jb .LBB0_6
; X64-NEXT:  # %bb.7: # %afterloop.90.clone
; X64-NEXT:    # in Loop: Header=BB0_5 Depth=1
; X64-NEXT:    vaddpd %ymm4, %ymm2, %ymm2
; X64-NEXT:    vaddpd %ymm5, %ymm3, %ymm3
; X64-NEXT:    vaddpd %ymm3, %ymm2, %ymm2
; X64-NEXT:    vextractf128 $1, %ymm2, %xmm3
; X64-NEXT:    vaddpd %xmm3, %xmm2, %xmm2
; X64-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; X64-NEXT:    vaddsd %xmm3, %xmm2, %xmm2
; X64-NEXT:    vaddsd %xmm2, %xmm0, %xmm0
; X64-NEXT:    movq (%rcx,%rax,8), %rdi
; X64-NEXT:    movq (%r8,%rax,8), %rax
; X64-NEXT:    xorl %r10d, %r10d
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_19: # %loop.75.clone
; X64-NEXT:    # Parent Loop BB0_5 Depth=1
; X64-NEXT:    # => This Loop Header: Depth=2
; X64-NEXT:    # Child Loop BB0_17 Depth 3
; X64-NEXT:    movq (%rdi,%r10,8), %r11
; X64-NEXT:    movq (%rax,%r10,8), %r15
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    movq $-4, %r12
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_17: # %loop.107.clone
; X64-NEXT:    # Parent Loop BB0_5 Depth=1
; X64-NEXT:    # Parent Loop BB0_19 Depth=2
; X64-NEXT:    # => This Inner Loop Header: Depth=3
; X64-NEXT:    vaddpd 32(%r11,%r12,8), %ymm2, %ymm2
; X64-NEXT:    vaddpd 32(%r15,%r12,8), %ymm2, %ymm2
; X64-NEXT:    addq $4, %r12
; X64-NEXT:    cmpq $4092, %r12 # imm = 0xFFC
; X64-NEXT:    jb .LBB0_17
; X64-NEXT:  # %bb.18: # %afterloop.107.clone
; X64-NEXT:    # in Loop: Header=BB0_19 Depth=2
; X64-NEXT:    vextractf128 $1, %ymm2, %xmm3
; X64-NEXT:    vaddpd %xmm3, %xmm2, %xmm2
; X64-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; X64-NEXT:    vaddsd %xmm3, %xmm2, %xmm2
; X64-NEXT:    vaddsd %xmm2, %xmm0, %xmm0
; X64-NEXT:    leaq 1(%r10), %r11
; X64-NEXT:    cmpq $4095, %r10 # imm = 0xFFF
; X64-NEXT:    movq %r11, %r10
; X64-NEXT:    jne .LBB0_19
; X64-NEXT:  # %bb.15: # %afterloop.75.clone
; X64-NEXT:    # in Loop: Header=BB0_5 Depth=1
; X64-NEXT:    leaq 1(%r9), %rax
; X64-NEXT:    addq $32768, %rsi # imm = 0x8000
; X64-NEXT:    cmpq %r14, %r9
; X64-NEXT:    movq %rax, %r9
; X64-NEXT:    jne .LBB0_5
; X64-NEXT:  .LBB0_16: # %for.cond.cleanup
; X64-NEXT:    addq $8, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 56
; X64-NEXT:    popq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    popq %r12
; X64-NEXT:    .cfi_def_cfa_offset 40
; X64-NEXT:    popq %r13
; X64-NEXT:    .cfi_def_cfa_offset 32
; X64-NEXT:    popq %r14
; X64-NEXT:    .cfi_def_cfa_offset 24
; X64-NEXT:    popq %r15
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    popq %rbp
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
; X64-NEXT:  .LBB0_20:
; X64-NEXT:    .cfi_def_cfa_offset 64
; X64-NEXT:    movq %r8, (%rsp) # 8-byte Spill
; X64-NEXT:    movq %rcx, %rbp
; X64-NEXT:    movq %rdx, %r12
; X64-NEXT:    movq %rsi, %r15
; X64-NEXT:    movq %rdi, %r13
; X64-NEXT:    callq __detect_cpu_core_type@PLT
; X64-NEXT:    movq %r13, %rdi
; X64-NEXT:    movq %r15, %rsi
; X64-NEXT:    movq %r12, %rdx
; X64-NEXT:    movq %rbp, %rcx
; X64-NEXT:    movq (%rsp), %r8 # 8-byte Reload
; X64-NEXT:    jmp .LBB0_3
;
; X86-LABEL: foo:
; X86:       # %bb.0: # %entry
; X86-NEXT:    pushl %ebp
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %ebp, -8
; X86-NEXT:    movl %esp, %ebp
; X86-NEXT:    .cfi_def_cfa_register %ebp
; X86-NEXT:    pushl %ebx
; X86-NEXT:    pushl %edi
; X86-NEXT:    pushl %esi
; X86-NEXT:    andl $-32, %esp
; X86-NEXT:    subl $160, %esp
; X86-NEXT:    .cfi_offset %esi, -20
; X86-NEXT:    .cfi_offset %edi, -16
; X86-NEXT:    .cfi_offset %ebx, -12
; X86-NEXT:    movl 32(%ebp), %esi
; X86-NEXT:    testl %esi, %esi
; X86-NEXT:    jle .LBB0_1
; X86-NEXT:  # %bb.2: # %for.body.preheader
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    movl %esi, %eax
; X86-NEXT:    addl $-1, %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl $0, %eax
; X86-NEXT:    adcl $-1, %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    #APP
; X86-NEXT:    rdpid %eax
; X86-NEXT:    #NO_APP
; X86-NEXT:    andl $1023, %eax # imm = 0x3FF
; X86-NEXT:    movzbl __cpu_core_type(%eax), %eax
; X86-NEXT:    testb %al, %al
; X86-NEXT:    je .LBB0_21
; X86-NEXT:  # %bb.3:
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    cmpb $32, %al
; X86-NEXT:    je .LBB0_19
; X86-NEXT:  .LBB0_4:
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_5: # %loop.73
; X86-NEXT:    # =>This Loop Header: Depth=1
; X86-NEXT:    # Child Loop BB0_6 Depth 2
; X86-NEXT:    # Child Loop BB0_8 Depth 2
; X86-NEXT:    # Child Loop BB0_9 Depth 3
; X86-NEXT:    movl %ecx, (%esp) # 4-byte Spill
; X86-NEXT:    vmovsd %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 8-byte Spill
; X86-NEXT:    movl %edx, %ecx
; X86-NEXT:    notl %ecx
; X86-NEXT:    addl %esi, %ecx
; X86-NEXT:    movl 16(%ebp), %eax
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl $-16, %ebx
; X86-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X86-NEXT:    movl 12(%ebp), %esi
; X86-NEXT:    movl 8(%ebp), %edi
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_6: # %loop.90
; X86-NEXT:    # Parent Loop BB0_5 Depth=1
; X86-NEXT:    # => This Inner Loop Header: Depth=2
; X86-NEXT:    vmovapd %ymm2, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovaps %ymm0, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X86-NEXT:    vxorpd %xmm5, %xmm5, %xmm5
; X86-NEXT:    vgatherqpd %ymm6, (%edi,%ymm0,8), %ymm5
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    vgatherqpd %ymm7, (%edi,%ymm6,8), %ymm0
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vgatherqpd %ymm7, (%edi,%ymm6,8), %ymm1
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm6 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vgatherqpd %ymm7, (%edi,%ymm6,8), %ymm2
; X86-NEXT:    movl %edx, %ecx
; X86-NEXT:    shll $12, %ecx
; X86-NEXT:    leal 17(%ecx,%ebx), %ecx
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm2 * mem) + ymm3
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm2 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm1 * mem) + ymm2
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm1 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm0 * mem) + ymm1
; X86-NEXT:    vmovapd %ymm1, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm0 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm5 * mem) + ymm4
; X86-NEXT:    addl $16, %ebx
; X86-NEXT:    cmpl $4080, %ebx # imm = 0xFF0
; X86-NEXT:    jb .LBB0_6
; X86-NEXT:  # %bb.7: # %afterloop.90
; X86-NEXT:    # in Loop: Header=BB0_5 Depth=1
; X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vaddpd %ymm3, %ymm0, %ymm0
; X86-NEXT:    vaddpd %ymm4, %ymm2, %ymm1
; X86-NEXT:    vaddpd %ymm1, %ymm0, %ymm0
; X86-NEXT:    vextractf128 $1, %ymm0, %xmm1
; X86-NEXT:    vaddpd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vmovsd {{[-0-9]+}}(%e{{[sb]}}p), %xmm3 # 8-byte Reload
; X86-NEXT:    # xmm3 = mem[0],zero
; X86-NEXT:    vaddsd %xmm0, %xmm3, %xmm3
; X86-NEXT:    movl 20(%ebp), %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl 24(%ebp), %eax
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    xorl %esi, %esi
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_8: # %loop.75
; X86-NEXT:    # Parent Loop BB0_5 Depth=1
; X86-NEXT:    # => This Loop Header: Depth=2
; X86-NEXT:    # Child Loop BB0_9 Depth 3
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl (%eax,%esi,4), %edx
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl (%eax,%esi,4), %ecx
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    xorl %edi, %edi
; X86-NEXT:    xorl %ebx, %ebx
; X86-NEXT:    xorl %esi, %esi
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_9: # %loop.107
; X86-NEXT:    # Parent Loop BB0_5 Depth=1
; X86-NEXT:    # Parent Loop BB0_8 Depth=2
; X86-NEXT:    # => This Inner Loop Header: Depth=3
; X86-NEXT:    vaddpd (%edx,%eax,8), %ymm1, %ymm0
; X86-NEXT:    vaddpd (%ecx,%eax,8), %ymm0, %ymm1
; X86-NEXT:    addl $4, %ebx
; X86-NEXT:    adcl $0, %esi
; X86-NEXT:    cmpl $4092, %eax # imm = 0xFFC
; X86-NEXT:    sbbl $0, %edi
; X86-NEXT:    movl %ebx, %eax
; X86-NEXT:    movl %esi, %edi
; X86-NEXT:    jb .LBB0_9
; X86-NEXT:  # %bb.10: # %afterloop.107
; X86-NEXT:    # in Loop: Header=BB0_8 Depth=2
; X86-NEXT:    vextractf128 $1, %ymm1, %xmm0
; X86-NEXT:    vaddpd %xmm0, %xmm1, %xmm0
; X86-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vaddsd %xmm0, %xmm3, %xmm3
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl $4095, %edx # imm = 0xFFF
; X86-NEXT:    orl %esi, %edx
; X86-NEXT:    movl %eax, %esi
; X86-NEXT:    jne .LBB0_8
; X86-NEXT:  # %bb.11: # %afterloop.75
; X86-NEXT:    # in Loop: Header=BB0_5 Depth=1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl (%esp), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Folded Reload
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Folded Reload
; X86-NEXT:    orl %edx, %esi
; X86-NEXT:    movl %eax, %edx
; X86-NEXT:    movl 32(%ebp), %esi
; X86-NEXT:    jne .LBB0_5
; X86-NEXT:    jmp .LBB0_13
; X86-NEXT:  .LBB0_1:
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    jmp .LBB0_13
; X86-NEXT:  .LBB0_21:
; X86-NEXT:    calll __detect_cpu_core_type@PLT
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    cmpb $32, %al
; X86-NEXT:    jne .LBB0_4
; X86-NEXT:  .LBB0_19:
; X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; X86-NEXT:    vpbroadcastd %xmm0, %ymm6
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    vmovdqa %ymm6, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_20: # %loop.73.clone
; X86-NEXT:    # =>This Loop Header: Depth=1
; X86-NEXT:    # Child Loop BB0_17 Depth 2
; X86-NEXT:    # Child Loop BB0_16 Depth 2
; X86-NEXT:    # Child Loop BB0_14 Depth 3
; X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vmovsd %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 8-byte Spill
; X86-NEXT:    movl %edx, %ecx
; X86-NEXT:    notl %ecx
; X86-NEXT:    addl %esi, %ecx
; X86-NEXT:    movl 16(%ebp), %eax
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl %eax, (%esp) # 4-byte Spill
; X86-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; X86-NEXT:    movl $-16, %edi
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X86-NEXT:    vxorpd %xmm5, %xmm5, %xmm5
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_17: # %loop.90.clone
; X86-NEXT:    # Parent Loop BB0_20 Depth=1
; X86-NEXT:    # => This Inner Loop Header: Depth=2
; X86-NEXT:    vmovdqa %ymm0, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    movl (%esp), %eax # 4-byte Reload
; X86-NEXT:    vmovdqu 68(%eax,%edi,4), %ymm0
; X86-NEXT:    vmovdqu 100(%eax,%edi,4), %ymm1
; X86-NEXT:    vpslld $3, %ymm0, %ymm0
; X86-NEXT:    vpaddd %ymm0, %ymm6, %ymm7
; X86-NEXT:    vpextrd $1, %xmm7, %edx
; X86-NEXT:    vpextrd $2, %xmm7, %ebx
; X86-NEXT:    vpextrd $3, %xmm7, %ecx
; X86-NEXT:    vpslld $3, %ymm1, %ymm1
; X86-NEXT:    vextracti128 $1, %ymm7, %xmm0
; X86-NEXT:    vpextrd $1, %xmm0, %eax
; X86-NEXT:    vpextrd $2, %xmm0, %esi
; X86-NEXT:    vpaddd {{[-0-9]+}}(%e{{[sb]}}p), %ymm1, %ymm6 # 32-byte Folded Reload
; X86-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; X86-NEXT:    vpextrd $3, %xmm0, %ebx
; X86-NEXT:    vmovhps {{.*#+}} xmm1 = xmm1[0,1],mem[0,1]
; X86-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    vmovd %xmm7, %ecx
; X86-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; X86-NEXT:    vpextrd $1, %xmm6, %ecx
; X86-NEXT:    vmovhpd {{.*#+}} xmm1 = xmm1[0],mem[0]
; X86-NEXT:    vmovd %xmm0, %edx
; X86-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; X86-NEXT:    vpextrd $2, %xmm6, %esi
; X86-NEXT:    vmovhps {{.*#+}} xmm0 = xmm0[0,1],mem[0,1]
; X86-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    vmovd %xmm6, %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; X86-NEXT:    vpextrd $3, %xmm6, %edx
; X86-NEXT:    vmovhpd {{.*#+}} xmm0 = xmm0[0],mem[0]
; X86-NEXT:    vextracti128 $1, %ymm6, %xmm6
; X86-NEXT:    vpextrd $1, %xmm6, %eax
; X86-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X86-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X86-NEXT:    vmovd %xmm6, %edx
; X86-NEXT:    vpextrd $2, %xmm6, %esi
; X86-NEXT:    vmovsd {{.*#+}} xmm2 = mem[0],zero
; X86-NEXT:    vpextrd $3, %xmm6, %ebx
; X86-NEXT:    vmovhpd {{.*#+}} xmm2 = xmm2[0],mem[0]
; X86-NEXT:    vinsertf128 $1, {{[-0-9]+}}(%e{{[sb]}}p), %ymm1, %ymm6 # 16-byte Folded Reload
; X86-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; X86-NEXT:    vmovhpd {{.*#+}} xmm1 = xmm1[0],mem[0]
; X86-NEXT:    vinsertf128 $1, {{[-0-9]+}}(%e{{[sb]}}p), %ymm0, %ymm0 # 16-byte Folded Reload
; X86-NEXT:    vinsertf128 $1, %xmm7, %ymm2, %ymm2
; X86-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X86-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X86-NEXT:    vinsertf128 $1, %xmm1, %ymm7, %ymm1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    shll $12, %eax
; X86-NEXT:    leal 17(%eax,%edi), %eax
; X86-NEXT:    movl 12(%ebp), %ecx
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm5 = (ymm1 * mem) + ymm5
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm2 * mem) + ymm4
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm0 * mem) + ymm3
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm0 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm0 = (ymm6 * mem) + ymm0
; X86-NEXT:    vmovdqa {{[-0-9]+}}(%e{{[sb]}}p), %ymm6 # 32-byte Reload
; X86-NEXT:    addl $16, %edi
; X86-NEXT:    cmpl $4080, %edi # imm = 0xFF0
; X86-NEXT:    jb .LBB0_17
; X86-NEXT:  # %bb.18: # %afterloop.90.clone
; X86-NEXT:    # in Loop: Header=BB0_20 Depth=1
; X86-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; X86-NEXT:    vaddpd %ymm5, %ymm3, %ymm1
; X86-NEXT:    vaddpd %ymm1, %ymm0, %ymm0
; X86-NEXT:    vextractf128 $1, %ymm0, %xmm1
; X86-NEXT:    vaddpd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vmovsd {{[-0-9]+}}(%e{{[sb]}}p), %xmm3 # 8-byte Reload
; X86-NEXT:    # xmm3 = mem[0],zero
; X86-NEXT:    vaddsd %xmm0, %xmm3, %xmm3
; X86-NEXT:    movl 20(%ebp), %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl 24(%ebp), %eax
; X86-NEXT:    movl (%eax,%ecx,4), %eax
; X86-NEXT:    movl %eax, (%esp) # 4-byte Spill
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_16: # %loop.75.clone
; X86-NEXT:    # Parent Loop BB0_20 Depth=1
; X86-NEXT:    # => This Loop Header: Depth=2
; X86-NEXT:    # Child Loop BB0_14 Depth 3
; X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl (%eax,%edx,4), %esi
; X86-NEXT:    movl (%esp), %eax # 4-byte Reload
; X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl (%eax,%edx,4), %edx
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    xorl %ebx, %ebx
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    xorl %edi, %edi
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_14: # %loop.107.clone
; X86-NEXT:    # Parent Loop BB0_20 Depth=1
; X86-NEXT:    # Parent Loop BB0_16 Depth=2
; X86-NEXT:    # => This Inner Loop Header: Depth=3
; X86-NEXT:    vaddpd (%esi,%ebx,8), %ymm2, %ymm0
; X86-NEXT:    vaddpd (%edx,%ebx,8), %ymm0, %ymm2
; X86-NEXT:    addl $4, %edi
; X86-NEXT:    adcl $0, %eax
; X86-NEXT:    cmpl $4092, %ebx # imm = 0xFFC
; X86-NEXT:    sbbl $0, %ecx
; X86-NEXT:    movl %edi, %ebx
; X86-NEXT:    movl %eax, %ecx
; X86-NEXT:    jb .LBB0_14
; X86-NEXT:  # %bb.15: # %afterloop.107.clone
; X86-NEXT:    # in Loop: Header=BB0_16 Depth=2
; X86-NEXT:    vextractf128 $1, %ymm2, %xmm0
; X86-NEXT:    vaddpd %xmm0, %xmm2, %xmm0
; X86-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vaddsd %xmm0, %xmm3, %xmm3
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl $4095, %edx # imm = 0xFFF
; X86-NEXT:    orl %esi, %edx
; X86-NEXT:    movl %eax, %edx
; X86-NEXT:    jne .LBB0_16
; X86-NEXT:  # %bb.12: # %afterloop.75.clone
; X86-NEXT:    # in Loop: Header=BB0_20 Depth=1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Folded Reload
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Folded Reload
; X86-NEXT:    orl %edx, %esi
; X86-NEXT:    movl %eax, %edx
; X86-NEXT:    movl 32(%ebp), %esi
; X86-NEXT:    jne .LBB0_20
; X86-NEXT:  .LBB0_13: # %for.cond.cleanup
; X86-NEXT:    vmovsd %xmm3, {{[0-9]+}}(%esp)
; X86-NEXT:    fldl {{[0-9]+}}(%esp)
; X86-NEXT:    leal -12(%ebp), %esp
; X86-NEXT:    popl %esi
; X86-NEXT:    popl %edi
; X86-NEXT:    popl %ebx
; X86-NEXT:    popl %ebp
; X86-NEXT:    .cfi_def_cfa %esp, 4
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
entry:
  %cmp70 = icmp sgt i32 %N, 0
  br i1 %cmp70, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %0 = zext i32 %N to i64
  %1 = add nsw i64 %0, -1
  br label %loop.73

for.cond.cleanup:                                 ; preds = %afterloop.75, %entry
  %dst_row.0.lcssa = phi double [ 0.000000e+00, %entry ], [ %31, %afterloop.75 ]
  ret double %dst_row.0.lcssa

loop.73:                                          ; preds = %afterloop.75, %for.body.preheader
  %i1.i64.0 = phi i64 [ 0, %for.body.preheader ], [ %nextivloop.73, %afterloop.75 ]
  %t3.0 = phi double [ 0.000000e+00, %for.body.preheader ], [ %31, %afterloop.75 ]
  %2 = trunc i64 %i1.i64.0 to i32
  %3 = xor i32 %2, -1
  %4 = add i32 %3, %N
  %5 = zext i32 %4 to i64
  %6 = getelementptr inbounds i32*, i32** %rowstart, i64 %5
  %gepload = load i32*, i32** %6, align 8, !tbaa !3
  br label %loop.90

loop.90:                                          ; preds = %loop.90, %loop.73
  %i2.i32.0 = phi i32 [ 0, %loop.73 ], [ %nextivloop.90, %loop.90 ]
  %t63.0 = phi <16 x double> [ zeroinitializer, %loop.73 ], [ %19, %loop.90 ]
  %7 = or i32 %i2.i32.0, 1
  %8 = zext i32 %7 to i64
  %9 = getelementptr inbounds i32, i32* %gepload, i64 %8
  %10 = bitcast i32* %9 to <16 x i32>*
  %gepload100 = load <16 x i32>, <16 x i32>* %10, align 4, !tbaa !7
  %11 = zext <16 x i32> %gepload100 to <16 x i64>
  %12 = getelementptr inbounds double, double* %dst, <16 x i64> %11
  %13 = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> %12, i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef), !tbaa !9
  %14 = shl i64 %i1.i64.0, 12
  %15 = add i64 %14, %8
  %16 = getelementptr inbounds double, double* %luval, i64 %15
  %17 = bitcast double* %16 to <16 x double>*
  %gepload101 = load <16 x double>, <16 x double>* %17, align 8, !tbaa !9
  %18 = fmul fast <16 x double> %13, %gepload101
  %19 = fadd fast <16 x double> %18, %t63.0
  %nextivloop.90 = add nuw nsw i32 %i2.i32.0, 16
  %condloop.90 = icmp ult i32 %i2.i32.0, 4080
  br i1 %condloop.90, label %loop.90, label %afterloop.90, !llvm.loop !11

afterloop.90:                                     ; preds = %loop.90
  %20 = call fast double @llvm.vector.reduce.fadd.v16f64(double %t3.0, <16 x double> %19)
  %21 = getelementptr inbounds double**, double*** %src1, i64 %5
  %gepload103 = load double**, double*** %21, align 8, !tbaa !16
  %22 = getelementptr inbounds double**, double*** %src2, i64 %5
  %gepload104 = load double**, double*** %22, align 8, !tbaa !16
  br label %loop.75

loop.75:                                          ; preds = %afterloop.107, %afterloop.90
  %i2.i64.0 = phi i64 [ 0, %afterloop.90 ], [ %nextivloop.75, %afterloop.107 ]
  %t3.1 = phi double [ %20, %afterloop.90 ], [ %31, %afterloop.107 ]
  %23 = getelementptr inbounds double*, double** %gepload103, i64 %i2.i64.0
  %gepload105 = load double*, double** %23, align 8, !tbaa !18
  %24 = getelementptr inbounds double*, double** %gepload104, i64 %i2.i64.0
  %gepload106 = load double*, double** %24, align 8, !tbaa !18
  br label %loop.107

loop.107:                                         ; preds = %loop.107, %loop.75
  %t75.0 = phi <4 x double> [ zeroinitializer, %loop.75 ], [ %30, %loop.107 ]
  %i3.i64.0 = phi i64 [ 0, %loop.75 ], [ %nextivloop.107, %loop.107 ]
  %25 = getelementptr inbounds double, double* %gepload105, i64 %i3.i64.0
  %26 = bitcast double* %25 to <4 x double>*
  %gepload107 = load <4 x double>, <4 x double>* %26, align 8, !tbaa !9
  %27 = fadd fast <4 x double> %gepload107, %t75.0
  %28 = getelementptr inbounds double, double* %gepload106, i64 %i3.i64.0
  %29 = bitcast double* %28 to <4 x double>*
  %gepload108 = load <4 x double>, <4 x double>* %29, align 8, !tbaa !9
  %30 = fadd fast <4 x double> %27, %gepload108
  %nextivloop.107 = add nuw nsw i64 %i3.i64.0, 4
  %condloop.107 = icmp ult i64 %i3.i64.0, 4092
  br i1 %condloop.107, label %loop.107, label %afterloop.107, !llvm.loop !20

afterloop.107:                                    ; preds = %loop.107
  %31 = call fast double @llvm.vector.reduce.fadd.v4f64(double %t3.1, <4 x double> %30)
  %nextivloop.75 = add nuw nsw i64 %i2.i64.0, 1
  %condloop.75.not = icmp eq i64 %i2.i64.0, 4095
  br i1 %condloop.75.not, label %afterloop.75, label %loop.75, !llvm.loop !21

afterloop.75:                                     ; preds = %afterloop.107
  %nextivloop.73 = add nuw nsw i64 %i1.i64.0, 1
  %condloop.73.not = icmp eq i64 %i1.i64.0, %1
  br i1 %condloop.73.not, label %for.cond.cleanup, label %loop.73, !llvm.loop !22
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare double @llvm.vector.reduce.fadd.v16f64(double, <16 x double>)

; Function Attrs: nofree nosync nounwind readnone willreturn
declare double @llvm.vector.reduce.fadd.v4f64(double, <4 x double>)

; Function Attrs: nofree nosync nounwind readonly willreturn
declare <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*>, i32 immarg, <16 x i1>, <16 x double>)

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.x.0.YYYYMMDD)"}
!3 = !{!4, !4, i64 0}
!4 = !{!"pointer@_ZTSPj", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C/C++ TBAA"}
!7 = !{!8, !8, i64 0}
!8 = !{!"int", !5, i64 0}
!9 = !{!10, !10, i64 0}
!10 = !{!"double", !5, i64 0}
!11 = distinct !{!11, !12, !13, !14, !15}
!12 = !{!"llvm.loop.mustprogress"}
!13 = !{!"llvm.loop.unroll.disable"}
!14 = !{!"llvm.loop.vectorize.width", i32 1}
!15 = !{!"llvm.loop.interleave.count", i32 1}
!16 = !{!17, !17, i64 0}
!17 = !{!"pointer@_ZTSPPd", !5, i64 0}
!18 = !{!19, !19, i64 0}
!19 = !{!"pointer@_ZTSPd", !5, i64 0}
!20 = distinct !{!20, !12, !13, !14, !15}
!21 = distinct !{!21, !12, !13}
!22 = distinct !{!22, !12, !13}
