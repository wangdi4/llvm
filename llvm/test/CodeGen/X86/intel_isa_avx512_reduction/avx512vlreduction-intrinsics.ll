; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_reduction
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512reduction,+avx512bw | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512reduction,+avx512bw | FileCheck %s --check-prefixes=X86

define <4 x i32> @test_int_x86_avx512reduction_vphraddbd128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddbd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddbd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddbd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddbd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddbd128(<16 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddbd128(<16 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddbd128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddbd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddbd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddbd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddbd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd128(<16 x i8> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd128(<16 x i8> %A, i16 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddbd256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddbd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddbd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddbd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddbd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddbd256(<32 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddbd256(<32 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddbd256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddbd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddbd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddbd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddbd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd256(<32 x i8> %A, i32 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddq128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddq128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddq256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddq256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq256(<4 x i64> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddwd128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddwd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddwd %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddwd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddwd %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddwd128(<8 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddwd128(<8 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddwd128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddwd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddwd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x43,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddwd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddwd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x43,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd128(<8 x i16> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd128(<8 x i16> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddwd256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddwd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddwd %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddwd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddwd %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddwd256(<16 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddwd256(<16 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddwd256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddwd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddwd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddwd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddwd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd256(<16 x i16> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd256(<16 x i16> %A, i16 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsbd128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsbd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsbd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsbd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsbd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsbd128(<16 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsbd128(<16 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsbd128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsbd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddsbd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd128(<16 x i8> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd128(<16 x i8> %A, i16 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsbd256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsbd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsbd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsbd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsbd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsbd256(<32 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsbd256(<32 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsbd256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsbd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddsbd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd256(<32 x i8> %A, i32 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddsq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsq128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsq128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddsq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddsq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddsq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddsq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsq256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsq256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddsq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddsq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddsq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq256(<4 x i64> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddswd128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddswd128:
; X64:       # %bb.0:
; X64-NEXT:    vphraddswd %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddswd128:
; X86:       # %bb.0:
; X86-NEXT:    vphraddswd %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddswd128(<8 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddswd128(<8 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddswd128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddswd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddswd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x44,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddswd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddswd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x44,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd128(<8 x i16> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd128(<8 x i16> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddswd256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddswd256:
; X64:       # %bb.0:
; X64-NEXT:    vphraddswd %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddswd256:
; X86:       # %bb.0:
; X86-NEXT:    vphraddswd %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddswd256(<16 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddswd256(<16 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddswd256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddswd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddswd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddswd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddswd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd256(<16 x i16> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrandb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrandb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrandb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrandb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrandb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrandb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrandb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrandb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrandb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrandb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrandb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrandb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrandb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrandb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrandd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandd128:
; X64:       # %bb.0:
; X64-NEXT:    vphrandd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandd128:
; X86:       # %bb.0:
; X86-NEXT:    vphrandd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrandd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrandd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrandd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrandd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandd256:
; X64:       # %bb.0:
; X64-NEXT:    vphrandd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandd256:
; X86:       # %bb.0:
; X86-NEXT:    vphrandd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrandd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrandd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrandd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphranddq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphranddq256:
; X64:       # %bb.0:
; X64-NEXT:    vphranddq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x45,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphranddq256:
; X86:       # %bb.0:
; X86-NEXT:    vphranddq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x45,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphranddq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphranddq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrandq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrandq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrandq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrandq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrandq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrandq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrandq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrandq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrandq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrandq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrandq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrandq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrandw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrandw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrandw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrandw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrandw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrandw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4d,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4d,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrandw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrandw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrandw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrandw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrandw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrandw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrandw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxd128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxd128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxd256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxd256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4a,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4a,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxsb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxsb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxsb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxsb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxsb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxsb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxsd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsd128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsd128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxsd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxsd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsd256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsd256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxsd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxsq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxsq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxsq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxsq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxsw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxsw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4b,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4b,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxsw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxsw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxsw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmind128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmind128:
; X64:       # %bb.0:
; X64-NEXT:    vphrmind %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmind128:
; X86:       # %bb.0:
; X86-NEXT:    vphrmind %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmind128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmind128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmind128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmind128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmind %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmind128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmind %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmind256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmind256:
; X64:       # %bb.0:
; X64-NEXT:    vphrmind %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmind256:
; X86:       # %bb.0:
; X86-NEXT:    vphrmind %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmind256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmind256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmind256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmind256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmind %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmind256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmind %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x48,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x48,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminsb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminsb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminsb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminsb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminsb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminsb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminsb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminsb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminsb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminsb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrminsd128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsd128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsd128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsd %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrminsd128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrminsd128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrminsd128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsd128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsd %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrminsd256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsd256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsd256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsd %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrminsd256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrminsd256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrminsd256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsd256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsd %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminsq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminsq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminsq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminsq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminsq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminsq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminsq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminsq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminsw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminsw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminsw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminsw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x49,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x49,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminsw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminsw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminsw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminsw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminsw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrorb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrorb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrorb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrorb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrorb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrorb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrorb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrorb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrorb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrorb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrorb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrorb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrorb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrorb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrord128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrord128:
; X64:       # %bb.0:
; X64-NEXT:    vphrord %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrord128:
; X86:       # %bb.0:
; X86-NEXT:    vphrord %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrord128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrord128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrord128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrord128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrord %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrord128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrord %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrord128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrord128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrord256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrord256:
; X64:       # %bb.0:
; X64-NEXT:    vphrord %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrord256:
; X86:       # %bb.0:
; X86-NEXT:    vphrord %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrord256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrord256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrord256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrord256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrord %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrord256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrord %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrord256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrord256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrordq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrordq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrordq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x46,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrordq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrordq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x46,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrordq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrordq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrorq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrorq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrorq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrorq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrorq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrorq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrorq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrorq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrorq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrorq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrorq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrorq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrorq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrorq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrorw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrorw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrorw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrorw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrorw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrorw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4e,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrorw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4e,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrorw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrorw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrorw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrorw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrorw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrorw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrorw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw256(<16 x i16> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrxorb128(<16 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorb128:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorb128:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorb %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x08,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrxorb128(<16 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrxorb128(<16 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrxorb128(<16 x i8> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorb128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorb128:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrxorb %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x09,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb128(<16 x i8> %A, i16 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb128(<16 x i8> %A, i16 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrxorb256(<32 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorb256:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorb256:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorb %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x28,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrxorb256(<32 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrxorb256(<32 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrxorb256(<32 x i8> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorb256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorb256:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrxorb %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x29,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb256(<32 x i8> %A, i32 %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb256(<32 x i8> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrxord128(<4 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxord128:
; X64:       # %bb.0:
; X64-NEXT:    vphrxord %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxord128:
; X86:       # %bb.0:
; X86-NEXT:    vphrxord %xmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x08,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrxord128(<4 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrxord128(<4 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrxord128(<4 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxord128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxord %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxord128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxord %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x09,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord128(<4 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord128(<4 x i32> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrxord256(<8 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxord256:
; X64:       # %bb.0:
; X64-NEXT:    vphrxord %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxord256:
; X86:       # %bb.0:
; X86-NEXT:    vphrxord %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrxord256(<8 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrxord256(<8 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrxord256(<8 x i32> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxord256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxord %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxord256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxord %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x29,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord256(<8 x i32> %A, i8 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord256(<8 x i32> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrxordq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxordq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrxordq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x47,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxordq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrxordq %ymm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x28,0x47,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrxordq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrxordq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrxorq128(<2 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorq128:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorq128:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorq %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x08,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrxorq128(<2 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrxorq128(<2 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrxorq128(<2 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorq128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorq128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxorq %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x09,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq128(<2 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq128(<2 x i64> %A, i8 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrxorq256(<4 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorq256:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorq256:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorq %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x28,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrxorq256(<4 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrxorq256(<4 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrxorq256(<4 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorq256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorq256:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxorq %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x29,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq256(<4 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq256(<4 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrxorw128(<8 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorw128:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorw128:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorw %xmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x08,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrxorw128(<8 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrxorw128(<8 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrxorw128(<8 x i16> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorw128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4f,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorw128:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxorw %xmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x09,0x4f,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw128(<8 x i16> %A, i8 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw128(<8 x i16> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrxorw256(<16 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorw256:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorw256:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorw %ymm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x28,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrxorw256(<16 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrxorw256(<16 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrxorw256(<16 x i16> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorw256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorw256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrxorw %ymm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x29,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw256(<16 x i16> %A, i16 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw256(<16 x i16> %A, i16 %B)

