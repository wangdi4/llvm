; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_reduction
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512reduction,+avx512bw,+avx512vl | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512reduction,+avx512bw,+avx512vl | FileCheck %s --check-prefixes=X86

define <4 x i32> @test_int_x86_avx512reduction_vphraddbd512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddbd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddbd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddbd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddbd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddbd512(<64 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddbd512(<64 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddbd512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddbd512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphraddbd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddbd512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphraddbd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd512(<64 x i8> %A, <64 x i1> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddbd512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddq512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddq512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddq512(<8 x i64> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddwd512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddwd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddwd %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddwd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddwd %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddwd512(<32 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddwd512(<32 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddwd512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddwd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddwd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x43,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddwd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddwd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x43,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd512(<32 x i16> %A, i32 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddwd512(<32 x i16> %A, i32 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsbd512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsbd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsbd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsbd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsbd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsbd512(<64 x i8> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsbd512(<64 x i8> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsbd512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphraddsbd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsbd512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphraddsbd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd512(<64 x i8> %A, <64 x i1> %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsbd512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddsd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddsd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddsd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddsd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddsd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphraddsq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddsq512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddsq512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphraddsq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphraddsq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphraddsq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddsq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddsq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphraddsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphraddsq512(<8 x i64> %A, i8 %B)

define <4 x i32> @test_int_x86_avx512reduction_vphraddswd512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphraddswd512:
; X64:       # %bb.0:
; X64-NEXT:    vphraddswd %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphraddswd512:
; X86:       # %bb.0:
; X86-NEXT:    vphraddswd %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphraddswd512(<32 x i16> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphraddswd512(<32 x i16> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphraddswd512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphraddswd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphraddswd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x44,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphraddswd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphraddswd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x44,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd512(<32 x i16> %A, i32 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphraddswd512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrandb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrandb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrandb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrandb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrandb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrandb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrandb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrandb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrandb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrandd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandd512:
; X64:       # %bb.0:
; X64-NEXT:    vphrandd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandd512:
; X86:       # %bb.0:
; X86-NEXT:    vphrandd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrandd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrandd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrandd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrandd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrandd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphranddq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphranddq512:
; X64:       # %bb.0:
; X64-NEXT:    vphranddq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x45,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphranddq512:
; X86:       # %bb.0:
; X86-NEXT:    vphranddq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x45,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphranddq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphranddq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrandq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrandq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrandq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrandq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrandq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrandq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrandq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrandq512(<8 x i64> %A, i8 %B)

define <4 x i64> @test_int_x86_avx512reduction_vphrandqq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandqq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrandqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x45,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandqq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrandqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x45,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i64> @llvm.x86.avx512reduction.vphrandqq512(<8 x i64> %A)
  ret <4 x i64> %ret
}

declare <4 x i64> @llvm.x86.avx512reduction.vphrandqq512(<8 x i64> %A)

define <8 x i16> @test_int_x86_avx512reduction_vphrandw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrandw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrandw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrandw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrandw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrandw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrandw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrandw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrandw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrandw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4d,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrandw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrandw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4d,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrandw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrmaxb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrmaxb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxd512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxd512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxq512(<8 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4a,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4a,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrmaxsb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrmaxsb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrmaxsb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrmaxsb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrmaxsb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrmaxsb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmaxsd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsd512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsd512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmaxsd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmaxsd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmaxsd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrmaxsq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrmaxsq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrmaxsq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrmaxsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrmaxsq512(<8 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrmaxsw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmaxsw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmaxsw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmaxsw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmaxsw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrmaxsw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrmaxsw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmaxsw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4b,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmaxsw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmaxsw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4b,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrmaxsw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrminb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrminb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrmind512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrmind512:
; X64:       # %bb.0:
; X64-NEXT:    vphrmind %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrmind512:
; X86:       # %bb.0:
; X86-NEXT:    vphrmind %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrmind512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrmind512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrmind512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrmind512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrmind %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrmind512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrmind %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrmind512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminq512(<8 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x48,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x48,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrminsb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrminsb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrminsb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrminsb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrminsb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrminsb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrminsb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrminsd512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsd512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsd512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsd %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrminsd512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrminsd512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrminsd512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsd512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminsd %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrminsd512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrminsq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrminsq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrminsq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrminsq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrminsq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrminsq512(<8 x i64> %A, i8 %B)

define <8 x i16> @test_int_x86_avx512reduction_vphrminsw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrminsw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrminsw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrminsw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrminsw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrminsw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrminsw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrminsw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrminsw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrminsw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x49,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrminsw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrminsw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x49,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrminsw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrorb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrorb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrorb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrorb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrorb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrorb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrorb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrorb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrorb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrord512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrord512:
; X64:       # %bb.0:
; X64-NEXT:    vphrord %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrord512:
; X86:       # %bb.0:
; X86-NEXT:    vphrord %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrord512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrord512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrord512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrord512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrord %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrord512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrord %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrord512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrord512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrordq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrordq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrordq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x46,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrordq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrordq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x46,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrordq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrordq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrorq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrorq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrorq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrorq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrorq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrorq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrorq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrorq512(<8 x i64> %A, i8 %B)

define <4 x i64> @test_int_x86_avx512reduction_vphrorqq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorqq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrorqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x46,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorqq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrorqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x46,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i64> @llvm.x86.avx512reduction.vphrorqq512(<8 x i64> %A)
  ret <4 x i64> %ret
}

declare <4 x i64> @llvm.x86.avx512reduction.vphrorqq512(<8 x i64> %A)

define <8 x i16> @test_int_x86_avx512reduction_vphrorw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrorw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrorw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrorw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrorw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrorw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrorw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrorw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrorw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrorw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4e,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrorw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrorw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4e,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrorw512(<32 x i16> %A, i32 %B)

define <16 x i8> @test_int_x86_avx512reduction_vphrxorb512(<64 x i8> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorb512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorb512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorb %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7c,0x48,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.vphrxorb512(<64 x i8> %A)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.vphrxorb512(<64 x i8> %A)

define <16 x i8> @test_int_x86_avx512reduction_mask_vphrxorb512(<64 x i8> %A, <64 x i1> %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorb512:
; X64:       # %bb.0:
; X64-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X64-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X64-NEXT:    vphrxorb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorb512:
; X86:       # %bb.0:
; X86-NEXT:    vpsllw $7, %zmm1, %zmm1 # encoding: [0x62,0xf1,0x75,0x48,0x71,0xf1,0x07]
; X86-NEXT:    vpmovb2m %zmm1, %k1 # encoding: [0x62,0xf2,0x7e,0x48,0x29,0xc9]
; X86-NEXT:    vphrxorb %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7c,0x49,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb512(<64 x i8> %A, <64 x i1> %B)
  ret <16 x i8> %ret
}

declare <16 x i8> @llvm.x86.avx512reduction.mask.vphrxorb512(<64 x i8> %A, <64 x i1> %B)

define <4 x i32> @test_int_x86_avx512reduction_vphrxord512(<16 x i32> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxord512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxord %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxord512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxord %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.vphrxord512(<16 x i32> %A)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.vphrxord512(<16 x i32> %A)

define <4 x i32> @test_int_x86_avx512reduction_mask_vphrxord512(<16 x i32> %A, i16 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxord512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxord %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxord512:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrxord %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0x7d,0x49,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord512(<16 x i32> %A, i16 %B)
  ret <4 x i32> %ret
}

declare <4 x i32> @llvm.x86.avx512reduction.mask.vphrxord512(<16 x i32> %A, i16 %B)

define <2 x i64> @test_int_x86_avx512reduction_vphrxordq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxordq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxordq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x47,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxordq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxordq %zmm0, %xmm0 # encoding: [0x62,0xf5,0x7d,0x48,0x47,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrxordq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrxordq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_vphrxorq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorq %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfd,0x48,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.vphrxorq512(<8 x i64> %A)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.vphrxorq512(<8 x i64> %A)

define <2 x i64> @test_int_x86_avx512reduction_mask_vphrxorq512(<8 x i64> %A, i8 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorq512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorq512:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vphrxorq %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfd,0x49,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq512(<8 x i64> %A, i8 %B)
  ret <2 x i64> %ret
}

declare <2 x i64> @llvm.x86.avx512reduction.mask.vphrxorq512(<8 x i64> %A, i8 %B)

define <4 x i64> @test_int_x86_avx512reduction_vphrxorqq512(<8 x i64> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorqq512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x47,0xc0]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorqq512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorqq %zmm0, %ymm0 # encoding: [0x62,0xf5,0xfd,0x48,0x47,0xc0]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x i64> @llvm.x86.avx512reduction.vphrxorqq512(<8 x i64> %A)
  ret <4 x i64> %ret
}

declare <4 x i64> @llvm.x86.avx512reduction.vphrxorqq512(<8 x i64> %A)

define <8 x i16> @test_int_x86_avx512reduction_vphrxorw512(<32 x i16> %A) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_vphrxorw512:
; X64:       # %bb.0:
; X64-NEXT:    vphrxorw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_vphrxorw512:
; X86:       # %bb.0:
; X86-NEXT:    vphrxorw %zmm0, %xmm0 # encoding: [0x62,0xf5,0xfc,0x48,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.vphrxorw512(<32 x i16> %A)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.vphrxorw512(<32 x i16> %A)

define <8 x i16> @test_int_x86_avx512reduction_mask_vphrxorw512(<32 x i16> %A, i32 %B) nounwind {
; X64-LABEL: test_int_x86_avx512reduction_mask_vphrxorw512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vphrxorw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4f,0xc0]
; X64-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512reduction_mask_vphrxorw512:
; X86:       # %bb.0:
; X86-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vphrxorw %zmm0, %xmm0 {%k1} # encoding: [0x62,0xf5,0xfc,0x49,0x4f,0xc0]
; X86-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw512(<32 x i16> %A, i32 %B)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512reduction.mask.vphrxorw512(<32 x i16> %A, i32 %B)

