; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
; REQUIRES: intel_feature_isa_avx512_reduction
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mattr=+avx512reduction,+avx512vl | FileCheck %s --check-prefixes=ALL,AVX512REDUCTION
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512vl | FileCheck %s --check-prefixes=ALL,AVX512F

define void @reduce_addq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_addq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphraddq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_addq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpaddq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpaddq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_addd_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_addd_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphraddd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_addd_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_addw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_addw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphraddwd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_addw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpaddw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpaddw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpaddw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpsrld $16, %xmm0, %xmm1
; AVX512F-NEXT:    vpaddw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.add.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.add.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.add.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.add.v16i16(<16 x i16>)

define void @reduce_andq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_andq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrandq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_andq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vandps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vandps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovlps %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.and.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_andd_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_andd_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrandd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_andd_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vandps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vandps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vandps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovss %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.and.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_andw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_andw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrandw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_andw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpand %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpand %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpand %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpsrld $16, %xmm0, %xmm1
; AVX512F-NEXT:    vpand %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.and.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.and.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.and.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.and.v16i16(<16 x i16>)

define void @reduce_orq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_orq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrorq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_orq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovlps %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.or.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_ord_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_ord_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrord %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_ord_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovss %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.or.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_orw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_orw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrorw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_orw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpsrld $16, %xmm0, %xmm1
; AVX512F-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.or.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.or.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.or.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.or.v16i16(<16 x i16>)

define void @reduce_xorq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_xorq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrxorq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_xorq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vxorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vxorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovlps %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.xor.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_xord_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_xord_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrxord %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_xord_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextractf128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vxorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vxorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vxorps %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovss %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.xor.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_xorw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_xorw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrxorw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_xorw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpsrld $16, %xmm0, %xmm1
; AVX512F-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.xor.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.xor.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.xor.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.xor.v16i16(<16 x i16>)

define void @reduce_smaxq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_smaxq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxsq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_smaxq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpmaxsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.smax.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_smaxd_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_smaxd_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxsd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_smaxd_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.smax.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_smaxw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_smaxw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxsw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_smaxw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxsw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpxord {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to4}, %xmm0, %xmm0
; AVX512F-NEXT:    vphminposuw %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    xorl $32767, %eax # imm = 0x7FFF
; AVX512F-NEXT:    movw %ax, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.smax.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.smax.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.smax.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.smax.v16i16(<16 x i16>)

define void @reduce_sminq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_sminq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.smin.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_smind_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_smind_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_smind_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.smin.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_sminw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_sminw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpxord {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to4}, %xmm0, %xmm0
; AVX512F-NEXT:    vphminposuw %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    xorl $32768, %eax # imm = 0x8000
; AVX512F-NEXT:    movw %ax, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.smin.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.smin.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.smin.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.smin.v16i16(<16 x i16>)

define void @reduce_umaxq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_umaxq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_umaxq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxuq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpmaxuq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.umax.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_umaxd_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_umaxd_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_umaxd_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.umax.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_umaxw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_umaxw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmaxw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_umaxw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpmaxuw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpternlogq $15, %xmm0, %xmm0, %xmm0
; AVX512F-NEXT:    vphminposuw %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    notl %eax
; AVX512F-NEXT:    movw %ax, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.umax.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.umax.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.umax.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.umax.v16i16(<16 x i16>)

define void @reduce_uminq_4xi64_mem(<4 x i64> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_uminq_4xi64_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_uminq_4xi64_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminuq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminuq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i64 @llvm.vector.reduce.umin.v4i64(<4 x i64> %in)
  store i64 %1, ptr %out
  ret void
}

define void @reduce_umind_8xi32_mem(<8 x i32> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_umind_8xi32_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrmind %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_umind_8xi32_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i32 @llvm.vector.reduce.umin.v8i32(<8 x i32> %in)
  store i32 %1, ptr %out
  ret void
}

define void @reduce_uminw_16xi16_mem(<16 x i16> %in, ptr %out) {
; AVX512REDUCTION-LABEL: reduce_uminw_16xi16_mem:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminw %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovw %xmm0, (%rdi)
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_uminw_16xi16_mem:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminuw %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vphminposuw %xmm0, %xmm0
; AVX512F-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %1 = call i16 @llvm.vector.reduce.umin.v16i16(<16 x i16> %in)
  store i16 %1, ptr %out
  ret void
}

declare i64 @llvm.vector.reduce.umin.v4i64(<4 x i64>)
declare i32 @llvm.vector.reduce.umin.v8i32(<8 x i32>)
declare i16 @llvm.vector.reduce.umin.v16i16(<16 x i16>)
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; ALL: {{.*}}
