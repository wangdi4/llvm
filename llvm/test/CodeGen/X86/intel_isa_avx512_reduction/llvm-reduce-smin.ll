; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_reduction
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mattr=+avx512reduction,+avx512vl | FileCheck %s --check-prefixes=ALL,AVX512REDUCTION
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512vl | FileCheck %s --check-prefixes=ALL,AVX512F

define i64 @reduce_sminq_13xi64(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminq_13xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vmovdqa64 64(%rdi), %zmm0
; AVX512REDUCTION-NEXT:    vpbroadcastq {{.*#+}} zmm1 = [9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807]
; AVX512REDUCTION-NEXT:    movb $32, %al
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    movb $64, %al
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    movb $-128, %al
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    vpminsq (%rdi), %zmm0, %zmm0
; AVX512REDUCTION-NEXT:    vphrminsq %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_13xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqa64 64(%rdi), %zmm0
; AVX512F-NEXT:    vpbroadcastq {{.*#+}} zmm1 = [9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807]
; AVX512F-NEXT:    movb $32, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    movb $64, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    movb $-128, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa64 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    vpminsq (%rdi), %zmm0, %zmm0
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsq %ymm1, %ymm0, %ymm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512F-NEXT:    vmovq %xmm0, %rcx
; AVX512F-NEXT:    cmpq %rax, %rcx
; AVX512F-NEXT:    cmovlq %rcx, %rax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <13 x i64>, ptr %p
  %res = tail call i64 @llvm.vector.reduce.smin.v13i64(<13 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_8xi64(<8 x i64> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminq_8xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsq %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_8xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsq %zmm1, %zmm0, %zmm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, %rax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %res = tail call i64 @llvm.vector.reduce.smin.v8i64(<8 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_7xi64(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminq_7xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vmovdqa64 (%rdi), %zmm0
; AVX512REDUCTION-NEXT:    movb $-128, %al
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vpbroadcastq {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    vphrminsq %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_7xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqa64 (%rdi), %zmm0
; AVX512F-NEXT:    movb $-128, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vpbroadcastq {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %zmm0 {%k1}
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsq %ymm1, %ymm0, %ymm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512F-NEXT:    vmovq %xmm0, %rcx
; AVX512F-NEXT:    cmpq %rax, %rcx
; AVX512F-NEXT:    cmovlq %rcx, %rax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <7 x i64>, ptr %p
  %res = tail call i64 @llvm.vector.reduce.smin.v7i64(<7 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_4xi64(<4 x i64> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminq_4xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_4xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, %rax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %res = tail call i64 @llvm.vector.reduce.smin.v4i64(<4 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_3xi64(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminq_3xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vbroadcastsd {{.*#+}} ymm0 = [9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807]
; AVX512REDUCTION-NEXT:    vblendps {{.*#+}} ymm0 = mem[0,1,2,3,4,5],ymm0[6,7]
; AVX512REDUCTION-NEXT:    vphrminsq %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_3xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vpbroadcastq {{.*#+}} ymm0 = [9223372036854775807,9223372036854775807,9223372036854775807,9223372036854775807]
; AVX512F-NEXT:    vpblendd {{.*#+}} ymm0 = mem[0,1,2,3,4,5],ymm0[6,7]
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512F-NEXT:    vmovq %xmm0, %rcx
; AVX512F-NEXT:    cmpq %rax, %rcx
; AVX512F-NEXT:    cmovlq %rcx, %rax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <3 x i64>, ptr %p
  %res = tail call i64 @llvm.vector.reduce.smin.v3i64(<3 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_2xi64(<2 x i64> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminq_2xi64:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsq %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovq %xmm0, %rax
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminq_2xi64:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsq %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovq %xmm0, %rax
; AVX512F-NEXT:    retq
  %res = tail call i64 @llvm.vector.reduce.smin.v2i64(<2 x i64> %vec)
  ret i64 %res
}

define i64 @reduce_sminq_1xi64(<1 x i64> %vec) {
; ALL-LABEL: reduce_sminq_1xi64:
; ALL:       # %bb.0:
; ALL-NEXT:    movq %rdi, %rax
; ALL-NEXT:    retq
  %res = tail call i64 @llvm.vector.reduce.smin.v1i64(<1 x i64> %vec)
  ret i64 %res
}

declare i64 @llvm.vector.reduce.smin.v13i64(<13 x i64>)
declare i64 @llvm.vector.reduce.smin.v8i64(<8 x i64>)
declare i64 @llvm.vector.reduce.smin.v7i64(<7 x i64>)
declare i64 @llvm.vector.reduce.smin.v4i64(<4 x i64>)
declare i64 @llvm.vector.reduce.smin.v3i64(<3 x i64>)
declare i64 @llvm.vector.reduce.smin.v2i64(<2 x i64>)
declare i64 @llvm.vector.reduce.smin.v1i64(<1 x i64>)

define i32 @reduce_sminw_17xi32(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminw_17xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vmovdqa {{.*#+}} xmm0 = <u,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647>
; AVX512REDUCTION-NEXT:    vpinsrd $0, 64(%rdi), %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovdqa64 {{.*#+}} zmm1 = <u,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647>
; AVX512REDUCTION-NEXT:    vinserti32x4 $0, %xmm0, %zmm1, %zmm0
; AVX512REDUCTION-NEXT:    vpminsd (%rdi), %zmm0, %zmm0
; AVX512REDUCTION-NEXT:    vphrminsd %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_17xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqa {{.*#+}} xmm0 = <u,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647>
; AVX512F-NEXT:    vpinsrd $0, 64(%rdi), %xmm0, %xmm0
; AVX512F-NEXT:    vmovdqa64 {{.*#+}} zmm1 = <u,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647>
; AVX512F-NEXT:    vinserti32x4 $0, %xmm0, %zmm1, %zmm0
; AVX512F-NEXT:    vpminsd (%rdi), %zmm0, %zmm0
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsd %ymm1, %ymm0, %ymm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512F-NEXT:    vmovd %xmm0, %ecx
; AVX512F-NEXT:    cmpl %eax, %ecx
; AVX512F-NEXT:    cmovll %ecx, %eax
; AVX512F-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vpextrd $3, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <17 x i32>, ptr %p
  %res = tail call i32 @llvm.vector.reduce.smin.v17i32(<17 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_16xi32(<16 x i32> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminw_16xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsd %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_16xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsd %zmm1, %zmm0, %zmm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %res = tail call i32 @llvm.vector.reduce.smin.v16i32(<16 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_13xi32(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminw_13xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vmovdqa64 (%rdi), %zmm0
; AVX512REDUCTION-NEXT:    vpbroadcastd {{.*#+}} zmm1 = [2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647]
; AVX512REDUCTION-NEXT:    movw $8192, %ax # imm = 0x2000
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    movw $16384, %ax # imm = 0x4000
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    movw $-32768, %ax # imm = 0x8000
; AVX512REDUCTION-NEXT:    kmovw %eax, %k1
; AVX512REDUCTION-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512REDUCTION-NEXT:    vphrminsd %zmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_13xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqa64 (%rdi), %zmm0
; AVX512F-NEXT:    vpbroadcastd {{.*#+}} zmm1 = [2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647]
; AVX512F-NEXT:    movw $8192, %ax # imm = 0x2000
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    movw $16384, %ax # imm = 0x4000
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    movw $-32768, %ax # imm = 0x8000
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqa32 %zmm1, %zmm0 {%k1}
; AVX512F-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512F-NEXT:    vpminsd %ymm1, %ymm0, %ymm0
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512F-NEXT:    vmovd %xmm0, %ecx
; AVX512F-NEXT:    cmpl %eax, %ecx
; AVX512F-NEXT:    cmovll %ecx, %eax
; AVX512F-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vpextrd $3, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <13 x i32>, ptr %p
  %res = tail call i32 @llvm.vector.reduce.smin.v13i32(<13 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_8xi32(<8 x i32> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminw_8xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_8xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %res = tail call i32 @llvm.vector.reduce.smin.v8i32(<8 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_7xi32(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminw_7xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vbroadcastss {{.*#+}} ymm0 = [2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647]
; AVX512REDUCTION-NEXT:    vblendps {{.*#+}} ymm0 = mem[0,1,2,3,4,5,6],ymm0[7]
; AVX512REDUCTION-NEXT:    vphrminsd %ymm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    vzeroupper
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_7xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vpbroadcastd {{.*#+}} ymm0 = [2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647,2147483647]
; AVX512F-NEXT:    vpblendd {{.*#+}} ymm0 = mem[0,1,2,3,4,5,6],ymm0[7]
; AVX512F-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512F-NEXT:    vmovd %xmm0, %ecx
; AVX512F-NEXT:    cmpl %eax, %ecx
; AVX512F-NEXT:    cmovll %ecx, %eax
; AVX512F-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vpextrd $3, %xmm0, %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovgel %ecx, %eax
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
  %vec= load <7 x i32>, ptr %p
  %res = tail call i32 @llvm.vector.reduce.smin.v7i32(<7 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_4xi32(<4 x i32> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminw_4xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vphrminsd %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_4xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    retq
  %res = tail call i32 @llvm.vector.reduce.smin.v4i32(<4 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_3xi32(ptr %p) {
; AVX512REDUCTION-LABEL: reduce_sminw_3xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    vmovdqa (%rdi), %xmm0
; AVX512REDUCTION-NEXT:    movl $2147483647, %eax # imm = 0x7FFFFFFF
; AVX512REDUCTION-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vphrminsd %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_3xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    movl (%rdi), %eax
; AVX512F-NEXT:    movl 4(%rdi), %ecx
; AVX512F-NEXT:    cmpl %ecx, %eax
; AVX512F-NEXT:    cmovll %eax, %ecx
; AVX512F-NEXT:    movl 8(%rdi), %eax
; AVX512F-NEXT:    cmpl %eax, %ecx
; AVX512F-NEXT:    cmovgel %eax, %ecx
; AVX512F-NEXT:    cmpl $2147483647, %ecx # imm = 0x7FFFFFFF
; AVX512F-NEXT:    movl $2147483647, %eax # imm = 0x7FFFFFFF
; AVX512F-NEXT:    cmovll %ecx, %eax
; AVX512F-NEXT:    retq
  %vec= load <3 x i32>, ptr %p
  %res = tail call i32 @llvm.vector.reduce.smin.v3i32(<3 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_2xi32(<2 x i32> %vec) {
; AVX512REDUCTION-LABEL: reduce_sminw_2xi32:
; AVX512REDUCTION:       # %bb.0:
; AVX512REDUCTION-NEXT:    movl $2147483647, %eax # imm = 0x7FFFFFFF
; AVX512REDUCTION-NEXT:    vpinsrd $2, %eax, %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vphrminsd %xmm0, %xmm0
; AVX512REDUCTION-NEXT:    vmovd %xmm0, %eax
; AVX512REDUCTION-NEXT:    retq
;
; AVX512F-LABEL: reduce_sminw_2xi32:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512F-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; AVX512F-NEXT:    vmovd %xmm0, %eax
; AVX512F-NEXT:    retq
  %res = tail call i32 @llvm.vector.reduce.smin.v2i32(<2 x i32> %vec)
  ret i32 %res
}

define i32 @reduce_sminw_1xi32(<1 x i32> %vec) {
; ALL-LABEL: reduce_sminw_1xi32:
; ALL:       # %bb.0:
; ALL-NEXT:    movl %edi, %eax
; ALL-NEXT:    retq
  %res = tail call i32 @llvm.vector.reduce.smin.v1i32(<1 x i32> %vec)
  ret i32 %res
}

declare i32 @llvm.vector.reduce.smin.v17i32(<17 x i32>)
declare i32 @llvm.vector.reduce.smin.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.smin.v13i32(<13 x i32>)
declare i32 @llvm.vector.reduce.smin.v8i32(<8 x i32>)
declare i32 @llvm.vector.reduce.smin.v7i32(<7 x i32>)
declare i32 @llvm.vector.reduce.smin.v4i32(<4 x i32>)
declare i32 @llvm.vector.reduce.smin.v3i32(<3 x i32>)
declare i32 @llvm.vector.reduce.smin.v2i32(<2 x i32>)
declare i32 @llvm.vector.reduce.smin.v1i32(<1 x i32>)
