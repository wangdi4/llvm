; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512f -mattr=+avx512bw -enable-intel-advanced-opts -O3 -vec-spill-threshold=8 | FileCheck %s
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@g4 = dso_local local_unnamed_addr global i64 0, align 8
@g7 = dso_local local_unnamed_addr global i8 0, align 1
@g8 = dso_local local_unnamed_addr global i16 0, align 2
@g9 = dso_local local_unnamed_addr global i8 0, align 1

; Function Attrs: nounwind uwtable
define dso_local float @foo(float %f, i32 %n) local_unnamed_addr #0 {
; CHECK-LABEL: foo:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %r13
; CHECK-NEXT:    pushq %r12
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    subq $56, %rsp
; CHECK-NEXT:    cmpl $9, %edi
; CHECK-NEXT:    jg .LBB0_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    vaddss {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm1
; CHECK-NEXT:    vmovss %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    vcvtss2sd %xmm0, %xmm0, %xmm1
; CHECK-NEXT:    movl $1, %eax
; CHECK-NEXT:    vmulsd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm1
; CHECK-NEXT:    vmovsd %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    subl %edi, %eax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movslq %edi, %rax
; CHECK-NEXT:    leaq (%rax,%rax), %rcx
; CHECK-NEXT:    leaq (%rcx,%rcx,4), %rcx
; CHECK-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; CHECK-NEXT:    vsubss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    andb $30, %cl
; CHECK-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    imulq $1717986919, %rax, %rdx # imm = 0x66666667
; CHECK-NEXT:    movq %rdx, %rcx
; CHECK-NEXT:    shrq $63, %rcx
; CHECK-NEXT:    shrq $36, %rdx
; CHECK-NEXT:    addl %ecx, %edx
; CHECK-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    imulq $1374389535, %rax, %rax # imm = 0x51EB851F
; CHECK-NEXT:    movq %rax, %rcx
; CHECK-NEXT:    shrq $63, %rcx
; CHECK-NEXT:    shrq $36, %rax
; CHECK-NEXT:    addl %ecx, %eax
; CHECK-NEXT:    imull $50, %eax, %eax
; CHECK-NEXT:    subl %eax, %edi
; CHECK-NEXT:    movl %edi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    callq foo1
; CHECK-NEXT:    jmp .LBB0_5
; CHECK-NEXT:  .LBB0_2: # %if.else
; CHECK-NEXT:    cmpl $21, %edi
; CHECK-NEXT:    jl .LBB0_3
; CHECK-NEXT:  # %bb.4: # %if.then13
; CHECK-NEXT:    vcvtss2sd %xmm0, %xmm0, %xmm1
; CHECK-NEXT:    vaddss {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm2
; CHECK-NEXT:    vmovss %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $2, %eax
; CHECK-NEXT:    subl %edi, %eax
; CHECK-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmulsd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm1
; CHECK-NEXT:    vmovsd %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    leaq (,%rax,4), %rcx
; CHECK-NEXT:    leaq (%rcx,%rcx,4), %rcx
; CHECK-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    vmovss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; CHECK-NEXT:    vsubss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $2290649225, %ecx # imm = 0x88888889
; CHECK-NEXT:    imulq %rax, %rcx
; CHECK-NEXT:    shrq $36, %rcx
; CHECK-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    movl $3435973837, %ecx # imm = 0xCCCCCCCD
; CHECK-NEXT:    imulq %rax, %rcx
; CHECK-NEXT:    shrq $37, %rcx
; CHECK-NEXT:    shll $3, %ecx
; CHECK-NEXT:    leal (%rcx,%rcx,4), %eax
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    subl %eax, %ecx
; CHECK-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; CHECK-NEXT:    andb $50, %dil
; CHECK-NEXT:    movl %edi, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    callq foo2
; CHECK-NEXT:    jmp .LBB0_5
; CHECK-NEXT:  .LBB0_3:
; CHECK-NEXT:    vpxorq %xmm16, %xmm16, %xmm16
; CHECK-NEXT:    vmovdqu64 %ymm16, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    vmovdqa64 %xmm16, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    movl $0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; CHECK-NEXT:  .LBB0_5: # %if.end31
; CHECK-NEXT:    #APP
; CHECK-NEXT:    #NO_APP
; CHECK-NEXT:    callq foo3
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movq %rax, g4(%rip)
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movb %al, g7(%rip)
; CHECK-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rax # 8-byte Reload
; CHECK-NEXT:    movw %ax, g8(%rip)
; CHECK-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax # 4-byte Reload
; CHECK-NEXT:    movb %al, g9(%rip)
; CHECK-NEXT:    vcvtsi2ssl {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm0 # 4-byte Folded Reload
; CHECK-NEXT:    vmulss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; CHECK-NEXT:    vcvtss2sd %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmulsd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 8-byte Folded Reload
; CHECK-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; CHECK-NEXT:    # xmm1 = mem[0],zero,zero,zero
; CHECK-NEXT:    vcvtss2sd %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vmulsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vcvtsd2ss %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    addq $56, %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r12
; CHECK-NEXT:    popq %r13
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    retq
entry:
  %cmp = icmp slt i32 %n, 10
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %conv = fpext float %f to double
  %conv1 = fadd float %f, 1.000000e+00
  %sub = sub nsw i32 1, %n
  %mul = fmul double %conv, 1.000000e+01
  %conv3 = sext i32 %n to i64
  %mul4 = mul nsw i64 %conv3, 10
  %conv7 = fsub float 2.000000e+01, %f
  %0 = trunc i32 %n to i8
  %conv8 = and i8 %0, 30
  %div = sdiv i32 %n, 40
  %conv9 = trunc i32 %div to i16
  %rem = srem i32 %n, 50
  %conv10 = trunc i32 %rem to i8
  tail call void @foo1() #2
  br label %if.end31

if.else:                                          ; preds = %entry
  %cmp11 = icmp sgt i32 %n, 20
  br i1 %cmp11, label %if.then13, label %if.end31

if.then13:                                        ; preds = %if.else
  %conv14 = fpext float %f to double
  %conv16 = fadd float %f, 2.000000e+00
  %sub17 = sub nsw i32 2, %n
  %mul19 = fmul double %conv14, 2.000000e+01
  %conv2061 = zext i32 %n to i64
  %mul21 = mul nuw nsw i64 %conv2061, 20
  %conv24 = fsub float 3.000000e+01, %f
  %div2562 = udiv i32 %n, 30
  %conv26 = trunc i32 %div2562 to i8
  %rem2763 = urem i32 %n, 40
  %conv28 = trunc i32 %rem2763 to i16
  %1 = trunc i32 %n to i8
  %conv30 = and i8 %1, 50
  tail call void @foo2() #2
  br label %if.end31

if.end31:                                         ; preds = %if.else, %if.then13, %if.then
  %r1.0 = phi float [ %conv1, %if.then ], [ %conv16, %if.then13 ], [ 0.000000e+00, %if.else ]
  %r2.0 = phi i32 [ %sub, %if.then ], [ %sub17, %if.then13 ], [ 0, %if.else ]
  %r3.0 = phi double [ %mul, %if.then ], [ %mul19, %if.then13 ], [ 0.000000e+00, %if.else ]
  %r4.0 = phi i64 [ %mul4, %if.then ], [ %mul21, %if.then13 ], [ 0, %if.else ]
  %r5.0 = phi float [ %conv7, %if.then ], [ %conv24, %if.then13 ], [ 0.000000e+00, %if.else ]
  %r7.0 = phi i8 [ %conv8, %if.then ], [ %conv26, %if.then13 ], [ 0, %if.else ]
  %r8.0 = phi i16 [ %conv9, %if.then ], [ %conv28, %if.then13 ], [ 0, %if.else ]
  %r9.0 = phi i8 [ %conv10, %if.then ], [ %conv30, %if.then13 ], [ 0, %if.else ]
  tail call void asm sideeffect "", "~{rbp},~{rbx},~{r12},~{r13},~{r14},~{r15},~{dirflag},~{fpsr},~{flags}"() #2
  tail call void @foo3() #2
  store i64 %r4.0, i64* @g4, align 8
  store i8 %r7.0, i8* @g7, align 1
  store i16 %r8.0, i16* @g8, align 2
  store i8 %r9.0, i8* @g9, align 1
  %conv32 = sitofp i32 %r2.0 to float
  %mul33 = fmul float %r1.0, %conv32
  %conv34 = fpext float %mul33 to double
  %mul35 = fmul double %r3.0, %conv34
  %conv36 = fpext float %r5.0 to double
  %mul37 = fmul double %mul35, %conv36
  %conv38 = fptrunc double %mul37 to float
  ret float %conv38
}

declare dso_local void @foo1() local_unnamed_addr #1

declare dso_local void @foo2() local_unnamed_addr #1

declare dso_local void @foo3() local_unnamed_addr #1

attributes #0 = { nounwind "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { nounwind }
