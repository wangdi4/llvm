; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512vl,avx512bw,avx512dq,fast-variable-shuffle -enable-intel-advanced-opts | FileCheck %s

define <8 x i32> @x264_pixel_satd_8x4(i8* %pix1, i32 %i_pix1, i8* %pix2, i32 %i_pix2) {
; CHECK-LABEL: x264_pixel_satd_8x4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movslq %esi, %rax
; CHECK-NEXT:    movslq %ecx, %rcx
; CHECK-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; CHECK-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; CHECK-NEXT:    vpunpcklbw {{.*#+}} xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1],xmm0[2],xmm1[2],xmm0[3],xmm1[3],xmm0[4],xmm1[4],xmm0[5],xmm1[5],xmm0[6],xmm1[6],xmm0[7],xmm1[7]
; CHECK-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; CHECK-NEXT:    vmovq {{.*#+}} xmm2 = mem[0],zero
; CHECK-NEXT:    vpunpcklbw {{.*#+}} xmm1 = xmm1[0],xmm2[0],xmm1[1],xmm2[1],xmm1[2],xmm2[2],xmm1[3],xmm2[3],xmm1[4],xmm2[4],xmm1[5],xmm2[5],xmm1[6],xmm2[6],xmm1[7],xmm2[7]
; CHECK-NEXT:    vmovdqa {{.*#+}} xmm2 = [0,8,2,10,4,12,6,14,1,9,3,11,5,13,7,15]
; CHECK-NEXT:    vpshufb %xmm2, %xmm1, %xmm1
; CHECK-NEXT:    vpmovzxbw {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero,xmm1[8],zero,xmm1[9],zero,xmm1[10],zero,xmm1[11],zero,xmm1[12],zero,xmm1[13],zero,xmm1[14],zero,xmm1[15],zero
; CHECK-NEXT:    vpshufb %xmm2, %xmm0, %xmm0
; CHECK-NEXT:    vpmovzxbw {{.*#+}} ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero,xmm0[8],zero,xmm0[9],zero,xmm0[10],zero,xmm0[11],zero,xmm0[12],zero,xmm0[13],zero,xmm0[14],zero,xmm0[15],zero
; CHECK-NEXT:    vpsubd %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    retq
entry:
  %idx.ext = sext i32 %i_pix1 to i64
  %idx.ext63 = sext i32 %i_pix2 to i64
  %add.ptr = getelementptr inbounds i8, i8* %pix1, i64 %idx.ext
  %add.ptr64 = getelementptr inbounds i8, i8* %pix2, i64 %idx.ext63
  %0 = bitcast i8* %pix1 to <8 x i8>*
  %1 = load <8 x i8>, <8 x i8>* %0, align 1
  %LoadCoalescingShuffle_ = shufflevector <8 x i8> %1, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %2 = bitcast i8* %add.ptr to <8 x i8>*
  %3 = bitcast i8* %pix2 to <8 x i8>*
  %4 = load <8 x i8>, <8 x i8>* %2, align 1
  %5 = load <8 x i8>, <8 x i8>* %3, align 1
  %LoadCoalescingShuffle_164 = shufflevector <8 x i8> %4, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_166 = shufflevector <8 x i8> %5, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_165 = shufflevector <8 x i8> %4, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %LoadCoalescingShuffle_163 = shufflevector <8 x i8> %1, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle125 = shufflevector <4 x i8> %LoadCoalescingShuffle_, <4 x i8> %LoadCoalescingShuffle_164, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %6 = bitcast i8* %add.ptr64 to <8 x i8>*
  %7 = load <8 x i8>, <8 x i8>* %6, align 1
  %LoadCoalescingShuffle_168 = shufflevector <8 x i8> %7, <8 x i8> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_169 = shufflevector <8 x i8> %7, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle126 = shufflevector <4 x i8> %LoadCoalescingShuffle_166, <4 x i8> %LoadCoalescingShuffle_168, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %SplitLoadShuffle = shufflevector <4 x i8> %LoadCoalescingShuffle_163, <4 x i8> %LoadCoalescingShuffle_165, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %LoadCoalescingShuffle_167 = shufflevector <8 x i8> %5, <8 x i8> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %8 = zext <8 x i8> %SplitLoadShuffle125 to <8 x i32>
  %9 = zext <8 x i8> %SplitLoadShuffle126 to <8 x i32>
  %10 = zext <8 x i8> %SplitLoadShuffle to <8 x i32>
  %SplitLoadShuffle124 = shufflevector <4 x i8> %LoadCoalescingShuffle_167, <4 x i8> %LoadCoalescingShuffle_169, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %11 = zext <8 x i8> %SplitLoadShuffle124 to <8 x i32>
  %12 = sub nsw <8 x i32> %10, %11
  %13 = shl nsw <8 x i32> %12, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %14 = sub nsw <8 x i32> %8, %9
  %15 = add nsw <8 x i32> %13, %14
  ret <8 x i32> %15
}
