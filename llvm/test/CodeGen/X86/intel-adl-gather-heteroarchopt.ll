; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mcpu=alderlake -enable-intel-advanced-opts -O3 | FileCheck %s --check-prefix=X64
; RUN: llc < %s -mtriple=i386-unknown-unknown  -mcpu=alderlake -enable-intel-advanced-opts -O3 | FileCheck %s --check-prefix=X86

; Function Attrs: nofree norecurse nosync nounwind readonly uwtable
define dso_local double @foo(double* noalias nocapture readonly %dst, double* noalias nocapture readonly %luval, i32** nocapture readonly %rowstart, i32* nocapture readnone %first_after_diagonal, i32 %N) local_unnamed_addr #0 {
; X64-LABEL: foo:
<<<<<<< HEAD
; X64:       # %bb.0: # %entry.new
; X64-NEXT:    subq $40, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 48
=======
; X64:       # %bb.0: # %entry
; X64-NEXT:    pushq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    subq $48, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 64
; X64-NEXT:    .cfi_offset %rbx, -16
; X64-NEXT:    testl %r8d, %r8d
; X64-NEXT:    jle .LBB0_1
; X64-NEXT:  # %bb.2: # %for.body.preheader
; X64-NEXT:    movl %r8d, %r9d
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    rdpid %rax
; X64-NEXT:    movzbl %al, %eax
; X64-NEXT:    movq __cpu_core_type@GOTPCREL(%rip), %rcx
; X64-NEXT:    movb (%rcx,%rax), %al
; X64-NEXT:    testb %al, %al
<<<<<<< HEAD
; X64-NEXT:    je .LBB0_9
; X64-NEXT:  # %bb.1:
; X64-NEXT:    cmpb $64, %al
; X64-NEXT:    jne .LBB0_10
; X64-NEXT:  .LBB0_2: # %entry
; X64-NEXT:    testl %r8d, %r8d
; X64-NEXT:    jle .LBB0_8
; X64-NEXT:  # %bb.3: # %for.body.preheader
; X64-NEXT:    movl %r8d, %r9d
=======
; X64-NEXT:    je .LBB0_11
; X64-NEXT:  .LBB0_3:
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    decq %r9
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:    xorl %r10d, %r10d
; X64-NEXT:    cmpb $64, %al
; X64-NEXT:    jne .LBB0_7
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_4: # %loop.39
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_5 Depth 2
; X64-NEXT:    movl %r10d, %eax
; X64-NEXT:    notl %eax
; X64-NEXT:    addl %r8d, %eax
; X64-NEXT:    movq (%rdx,%rax,8), %r11
; X64-NEXT:    xorl %ecx, %ecx
; X64-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X64-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_5: # %loop.48
; X64-NEXT:    # Parent Loop BB0_4 Depth=1
; X64-NEXT:    # => This Inner Loop Header: Depth=2
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm7, %xmm7, %xmm7
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm7
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm8, %xmm8, %xmm8
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm8
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm9, %xmm9, %xmm9
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm9
; X64-NEXT:    vpmovzxdq {{.*#+}} ymm5 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X64-NEXT:    vpcmpeqd %ymm6, %ymm6, %ymm6
; X64-NEXT:    vxorpd %xmm10, %xmm10, %xmm10
; X64-NEXT:    vgatherqpd %ymm6, (%rdi,%ymm5,8), %ymm10
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm10 * mem) + ymm3
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm9 * mem) + ymm2
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm8 * mem) + ymm1
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm7 * mem) + ymm4
; X64-NEXT:    addq $16, %rcx
; X64-NEXT:    leal -16(%rcx), %eax
; X64-NEXT:    cmpl $4080, %eax # imm = 0xFF0
; X64-NEXT:    jb .LBB0_5
; X64-NEXT:  # %bb.6: # %afterloop.48
; X64-NEXT:    # in Loop: Header=BB0_4 Depth=1
; X64-NEXT:    vaddpd %ymm3, %ymm1, %ymm1
; X64-NEXT:    vaddpd %ymm4, %ymm2, %ymm2
; X64-NEXT:    vaddpd %ymm2, %ymm1, %ymm1
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vpermilpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    addq $32768, %rsi # imm = 0x8000
; X64-NEXT:    cmpq %r9, %r10
; X64-NEXT:    leaq 1(%r10), %r10
; X64-NEXT:    jne .LBB0_4
<<<<<<< HEAD
; X64-NEXT:  # %bb.7: # %for.cond.cleanup
; X64-NEXT:    addq $40, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
; X64-NEXT:  .LBB0_9:
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    movl %r8d, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    movq %rsi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    callq __detect_cpu_core_type_1n@PLT
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdi # 8-byte Reload
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; X64-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %r8d # 4-byte Reload
; X64-NEXT:    cmpb $64, %al
; X64-NEXT:    je .LBB0_2
; X64-NEXT:  .LBB0_10: # %entry.clone
; X64-NEXT:    testl %r8d, %r8d
; X64-NEXT:    jle .LBB0_8
; X64-NEXT:  # %bb.11: # %for.body.preheader.clone
; X64-NEXT:    movl %r8d, %r9d
; X64-NEXT:    decq %r9
; X64-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X64-NEXT:    xorl %r10d, %r10d
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_12: # %loop.39.clone
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_13 Depth 2
=======
; X64-NEXT:  .LBB0_10: # %for.cond.cleanup
; X64-NEXT:    addq $48, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    popq %rbx
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
; X64-NEXT:    .p2align 4, 0x90
; X64-NEXT:  .LBB0_7: # %loop.39.clone
; X64-NEXT:    # =>This Loop Header: Depth=1
; X64-NEXT:    # Child Loop BB0_8 Depth 2
; X64-NEXT:    .cfi_def_cfa_offset 64
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    movl %r10d, %eax
; X64-NEXT:    notl %eax
; X64-NEXT:    addl %r8d, %eax
; X64-NEXT:    movq (%rdx,%rax,8), %r11
; X64-NEXT:    vxorpd %xmm9, %xmm9, %xmm9
; X64-NEXT:    xorl %ecx, %ecx
; X64-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X64-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X64-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X64-NEXT:    .p2align 4, 0x90
<<<<<<< HEAD
; X64-NEXT:  .LBB0_13: # %loop.48.clone
; X64-NEXT:    # Parent Loop BB0_12 Depth=1
=======
; X64-NEXT:  .LBB0_8: # %loop.48.clone
; X64-NEXT:    # Parent Loop BB0_7 Depth=1
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    # => This Inner Loop Header: Depth=2
; X64-NEXT:    movl 8(%r11,%rcx,4), %eax
; X64-NEXT:    vmovsd {{.*#+}} xmm5 = mem[0],zero
; X64-NEXT:    movl 12(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm8 = xmm5[0],mem[0]
; X64-NEXT:    movl 24(%r11,%rcx,4), %eax
; X64-NEXT:    vmovsd {{.*#+}} xmm6 = mem[0],zero
; X64-NEXT:    movl 28(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm6 = xmm6[0],mem[0]
; X64-NEXT:    movl 16(%r11,%rcx,4), %eax
; X64-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X64-NEXT:    movl 20(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X64-NEXT:    movl 40(%r11,%rcx,4), %eax
; X64-NEXT:    vmovsd {{.*#+}} xmm5 = mem[0],zero
; X64-NEXT:    movl 44(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm5 = xmm5[0],mem[0]
; X64-NEXT:    movl 32(%r11,%rcx,4), %eax
; X64-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; X64-NEXT:    movl 36(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm1 = xmm1[0],mem[0]
; X64-NEXT:    movl 56(%r11,%rcx,4), %eax
; X64-NEXT:    vinsertf128 $1, %xmm6, %ymm7, %ymm6
; X64-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X64-NEXT:    movl 60(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X64-NEXT:    movl 48(%r11,%rcx,4), %eax
; X64-NEXT:    vinsertf128 $1, %xmm5, %ymm1, %ymm1
; X64-NEXT:    vmovsd {{.*#+}} xmm5 = mem[0],zero
; X64-NEXT:    movl 52(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm5 = xmm5[0],mem[0]
; X64-NEXT:    movl (%r11,%rcx,4), %eax
; X64-NEXT:    vinsertf128 $1, %xmm7, %ymm5, %ymm5
; X64-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X64-NEXT:    movl 4(%r11,%rcx,4), %eax
; X64-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm5 * mem) + ymm4
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm1 * mem) + ymm3
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm6 * mem) + ymm2
; X64-NEXT:    vinsertf128 $1, %xmm8, %ymm7, %ymm1
; X64-NEXT:    vfmadd231pd {{.*#+}} ymm9 = (ymm1 * mem) + ymm9
; X64-NEXT:    addq $16, %rcx
; X64-NEXT:    leal -16(%rcx), %eax
; X64-NEXT:    cmpl $4080, %eax # imm = 0xFF0
<<<<<<< HEAD
; X64-NEXT:    jb .LBB0_13
; X64-NEXT:  # %bb.14: # %afterloop.48.clone
; X64-NEXT:    # in Loop: Header=BB0_12 Depth=1
=======
; X64-NEXT:    jb .LBB0_8
; X64-NEXT:  # %bb.9: # %afterloop.48.clone
; X64-NEXT:    # in Loop: Header=BB0_7 Depth=1
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    vaddpd %ymm3, %ymm9, %ymm1
; X64-NEXT:    vaddpd %ymm4, %ymm2, %ymm2
; X64-NEXT:    vaddpd %ymm2, %ymm1, %ymm1
; X64-NEXT:    vextractf128 $1, %ymm1, %xmm2
; X64-NEXT:    vaddpd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vpermilpd {{.*#+}} xmm2 = xmm1[1,0]
; X64-NEXT:    vaddsd %xmm2, %xmm1, %xmm1
; X64-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X64-NEXT:    addq $32768, %rsi # imm = 0x8000
; X64-NEXT:    cmpq %r9, %r10
; X64-NEXT:    leaq 1(%r10), %r10
<<<<<<< HEAD
; X64-NEXT:    jne .LBB0_12
; X64-NEXT:  # %bb.15: # %for.cond.cleanup.clone
; X64-NEXT:    addq $40, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
; X64-NEXT:  .LBB0_8: # %entry
; X64-NEXT:    .cfi_def_cfa_offset 48
; X64-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; X64-NEXT:    addq $40, %rsp
=======
; X64-NEXT:    jne .LBB0_7
; X64-NEXT:    jmp .LBB0_10
; X64-NEXT:  .LBB0_1: # %entry
; X64-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; X64-NEXT:    addq $48, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 16
; X64-NEXT:    popq %rbx
>>>>>>> 968f5daa887d94b3092af624db5080fbd721ba0f
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    retq
; X64-NEXT:  .LBB0_11:
; X64-NEXT:    .cfi_def_cfa_offset 64
; X64-NEXT:    movl %r8d, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    movq %rsi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    movq %rdi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; X64-NEXT:    callq __detect_cpu_core_type_1n@PLT
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r9 # 8-byte Reload
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdi # 8-byte Reload
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rdx # 8-byte Reload
; X64-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %r8d # 4-byte Reload
; X64-NEXT:    jmp .LBB0_3
;
; X86-LABEL: foo:
; X86:       # %bb.0: # %entry
; X86-NEXT:    pushl %ebp
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %ebp, -8
; X86-NEXT:    movl %esp, %ebp
; X86-NEXT:    .cfi_def_cfa_register %ebp
; X86-NEXT:    pushl %ebx
; X86-NEXT:    pushl %edi
; X86-NEXT:    pushl %esi
; X86-NEXT:    andl $-32, %esp
; X86-NEXT:    subl $224, %esp
; X86-NEXT:    .cfi_offset %esi, -20
; X86-NEXT:    .cfi_offset %edi, -16
; X86-NEXT:    .cfi_offset %ebx, -12
; X86-NEXT:    movl 24(%ebp), %esi
; X86-NEXT:    testl %esi, %esi
; X86-NEXT:    jle .LBB0_1
; X86-NEXT:  # %bb.2: # %for.body.preheader
; X86-NEXT:    xorl %ebx, %ebx
; X86-NEXT:    movl %esi, %eax
; X86-NEXT:    addl $-1, %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movl $0, %eax
; X86-NEXT:    adcl $-1, %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    rdpid %eax
; X86-NEXT:    movzbl %al, %eax
; X86-NEXT:    movb __cpu_core_type(%eax), %al
; X86-NEXT:    testb %al, %al
; X86-NEXT:    je .LBB0_13
; X86-NEXT:  # %bb.3:
; X86-NEXT:    movl 12(%ebp), %ecx
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    cmpb $64, %al
; X86-NEXT:    jne .LBB0_8
; X86-NEXT:  .LBB0_4:
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_5: # %loop.39
; X86-NEXT:    # =>This Loop Header: Depth=1
; X86-NEXT:    # Child Loop BB0_6 Depth 2
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vmovsd %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 8-byte Spill
; X86-NEXT:    movl %ebx, %edx
; X86-NEXT:    notl %edx
; X86-NEXT:    addl %esi, %edx
; X86-NEXT:    movl 16(%ebp), %esi
; X86-NEXT:    movl (%esi,%edx,4), %esi
; X86-NEXT:    movl $-16, %edi
; X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    shll $12, %ebx
; X86-NEXT:    vxorpd %xmm5, %xmm5, %xmm5
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vxorps %xmm3, %xmm3, %xmm3
; X86-NEXT:    movl 8(%ebp), %eax
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_6: # %loop.48
; X86-NEXT:    # Parent Loop BB0_5 Depth=1
; X86-NEXT:    # => This Inner Loop Header: Depth=2
; X86-NEXT:    vmovaps %ymm3, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm3 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X86-NEXT:    vgatherqpd %ymm7, (%eax,%ymm3,8), %ymm4
; X86-NEXT:    vmovapd %ymm4, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm4 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    vgatherqpd %ymm7, (%eax,%ymm4,8), %ymm3
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vmovapd %ymm1, %ymm4
; X86-NEXT:    vxorpd %xmm1, %xmm1, %xmm1
; X86-NEXT:    vgatherqpd %ymm7, (%eax,%ymm0,8), %ymm1
; X86-NEXT:    vpmovzxdq {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero
; X86-NEXT:    vmovdqa %ymm0, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vpcmpeqd %ymm7, %ymm7, %ymm7
; X86-NEXT:    vmovapd %ymm2, %ymm0
; X86-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm6 # 32-byte Reload
; X86-NEXT:    vgatherqpd %ymm7, (%eax,%ymm6,8), %ymm2
; X86-NEXT:    leal 16(%ebx,%edi), %edx
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; X86-NEXT:    vmovapd %ymm0, %ymm2
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm1 * mem) + ymm4
; X86-NEXT:    vmovapd %ymm4, %ymm1
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm5 = (ymm3 * mem) + ymm5
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm3 # 32-byte Reload
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm4 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm4 * mem) + ymm3
; X86-NEXT:    addl $16, %edi
; X86-NEXT:    cmpl $4080, %edi # imm = 0xFF0
; X86-NEXT:    jb .LBB0_6
; X86-NEXT:  # %bb.7: # %afterloop.48
; X86-NEXT:    # in Loop: Header=BB0_5 Depth=1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edi # 4-byte Reload
; X86-NEXT:    movl %edi, %edx
; X86-NEXT:    addl $1, %edx
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    movl %eax, %esi
; X86-NEXT:    adcl $0, %esi
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %edi # 4-byte Folded Reload
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Folded Reload
; X86-NEXT:    orl %edi, %eax
; X86-NEXT:    vaddpd %ymm2, %ymm5, %ymm0
; X86-NEXT:    vaddpd %ymm3, %ymm1, %ymm1
; X86-NEXT:    vaddpd %ymm1, %ymm0, %ymm0
; X86-NEXT:    vextractf128 $1, %ymm0, %xmm1
; X86-NEXT:    vaddpd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vpermilpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vmovsd {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 8-byte Reload
; X86-NEXT:    # xmm1 = mem[0],zero
; X86-NEXT:    vaddsd %xmm0, %xmm1, %xmm0
; X86-NEXT:    movl %edx, %ebx
; X86-NEXT:    movl %esi, %eax
; X86-NEXT:    movl 24(%ebp), %esi
; X86-NEXT:    jne .LBB0_5
; X86-NEXT:    jmp .LBB0_12
; X86-NEXT:  .LBB0_1: # %entry
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    jmp .LBB0_12
; X86-NEXT:  .LBB0_13:
; X86-NEXT:    calll __detect_cpu_core_type_1n@PLT
; X86-NEXT:    movl 12(%ebp), %ecx
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    cmpb $64, %al
; X86-NEXT:    je .LBB0_4
; X86-NEXT:  .LBB0_8:
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_9: # %loop.39.clone
; X86-NEXT:    # =>This Loop Header: Depth=1
; X86-NEXT:    # Child Loop BB0_10 Depth 2
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vmovsd %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 8-byte Spill
; X86-NEXT:    movl %ebx, %eax
; X86-NEXT:    notl %eax
; X86-NEXT:    addl %esi, %eax
; X86-NEXT:    movl 16(%ebp), %ecx
; X86-NEXT:    movl (%ecx,%eax,4), %eax
; X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-NEXT:    movl $-16, %edi
; X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    shll $12, %ebx
; X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X86-NEXT:    vxorpd %xmm3, %xmm3, %xmm3
; X86-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    .p2align 4, 0x90
; X86-NEXT:  .LBB0_10: # %loop.48.clone
; X86-NEXT:    # Parent Loop BB0_9 Depth=1
; X86-NEXT:    # => This Inner Loop Header: Depth=2
; X86-NEXT:    vmovaps %ymm2, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovapd %ymm0, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    vmovdqu 64(%edx,%edi,4), %xmm0
; X86-NEXT:    vmovdqu 80(%edx,%edi,4), %xmm5
; X86-NEXT:    vpextrd $1, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; X86-NEXT:    vpextrd $2, %xmm0, %eax
; X86-NEXT:    vmovd %xmm0, %esi
; X86-NEXT:    vpextrd $3, %xmm0, %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; X86-NEXT:    vpextrd $1, %xmm5, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; X86-NEXT:    vmovhps {{.*#+}} xmm0 = xmm0[0,1],mem[0,1]
; X86-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    vpextrd $2, %xmm5, %ebx
; X86-NEXT:    vpextrd $3, %xmm5, %ecx
; X86-NEXT:    movl 8(%ebp), %eax
; X86-NEXT:    vmovsd {{.*#+}} xmm6 = mem[0],zero
; X86-NEXT:    vmovdqu 96(%edx,%edi,4), %xmm7
; X86-NEXT:    movl 8(%ebp), %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    vmovhpd {{.*#+}} xmm6 = xmm6[0],mem[0]
; X86-NEXT:    vpextrd $1, %xmm7, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; X86-NEXT:    movl 8(%ebp), %esi
; X86-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; X86-NEXT:    vpextrd $2, %xmm7, %esi
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovhpd {{.*#+}} xmm1 = xmm1[0],mem[0]
; X86-NEXT:    vmovd %xmm5, %ecx
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm5 = mem[0],zero
; X86-NEXT:    vpextrd $3, %xmm7, %ecx
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    vmovhpd {{.*#+}} xmm5 = xmm5[0],mem[0]
; X86-NEXT:    vmovdqu 112(%edx,%edi,4), %xmm0
; X86-NEXT:    vpextrd $1, %xmm0, %edx
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm2 = mem[0],zero
; X86-NEXT:    movl 8(%ebp), %esi
; X86-NEXT:    vmovhpd {{.*#+}} xmm2 = xmm2[0],mem[0]
; X86-NEXT:    vmovd %xmm7, %ecx
; X86-NEXT:    vpextrd $2, %xmm0, %esi
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm7 = mem[0],zero
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    vmovhpd {{.*#+}} xmm7 = xmm7[0],mem[0]
; X86-NEXT:    vmovd %xmm0, %eax
; X86-NEXT:    vpextrd $3, %xmm0, %ecx
; X86-NEXT:    vinsertf128 $1, {{[-0-9]+}}(%e{{[sb]}}p), %ymm6, %ymm0 # 16-byte Folded Reload
; X86-NEXT:    vinsertf128 $1, %xmm1, %ymm5, %ymm1
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovsd {{.*#+}} xmm5 = mem[0],zero
; X86-NEXT:    movl 12(%ebp), %esi
; X86-NEXT:    movl 8(%ebp), %ebx
; X86-NEXT:    vmovhpd {{.*#+}} xmm5 = xmm5[0],mem[0]
; X86-NEXT:    movl 8(%ebp), %ecx
; X86-NEXT:    vinsertf128 $1, %xmm2, %ymm7, %ymm2
; X86-NEXT:    vmovsd {{.*#+}} xmm6 = mem[0],zero
; X86-NEXT:    vmovhpd {{.*#+}} xmm6 = xmm6[0],mem[0]
; X86-NEXT:    vinsertf128 $1, %xmm5, %ymm6, %ymm5
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; X86-NEXT:    leal 16(%eax,%edi), %eax
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm4 = (ymm5 * mem) + ymm4
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm3 = (ymm2 * mem) + ymm3
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm2 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm2 = (ymm1 * mem) + ymm2
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm1 # 32-byte Reload
; X86-NEXT:    vfmadd231pd {{.*#+}} ymm1 = (ymm0 * mem) + ymm1
; X86-NEXT:    vmovapd %ymm1, {{[-0-9]+}}(%e{{[sb]}}p) # 32-byte Spill
; X86-NEXT:    vmovapd {{[-0-9]+}}(%e{{[sb]}}p), %ymm0 # 32-byte Reload
; X86-NEXT:    addl $16, %edi
; X86-NEXT:    cmpl $4080, %edi # imm = 0xFF0
; X86-NEXT:    jb .LBB0_10
; X86-NEXT:  # %bb.11: # %afterloop.48.clone
; X86-NEXT:    # in Loop: Header=BB0_9 Depth=1
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; X86-NEXT:    movl %edx, %eax
; X86-NEXT:    addl $1, %eax
; X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Reload
; X86-NEXT:    movl %esi, %ecx
; X86-NEXT:    adcl $0, %ecx
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Folded Reload
; X86-NEXT:    xorl {{[-0-9]+}}(%e{{[sb]}}p), %esi # 4-byte Folded Reload
; X86-NEXT:    orl %edx, %esi
; X86-NEXT:    vaddpd %ymm3, %ymm0, %ymm0
; X86-NEXT:    vaddpd %ymm4, %ymm2, %ymm1
; X86-NEXT:    vaddpd %ymm1, %ymm0, %ymm0
; X86-NEXT:    vextractf128 $1, %ymm0, %xmm1
; X86-NEXT:    vaddpd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vpermilpd {{.*#+}} xmm1 = xmm0[1,0]
; X86-NEXT:    vaddsd %xmm1, %xmm0, %xmm0
; X86-NEXT:    vmovsd {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 8-byte Reload
; X86-NEXT:    # xmm1 = mem[0],zero
; X86-NEXT:    vaddsd %xmm0, %xmm1, %xmm0
; X86-NEXT:    movl %eax, %ebx
; X86-NEXT:    movl %ecx, %eax
; X86-NEXT:    movl 24(%ebp), %esi
; X86-NEXT:    jne .LBB0_9
; X86-NEXT:  .LBB0_12: # %for.cond.cleanup
; X86-NEXT:    vmovsd %xmm0, {{[0-9]+}}(%esp)
; X86-NEXT:    fldl {{[0-9]+}}(%esp)
; X86-NEXT:    leal -12(%ebp), %esp
; X86-NEXT:    popl %esi
; X86-NEXT:    popl %edi
; X86-NEXT:    popl %ebx
; X86-NEXT:    popl %ebp
; X86-NEXT:    .cfi_def_cfa %esp, 4
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
entry:
  %cmp26 = icmp sgt i32 %N, 0
  br i1 %cmp26, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %0 = zext i32 %N to i64
  %1 = add nsw i64 %0, -1
  br label %loop.39

for.cond.cleanup:                                 ; preds = %afterloop.48, %entry
  %dst_row.0.lcssa = phi double [ 0.000000e+00, %entry ], [ %19, %afterloop.48 ]
  ret double %dst_row.0.lcssa

loop.39:                                          ; preds = %afterloop.48, %for.body.preheader
  %i1.i64.0 = phi i64 [ 0, %for.body.preheader ], [ %nextivloop.39, %afterloop.48 ]
  %t3.0 = phi double [ 0.000000e+00, %for.body.preheader ], [ %19, %afterloop.48 ]
  %2 = trunc i64 %i1.i64.0 to i32
  %3 = xor i32 %2, -1
  %4 = add i32 %3, %N
  %5 = zext i32 %4 to i64
  %6 = getelementptr inbounds i32*, i32** %rowstart, i64 %5
  %gepload = load i32*, i32** %6, align 8, !tbaa !3
  br label %loop.48

loop.48:                                          ; preds = %loop.48, %loop.39
  %t38.0 = phi <16 x double> [ zeroinitializer, %loop.39 ], [ %18, %loop.48 ]
  %i2.i32.0 = phi i32 [ 0, %loop.39 ], [ %nextivloop.48, %loop.48 ]
  %7 = zext i32 %i2.i32.0 to i64
  %8 = getelementptr inbounds i32, i32* %gepload, i64 %7
  %9 = bitcast i32* %8 to <16 x i32>*
  %gepload38 = load <16 x i32>, <16 x i32>* %9, align 4, !tbaa !7
  %10 = zext <16 x i32> %gepload38 to <16 x i64>
  %11 = getelementptr inbounds double, double* %dst, <16 x i64> %10
  %12 = call <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*> %11, i32 8, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x double> undef), !tbaa !9
  %13 = shl i64 %i1.i64.0, 12
  %14 = add i64 %13, %7
  %15 = getelementptr inbounds double, double* %luval, i64 %14
  %16 = bitcast double* %15 to <16 x double>*
  %gepload39 = load <16 x double>, <16 x double>* %16, align 8, !tbaa !9
  %17 = fmul fast <16 x double> %12, %gepload39
  %18 = fadd fast <16 x double> %17, %t38.0
  %nextivloop.48 = add nuw nsw i32 %i2.i32.0, 16
  %condloop.48 = icmp ult i32 %i2.i32.0, 4080
  br i1 %condloop.48, label %loop.48, label %afterloop.48, !llvm.loop !11

afterloop.48:                                     ; preds = %loop.48
  %19 = call fast double @llvm.vector.reduce.fadd.v16f64(double %t3.0, <16 x double> %18)
  %nextivloop.39 = add nuw nsw i64 %i1.i64.0, 1
  %condloop.39.not = icmp eq i64 %i1.i64.0, %1
  br i1 %condloop.39.not, label %for.cond.cleanup, label %loop.39, !llvm.loop !16
}

; Function Attrs: nofree nosync nounwind readnone willreturn
declare double @llvm.vector.reduce.fadd.v16f64(double, <16 x double>) #1

; Function Attrs: nofree nosync nounwind readonly willreturn
declare <16 x double> @llvm.masked.gather.v16f64.v16p0f64(<16 x double*>, i32 immarg, <16 x i1>, <16 x double>) #2

attributes #0 = { nofree norecurse nosync nounwind readonly uwtable "denormal-fp-math"="preserve-sign,preserve-sign" "denormal-fp-math-f32"="ieee,ieee" "frame-pointer"="none" "loopopt-pipeline"="full" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="alderlake" "target-features"="+adx,+aes,+avx,+avx2,+avxvnni,+bmi,+bmi2,+cldemote,+clflushopt,+clwb,+crc32,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+gfni,+hreset,+invpcid,+kl,+lzcnt,+mmx,+movbe,+movdir64b,+movdiri,+pclmul,+pconfig,+pku,+popcnt,+prfchw,+ptwrite,+rdpid,+rdrnd,+rdseed,+sahf,+serialize,+sgx,+sha,+shstk,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+vaes,+vpclmulqdq,+waitpkg,+widekl,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "unsafe-fp-math"="true" }
attributes #1 = { nofree nosync nounwind readnone willreturn }
attributes #2 = { nofree nosync nounwind readonly willreturn }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.x.0.YYYYMMDD)"}
!3 = !{!4, !4, i64 0}
!4 = !{!"pointer@_ZTSPj", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C/C++ TBAA"}
!7 = !{!8, !8, i64 0}
!8 = !{!"int", !5, i64 0}
!9 = !{!10, !10, i64 0}
!10 = !{!"double", !5, i64 0}
!11 = distinct !{!11, !12, !13, !14, !15}
!12 = !{!"llvm.loop.mustprogress"}
!13 = !{!"llvm.loop.unroll.disable"}
!14 = !{!"llvm.loop.vectorize.width", i32 1}
!15 = !{!"llvm.loop.interleave.count", i32 1}
!16 = distinct !{!16, !12, !13}
