; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_vnni_int16
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vnniint16 | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vnniint16 | FileCheck %s --check-prefixes=X86

define <16 x i32> @test_int_x86_vpdpwsud512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwsud512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwsud %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd2,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwsud512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwsud %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd2,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwsud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwsud512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwsud512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwsud %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x76,0x49,0xd2,0xe2]
; X64-NEXT:    vpdpwsud %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd2,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwsud512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwsud %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x76,0x49,0xd2,0xda]
; X86-NEXT:    vpdpwsud 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd2,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwsud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwsud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwsud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

define <16 x i32> @test_int_x86_vpdpwsuds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwsuds512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwsuds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd3,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwsuds512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwsuds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd3,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwsuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwsuds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwsuds512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwsuds %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x76,0x49,0xd3,0xe2]
; X64-NEXT:    vpdpwsuds %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd3,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwsuds512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwsuds %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x76,0x49,0xd3,0xda]
; X86-NEXT:    vpdpwsuds 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x76,0x48,0xd3,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwsuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwsuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwsuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

define <16 x i32> @test_int_x86_vpdpwusd512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwusd512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwusd %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd2,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwusd512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwusd %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd2,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwusd.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwusd512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwusd512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwusd %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0xd2,0xe2]
; X64-NEXT:    vpdpwusd %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd2,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwusd512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwusd %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0xd2,0xda]
; X86-NEXT:    vpdpwusd 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd2,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwusd.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwusd.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwusd.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

define <16 x i32> @test_int_x86_vpdpwusds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwusds512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwusds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd3,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwusds512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwusds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd3,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwusds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwusds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwusds512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwusds %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0xd3,0xe2]
; X64-NEXT:    vpdpwusds %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd3,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwusds512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwusds %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0xd3,0xda]
; X86-NEXT:    vpdpwusds 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x75,0x48,0xd3,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwusds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwusds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwusds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

define <16 x i32> @test_int_x86_vpdpwuud512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwuud512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwuud %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd2,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwuud512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwuud %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd2,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwuud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwuud512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwuud512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwuud %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x74,0x49,0xd2,0xe2]
; X64-NEXT:    vpdpwuud %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd2,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwuud512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwuud %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x74,0x49,0xd2,0xda]
; X86-NEXT:    vpdpwuud 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd2,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwuud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwuud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwuud.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

define <16 x i32> @test_int_x86_vpdpwuuds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C) nounwind {
; X64-LABEL: test_int_x86_vpdpwuuds512:
; X64:       # %bb.0:
; X64-NEXT:    vpdpwuuds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd3,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vpdpwuuds512:
; X86:       # %bb.0:
; X86-NEXT:    vpdpwuuds %zmm2, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd3,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i32> @llvm.x86.avx2.vpdpwuuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  ret <16 x i32> %ret
}

define <16 x i32> @test_int_x86_mask_vpdpwuuds512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C, <16 x i32> %D, i16 %E) nounwind {
; X64-LABEL: test_int_x86_mask_vpdpwuuds512:
; X64:       # %bb.0:
; X64-NEXT:    kmovw %edi, %k1 # encoding: [0xc5,0xf8,0x92,0xcf]
; X64-NEXT:    vmovdqa64 %zmm0, %zmm4 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xe0]
; X64-NEXT:    vpdpwuuds %zmm2, %zmm1, %zmm4 {%k1} # encoding: [0x62,0xf2,0x74,0x49,0xd3,0xe2]
; X64-NEXT:    vpdpwuuds %zmm3, %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd3,0xc3]
; X64-NEXT:    vpaddd %zmm0, %zmm4, %zmm4 {%k1} # encoding: [0x62,0xf1,0x5d,0x49,0xfe,0xe0]
; X64-NEXT:    vmovdqa64 %zmm4, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc4]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vpdpwuuds512:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebp # encoding: [0x55]
; X86-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; X86-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; X86-NEXT:    subl $64, %esp # encoding: [0x83,0xec,0x40]
; X86-NEXT:    kmovw 72(%ebp), %k1 # encoding: [0xc5,0xf8,0x90,0x4d,0x48]
; X86-NEXT:    vmovdqa64 %zmm0, %zmm3 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xd8]
; X86-NEXT:    vpdpwuuds %zmm2, %zmm1, %zmm3 {%k1} # encoding: [0x62,0xf2,0x74,0x49,0xd3,0xda]
; X86-NEXT:    vpdpwuuds 8(%ebp), %zmm1, %zmm0 # encoding: [0x62,0xf2,0x74,0x48,0xd3,0x85,0x08,0x00,0x00,0x00]
; X86-NEXT:    vpaddd %zmm0, %zmm3, %zmm3 {%k1} # encoding: [0x62,0xf1,0x65,0x49,0xfe,0xd8]
; X86-NEXT:    vmovdqa64 %zmm3, %zmm0 # encoding: [0x62,0xf1,0xfd,0x48,0x6f,0xc3]
; X86-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; X86-NEXT:    popl %ebp # encoding: [0x5d]
; X86-NEXT:    retl # encoding: [0xc3]
  %1 = call <16 x i32> @llvm.x86.avx2.vpdpwuuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)
  %2 = bitcast i16 %E to <16 x i1>
  %3 = select <16 x i1> %2, <16 x i32> %1, <16 x i32> %A
  %4 = call <16 x i32> @llvm.x86.avx2.vpdpwuuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %D)
  %5 = select <16 x i1> %2, <16 x i32> %4, <16 x i32> zeroinitializer
  %ret = add <16 x i32> %3, %5
  ret <16 x i32> %ret
}
declare <16 x i32> @llvm.x86.avx2.vpdpwuuds.512(<16 x i32> %A, <16 x i32> %B, <16 x i32> %C)

