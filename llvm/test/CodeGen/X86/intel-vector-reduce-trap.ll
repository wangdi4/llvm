; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512bw -trap-on-reduce-intrin | FileCheck %s

define float @test_reduce_fadd_ps(<16 x float> %a) {
; CHECK-LABEL: test_reduce_fadd_ps:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextractf64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vaddps %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; CHECK-NEXT:    vaddss %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call fast float @llvm.vector.reduce.fadd.v16f32(float -0.000000e+00, <16 x float> %a) #5
  ret float %0
}

define float @test_reduce_fmul_ps(<16 x float> %a) {
; CHECK-LABEL: test_reduce_fmul_ps:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextractf64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vmulps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; CHECK-NEXT:    vmulps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmulss %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call fast float @llvm.vector.reduce.fmul.v16f32(float 1.0, <16 x float> %a)
  ret float %0
}

define double @test_reduce_fmax_pd(<8 x double> %a) {
; CHECK-LABEL: test_reduce_fmax_pd:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextractf64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vmaxpd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vmaxpd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; CHECK-NEXT:    vmaxsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call fast double @llvm.vector.reduce.fmax.v8f64(<8 x double> %a)
  ret double %0
}

define double @test_reduce_fmin_pd(<8 x double> %a) {
; CHECK-LABEL: test_reduce_fmin_pd:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextractf64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vminpd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vminpd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; CHECK-NEXT:    vminsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call fast double @llvm.vector.reduce.fmin.v8f64(<8 x double> %a)
  ret double %0
}

define i32 @test_reduce_add(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_add:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_mul(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_mul:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpmulld %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.mul.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_and(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_and:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpandd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpand %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpand %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpand %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.and.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_or(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_or:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpord %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.or.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_xor(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_xor:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpxord %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.xor.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_smax(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_smax:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpmaxsd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpmaxsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.smax.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_umax(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_umax:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpmaxud %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpmaxud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.umax.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_smin(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_smin:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpminsd %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpminsd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.smin.v16i32(<16 x i32> %a)
  ret i32 %0
}

define i32 @test_reduce_umin(<16 x i32> %a) {
; CHECK-LABEL: test_reduce_umin:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    int3
; CHECK-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; CHECK-NEXT:    vpminud %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; CHECK-NEXT:    vpminud %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovd %xmm0, %eax
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = tail call i32 @llvm.vector.reduce.umin.v16i32(<16 x i32> %a)
  ret i32 %0
}

declare float @llvm.vector.reduce.fadd.v16f32(float, <16 x float>)
declare float @llvm.vector.reduce.fmul.v16f32(float, <16 x float>)
declare double @llvm.vector.reduce.fmax.v8f64(<8 x double>)
declare double @llvm.vector.reduce.fmin.v8f64(<8 x double>)
declare i32 @llvm.vector.reduce.add.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.mul.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.and.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.or.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.xor.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.smax.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.umax.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.smin.v16i32(<16 x i32>)
declare i32 @llvm.vector.reduce.umin.v16i32(<16 x i32>)
