; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -x86-lower-matrix-intrinsics -mtriple=x86_64-unknown-unknown -mcpu=sapphirerapids %s -S | FileCheck %s

define <4 x i32> @test_extract_slice_4x4(<16 x i32> %mat, i32 %x, i32 %y) {
; CHECK-LABEL: @test_extract_slice_4x4(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca <16 x i32>, align 64
; CHECK-NEXT:    store <16 x i32> [[MAT:%.*]], ptr [[TMP0]], align 64
; CHECK-NEXT:    [[TMP1:%.*]] = mul i32 [[X:%.*]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = add i32 [[TMP1]], [[Y:%.*]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr i32, ptr [[TMP0]], i32 [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = load <4 x i32>, ptr [[TMP3]], align 16
; CHECK-NEXT:    ret <4 x i32> [[TMP4]]
;
enrty:
  %0 = call <4 x i32> @llvm.experimental.matrix.extract.row.slice.v4i32.v16i32(<16 x i32> %mat, i32 %x, i32 %y, i32 4, i32 4, i32 4, metadata !"matrix.rowmajor")
  ret <4 x i32> %0
}

define <4 x i32> @test_extract_slice_3x5(<15 x i32> %mat) {
; CHECK-LABEL: @test_extract_slice_3x5(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca <15 x i32>, align 64
; CHECK-NEXT:    store <15 x i32> [[MAT:%.*]], ptr [[TMP0]], align 64
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr i32, ptr [[TMP0]], i32 13
; CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[TMP1]], align 16
; CHECK-NEXT:    ret <4 x i32> [[TMP2]]
;
enrty:
  %0 = call <4 x i32> @llvm.experimental.matrix.extract.row.slice.v4i32.v15i32(<15 x i32> %mat, i32 2, i32 3, i32 4, i32 3, i32 5, metadata !"matrix.rowmajor")
  ret <4 x i32> %0
}

define <16 x i32> @test_insert_slice_4x4(<16 x i32> %mat, <4 x i32> %slice, i32 %x, i32 %y) {
; CHECK-LABEL: @test_insert_slice_4x4(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca <16 x i32>, align 64
; CHECK-NEXT:    store <16 x i32> [[MAT:%.*]], ptr [[TMP0]], align 64
; CHECK-NEXT:    [[TMP1:%.*]] = mul i32 [[X:%.*]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = add i32 [[TMP1]], [[Y:%.*]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr i32, ptr [[TMP0]], i32 [[TMP2]]
; CHECK-NEXT:    store <4 x i32> [[SLICE:%.*]], ptr [[TMP3]], align 16
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i32>, ptr [[TMP0]], align 64
; CHECK-NEXT:    ret <16 x i32> [[TMP4]]
;
enrty:
  %0 = call <16 x i32> @llvm.experimental.matrix.insert.row.slice.v16i32.v4i32(<16 x i32> %mat, <4 x i32> %slice, i32 %x, i32 %y, i32 4, i32 4, i32 4, metadata !"matrix.rowmajor")
  ret <16 x i32> %0
}

define <15 x i32> @test_insert_slice_3x5(<15 x i32> %mat, <4 x i32> %slice) {
; CHECK-LABEL: @test_insert_slice_3x5(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca <15 x i32>, align 64
; CHECK-NEXT:    store <15 x i32> [[MAT:%.*]], ptr [[TMP0]], align 64
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr i32, ptr [[TMP0]], i32 13
; CHECK-NEXT:    store <4 x i32> [[SLICE:%.*]], ptr [[TMP1]], align 16
; CHECK-NEXT:    [[TMP2:%.*]] = load <15 x i32>, ptr [[TMP0]], align 64
; CHECK-NEXT:    ret <15 x i32> [[TMP2]]
;
enrty:
  %0 = call <15 x i32> @llvm.experimental.matrix.insert.row.slice.v15i32.v4i32(<15 x i32> %mat, <4 x i32> %slice, i32 2, i32 3, i32 4, i32 3, i32 5, metadata !"matrix.rowmajor")
  ret <15 x i32> %0
}

define void @test_load_store_different_addrspace(ptr addrspace(4) %ptr, i64 %stride, ptr addrspace(4) %dst) {
; CHECK-LABEL: @test_load_store_different_addrspace(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr addrspace(4) [[PTR:%.*]] to ptr
; CHECK-NEXT:    [[TMP1:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP2:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 8, ptr [[TMP0]], i64 [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i8> @llvm.x86.cast.tile.to.vector.v8i8(x86_amx [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = addrspacecast ptr addrspace(4) [[DST:%.*]] to ptr
; CHECK-NEXT:    [[TMP5:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP6:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v8i8(<8 x i8> [[TMP3]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 1, i16 8, ptr [[TMP4]], i64 [[TMP5]], x86_amx [[TMP6]])
; CHECK-NEXT:    ret void
;
enrty:
  %0 = call <8 x i8> @llvm.experimental.matrix.load.v8i8.p4(ptr addrspace(4) %ptr, i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  call void @llvm.experimental.matrix.store.v8i8.p4(<8 x i8> %0, ptr addrspace(4) %dst, i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define <16 x i8> @test_fill_int8_rowmajor() {
; CHECK-LABEL: @test_fill_int8_rowmajor(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 4)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i8> @llvm.x86.cast.tile.to.vector.v16i8(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i8> [[TMP1]]
;
enrty:
  %0 = call <16 x i8> @llvm.experimental.matrix.fill.v16i8.i8(i8 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x i8> %0
}

define <16 x i8> @test_fill_int8_packedb() {
; CHECK-LABEL: @test_fill_int8_packedb(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 1, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i8> @llvm.x86.cast.tile.to.vector.v16i8(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i8> [[TMP1]]
;
enrty:
  %0 = call <16 x i8> @llvm.experimental.matrix.fill.v16i8.i8(i8 0, i32 4, i32 4, metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x i8> %0
}

define <16 x i16> @test_fill_bf16_rowmajor() {
; CHECK-LABEL: @test_fill_bf16_rowmajor(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 8)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i16> @llvm.x86.cast.tile.to.vector.v16i16(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i16> [[TMP1]]
;
enrty:
  %0 = call <16 x i16> @llvm.experimental.matrix.fill.v16i16.i16(i16 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x i16> %0
}

define <16 x i16> @test_fill_bf16_packedb() {
; CHECK-LABEL: @test_fill_bf16_packedb(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 2, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i16> @llvm.x86.cast.tile.to.vector.v16i16(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i16> [[TMP1]]
;
enrty:
  %0 = call <16 x i16> @llvm.experimental.matrix.fill.v16i16.i16(i16 0, i32 4, i32 4, metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x i16> %0
}

define <16 x float> @test_fill_float_rowmajor() {
; CHECK-LABEL: @test_fill_float_rowmajor(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x float> @llvm.x86.cast.tile.to.vector.v16f32(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x float> [[TMP1]]
;
enrty:
  %0 = call <16 x float> @llvm.experimental.matrix.fill.v16f32.f32(float 0.000000e+00, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x float> %0
}

define <16 x i32> @test_fill_i32_rowmajor() {
; CHECK-LABEL: @test_fill_i32_rowmajor(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i32> @llvm.x86.cast.tile.to.vector.v16i32(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i32> [[TMP1]]
;
enrty:
  %0 = call <16 x i32> @llvm.experimental.matrix.fill.v16i32.i32(i32 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret <16 x i32> %0
}

define <16 x i8> @test_fill_int8_usea() {
; CHECK-LABEL: @test_fill_int8_usea(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 4)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i8> @llvm.x86.cast.tile.to.vector.v16i8(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i8> [[TMP1]]
;
enrty:
  %0 = call <16 x i8> @llvm.experimental.matrix.fill.v16i8.i8(i8 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  ret <16 x i8> %0
}

define <16 x i8> @test_fill_int8_useb() {
; CHECK-LABEL: @test_fill_int8_useb(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 1, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i8> @llvm.x86.cast.tile.to.vector.v16i8(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i8> [[TMP1]]
;
enrty:
  %0 = call <16 x i8> @llvm.experimental.matrix.fill.v16i8.i8(i8 0, i32 4, i32 4, metadata !"matrix.packed", metadata !"scope.subgroup", metadata !"matrix.use.b")
  ret <16 x i8> %0
}

define <16 x i16> @test_fill_bf16_usea() {
; CHECK-LABEL: @test_fill_bf16_usea(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 8)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i16> @llvm.x86.cast.tile.to.vector.v16i16(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i16> [[TMP1]]
;
enrty:
  %0 = call <16 x i16> @llvm.experimental.matrix.fill.v16i16.i16(i16 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  ret <16 x i16> %0
}

define <16 x i16> @test_fill_bf16_useb() {
; CHECK-LABEL: @test_fill_bf16_useb(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 2, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i16> @llvm.x86.cast.tile.to.vector.v16i16(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i16> [[TMP1]]
;
enrty:
  %0 = call <16 x i16> @llvm.experimental.matrix.fill.v16i16.i16(i16 0, i32 4, i32 4, metadata !"matrix.packed", metadata !"scope.subgroup", metadata !"matrix.use.b")
  ret <16 x i16> %0
}

define <16 x float> @test_fill_float_usec() {
; CHECK-LABEL: @test_fill_float_usec(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x float> @llvm.x86.cast.tile.to.vector.v16f32(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x float> [[TMP1]]
;
enrty:
  %0 = call <16 x float> @llvm.experimental.matrix.fill.v16f32.f32(float 0.000000e+00, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret <16 x float> %0
}

define <16 x i32> @test_fill_i32_usec() {
; CHECK-LABEL: @test_fill_i32_usec(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = call x86_amx @llvm.x86.tilezero.internal(i16 4, i16 16)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i32> @llvm.x86.cast.tile.to.vector.v16i32(x86_amx [[TMP0]])
; CHECK-NEXT:    ret <16 x i32> [[TMP1]]
;
enrty:
  %0 = call <16 x i32> @llvm.experimental.matrix.fill.v16i32.i32(i32 0, i32 4, i32 4, metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret <16 x i32> %0
}

define void @test_load_store_same_addrspace(ptr %ptr, i64 %stride, ptr %dst) {
; CHECK-LABEL: @test_load_store_same_addrspace(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP1:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 8, ptr [[PTR:%.*]], i64 [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <8 x i8> @llvm.x86.cast.tile.to.vector.v8i8(x86_amx [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP4:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v8i8(<8 x i8> [[TMP2]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 1, i16 8, ptr [[DST:%.*]], i64 [[TMP3]], x86_amx [[TMP4]])
; CHECK-NEXT:    ret void
;
enrty:
  %0 = call <8 x i8> @llvm.experimental.matrix.load.v8i8.p0(ptr %ptr, i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  call void @llvm.experimental.matrix.store.v8i8.p0(<8 x i8> %0, ptr %dst, i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_int8(ptr addrspace(4) %ptr, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_int8(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTR:%.*]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 4, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <12 x i8> @llvm.x86.cast.tile.to.vector.v12i8(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 20, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <20 x i8> @llvm.x86.cast.tile.to.vector.v20i8(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 20, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbssd.internal(i16 3, i16 20, i16 4, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP19:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP21:%.*]] = call x86_amx @llvm.x86.tdpbsud.internal(i16 3, i16 20, i16 4, x86_amx [[TMP18]], x86_amx [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    [[TMP22:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP21]])
; CHECK-NEXT:    [[TMP23:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP24:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP25:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP26:%.*]] = call x86_amx @llvm.x86.tdpbusd.internal(i16 3, i16 20, i16 4, x86_amx [[TMP23]], x86_amx [[TMP24]], x86_amx [[TMP25]])
; CHECK-NEXT:    [[TMP27:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP26]])
; CHECK-NEXT:    [[TMP28:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP29:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP30:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP31:%.*]] = call x86_amx @llvm.x86.tdpbuud.internal(i16 3, i16 20, i16 4, x86_amx [[TMP28]], x86_amx [[TMP29]], x86_amx [[TMP30]])
; CHECK-NEXT:    [[TMP32:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP31]])
; CHECK-NEXT:    [[TMP33:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP34:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP35:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 3, i16 20, ptr [[TMP33]], i64 [[TMP34]], x86_amx [[TMP35]])
; CHECK-NEXT:    ret void
;
  %A = call <12 x i8> @llvm.experimental.matrix.load.v12i8.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 4, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %B = call <20 x i8> @llvm.experimental.matrix.load.v20i8.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 4, i32 5, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %C = call <15 x i32> @llvm.experimental.matrix.load.v15i32.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 5, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %D = call <15 x i32> @llvm.experimental.matrix.mad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %E = call <15 x i32> @llvm.experimental.matrix.sumad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %F = call <15 x i32> @llvm.experimental.matrix.usmad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %G = call <15 x i32> @llvm.experimental.matrix.uumad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v15i32.p4(<15 x i32> %D, ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 5, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_bf16(ptr addrspace(4) %ptrA, ptr addrspace(4) %ptrB, ptr addrspace(4) %ptrC, ptr addrspace(4) %ptrD, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_bf16(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTRA:%.*]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTRB:%.*]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTRC:%.*]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbf16ps.internal(i16 15, i16 60, i16 60, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(4) [[PTRD:%.*]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 15, i16 60, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4(ptr addrspace(4) %ptrA, i64 %Stride, i1 false, i32 15, i32 30, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %B = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4(ptr addrspace(4) %ptrB, i64 %Stride, i1 false, i32 30, i32 15, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %C = call <225 x float> @llvm.experimental.matrix.load.v225f32.p4(ptr addrspace(4) %ptrC, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  %D = call <225 x float> @llvm.experimental.matrix.mad.v225f32.v450i16.v450i16(<450 x i16> %A, <450 x i16> %B, <225 x float> %C, i32 15, i32 30, i32 15, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v225f32.p4(<225 x float> %D, ptr addrspace(4) %ptrD, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.unnecessary")
  ret void
}

define void @load_mad_store_int8_with_use(ptr addrspace(4) %ptr, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_int8_with_use(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTR:%.*]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 4, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <12 x i8> @llvm.x86.cast.tile.to.vector.v12i8(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 20, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <20 x i8> @llvm.x86.cast.tile.to.vector.v20i8(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 20, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbssd.internal(i16 3, i16 20, i16 4, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP19:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP21:%.*]] = call x86_amx @llvm.x86.tdpbsud.internal(i16 3, i16 20, i16 4, x86_amx [[TMP18]], x86_amx [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    [[TMP22:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP21]])
; CHECK-NEXT:    [[TMP23:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP24:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP25:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP26:%.*]] = call x86_amx @llvm.x86.tdpbusd.internal(i16 3, i16 20, i16 4, x86_amx [[TMP23]], x86_amx [[TMP24]], x86_amx [[TMP25]])
; CHECK-NEXT:    [[TMP27:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP26]])
; CHECK-NEXT:    [[TMP28:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP29:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP30:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP31:%.*]] = call x86_amx @llvm.x86.tdpbuud.internal(i16 3, i16 20, i16 4, x86_amx [[TMP28]], x86_amx [[TMP29]], x86_amx [[TMP30]])
; CHECK-NEXT:    [[TMP32:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP31]])
; CHECK-NEXT:    [[TMP33:%.*]] = addrspacecast ptr addrspace(4) [[PTR]] to ptr
; CHECK-NEXT:    [[TMP34:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP35:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 3, i16 20, ptr [[TMP33]], i64 [[TMP34]], x86_amx [[TMP35]])
; CHECK-NEXT:    ret void
;
  %A = call <12 x i8> @llvm.experimental.matrix.load.v12i8.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 4, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  %B = call <20 x i8> @llvm.experimental.matrix.load.v20i8.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 4, i32 5, metadata !"matrix.rowmajor", metadata !"matrix.packed", metadata !"scope.subgroup", metadata !"matrix.use.b")
  %C = call <15 x i32> @llvm.experimental.matrix.load.v15i32.p4(ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 5, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  %D = call <15 x i32> @llvm.experimental.matrix.mad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %E = call <15 x i32> @llvm.experimental.matrix.sumad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %F = call <15 x i32> @llvm.experimental.matrix.usmad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  %G = call <15 x i32> @llvm.experimental.matrix.uumad.v15i32.v12i8.v20i8(<12 x i8> %A, <20 x i8> %B, <15 x i32> %C, i32 3, i32 4, i32 5, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v15i32.p4(<15 x i32> %D, ptr addrspace(4) %ptr, i64 %Stride, i1 false, i32 3, i32 5, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret void
}

define void @load_mad_store_bf16_with_use(ptr addrspace(4) %ptrA, ptr addrspace(4) %ptrB, ptr addrspace(4) %ptrC, ptr addrspace(4) %ptrD, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_bf16_with_use(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast ptr addrspace(4) [[PTRA:%.*]] to ptr
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[PTRB:%.*]] to ptr
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast ptr addrspace(4) [[PTRC:%.*]] to ptr
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, ptr [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbf16ps.internal(i16 15, i16 60, i16 60, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast ptr addrspace(4) [[PTRD:%.*]] to ptr
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 15, i16 60, ptr [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4(ptr addrspace(4) %ptrA, i64 %Stride, i1 false, i32 15, i32 30, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.a")
  %B = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4(ptr addrspace(4) %ptrB, i64 %Stride, i1 false, i32 30, i32 15, metadata !"matrix.rowmajor", metadata !"matrix.packed", metadata !"scope.subgroup", metadata !"matrix.use.b")
  %C = call <225 x float> @llvm.experimental.matrix.load.v225f32.p4(ptr addrspace(4) %ptrC, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  %D = call <225 x float> @llvm.experimental.matrix.mad.v225f32.v450i16.v450i16(<450 x i16> %A, <450 x i16> %B, <225 x float> %C, i32 15, i32 30, i32 15, metadata !"scope.subgroup", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none", metadata !"matrix.reinterpret.type.none")
  call void @llvm.experimental.matrix.store.v225f32.p4(<225 x float> %D, ptr addrspace(4) %ptrD, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.columnmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup", metadata !"matrix.use.accumulator")
  ret void
}

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <225 x float> @llvm.experimental.matrix.mad.v225f32.v450i16.v450i16(<450 x i16>, <450 x i16>, <225 x float>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <12 x i8> @llvm.experimental.matrix.load.v12i8.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <20 x i8> @llvm.experimental.matrix.load.v20i8.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <15 x i32> @llvm.experimental.matrix.load.v15i32.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <15 x i32> @llvm.experimental.matrix.mad.v15i32.v12i8.v20i8(<12 x i8>, <20 x i8>, <15 x i32>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <15 x i32> @llvm.experimental.matrix.sumad.v15i32.v12i8.v20i8(<12 x i8>, <20 x i8>, <15 x i32>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <15 x i32> @llvm.experimental.matrix.usmad.v15i32.v12i8.v20i8(<12 x i8>, <20 x i8>, <15 x i32>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <15 x i32> @llvm.experimental.matrix.uumad.v15i32.v12i8.v20i8(<12 x i8>, <20 x i8>, <15 x i32>, i32, i32, i32, metadata, metadata, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v15i32.p4(<15 x i32>, ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <8 x i8> @llvm.experimental.matrix.load.v8i8.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v8i8.p4(<8 x i8>, ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <8 x i8> @llvm.experimental.matrix.load.v8i8.p0(ptr, i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v8i8.p0(<8 x i8>, ptr, i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <225 x float> @llvm.experimental.matrix.load.v225f32.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(read)
declare <450 x i16> @llvm.experimental.matrix.load.v450i16.p4(ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #1

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(write)
declare void @llvm.experimental.matrix.store.v225f32.p4(<225 x float>, ptr addrspace(4), i64, i1, i32, i32, metadata, metadata, metadata, metadata) #2

; Function Attrs: nocallback nofree nosync nounwind willreturn
declare <4 x i32> @llvm.experimental.matrix.extract.row.slice.v4i32.v16i32(<16 x i32>, i32, i32, i32 immarg, i32 immarg, i32 immarg, metadata) #3

; Function Attrs: nocallback nofree nosync nounwind willreturn
declare <4 x i32> @llvm.experimental.matrix.extract.row.slice.v4i32.v15i32(<15 x i32>, i32, i32, i32 immarg, i32 immarg, i32 immarg, metadata) #3

; Function Attrs: nocallback nofree nosync nounwind willreturn
declare <16 x i32> @llvm.experimental.matrix.insert.row.slice.v16i32.v4i32(<16 x i32>, <4 x i32>, i32, i32, i32 immarg, i32 immarg, i32 immarg, metadata) #3

; Function Attrs: nocallback nofree nosync nounwind willreturn
declare <15 x i32> @llvm.experimental.matrix.insert.row.slice.v15i32.v4i32(<15 x i32>, <4 x i32>, i32, i32, i32 immarg, i32 immarg, i32 immarg, metadata) #3

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <16 x i8> @llvm.experimental.matrix.fill.v16i8.i8(i8, i32, i32, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <16 x i16> @llvm.experimental.matrix.fill.v16i16.i16(i16, i32, i32, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <16 x float> @llvm.experimental.matrix.fill.v16f32.f32(float, i32, i32, metadata, metadata, metadata) #0

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none)
declare <16 x i32> @llvm.experimental.matrix.fill.v16i32.i32(i32, i32, i32, metadata, metadata, metadata) #0

attributes #0 = { nocallback nofree nosync nounwind willreturn memory(none) }
attributes #1 = { nocallback nofree nosync nounwind willreturn memory(read) }
attributes #2 = { nocallback nofree nosync nounwind willreturn memory(write) }
attributes #3 = { nocallback nofree nosync nounwind willreturn }
