; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p | FileCheck %s

define float @test_v2f32(<2 x float> %a0) {
; CHECK-LABEL: test_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vminss %xmm0, %xmm2, %xmm1
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovss %xmm2, %xmm1, %xmm1 {%k1}
; CHECK-NEXT:    vmovaps %xmm1, %xmm0
; CHECK-NEXT:    retq
  %1 = call float @llvm.vector.reduce.fmin.v2f32(<2 x float> %a0)
  ret float %1
}

define float @test_v4f32(<4 x float> %a0) {
; CHECK-LABEL: test_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vshufps {{.*#+}} xmm1 = xmm0[3,3,3,3]
; CHECK-NEXT:    vshufpd {{.*#+}} xmm2 = xmm0[1,0]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm0[1,1,3,3]
; CHECK-NEXT:    vminss %xmm0, %xmm3, %xmm4
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovss %xmm3, %xmm4, %xmm4 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm4, %xmm4, %k1
; CHECK-NEXT:    vminss %xmm4, %xmm2, %xmm0
; CHECK-NEXT:    vmovss %xmm2, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    retq
  %1 = call float @llvm.vector.reduce.fmin.v4f32(<4 x float> %a0)
  ret float %1
}

define float @test_v8f32(<8 x float> %a0) {
; CHECK-LABEL: test_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vshufps {{.*#+}} xmm2 = xmm1[3,3,3,3]
; CHECK-NEXT:    vshufpd {{.*#+}} xmm3 = xmm1[1,0]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm4 = xmm1[1,1,3,3]
; CHECK-NEXT:    vshufps {{.*#+}} xmm5 = xmm0[3,3,3,3]
; CHECK-NEXT:    vshufpd {{.*#+}} xmm6 = xmm0[1,0]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm7 = xmm0[1,1,3,3]
; CHECK-NEXT:    vminss %xmm0, %xmm7, %xmm8
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovss %xmm7, %xmm8, %xmm8 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm8, %xmm8, %k1
; CHECK-NEXT:    vminss %xmm8, %xmm6, %xmm0
; CHECK-NEXT:    vmovss %xmm6, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm5, %xmm0
; CHECK-NEXT:    vmovss %xmm5, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm4, %xmm0
; CHECK-NEXT:    vmovss %xmm4, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm3, %xmm0
; CHECK-NEXT:    vmovss %xmm3, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordss %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminss %xmm0, %xmm2, %xmm0
; CHECK-NEXT:    vmovss %xmm2, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
  %1 = call float @llvm.vector.reduce.fmin.v8f32(<8 x float> %a0)
  ret float %1
}

define double @test_v2f64(<2 x double> %a0) {
; CHECK-LABEL: test_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vshufpd {{.*#+}} xmm2 = xmm0[1,0]
; CHECK-NEXT:    vminsd %xmm0, %xmm2, %xmm1
; CHECK-NEXT:    vcmpunordsd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovsd %xmm2, %xmm1, %xmm1 {%k1}
; CHECK-NEXT:    vmovapd %xmm1, %xmm0
; CHECK-NEXT:    retq
  %1 = call double @llvm.vector.reduce.fmin.v2f64(<2 x double> %a0)
  ret double %1
}

define double @test_v3f64(<3 x double> %a0) {
; CHECK-LABEL: test_v3f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; CHECK-NEXT:    vminsd %xmm0, %xmm1, %xmm2
; CHECK-NEXT:    vcmpunordsd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovsd %xmm1, %xmm2, %xmm2 {%k1}
; CHECK-NEXT:    vcmpunordsd %xmm2, %xmm2, %k1
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vminsd %xmm2, %xmm1, %xmm0
; CHECK-NEXT:    vmovsd %xmm1, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
  %1 = call double @llvm.vector.reduce.fmin.v3f64(<3 x double> %a0)
  ret double %1
}

define double @test_v4f64(<4 x double> %a0) {
; CHECK-LABEL: test_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vshufpd {{.*#+}} xmm2 = xmm1[1,0]
; CHECK-NEXT:    vshufpd {{.*#+}} xmm3 = xmm0[1,0]
; CHECK-NEXT:    vminsd %xmm0, %xmm3, %xmm4
; CHECK-NEXT:    vcmpunordsd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovsd %xmm3, %xmm4, %xmm4 {%k1}
; CHECK-NEXT:    vcmpunordsd %xmm4, %xmm4, %k1
; CHECK-NEXT:    vminsd %xmm4, %xmm1, %xmm0
; CHECK-NEXT:    vmovsd %xmm1, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vcmpunordsd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vminsd %xmm0, %xmm2, %xmm0
; CHECK-NEXT:    vmovsd %xmm2, %xmm0, %xmm0 {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
  %1 = call double @llvm.vector.reduce.fmin.v4f64(<4 x double> %a0)
  ret double %1
}
declare float @llvm.vector.reduce.fmin.v2f32(<2 x float>)
declare float @llvm.vector.reduce.fmin.v4f32(<4 x float>)
declare float @llvm.vector.reduce.fmin.v8f32(<8 x float>)
declare double @llvm.vector.reduce.fmin.v2f64(<2 x double>)
declare double @llvm.vector.reduce.fmin.v3f64(<3 x double>)
declare double @llvm.vector.reduce.fmin.v4f64(<4 x double>)
