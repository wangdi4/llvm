; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p -show-mc-encoding | FileCheck %s --check-prefixes=AVX256P

define dso_local <4 x i64> @test_mm256_mask_gf2p8affineinv_epi64_epi8(<4 x i64> noundef %S, i32 noundef %U, <4 x i64> noundef %A, <4 x i64> noundef %B) #0 {
; AVX256P-LABEL: test_mm256_mask_gf2p8affineinv_epi64_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 16
; AVX256P-NEXT:    .cfi_offset %rbp, -16
; AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; AVX256P-NEXT:    andq $-32, %rsp # encoding: [0x48,0x83,0xe4,0xe0]
; AVX256P-NEXT:    subq $160, %rsp # encoding: [0x48,0x81,0xec,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    movl %edi, {{[0-9]+}}(%rsp) # encoding: [0x89,0x7c,0x24,0x1c]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x4c,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x54,0x24,0x20]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4c,0x24,0x40]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x44,0x24,0x60]
; AVX256P-NEXT:    kmovd {{[0-9]+}}(%rsp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x1c]
; AVX256P-NEXT:    vgf2p8affineinvqb $1, {{[0-9]+}}(%rsp), %ymm1, %ymm0 {%k1} # encoding: [0x62,0xf3,0xf5,0x29,0xcf,0x44,0x24,0x01,0x01]
; AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %S.addr = alloca <4 x i64>, align 32
  %U.addr = alloca i32, align 4
  %A.addr = alloca <4 x i64>, align 32
  %B.addr = alloca <4 x i64>, align 32
  store <4 x i64> %S, ptr %S.addr, align 32
  store i32 %U, ptr %U.addr, align 4
  store <4 x i64> %A, ptr %A.addr, align 32
  store <4 x i64> %B, ptr %B.addr, align 32
  %0 = load i32, ptr %U.addr, align 4
  %1 = load <4 x i64>, ptr %A.addr, align 32
  %2 = bitcast <4 x i64> %1 to <32 x i8>
  %3 = load <4 x i64>, ptr %B.addr, align 32
  %4 = bitcast <4 x i64> %3 to <32 x i8>
  %5 = call <32 x i8> @llvm.x86.vgf2p8affineinvqb.256(<32 x i8> %2, <32 x i8> %4, i8 1)
  %6 = bitcast <32 x i8> %5 to <4 x i64>
  %7 = bitcast <4 x i64> %6 to <32 x i8>
  %8 = load <4 x i64>, ptr %S.addr, align 32
  %9 = bitcast <4 x i64> %8 to <32 x i8>
  %10 = bitcast i32 %0 to <32 x i1>
  %11 = select <32 x i1> %10, <32 x i8> %7, <32 x i8> %9
  %12 = bitcast <32 x i8> %11 to <4 x i64>
  ret <4 x i64> %12
}

; Function Attrs: nounwind readnone
declare <32 x i8> @llvm.x86.vgf2p8affineinvqb.256(<32 x i8>, <32 x i8>, i8 immarg) #1

; Function Attrs: noinline nounwind optnone
define dso_local <4 x i64> @test_mm256_maskz_gf2p8affineinv_epi64_epi8(i32 noundef %U, <4 x i64> noundef %A, <4 x i64> noundef %B) #0 {
; AVX256P-LABEL: test_mm256_maskz_gf2p8affineinv_epi64_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 16
; AVX256P-NEXT:    .cfi_offset %rbp, -16
; AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; AVX256P-NEXT:    andq $-32, %rsp # encoding: [0x48,0x83,0xe4,0xe0]
; AVX256P-NEXT:    subq $160, %rsp # encoding: [0x48,0x81,0xec,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    movl %edi, {{[0-9]+}}(%rsp) # encoding: [0x89,0x7c,0x24,0x1c]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x4c,0x24,0x20]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4c,0x24,0x40]
; AVX256P-NEXT:    vxorps %xmm0, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x57,0xc0]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x44,0x24,0x60]
; AVX256P-NEXT:    kmovd {{[0-9]+}}(%rsp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x1c]
; AVX256P-NEXT:    vgf2p8affineinvqb $1, {{[0-9]+}}(%rsp), %ymm1, %ymm0 {%k1} # encoding: [0x62,0xf3,0xf5,0x29,0xcf,0x44,0x24,0x01,0x01]
; AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %.compoundliteral.i = alloca <4 x i64>, align 32
  %U.addr = alloca i32, align 4
  %A.addr = alloca <4 x i64>, align 32
  %B.addr = alloca <4 x i64>, align 32
  store i32 %U, ptr %U.addr, align 4
  store <4 x i64> %A, ptr %A.addr, align 32
  store <4 x i64> %B, ptr %B.addr, align 32
  %0 = load i32, ptr %U.addr, align 4
  %1 = load <4 x i64>, ptr %A.addr, align 32
  %2 = bitcast <4 x i64> %1 to <32 x i8>
  %3 = load <4 x i64>, ptr %B.addr, align 32
  %4 = bitcast <4 x i64> %3 to <32 x i8>
  %5 = call <32 x i8> @llvm.x86.vgf2p8affineinvqb.256(<32 x i8> %2, <32 x i8> %4, i8 1)
  %6 = bitcast <32 x i8> %5 to <4 x i64>
  %7 = bitcast <4 x i64> %6 to <32 x i8>
  store <4 x i64> zeroinitializer, ptr %.compoundliteral.i, align 32
  %8 = load <4 x i64>, ptr %.compoundliteral.i, align 32
  %9 = bitcast <4 x i64> %8 to <32 x i8>
  %10 = bitcast i32 %0 to <32 x i1>
  %11 = select <32 x i1> %10, <32 x i8> %7, <32 x i8> %9
  %12 = bitcast <32 x i8> %11 to <4 x i64>
  ret <4 x i64> %12
}

; Function Attrs: noinline nounwind optnone
define dso_local <2 x i64> @test_mm_mask_gf2p8affineinv_epi64_epi8(<2 x i64> noundef %S, i16 noundef zeroext %U, <2 x i64> noundef %A, <2 x i64> noundef %B) #2 {
; AVX256P-LABEL: test_mm_mask_gf2p8affineinv_epi64_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xe8]
; AVX256P-NEXT:    movw %di, -{{[0-9]+}}(%rsp) # encoding: [0x66,0x89,0x7c,0x24,0xc6]
; AVX256P-NEXT:    vmovaps %xmm1, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x4c,0x24,0xd8]
; AVX256P-NEXT:    vmovaps %xmm2, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x54,0x24,0xc8]
; AVX256P-NEXT:    vmovdqa -{{[0-9]+}}(%rsp), %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0x4c,0x24,0xd8]
; AVX256P-NEXT:    vmovdqa -{{[0-9]+}}(%rsp), %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0x44,0x24,0xe8]
; AVX256P-NEXT:    kmovw -{{[0-9]+}}(%rsp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0xc6]
; AVX256P-NEXT:    vgf2p8affineinvqb $1, -{{[0-9]+}}(%rsp), %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf3,0xf5,0x09,0xcf,0x84,0x24,0xc8,0xff,0xff,0xff,0x01]
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %S.addr = alloca <2 x i64>, align 16
  %U.addr = alloca i16, align 2
  %A.addr = alloca <2 x i64>, align 16
  %B.addr = alloca <2 x i64>, align 16
  store <2 x i64> %S, ptr %S.addr, align 16
  store i16 %U, ptr %U.addr, align 2
  store <2 x i64> %A, ptr %A.addr, align 16
  store <2 x i64> %B, ptr %B.addr, align 16
  %0 = load i16, ptr %U.addr, align 2
  %1 = load <2 x i64>, ptr %A.addr, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = load <2 x i64>, ptr %B.addr, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = call <16 x i8> @llvm.x86.vgf2p8affineinvqb.128(<16 x i8> %2, <16 x i8> %4, i8 1)
  %6 = bitcast <16 x i8> %5 to <2 x i64>
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  %8 = load <2 x i64>, ptr %S.addr, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %10 = bitcast i16 %0 to <16 x i1>
  %11 = select <16 x i1> %10, <16 x i8> %7, <16 x i8> %9
  %12 = bitcast <16 x i8> %11 to <2 x i64>
  ret <2 x i64> %12
}

; Function Attrs: nounwind readnone
declare <16 x i8> @llvm.x86.vgf2p8affineinvqb.128(<16 x i8>, <16 x i8>, i8 immarg) #1

; Function Attrs: noinline nounwind optnone
define dso_local <2 x i64> @test_mm_maskz_gf2p8affineinv_epi64_epi8(i16 noundef zeroext %U, <2 x i64> noundef %A, <2 x i64> noundef %B) #2 {
; AVX256P-LABEL: test_mm_maskz_gf2p8affineinv_epi64_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    movw %di, -{{[0-9]+}}(%rsp) # encoding: [0x66,0x89,0x7c,0x24,0xc6]
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xd8]
; AVX256P-NEXT:    vmovaps %xmm1, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x4c,0x24,0xc8]
; AVX256P-NEXT:    vmovdqa -{{[0-9]+}}(%rsp), %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0x4c,0x24,0xd8]
; AVX256P-NEXT:    vxorps %xmm0, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x57,0xc0]
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xe8]
; AVX256P-NEXT:    vmovdqa -{{[0-9]+}}(%rsp), %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0x44,0x24,0xe8]
; AVX256P-NEXT:    kmovw -{{[0-9]+}}(%rsp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0xc6]
; AVX256P-NEXT:    vgf2p8affineinvqb $1, -{{[0-9]+}}(%rsp), %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf3,0xf5,0x09,0xcf,0x84,0x24,0xc8,0xff,0xff,0xff,0x01]
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %.compoundliteral.i = alloca <2 x i64>, align 16
  %U.addr = alloca i16, align 2
  %A.addr = alloca <2 x i64>, align 16
  %B.addr = alloca <2 x i64>, align 16
  store i16 %U, ptr %U.addr, align 2
  store <2 x i64> %A, ptr %A.addr, align 16
  store <2 x i64> %B, ptr %B.addr, align 16
  %0 = load i16, ptr %U.addr, align 2
  %1 = load <2 x i64>, ptr %A.addr, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = load <2 x i64>, ptr %B.addr, align 16
  %4 = bitcast <2 x i64> %3 to <16 x i8>
  %5 = call <16 x i8> @llvm.x86.vgf2p8affineinvqb.128(<16 x i8> %2, <16 x i8> %4, i8 1)
  %6 = bitcast <16 x i8> %5 to <2 x i64>
  %7 = bitcast <2 x i64> %6 to <16 x i8>
  store <2 x i64> zeroinitializer, ptr %.compoundliteral.i, align 16
  %8 = load <2 x i64>, ptr %.compoundliteral.i, align 16
  %9 = bitcast <2 x i64> %8 to <16 x i8>
  %10 = bitcast i16 %0 to <16 x i1>
  %11 = select <16 x i1> %10, <16 x i8> %7, <16 x i8> %9
  %12 = bitcast <16 x i8> %11 to <2 x i64>
  ret <2 x i64> %12
}

