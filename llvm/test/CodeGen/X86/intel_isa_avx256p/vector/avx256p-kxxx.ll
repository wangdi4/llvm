; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+avx256p -show-mc-encoding | FileCheck %s --check-prefixes=AVX256P
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p -show-mc-encoding | FileCheck %s --check-prefixes=X64-AVX256P

define dso_local zeroext i8 @test_kadd_mask8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kadd_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xc3,0x04]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xca,0x04]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x48]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xd0,0x04]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x68]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x45,0x28]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xd9,0x04]
; AVX256P-NEXT:    vmovdqa 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 200(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k4 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xe0,0x04]
; AVX256P-NEXT:    vmovdqa 232(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xe9,0x04]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x4c,0x24,0x40]
; AVX256P-NEXT:    kshiftlb $4, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x04]
; AVX256P-NEXT:    korb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x45,0xc0]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xca,0x04]
; AVX256P-NEXT:    korb %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x45,0xc9]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    kaddb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x4a,0xc1]
; AVX256P-NEXT:    kshiftlb $4, %k4, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xcc,0x04]
; AVX256P-NEXT:    korb %k1, %k5, %k1 # encoding: [0xc5,0xd5,0x45,0xc9]
; AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kadd_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovdqa 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x45,0x50]
; X64-AVX256P-NEXT:    vmovdqa 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovdqa 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x55,0x10]
; X64-AVX256P-NEXT:    vmovdqa 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    kaddb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x4a,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm8, %ymm10, %k1 # encoding: [0x62,0xd3,0xad,0x28,0x1f,0xc8,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm9, %ymm11, %k2 # encoding: [0x62,0xd3,0xa5,0x28,0x1f,0xd1,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = load <8 x i64>, ptr %__F.addr, align 64
  %2 = load <8 x i64>, ptr %__A.addr, align 64
  %3 = load <8 x i64>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i64> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = load <8 x i64>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i64> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = call <8 x i1> @llvm.x86.avx512.kadd.b(<8 x i1> %12, <8 x i1> %13)
  %15 = bitcast <8 x i1> %14 to i8
  %16 = icmp ne <8 x i64> %0, %1
  %17 = bitcast i8 %15 to <8 x i1>
  %18 = and <8 x i1> %16, %17
  %19 = bitcast <8 x i1> %18 to i8
  ret i8 %19
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kadd_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kadd_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k2, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xca]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    kaddw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x4a,0xc1]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k1 # encoding: [0xc5,0xd5,0x4b,0xcc]
; AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kadd_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    kaddw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x4a,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = call <16 x i1> @llvm.x86.avx512.kadd.w(<16 x i1> %18, <16 x i1> %19)
  %21 = bitcast <16 x i1> %20 to i16
  %22 = icmp ne <16 x i32> %1, %3
  %23 = bitcast i16 %21 to <16 x i1>
  %24 = and <16 x i1> %22, %23
  %25 = bitcast <16 x i1> %24 to i16
  ret i16 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kadd_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kadd_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k2, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xca]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; AVX256P-NEXT:    kaddd %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x4a,0xc1]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k1 # encoding: [0xc5,0xd4,0x4b,0xcc]
; AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kadd_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; X64-AVX256P-NEXT:    kaddd %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x4a,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = call <32 x i1> @llvm.x86.avx512.kadd.d(<32 x i1> %18, <32 x i1> %19)
  %21 = bitcast <32 x i1> %20 to i32
  %22 = icmp ne <32 x i16> %1, %3
  %23 = bitcast i32 %21 to <32 x i1>
  %24 = and <32 x i1> %22, %23
  %25 = bitcast <32 x i1> %24 to i32
  ret i32 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kand_mask8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kand_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xc3,0x04]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xca,0x04]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x48]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xd0,0x04]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x68]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x45,0x28]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xd9,0x04]
; AVX256P-NEXT:    vmovdqa 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 200(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k4 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xe0,0x04]
; AVX256P-NEXT:    vmovdqa 232(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xe9,0x04]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x4c,0x24,0x40]
; AVX256P-NEXT:    kshiftlb $4, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x04]
; AVX256P-NEXT:    korb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x45,0xc0]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xca,0x04]
; AVX256P-NEXT:    korb %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x45,0xc9]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    kshiftlb $4, %k4, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd4,0x04]
; AVX256P-NEXT:    korb %k2, %k5, %k2 # encoding: [0xc5,0xd5,0x45,0xd2]
; AVX256P-NEXT:    kandb %k1, %k2, %k1 # encoding: [0xc5,0xed,0x41,0xc9]
; AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kand_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovdqa 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x45,0x50]
; X64-AVX256P-NEXT:    vmovdqa 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovdqa 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x55,0x10]
; X64-AVX256P-NEXT:    vmovdqa 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm8, %ymm10, %k2 # encoding: [0x62,0xd3,0xad,0x28,0x1f,0xd0,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm9, %ymm11, %k3 # encoding: [0x62,0xd3,0xa5,0x28,0x1f,0xd9,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k3, %k3 # encoding: [0xc4,0xe3,0x79,0x32,0xdb,0x04]
; X64-AVX256P-NEXT:    korb %k3, %k2, %k2 # encoding: [0xc5,0xed,0x45,0xd3]
; X64-AVX256P-NEXT:    kandb %k1, %k2, %k1 # encoding: [0xc5,0xed,0x41,0xc9]
; X64-AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = load <8 x i64>, ptr %__F.addr, align 64
  %2 = load <8 x i64>, ptr %__A.addr, align 64
  %3 = load <8 x i64>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i64> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = load <8 x i64>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i64> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = and <8 x i1> %12, %13
  %15 = bitcast <8 x i1> %14 to i8
  %16 = icmp ne <8 x i64> %0, %1
  %17 = bitcast i8 %15 to <8 x i1>
  %18 = and <8 x i1> %16, %17
  %19 = bitcast <8 x i1> %18 to i8
  ret i8 %19
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kand_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kand_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k2, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xca]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k2 # encoding: [0xc5,0xd5,0x4b,0xd4]
; AVX256P-NEXT:    kandw %k1, %k2, %k1 # encoding: [0xc5,0xec,0x41,0xc9]
; AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kand_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k2 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x54,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k2, %k3, %k2 # encoding: [0xc5,0xe5,0x4b,0xd2]
; X64-AVX256P-NEXT:    kandw %k1, %k2, %k1 # encoding: [0xc5,0xec,0x41,0xc9]
; X64-AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = and <16 x i1> %18, %19
  %21 = bitcast <16 x i1> %20 to i16
  %22 = icmp ne <16 x i32> %1, %3
  %23 = bitcast i16 %21 to <16 x i1>
  %24 = and <16 x i1> %22, %23
  %25 = bitcast <16 x i1> %24 to i16
  ret i16 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kand_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kand_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k2, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xca]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k2 # encoding: [0xc5,0xd4,0x4b,0xd4]
; AVX256P-NEXT:    kandd %k1, %k2, %k1 # encoding: [0xc4,0xe1,0xed,0x41,0xc9]
; AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kand_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x54,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k2, %k3, %k2 # encoding: [0xc5,0xe4,0x4b,0xd2]
; X64-AVX256P-NEXT:    kandd %k1, %k2, %k1 # encoding: [0xc4,0xe1,0xed,0x41,0xc9]
; X64-AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = and <32 x i1> %18, %19
  %21 = bitcast <32 x i1> %20 to i32
  %22 = icmp ne <32 x i16> %1, %3
  %23 = bitcast i32 %21 to <32 x i1>
  %24 = and <32 x i1> %22, %23
  %25 = bitcast <32 x i1> %24 to i32
  ret i32 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kandn_mask8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kandn_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqq %ymm3, %ymm1, %k0 # encoding: [0x62,0xf2,0xf5,0x28,0x29,0xc3]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqq %ymm2, %ymm0, %k2 # encoding: [0x62,0xf2,0xfd,0x28,0x29,0xd2]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xda,0x04]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x48]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k4 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xe0,0x04]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x68]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x45,0x28]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xe9,0x04]
; AVX256P-NEXT:    vmovdqa 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 200(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k6 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xf0,0x04]
; AVX256P-NEXT:    vmovdqa 232(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k7 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xf9,0x04]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x4c,0x24,0x40]
; AVX256P-NEXT:    kshiftlb $4, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x04]
; AVX256P-NEXT:    korb %k0, %k2, %k0 # encoding: [0xc5,0xed,0x45,0xc0]
; AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; AVX256P-NEXT:    korb %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x45,0xc9]
; AVX256P-NEXT:    kshiftlb $4, %k4, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd4,0x04]
; AVX256P-NEXT:    korb %k2, %k5, %k2 # encoding: [0xc5,0xd5,0x45,0xd2]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k2, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x54,0x24,0x3e]
; AVX256P-NEXT:    kshiftlb $4, %k6, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xce,0x04]
; AVX256P-NEXT:    korb %k1, %k7, %k1 # encoding: [0xc5,0xc5,0x45,0xc9]
; AVX256P-NEXT:    kandb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x41,0xca]
; AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kandn_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovdqa 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x45,0x50]
; X64-AVX256P-NEXT:    vmovdqa 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovdqa 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x55,0x10]
; X64-AVX256P-NEXT:    vmovdqa 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpeqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf2,0xfd,0x28,0x29,0xc2]
; X64-AVX256P-NEXT:    vpcmpeqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf2,0xf5,0x28,0x29,0xcb]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xca,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xd3,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k2 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xd6,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k3 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xdf,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k3, %k3 # encoding: [0xc4,0xe3,0x79,0x32,0xdb,0x04]
; X64-AVX256P-NEXT:    korb %k3, %k2, %k2 # encoding: [0xc5,0xed,0x45,0xd3]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k2, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x54,0x24,0x3e]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm8, %ymm10, %k1 # encoding: [0x62,0xd3,0xad,0x28,0x1f,0xc8,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm9, %ymm11, %k3 # encoding: [0x62,0xd3,0xa5,0x28,0x1f,0xd9,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k3, %k3 # encoding: [0xc4,0xe3,0x79,0x32,0xdb,0x04]
; X64-AVX256P-NEXT:    korb %k3, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xcb]
; X64-AVX256P-NEXT:    kandb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x41,0xca]
; X64-AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = load <8 x i64>, ptr %__F.addr, align 64
  %2 = load <8 x i64>, ptr %__A.addr, align 64
  %3 = load <8 x i64>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i64> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = load <8 x i64>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i64> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = xor <8 x i1> %12, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %15 = and <8 x i1> %14, %13
  %16 = bitcast <8 x i1> %15 to i8
  %17 = icmp ne <8 x i64> %0, %1
  %18 = bitcast i8 %16 to <8 x i1>
  %19 = and <8 x i1> %17, %18
  %20 = bitcast <8 x i1> %19 to i8
  ret i8 %20
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kandn_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kandn_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqd %ymm1, %ymm0, %k0 # encoding: [0x62,0xf1,0x7d,0x28,0x76,0xc1]
; AVX256P-NEXT:    vpcmpneqd %ymm1, %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0xc9,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqd %ymm1, %ymm0, %k2 # encoding: [0x62,0xf1,0x7d,0x28,0x76,0xd1]
; AVX256P-NEXT:    vpcmpneqd %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0xd9,0x04]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k6 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x74,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k7 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x7c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k2, %k0 # encoding: [0xc5,0xed,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xc9]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k2 # encoding: [0xc5,0xd5,0x4b,0xd4]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k2, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x54,0x24,0x3c]
; AVX256P-NEXT:    kunpckbw %k6, %k7, %k1 # encoding: [0xc5,0xc5,0x4b,0xce]
; AVX256P-NEXT:    kandw %k2, %k1, %k1 # encoding: [0xc5,0xf4,0x41,0xca]
; AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kandn_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xa4,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xac,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpeqd %ymm5, %ymm3, %k0 # encoding: [0x62,0xf1,0x65,0x28,0x76,0xc5]
; X64-AVX256P-NEXT:    vpcmpeqd %ymm4, %ymm2, %k1 # encoding: [0x62,0xf1,0x6d,0x28,0x76,0xcc]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vpcmpneqd %ymm5, %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0xcd,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd %ymm4, %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0xd4,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k2 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x54,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k3 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k2, %k3, %k2 # encoding: [0xc5,0xe5,0x4b,0xd2]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k2, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x54,0x24,0x3c]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandw %k2, %k1, %k1 # encoding: [0xc5,0xf4,0x41,0xca]
; X64-AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = xor <16 x i1> %18, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %21 = and <16 x i1> %20, %19
  %22 = bitcast <16 x i1> %21 to i16
  %23 = icmp ne <16 x i32> %1, %3
  %24 = bitcast i16 %22 to <16 x i1>
  %25 = and <16 x i1> %23, %24
  %26 = bitcast <16 x i1> %25 to i16
  ret i16 %26
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kandn_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kandn_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqw %ymm1, %ymm0, %k0 # encoding: [0x62,0xf1,0x7d,0x28,0x75,0xc1]
; AVX256P-NEXT:    vpcmpneqw %ymm1, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0xc9,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpeqw %ymm1, %ymm0, %k2 # encoding: [0x62,0xf1,0x7d,0x28,0x75,0xd1]
; AVX256P-NEXT:    vpcmpneqw %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0xd9,0x04]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k6 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x74,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k7 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x7c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k2, %k0 # encoding: [0xc5,0xec,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k1, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xc9]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k2 # encoding: [0xc5,0xd4,0x4b,0xd4]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k2, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x54,0x24,0x38]
; AVX256P-NEXT:    kunpckwd %k6, %k7, %k1 # encoding: [0xc5,0xc4,0x4b,0xce]
; AVX256P-NEXT:    kandd %k2, %k1, %k1 # encoding: [0xc4,0xe1,0xf5,0x41,0xca]
; AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kandn_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xa4,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xac,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpeqw %ymm5, %ymm3, %k0 # encoding: [0x62,0xf1,0x65,0x28,0x75,0xc5]
; X64-AVX256P-NEXT:    vpcmpeqw %ymm4, %ymm2, %k1 # encoding: [0x62,0xf1,0x6d,0x28,0x75,0xcc]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vpcmpneqw %ymm5, %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0xcd,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw %ymm4, %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0xd4,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k2 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x54,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k3 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x5c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k2, %k3, %k2 # encoding: [0xc5,0xe4,0x4b,0xd2]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k2, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x54,0x24,0x38]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandd %k2, %k1, %k1 # encoding: [0xc4,0xe1,0xf5,0x41,0xca]
; X64-AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = xor <32 x i1> %18, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %21 = and <32 x i1> %20, %19
  %22 = bitcast <32 x i1> %21 to i32
  %23 = icmp ne <32 x i16> %1, %3
  %24 = bitcast i32 %22 to <32 x i1>
  %25 = and <32 x i1> %23, %24
  %26 = bitcast <32 x i1> %25 to i32
  ret i32 %26
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_knot_mask8(i8 noundef zeroext %a) #1 {
; AVX256P-LABEL: test_knot_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %eax # encoding: [0x50]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x08]
; AVX256P-NEXT:    movb %al, {{[0-9]+}}(%esp) # encoding: [0x88,0x44,0x24,0x03]
; AVX256P-NEXT:    notb %al # encoding: [0xf6,0xd0]
; AVX256P-NEXT:    popl %ecx # encoding: [0x59]
; AVX256P-NEXT:    .cfi_def_cfa_offset 4
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_knot_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    movl %edi, %eax # encoding: [0x89,0xf8]
; X64-AVX256P-NEXT:    movb %al, -{{[0-9]+}}(%rsp) # encoding: [0x88,0x44,0x24,0xfe]
; X64-AVX256P-NEXT:    movb %al, -{{[0-9]+}}(%rsp) # encoding: [0x88,0x44,0x24,0xff]
; X64-AVX256P-NEXT:    notb %al # encoding: [0xf6,0xd0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__M.addr.i = alloca i8, align 1
  %a.addr = alloca i8, align 1
  store i8 %a, ptr %a.addr, align 1
  %0 = load i8, ptr %a.addr, align 1
  store i8 %0, ptr %__M.addr.i, align 1
  %1 = load i8, ptr %__M.addr.i, align 1
  %2 = bitcast i8 %1 to <8 x i1>
  %3 = xor <8 x i1> %2, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %4 = bitcast <8 x i1> %3 to i8
  ret i8 %4
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_mm512_knot(i16 noundef zeroext %a) #1 {
; AVX256P-LABEL: test_mm512_knot:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %eax # encoding: [0x50]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; AVX256P-NEXT:    movw %ax, {{[0-9]+}}(%esp) # encoding: [0x66,0x89,0x44,0x24,0x02]
; AVX256P-NEXT:    notl %eax # encoding: [0xf7,0xd0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    popl %ecx # encoding: [0x59]
; AVX256P-NEXT:    .cfi_def_cfa_offset 4
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_mm512_knot:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    movl %edi, %eax # encoding: [0x89,0xf8]
; X64-AVX256P-NEXT:    movw %ax, -{{[0-9]+}}(%rsp) # encoding: [0x66,0x89,0x44,0x24,0xfc]
; X64-AVX256P-NEXT:    movw %ax, -{{[0-9]+}}(%rsp) # encoding: [0x66,0x89,0x44,0x24,0xfe]
; X64-AVX256P-NEXT:    notl %eax # encoding: [0xf7,0xd0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__M.addr.i = alloca i16, align 2
  %a.addr = alloca i16, align 2
  store i16 %a, ptr %a.addr, align 2
  %0 = load i16, ptr %a.addr, align 2
  store i16 %0, ptr %__M.addr.i, align 2
  %1 = load i16, ptr %__M.addr.i, align 2
  %2 = bitcast i16 %1 to <16 x i1>
  %3 = xor <16 x i1> %2, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %4 = bitcast <16 x i1> %3 to i16
  ret i16 %4
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_knot_mask32(i32 noundef %a) #1 {
; AVX256P-LABEL: test_knot_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %eax # encoding: [0x50]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; AVX256P-NEXT:    movl %eax, (%esp) # encoding: [0x89,0x04,0x24]
; AVX256P-NEXT:    notl %eax # encoding: [0xf7,0xd0]
; AVX256P-NEXT:    popl %ecx # encoding: [0x59]
; AVX256P-NEXT:    .cfi_def_cfa_offset 4
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_knot_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    movl %edi, %eax # encoding: [0x89,0xf8]
; X64-AVX256P-NEXT:    movl %edi, -{{[0-9]+}}(%rsp) # encoding: [0x89,0x7c,0x24,0xf8]
; X64-AVX256P-NEXT:    movl %edi, -{{[0-9]+}}(%rsp) # encoding: [0x89,0x7c,0x24,0xfc]
; X64-AVX256P-NEXT:    notl %eax # encoding: [0xf7,0xd0]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__M.addr.i = alloca i32, align 4
  %a.addr = alloca i32, align 4
  store i32 %a, ptr %a.addr, align 4
  %0 = load i32, ptr %a.addr, align 4
  store i32 %0, ptr %__M.addr.i, align 4
  %1 = load i32, ptr %__M.addr.i, align 4
  %2 = bitcast i32 %1 to <32 x i1>
  %3 = xor <32 x i1> %2, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %4 = bitcast <32 x i1> %3 to i32
  ret i32 %4
}

define void @mask32_mem(ptr %ptr) {
; AVX256P-LABEL: mask32_mem:
; AVX256P:       # %bb.0:
; AVX256P-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; AVX256P-NEXT:    kmovd (%eax), %k0 # encoding: [0xc4,0xe1,0xf9,0x90,0x00]
; AVX256P-NEXT:    knotd %k0, %k0 # encoding: [0xc4,0xe1,0xf9,0x44,0xc0]
; AVX256P-NEXT:    kmovd %k0, (%eax) # encoding: [0xc4,0xe1,0xf9,0x91,0x00]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: mask32_mem:
; X64-AVX256P:       # %bb.0:
; X64-AVX256P-NEXT:    kmovd (%rdi), %k0 # encoding: [0xc4,0xe1,0xf9,0x90,0x07]
; X64-AVX256P-NEXT:    knotd %k0, %k0 # encoding: [0xc4,0xe1,0xf9,0x44,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, (%rdi) # encoding: [0xc4,0xe1,0xf9,0x91,0x07]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
  %x = load i32, ptr %ptr, align 4
  %m0 = bitcast i32 %x to <32 x i1>
  %m1 = xor <32 x i1> %m0, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true,
                            i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true,
                            i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true,
                            i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %ret = bitcast <32 x i1> %m1 to i32
  store i32 %ret, ptr %ptr, align 4
  ret void
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kor_mask8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kor_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xc3,0x04]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xca,0x04]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x48]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xd0,0x04]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x68]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x45,0x28]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xd9,0x04]
; AVX256P-NEXT:    vmovdqa 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 200(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k4 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xe0,0x04]
; AVX256P-NEXT:    vmovdqa 232(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xe9,0x04]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x4c,0x24,0x40]
; AVX256P-NEXT:    kshiftlb $4, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x04]
; AVX256P-NEXT:    korb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x45,0xc0]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xca,0x04]
; AVX256P-NEXT:    korb %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x45,0xc9]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; AVX256P-NEXT:    kshiftlb $4, %k4, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xcc,0x04]
; AVX256P-NEXT:    korb %k1, %k5, %k1 # encoding: [0xc5,0xd5,0x45,0xc9]
; AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kor_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovdqa 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x45,0x50]
; X64-AVX256P-NEXT:    vmovdqa 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovdqa 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x55,0x10]
; X64-AVX256P-NEXT:    vmovdqa 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm8, %ymm10, %k1 # encoding: [0x62,0xd3,0xad,0x28,0x1f,0xc8,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm9, %ymm11, %k2 # encoding: [0x62,0xd3,0xa5,0x28,0x1f,0xd1,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = load <8 x i64>, ptr %__F.addr, align 64
  %2 = load <8 x i64>, ptr %__A.addr, align 64
  %3 = load <8 x i64>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i64> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = load <8 x i64>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i64> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = or <8 x i1> %12, %13
  %15 = bitcast <8 x i1> %14 to i8
  %16 = icmp ne <8 x i64> %0, %1
  %17 = bitcast i8 %15 to <8 x i1>
  %18 = and <8 x i1> %16, %17
  %19 = bitcast <8 x i1> %18 to i8
  ret i8 %19
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kor_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kor_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k2, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xca]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    korw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x45,0xc1]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k1 # encoding: [0xc5,0xd5,0x4b,0xcc]
; AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kor_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    korw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = or <16 x i1> %18, %19
  %21 = bitcast <16 x i1> %20 to i16
  %22 = icmp ne <16 x i32> %1, %3
  %23 = bitcast i16 %21 to <16 x i1>
  %24 = and <16 x i1> %22, %23
  %25 = bitcast <16 x i1> %24 to i16
  ret i16 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kor_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kor_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k2, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xca]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; AVX256P-NEXT:    kord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x45,0xc1]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k1 # encoding: [0xc5,0xd4,0x4b,0xcc]
; AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kor_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; X64-AVX256P-NEXT:    kord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = or <32 x i1> %18, %19
  %21 = bitcast <32 x i1> %20 to i32
  %22 = icmp ne <32 x i16> %1, %3
  %23 = bitcast i32 %21 to <32 x i1>
  %24 = and <32 x i1> %22, %23
  %25 = bitcast <32 x i1> %24 to i32
  ret i32 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kxor_mask8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kxor_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xc3,0x04]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xca,0x04]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x48]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k2 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xd0,0x04]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x4d,0x68]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x45,0x28]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xd9,0x04]
; AVX256P-NEXT:    vmovdqa 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 200(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm0, %ymm1, %k4 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xe0,0x04]
; AVX256P-NEXT:    vmovdqa 232(%ebp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8d,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqq %ymm1, %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xe9,0x04]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x4c,0x24,0x40]
; AVX256P-NEXT:    kshiftlb $4, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x04]
; AVX256P-NEXT:    korb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x45,0xc0]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xca,0x04]
; AVX256P-NEXT:    korb %k1, %k3, %k1 # encoding: [0xc5,0xe5,0x45,0xc9]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    kxorb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x47,0xc1]
; AVX256P-NEXT:    kshiftlb $4, %k4, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xcc,0x04]
; AVX256P-NEXT:    korb %k1, %k5, %k1 # encoding: [0xc5,0xd5,0x45,0xc9]
; AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxor_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovdqa 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x45,0x50]
; X64-AVX256P-NEXT:    vmovdqa 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovdqa 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x55,0x10]
; X64-AVX256P-NEXT:    vmovdqa 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x6f,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7d,0x7f,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    kxorb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x47,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm8, %ymm10, %k1 # encoding: [0x62,0xd3,0xad,0x28,0x1f,0xc8,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm9, %ymm11, %k2 # encoding: [0x62,0xd3,0xa5,0x28,0x1f,0xd1,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kandb %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = load <8 x i64>, ptr %__F.addr, align 64
  %2 = load <8 x i64>, ptr %__A.addr, align 64
  %3 = load <8 x i64>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i64> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = load <8 x i64>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i64> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = xor <8 x i1> %12, %13
  %15 = bitcast <8 x i1> %14 to i8
  %16 = icmp ne <8 x i64> %0, %1
  %17 = bitcast i8 %15 to <8 x i1>
  %18 = and <8 x i1> %16, %17
  %19 = bitcast <8 x i1> %18 to i8
  ret i8 %19
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kxor_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kxor_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k2, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xca]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    kxorw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x47,0xc1]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k1 # encoding: [0xc5,0xd5,0x4b,0xcc]
; AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxor_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    kxorw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x47,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = xor <16 x i1> %18, %19
  %21 = bitcast <16 x i1> %20 to i16
  %22 = icmp ne <16 x i32> %1, %3
  %23 = bitcast i16 %21 to <16 x i1>
  %24 = and <16 x i1> %22, %23
  %25 = bitcast <16 x i1> %24 to i16
  ret i16 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kxor_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kxor_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k2, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xca]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; AVX256P-NEXT:    kxord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x47,0xc1]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k1 # encoding: [0xc5,0xd4,0x4b,0xcc]
; AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxor_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; X64-AVX256P-NEXT:    kxord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x47,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandd %k0, %k1, %k0 # encoding: [0xc4,0xe1,0xf5,0x41,0xc0]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = xor <32 x i1> %18, %19
  %21 = bitcast <32 x i1> %20 to i32
  %22 = icmp ne <32 x i16> %1, %3
  %23 = bitcast i32 %21 to <32 x i1>
  %24 = and <32 x i1> %22, %23
  %25 = bitcast <32 x i1> %24 to i32
  ret i32 %25
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_ktest_mask8_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_ktest_mask8_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x68]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x6d,0x28]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x75,0x48]
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x7d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x64,0x24,0x60]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x5c,0x24,0x40]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcf,0x04]
; AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm5, %k1 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xcb,0x04]
; AVX256P-NEXT:    vpcmpneqq %ymm4, %ymm6, %k2 # encoding: [0x62,0xf3,0xcd,0x28,0x1f,0xd4,0x04]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x2f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x2e]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    ktestb %k1, %k0 # encoding: [0xc5,0xf9,0x99,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    kmovb {{[0-9]+}}(%esp), %k0 # encoding: [0xc5,0xf9,0x90,0x44,0x24,0x2f]
; AVX256P-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x2e]
; AVX256P-NEXT:    ktestb %k1, %k0 # encoding: [0xc5,0xf9,0x99,0xc1]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_ktest_mask8_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x2f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x2e]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    ktestb %k1, %k0 # encoding: [0xc5,0xf9,0x99,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    kmovb {{[0-9]+}}(%rsp), %k0 # encoding: [0xc5,0xf9,0x90,0x44,0x24,0x2f]
; X64-AVX256P-NEXT:    kmovb {{[0-9]+}}(%rsp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x2e]
; X64-AVX256P-NEXT:    ktestb %k1, %k0 # encoding: [0xc5,0xf9,0x99,0xc1]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = load <8 x i64>, ptr %__B.addr, align 64
  %2 = icmp ne <8 x i64> %0, %1
  %3 = bitcast <8 x i1> %2 to i8
  %4 = load <8 x i64>, ptr %__C.addr, align 64
  %5 = load <8 x i64>, ptr %__D.addr, align 64
  %6 = icmp ne <8 x i64> %4, %5
  %7 = bitcast <8 x i1> %6 to i8
  %8 = load ptr, ptr %CF.addr, align 8
  store i8 %3, ptr %__A.addr.i, align 1
  store i8 %7, ptr %__B.addr.i, align 1
  store ptr %8, ptr %__C.addr.i, align 8
  %9 = load i8, ptr %__A.addr.i, align 1
  %10 = load i8, ptr %__B.addr.i, align 1
  %11 = bitcast i8 %9 to <8 x i1>
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = call i32 @llvm.x86.avx512.ktestc.b(<8 x i1> %11, <8 x i1> %12)
  %conv.i = trunc i32 %13 to i8
  %14 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %14, align 1
  %15 = load i8, ptr %__A.addr.i, align 1
  %16 = load i8, ptr %__B.addr.i, align 1
  %17 = bitcast i8 %15 to <8 x i1>
  %18 = bitcast i8 %16 to <8 x i1>
  %19 = call i32 @llvm.x86.avx512.ktestz.b(<8 x i1> %17, <8 x i1> %18)
  %conv1.i = trunc i32 %19 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_ktest_mask16_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_ktest_mask16_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x5d,0x68]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x6d,0x28]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x75,0x48]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x7d,0x08]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x5c,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x64,0x24,0x60]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm1, %k0 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x44,0x24,0x06,0x04]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x07,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x2e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x2c]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    ktestw %k1, %k0 # encoding: [0xc5,0xf8,0x99,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    kmovw {{[0-9]+}}(%esp), %k0 # encoding: [0xc5,0xf8,0x90,0x44,0x24,0x2e]
; AVX256P-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x2c]
; AVX256P-NEXT:    ktestw %k1, %k0 # encoding: [0xc5,0xf8,0x99,0xc1]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_ktest_mask16_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k0 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x44,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x2e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x2c]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    ktestw %k1, %k0 # encoding: [0xc5,0xf8,0x99,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    kmovw {{[0-9]+}}(%rsp), %k0 # encoding: [0xc5,0xf8,0x90,0x44,0x24,0x2e]
; X64-AVX256P-NEXT:    kmovw {{[0-9]+}}(%rsp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x2c]
; X64-AVX256P-NEXT:    ktestw %k1, %k0 # encoding: [0xc5,0xf8,0x99,0xc1]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__B.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = icmp ne <16 x i32> %1, %3
  %5 = bitcast <16 x i1> %4 to i16
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = load <8 x i64>, ptr %__D.addr, align 64
  %9 = bitcast <8 x i64> %8 to <16 x i32>
  %10 = icmp ne <16 x i32> %7, %9
  %11 = bitcast <16 x i1> %10 to i16
  %12 = load ptr, ptr %CF.addr, align 8
  store i16 %5, ptr %__A.addr.i, align 2
  store i16 %11, ptr %__B.addr.i, align 2
  store ptr %12, ptr %__C.addr.i, align 8
  %13 = load i16, ptr %__A.addr.i, align 2
  %14 = load i16, ptr %__B.addr.i, align 2
  %15 = bitcast i16 %13 to <16 x i1>
  %16 = bitcast i16 %14 to <16 x i1>
  %17 = call i32 @llvm.x86.avx512.ktestc.w(<16 x i1> %15, <16 x i1> %16)
  %conv.i = trunc i32 %17 to i8
  %18 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %18, align 1
  %19 = load i16, ptr %__A.addr.i, align 2
  %20 = load i16, ptr %__B.addr.i, align 2
  %21 = bitcast i16 %19 to <16 x i1>
  %22 = bitcast i16 %20 to <16 x i1>
  %23 = call i32 @llvm.x86.avx512.ktestz.w(<16 x i1> %21, <16 x i1> %22)
  %conv1.i = trunc i32 %23 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_ktest_mask32_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_ktest_mask32_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x5d,0x68]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x6d,0x28]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x75,0x48]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x7d,0x08]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x5c,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x64,0x24,0x60]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x44,0x24,0x06,0x04]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x07,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x2c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x28]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    ktestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x99,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    kmovd {{[0-9]+}}(%esp), %k0 # encoding: [0xc4,0xe1,0xf9,0x90,0x44,0x24,0x2c]
; AVX256P-NEXT:    kmovd {{[0-9]+}}(%esp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x28]
; AVX256P-NEXT:    ktestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x99,0xc1]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_ktest_mask32_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x44,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x2c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x28]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    ktestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x99,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    kmovd {{[0-9]+}}(%rsp), %k0 # encoding: [0xc4,0xe1,0xf9,0x90,0x44,0x24,0x2c]
; X64-AVX256P-NEXT:    kmovd {{[0-9]+}}(%rsp), %k1 # encoding: [0xc4,0xe1,0xf9,0x90,0x4c,0x24,0x28]
; X64-AVX256P-NEXT:    ktestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x99,0xc1]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__B.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = icmp ne <32 x i16> %1, %3
  %5 = bitcast <32 x i1> %4 to i32
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = load <8 x i64>, ptr %__D.addr, align 64
  %9 = bitcast <8 x i64> %8 to <32 x i16>
  %10 = icmp ne <32 x i16> %7, %9
  %11 = bitcast <32 x i1> %10 to i32
  %12 = load ptr, ptr %CF.addr, align 8
  store i32 %5, ptr %__A.addr.i, align 4
  store i32 %11, ptr %__B.addr.i, align 4
  store ptr %12, ptr %__C.addr.i, align 8
  %13 = load i32, ptr %__A.addr.i, align 4
  %14 = load i32, ptr %__B.addr.i, align 4
  %15 = bitcast i32 %13 to <32 x i1>
  %16 = bitcast i32 %14 to <32 x i1>
  %17 = call i32 @llvm.x86.avx512.ktestc.d(<32 x i1> %15, <32 x i1> %16)
  %conv.i = trunc i32 %17 to i8
  %18 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %18, align 1
  %19 = load i32, ptr %__A.addr.i, align 4
  %20 = load i32, ptr %__B.addr.i, align 4
  %21 = bitcast i32 %19 to <32 x i1>
  %22 = bitcast i32 %20 to <32 x i1>
  %23 = call i32 @llvm.x86.avx512.ktestz.d(<32 x i1> %21, <32 x i1> %22)
  %conv1.i = trunc i32 %23 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kortest_mask8_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_kortest_mask8_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x5d,0x68]
; AVX256P-NEXT:    vmovdqa 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x6d,0x28]
; AVX256P-NEXT:    vmovdqa 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x75,0x48]
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x7d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x64,0x24,0x60]
; AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x5c,0x24,0x40]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcf,0x04]
; AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm5, %k1 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xcb,0x04]
; AVX256P-NEXT:    vpcmpneqq %ymm4, %ymm6, %k2 # encoding: [0x62,0xf3,0xcd,0x28,0x1f,0xd4,0x04]
; AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x2f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x2e]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    kortestb %k1, %k0 # encoding: [0xc5,0xf9,0x98,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    movzbl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x2f]
; AVX256P-NEXT:    orb {{[0-9]+}}(%esp), %al # encoding: [0x0a,0x44,0x24,0x2e]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kortest_mask8_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm2, %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x1f,0xc2,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm3, %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x04]
; X64-AVX256P-NEXT:    korb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x45,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm6, %ymm4, %k1 # encoding: [0x62,0xf3,0xdd,0x28,0x1f,0xce,0x04]
; X64-AVX256P-NEXT:    vpcmpneqq %ymm7, %ymm5, %k2 # encoding: [0x62,0xf3,0xd5,0x28,0x1f,0xd7,0x04]
; X64-AVX256P-NEXT:    kshiftlb $4, %k2, %k2 # encoding: [0xc4,0xe3,0x79,0x32,0xd2,0x04]
; X64-AVX256P-NEXT:    korb %k2, %k1, %k1 # encoding: [0xc5,0xf5,0x45,0xca]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x2f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x2e]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    kortestb %k1, %k0 # encoding: [0xc5,0xf9,0x98,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    movzbl {{[0-9]+}}(%rsp), %eax # encoding: [0x0f,0xb6,0x44,0x24,0x2f]
; X64-AVX256P-NEXT:    orb {{[0-9]+}}(%rsp), %al # encoding: [0x0a,0x44,0x24,0x2e]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = load <8 x i64>, ptr %__B.addr, align 64
  %2 = icmp ne <8 x i64> %0, %1
  %3 = bitcast <8 x i1> %2 to i8
  %4 = load <8 x i64>, ptr %__C.addr, align 64
  %5 = load <8 x i64>, ptr %__D.addr, align 64
  %6 = icmp ne <8 x i64> %4, %5
  %7 = bitcast <8 x i1> %6 to i8
  %8 = load ptr, ptr %CF.addr, align 8
  store i8 %3, ptr %__A.addr.i, align 1
  store i8 %7, ptr %__B.addr.i, align 1
  store ptr %8, ptr %__C.addr.i, align 8
  %9 = load i8, ptr %__A.addr.i, align 1
  %10 = load i8, ptr %__B.addr.i, align 1
  %11 = bitcast i8 %9 to <8 x i1>
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = or <8 x i1> %11, %12
  %14 = bitcast <8 x i1> %13 to i8
  %15 = icmp eq i8 %14, -1
  %16 = zext i1 %15 to i32
  %conv.i = trunc i32 %16 to i8
  %17 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %17, align 1
  %18 = load i8, ptr %__A.addr.i, align 1
  %19 = load i8, ptr %__B.addr.i, align 1
  %20 = bitcast i8 %18 to <8 x i1>
  %21 = bitcast i8 %19 to <8 x i1>
  %22 = or <8 x i1> %20, %21
  %23 = bitcast <8 x i1> %22 to i8
  %24 = icmp eq i8 %23, 0
  %25 = zext i1 %24 to i32
  %conv1.i = trunc i32 %25 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kortest_mask16_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_kortest_mask16_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x5d,0x68]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x6d,0x28]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x75,0x48]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x7d,0x08]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x5c,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x64,0x24,0x60]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm1, %k0 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x44,0x24,0x06,0x04]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x07,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x2e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x2c]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    kortestw %k1, %k0 # encoding: [0xc5,0xf8,0x98,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    movzwl {{[0-9]+}}(%esp), %eax # encoding: [0x0f,0xb7,0x44,0x24,0x2e]
; AVX256P-NEXT:    orw {{[0-9]+}}(%esp), %ax # encoding: [0x66,0x0b,0x44,0x24,0x2c]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kortest_mask16_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k0 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x44,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x2e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x2c]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    kortestw %k1, %k0 # encoding: [0xc5,0xf8,0x98,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    movzwl {{[0-9]+}}(%rsp), %eax # encoding: [0x0f,0xb7,0x44,0x24,0x2e]
; X64-AVX256P-NEXT:    orw {{[0-9]+}}(%rsp), %ax # encoding: [0x66,0x0b,0x44,0x24,0x2c]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__B.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = icmp ne <16 x i32> %1, %3
  %5 = bitcast <16 x i1> %4 to i16
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = load <8 x i64>, ptr %__D.addr, align 64
  %9 = bitcast <8 x i64> %8 to <16 x i32>
  %10 = icmp ne <16 x i32> %7, %9
  %11 = bitcast <16 x i1> %10 to i16
  %12 = load ptr, ptr %CF.addr, align 8
  store i16 %5, ptr %__A.addr.i, align 2
  store i16 %11, ptr %__B.addr.i, align 2
  store ptr %12, ptr %__C.addr.i, align 8
  %13 = load i16, ptr %__A.addr.i, align 2
  %14 = load i16, ptr %__B.addr.i, align 2
  %15 = bitcast i16 %13 to <16 x i1>
  %16 = bitcast i16 %14 to <16 x i1>
  %17 = or <16 x i1> %15, %16
  %18 = bitcast <16 x i1> %17 to i16
  %19 = icmp eq i16 %18, -1
  %20 = zext i1 %19 to i32
  %conv.i = trunc i32 %20 to i8
  %21 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %21, align 1
  %22 = load i16, ptr %__A.addr.i, align 2
  %23 = load i16, ptr %__B.addr.i, align 2
  %24 = bitcast i16 %22 to <16 x i1>
  %25 = bitcast i16 %23 to <16 x i1>
  %26 = or <16 x i1> %24, %25
  %27 = bitcast <16 x i1> %26 to i16
  %28 = icmp eq i16 %27, 0
  %29 = zext i1 %28 to i32
  %conv1.i = trunc i32 %29 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kortest_mask32_u8(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, ptr noundef %CF) #0 {
; AVX256P-LABEL: test_kortest_mask32_u8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $384, %esp # encoding: [0x81,0xec,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x180
; AVX256P-NEXT:    movl 168(%ebp), %eax # encoding: [0x8b,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x5d,0x68]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xa5,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x6d,0x28]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm6 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x75,0x48]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm7 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x7d,0x08]
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x5c,0x24,0x40]
; AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x64,0x24,0x60]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x30]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x44,0x24,0x06,0x04]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x07,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x2c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x28]
; AVX256P-NEXT:    movl %eax, {{[0-9]+}}(%esp) # encoding: [0x89,0x44,0x24,0x38]
; AVX256P-NEXT:    kortestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x98,0xc1]
; AVX256P-NEXT:    setb (%eax) # encoding: [0x0f,0x92,0x00]
; AVX256P-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x2c]
; AVX256P-NEXT:    orl {{[0-9]+}}(%esp), %eax # encoding: [0x0b,0x44,0x24,0x28]
; AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kortest_mask32_u8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $384, %rsp # encoding: [0x48,0x81,0xec,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x180
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x74,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x7c,0x24,0x60]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x30]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k0 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x44,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x2c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x28]
; X64-AVX256P-NEXT:    movq %rdi, {{[0-9]+}}(%rsp) # encoding: [0x48,0x89,0x7c,0x24,0x38]
; X64-AVX256P-NEXT:    kortestd %k1, %k0 # encoding: [0xc4,0xe1,0xf9,0x98,0xc1]
; X64-AVX256P-NEXT:    setb (%rdi) # encoding: [0x0f,0x92,0x07]
; X64-AVX256P-NEXT:    movl {{[0-9]+}}(%rsp), %eax # encoding: [0x8b,0x44,0x24,0x2c]
; X64-AVX256P-NEXT:    orl {{[0-9]+}}(%rsp), %eax # encoding: [0x0b,0x44,0x24,0x28]
; X64-AVX256P-NEXT:    sete %al # encoding: [0x0f,0x94,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__C.addr.i = alloca ptr, align 8
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %CF.addr = alloca ptr, align 8
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store ptr %CF, ptr %CF.addr, align 8
  %0 = load <8 x i64>, ptr %__A.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__B.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = icmp ne <32 x i16> %1, %3
  %5 = bitcast <32 x i1> %4 to i32
  %6 = load <8 x i64>, ptr %__C.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = load <8 x i64>, ptr %__D.addr, align 64
  %9 = bitcast <8 x i64> %8 to <32 x i16>
  %10 = icmp ne <32 x i16> %7, %9
  %11 = bitcast <32 x i1> %10 to i32
  %12 = load ptr, ptr %CF.addr, align 8
  store i32 %5, ptr %__A.addr.i, align 4
  store i32 %11, ptr %__B.addr.i, align 4
  store ptr %12, ptr %__C.addr.i, align 8
  %13 = load i32, ptr %__A.addr.i, align 4
  %14 = load i32, ptr %__B.addr.i, align 4
  %15 = bitcast i32 %13 to <32 x i1>
  %16 = bitcast i32 %14 to <32 x i1>
  %17 = or <32 x i1> %15, %16
  %18 = bitcast <32 x i1> %17 to i32
  %19 = icmp eq i32 %18, -1
  %20 = zext i1 %19 to i32
  %conv.i = trunc i32 %20 to i8
  %21 = load ptr, ptr %__C.addr.i, align 8
  store i8 %conv.i, ptr %21, align 1
  %22 = load i32, ptr %__A.addr.i, align 4
  %23 = load i32, ptr %__B.addr.i, align 4
  %24 = bitcast i32 %22 to <32 x i1>
  %25 = bitcast i32 %23 to <32 x i1>
  %26 = or <32 x i1> %24, %25
  %27 = bitcast <32 x i1> %26 to i32
  %28 = icmp eq i32 %27, 0
  %29 = zext i1 %28 to i32
  %conv1.i = trunc i32 %29 to i8
  ret i8 %conv1.i
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i8 @test_kxnor_mask8(<8 x i32> noundef %__A, <8 x i32> noundef %__B, <8 x i32> noundef %__C, <8 x i32> noundef %__D, <8 x i32> noundef %__E, <8 x i32> noundef %__F) #0 {
; AVX256P-LABEL: test_kxnor_mask8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $448, %esp # encoding: [0x81,0xec,0xc0,0x01,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x1C0
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x5d,0x48]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x65,0x28]
; AVX256P-NEXT:    vmovdqa 8(%ebp), %ymm5 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x6d,0x08]
; AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovdqa %ymm5, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0xac,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x5c,0x24,0x40]
; AVX256P-NEXT:    vpcmpneqd %ymm1, %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0xc1,0x04]
; AVX256P-NEXT:    vpcmpneqd %ymm5, %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0xcd,0x04]
; AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; AVX256P-NEXT:    kxnorb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x46,0xc1]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxnor_mask8:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $448, %rsp # encoding: [0x48,0x81,0xec,0xc0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x1C0
; X64-AVX256P-NEXT:    vmovdqa %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x8c,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x94,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x9c,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x6c,0x24,0x40]
; X64-AVX256P-NEXT:    vpcmpneqd %ymm1, %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0xc1,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd %ymm3, %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0xcb,0x04]
; X64-AVX256P-NEXT:    kmovb %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x44,0x24,0x3f]
; X64-AVX256P-NEXT:    kmovb %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf9,0x91,0x4c,0x24,0x3e]
; X64-AVX256P-NEXT:    kxnorb %k1, %k0, %k0 # encoding: [0xc5,0xfd,0x46,0xc1]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $al killed $al killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i8, align 1
  %__B.addr.i = alloca i8, align 1
  %__A.addr = alloca <8 x i32>, align 64
  %__B.addr = alloca <8 x i32>, align 64
  %__C.addr = alloca <8 x i32>, align 64
  %__D.addr = alloca <8 x i32>, align 64
  %__E.addr = alloca <8 x i32>, align 64
  %__F.addr = alloca <8 x i32>, align 64
  store <8 x i32> %__A, ptr %__A.addr, align 64
  store <8 x i32> %__B, ptr %__B.addr, align 64
  store <8 x i32> %__C, ptr %__C.addr, align 64
  store <8 x i32> %__D, ptr %__D.addr, align 64
  store <8 x i32> %__E, ptr %__E.addr, align 64
  store <8 x i32> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i32>, ptr %__E.addr, align 64
  %1 = load <8 x i32>, ptr %__F.addr, align 64
  %2 = load <8 x i32>, ptr %__A.addr, align 64
  %3 = load <8 x i32>, ptr %__B.addr, align 64
  %4 = icmp ne <8 x i32> %2, %3
  %5 = bitcast <8 x i1> %4 to i8
  %6 = load <8 x i32>, ptr %__C.addr, align 64
  %7 = load <8 x i32>, ptr %__D.addr, align 64
  %8 = icmp ne <8 x i32> %6, %7
  %9 = bitcast <8 x i1> %8 to i8
  store i8 %5, ptr %__A.addr.i, align 1
  store i8 %9, ptr %__B.addr.i, align 1
  %10 = load i8, ptr %__A.addr.i, align 1
  %11 = load i8, ptr %__B.addr.i, align 1
  %12 = bitcast i8 %10 to <8 x i1>
  %13 = bitcast i8 %11 to <8 x i1>
  %14 = xor <8 x i1> %12, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %15 = xor <8 x i1> %14, %13
  %16 = bitcast <8 x i1> %15 to i8
  %17 = icmp ne <8 x i32> %0, %1
  %18 = bitcast i8 %16 to <8 x i1>
  %19 = bitcast <8 x i1> %18 to i8
  ret i8 %19
}

; Function Attrs: noinline nounwind optnone
define dso_local zeroext i16 @test_kxnor_mask16(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kxnor_mask16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; AVX256P-NEXT:    kunpckbw %k2, %k3, %k1 # encoding: [0xc5,0xe5,0x4b,0xca]
; AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%esp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; AVX256P-NEXT:    kxorw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x47,0xc1]
; AVX256P-NEXT:    kunpckbw %k4, %k5, %k1 # encoding: [0xc5,0xd5,0x4b,0xcc]
; AVX256P-NEXT:    kandnw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x42,0xc1]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxnor_mask16:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k0, %k1, %k0 # encoding: [0xc5,0xf5,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0x65,0x28,0x1f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0x6d,0x28,0x1f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovw %k0, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x44,0x24,0x3e]
; X64-AVX256P-NEXT:    kmovw %k1, {{[0-9]+}}(%rsp) # encoding: [0xc5,0xf8,0x91,0x4c,0x24,0x3c]
; X64-AVX256P-NEXT:    kxorw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x47,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0x75,0x28,0x1f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqd {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0x7d,0x28,0x1f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckbw %k1, %k2, %k1 # encoding: [0xc5,0xed,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandnw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x42,0xc1]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    # kill: def $ax killed $ax killed $eax
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i16, align 2
  %__B.addr.i = alloca i16, align 2
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <16 x i32>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <16 x i32>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <16 x i32>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <16 x i32>
  %8 = icmp ne <16 x i32> %5, %7
  %9 = bitcast <16 x i1> %8 to i16
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <16 x i32>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <16 x i32>
  %14 = icmp ne <16 x i32> %11, %13
  %15 = bitcast <16 x i1> %14 to i16
  store i16 %9, ptr %__A.addr.i, align 2
  store i16 %15, ptr %__B.addr.i, align 2
  %16 = load i16, ptr %__A.addr.i, align 2
  %17 = load i16, ptr %__B.addr.i, align 2
  %18 = bitcast i16 %16 to <16 x i1>
  %19 = bitcast i16 %17 to <16 x i1>
  %20 = xor <16 x i1> %18, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %21 = xor <16 x i1> %20, %19
  %22 = bitcast <16 x i1> %21 to i16
  %23 = icmp ne <16 x i32> %1, %3
  %24 = bitcast i16 %22 to <16 x i1>
  %25 = and <16 x i1> %23, %24
  %26 = bitcast <16 x i1> %25 to i16
  ret i16 %26
}

; Function Attrs: noinline nounwind optnone
define dso_local i32 @test_kxnor_mask32(<8 x i64> noundef %__A, <8 x i64> noundef %__B, <8 x i64> noundef %__C, <8 x i64> noundef %__D, <8 x i64> noundef %__E, <8 x i64> noundef %__F) #0 {
; AVX256P-LABEL: test_kxnor_mask32:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushl %ebp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 8
; AVX256P-NEXT:    .cfi_offset %ebp, -8
; AVX256P-NEXT:    movl %esp, %ebp # encoding: [0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %ebp
; AVX256P-NEXT:    andl $-64, %esp # encoding: [0x83,0xe4,0xc0]
; AVX256P-NEXT:    subl $512, %esp # encoding: [0x81,0xec,0x00,0x02,0x00,0x00]
; AVX256P-NEXT:    # imm = 0x200
; AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 8(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x08]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x60,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 72(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x48]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 40(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x28]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps 104(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x45,0x68]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xc0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 136(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x88,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xe0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 200(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xc8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 168(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xa8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps 232(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0xe8,0x00,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x40]
; AVX256P-NEXT:    vmovaps 264(%ebp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x85,0x08,0x01,0x00,0x00]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%esp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x60]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k0 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x44,0x24,0x0a,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k1 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x00,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x06,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x20,0x01,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k3 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x5c,0x24,0x07,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0x80,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k4 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x64,0x24,0x02,0x04]
; AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%esp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%esp), %ymm0, %k5 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x6c,0x24,0x03,0x04]
; AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; AVX256P-NEXT:    kunpckwd %k2, %k3, %k1 # encoding: [0xc5,0xe4,0x4b,0xca]
; AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%esp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; AVX256P-NEXT:    kxord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x47,0xc1]
; AVX256P-NEXT:    kunpckwd %k4, %k5, %k1 # encoding: [0xc5,0xd4,0x4b,0xcc]
; AVX256P-NEXT:    kandnd %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x42,0xc1]
; AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; AVX256P-NEXT:    movl %ebp, %esp # encoding: [0x89,0xec]
; AVX256P-NEXT:    popl %ebp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %esp, 4
; AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; AVX256P-NEXT:    retl # encoding: [0xc3]
;
; X64-AVX256P-LABEL: test_kxnor_mask32:
; X64-AVX256P:       # %bb.0: # %entry
; X64-AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; X64-AVX256P-NEXT:    .cfi_def_cfa_offset 16
; X64-AVX256P-NEXT:    .cfi_offset %rbp, -16
; X64-AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; X64-AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; X64-AVX256P-NEXT:    andq $-64, %rsp # encoding: [0x48,0x83,0xe4,0xc0]
; X64-AVX256P-NEXT:    subq $512, %rsp # encoding: [0x48,0x81,0xec,0x00,0x02,0x00,0x00]
; X64-AVX256P-NEXT:    # imm = 0x200
; X64-AVX256P-NEXT:    vmovaps 80(%rbp), %ymm8 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x45,0x50]
; X64-AVX256P-NEXT:    vmovaps 112(%rbp), %ymm9 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x4d,0x70]
; X64-AVX256P-NEXT:    vmovaps 16(%rbp), %ymm10 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x55,0x10]
; X64-AVX256P-NEXT:    vmovaps 48(%rbp), %ymm11 # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x28,0x5d,0x30]
; X64-AVX256P-NEXT:    vmovaps %ymm1, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x8c,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x84,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm2, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x94,0x24,0x40,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm3, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x9c,0x24,0x60,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm5, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xac,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm4, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xa4,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm6, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xb4,0x24,0xc0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm7, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0xbc,0x24,0xe0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm11, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x9c,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm10, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x94,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovaps %ymm8, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x44,0x24,0x40]
; X64-AVX256P-NEXT:    vmovaps %ymm9, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0x7c,0x29,0x4c,0x24,0x60]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x84,0x24,0xa0,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x8c,0x24,0x80,0x00,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0xa0,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x80,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k0 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x44,0x24,0x0a,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k1 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x4c,0x24,0x0b,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x4b,0xc0]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x94,0x24,0x20,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vmovdqa {{[0-9]+}}(%rsp), %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0x9c,0x24,0x00,0x01,0x00,0x00]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm3, %k1 # encoding: [0x62,0xf3,0xe5,0x28,0x3f,0x4c,0x24,0x06,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm2, %k2 # encoding: [0x62,0xf3,0xed,0x28,0x3f,0x54,0x24,0x07,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kmovd %k0, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x44,0x24,0x3c]
; X64-AVX256P-NEXT:    kmovd %k1, {{[0-9]+}}(%rsp) # encoding: [0xc4,0xe1,0xf9,0x91,0x4c,0x24,0x38]
; X64-AVX256P-NEXT:    kxord %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x47,0xc1]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm1, %k1 # encoding: [0x62,0xf3,0xf5,0x28,0x3f,0x4c,0x24,0x02,0x04]
; X64-AVX256P-NEXT:    vpcmpneqw {{[0-9]+}}(%rsp), %ymm0, %k2 # encoding: [0x62,0xf3,0xfd,0x28,0x3f,0x54,0x24,0x03,0x04]
; X64-AVX256P-NEXT:    kunpckwd %k1, %k2, %k1 # encoding: [0xc5,0xec,0x4b,0xc9]
; X64-AVX256P-NEXT:    kandnd %k1, %k0, %k0 # encoding: [0xc4,0xe1,0xfd,0x42,0xc1]
; X64-AVX256P-NEXT:    kmovd %k0, %eax # encoding: [0xc5,0xfb,0x93,0xc0]
; X64-AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; X64-AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; X64-AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; X64-AVX256P-NEXT:    vzeroupper # encoding: [0xc5,0xf8,0x77]
; X64-AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca i32, align 4
  %__B.addr.i = alloca i32, align 4
  %__A.addr = alloca <8 x i64>, align 64
  %__B.addr = alloca <8 x i64>, align 64
  %__C.addr = alloca <8 x i64>, align 64
  %__D.addr = alloca <8 x i64>, align 64
  %__E.addr = alloca <8 x i64>, align 64
  %__F.addr = alloca <8 x i64>, align 64
  store <8 x i64> %__A, ptr %__A.addr, align 64
  store <8 x i64> %__B, ptr %__B.addr, align 64
  store <8 x i64> %__C, ptr %__C.addr, align 64
  store <8 x i64> %__D, ptr %__D.addr, align 64
  store <8 x i64> %__E, ptr %__E.addr, align 64
  store <8 x i64> %__F, ptr %__F.addr, align 64
  %0 = load <8 x i64>, ptr %__E.addr, align 64
  %1 = bitcast <8 x i64> %0 to <32 x i16>
  %2 = load <8 x i64>, ptr %__F.addr, align 64
  %3 = bitcast <8 x i64> %2 to <32 x i16>
  %4 = load <8 x i64>, ptr %__A.addr, align 64
  %5 = bitcast <8 x i64> %4 to <32 x i16>
  %6 = load <8 x i64>, ptr %__B.addr, align 64
  %7 = bitcast <8 x i64> %6 to <32 x i16>
  %8 = icmp ne <32 x i16> %5, %7
  %9 = bitcast <32 x i1> %8 to i32
  %10 = load <8 x i64>, ptr %__C.addr, align 64
  %11 = bitcast <8 x i64> %10 to <32 x i16>
  %12 = load <8 x i64>, ptr %__D.addr, align 64
  %13 = bitcast <8 x i64> %12 to <32 x i16>
  %14 = icmp ne <32 x i16> %11, %13
  %15 = bitcast <32 x i1> %14 to i32
  store i32 %9, ptr %__A.addr.i, align 4
  store i32 %15, ptr %__B.addr.i, align 4
  %16 = load i32, ptr %__A.addr.i, align 4
  %17 = load i32, ptr %__B.addr.i, align 4
  %18 = bitcast i32 %16 to <32 x i1>
  %19 = bitcast i32 %17 to <32 x i1>
  %20 = xor <32 x i1> %18, <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>
  %21 = xor <32 x i1> %20, %19
  %22 = bitcast <32 x i1> %21 to i32
  %23 = icmp ne <32 x i16> %1, %3
  %24 = bitcast i32 %22 to <32 x i1>
  %25 = and <32 x i1> %23, %24
  %26 = bitcast <32 x i1> %25 to i32
  ret i32 %26
}

declare <8 x i1> @llvm.x86.avx512.kadd.b(<8 x i1>, <8 x i1>) #2
declare <16 x i1> @llvm.x86.avx512.kadd.w(<16 x i1>, <16 x i1>) #2
declare <32 x i1> @llvm.x86.avx512.kadd.d(<32 x i1>, <32 x i1>) #2
declare i32 @llvm.x86.avx512.ktestc.b(<8 x i1>, <8 x i1>) #2
declare i32 @llvm.x86.avx512.ktestz.b(<8 x i1>, <8 x i1>) #2
declare i32 @llvm.x86.avx512.ktestc.w(<16 x i1>, <16 x i1>) #2
declare i32 @llvm.x86.avx512.ktestz.w(<16 x i1>, <16 x i1>) #2
declare i32 @llvm.x86.avx512.ktestc.d(<32 x i1>, <32 x i1>) #2
declare i32 @llvm.x86.avx512.ktestz.d(<32 x i1>, <32 x i1>) #2
