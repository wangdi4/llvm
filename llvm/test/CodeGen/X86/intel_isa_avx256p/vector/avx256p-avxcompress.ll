; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p --show-mc-encoding | FileCheck %s --check-prefixes=AVX256P

define dso_local <2 x i64> @test_mm_popcnt_avx_epi8(<2 x i64> noundef %A) #0 {
; AVX256P-LABEL: test_mm_popcnt_avx_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xe8]
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xd8]
; AVX256P-NEXT:    vpopcntb -{{[0-9]+}}(%rsp), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0x54,0x84,0x24,0xd8,0xff,0xff,0xff]
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca <2 x i64>, align 16
  %A.addr = alloca <2 x i64>, align 16
  store <2 x i64> %A, ptr %A.addr, align 16
  %0 = load <2 x i64>, ptr %A.addr, align 16
  store <2 x i64> %0, ptr %__A.addr.i, align 16
  %1 = load <2 x i64>, ptr %__A.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <16 x i8>
  %3 = call <16 x i8> @llvm.ctpop.v16i8(<16 x i8> %2)
  %4 = bitcast <16 x i8> %3 to <2 x i64>
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone
define dso_local <4 x i64> @test_mm256_popcnt_avx_epi8(<4 x i64> noundef %A) #1 {
; AVX256P-LABEL: test_mm256_popcnt_avx_epi8:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 16
; AVX256P-NEXT:    .cfi_offset %rbp, -16
; AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; AVX256P-NEXT:    andq $-32, %rsp # encoding: [0x48,0x83,0xe4,0xe0]
; AVX256P-NEXT:    subq $96, %rsp # encoding: [0x48,0x83,0xec,0x60]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x20]
; AVX256P-NEXT:    vmovaps %ymm0, (%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x04,0x24]
; AVX256P-NEXT:    vpopcntb (%rsp), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0x54,0x04,0x24]
; AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca <4 x i64>, align 32
  %A.addr = alloca <4 x i64>, align 32
  store <4 x i64> %A, ptr %A.addr, align 32
  %0 = load <4 x i64>, ptr %A.addr, align 32
  store <4 x i64> %0, ptr %__A.addr.i, align 32
  %1 = load <4 x i64>, ptr %__A.addr.i, align 32
  %2 = bitcast <4 x i64> %1 to <32 x i8>
  %3 = call <32 x i8> @llvm.ctpop.v32i8(<32 x i8> %2)
  %4 = bitcast <32 x i8> %3 to <4 x i64>
  ret <4 x i64> %4
}

; Function Attrs: noinline nounwind optnone
define dso_local <2 x i64> @test_mm_popcnt_avx_epi16(<2 x i64> noundef %A) #0 {
; AVX256P-LABEL: test_mm_popcnt_avx_epi16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xe8]
; AVX256P-NEXT:    vmovaps %xmm0, -{{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x44,0x24,0xd8]
; AVX256P-NEXT:    vpopcntw -{{[0-9]+}}(%rsp), %xmm0 # encoding: [0x62,0xf2,0xfd,0x08,0x54,0x84,0x24,0xd8,0xff,0xff,0xff]
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca <2 x i64>, align 16
  %A.addr = alloca <2 x i64>, align 16
  store <2 x i64> %A, ptr %A.addr, align 16
  %0 = load <2 x i64>, ptr %A.addr, align 16
  store <2 x i64> %0, ptr %__A.addr.i, align 16
  %1 = load <2 x i64>, ptr %__A.addr.i, align 16
  %2 = bitcast <2 x i64> %1 to <8 x i16>
  %3 = call <8 x i16> @llvm.ctpop.v8i16(<8 x i16> %2)
  %4 = bitcast <8 x i16> %3 to <2 x i64>
  ret <2 x i64> %4
}

; Function Attrs: noinline nounwind optnone
define dso_local <4 x i64> @test_mm256_popcnt_avx_epi16(<4 x i64> noundef %A) #1 {
; AVX256P-LABEL: test_mm256_popcnt_avx_epi16:
; AVX256P:       # %bb.0: # %entry
; AVX256P-NEXT:    pushq %rbp # encoding: [0x55]
; AVX256P-NEXT:    .cfi_def_cfa_offset 16
; AVX256P-NEXT:    .cfi_offset %rbp, -16
; AVX256P-NEXT:    movq %rsp, %rbp # encoding: [0x48,0x89,0xe5]
; AVX256P-NEXT:    .cfi_def_cfa_register %rbp
; AVX256P-NEXT:    andq $-32, %rsp # encoding: [0x48,0x83,0xe4,0xe0]
; AVX256P-NEXT:    subq $96, %rsp # encoding: [0x48,0x83,0xec,0x60]
; AVX256P-NEXT:    vmovaps %ymm0, {{[0-9]+}}(%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x44,0x24,0x20]
; AVX256P-NEXT:    vmovaps %ymm0, (%rsp) # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x29,0x04,0x24]
; AVX256P-NEXT:    vpopcntw (%rsp), %ymm0 # encoding: [0x62,0xf2,0xfd,0x28,0x54,0x04,0x24]
; AVX256P-NEXT:    movq %rbp, %rsp # encoding: [0x48,0x89,0xec]
; AVX256P-NEXT:    popq %rbp # encoding: [0x5d]
; AVX256P-NEXT:    .cfi_def_cfa %rsp, 8
; AVX256P-NEXT:    retq # encoding: [0xc3]
entry:
  %__A.addr.i = alloca <4 x i64>, align 32
  %A.addr = alloca <4 x i64>, align 32
  store <4 x i64> %A, ptr %A.addr, align 32
  %0 = load <4 x i64>, ptr %A.addr, align 32
  store <4 x i64> %0, ptr %__A.addr.i, align 32
  %1 = load <4 x i64>, ptr %__A.addr.i, align 32
  %2 = bitcast <4 x i64> %1 to <16 x i16>
  %3 = call <16 x i16> @llvm.ctpop.v16i16(<16 x i16> %2)
  %4 = bitcast <16 x i16> %3 to <4 x i64>
  ret <4 x i64> %4
}

declare <16 x i8> @llvm.ctpop.v16i8(<16 x i8>) #2
declare <32 x i8> @llvm.ctpop.v32i8(<32 x i8>) #2
declare <8 x i16> @llvm.ctpop.v8i16(<8 x i16>) #2
declare <16 x i16> @llvm.ctpop.v16i16(<16 x i16>) #2
