; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p,+fast-variable-crosslane-shuffle,+fast-variable-perlane-shuffle --show-mc-encoding | FileCheck %s --check-prefixes=CHECK
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p,+fast-variable-perlane-shuffle --show-mc-encoding | FileCheck %s --check-prefixes=CHECK

define <2 x i32> @umulo_v2i32(<2 x i32> %a0, <2 x i32> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmovzxdq %xmm1, %xmm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x35,0xc9]
; CHECK-NEXT:    # xmm1 = xmm1[0],zero,xmm1[1],zero
; CHECK-NEXT:    vpmovzxdq %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x35,0xc0]
; CHECK-NEXT:    # xmm0 = xmm0[0],zero,xmm0[1],zero
; CHECK-NEXT:    vpmuludq %xmm1, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xf4,0xc1]
; CHECK-NEXT:    vpshufd $232, %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xc8,0xe8]
; CHECK-NEXT:    # xmm1 = xmm0[0,2,2,3]
; CHECK-NEXT:    vpshufd $237, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xc0,0xed]
; CHECK-NEXT:    # xmm0 = xmm0[1,3,2,3]
; CHECK-NEXT:    vpxor %xmm2, %xmm2, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xe9,0xef,0xd2]
; CHECK-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0 # encoding: [0xc5,0xf9,0x76,0xc2]
; CHECK-NEXT:    vpternlogq $15, %xmm0, %xmm0, %xmm0 # encoding: [0x62,0xf3,0xfd,0x08,0x25,0xc0,0x0f]
; CHECK-NEXT:    vmovq %xmm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xd6,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<2 x i32>, <2 x i1>} @llvm.umul.with.overflow.v2i32(<2 x i32> %a0, <2 x i32> %a1)
  %val = extractvalue {<2 x i32>, <2 x i1>} %t, 0
  %obit = extractvalue {<2 x i32>, <2 x i1>} %t, 1
  %res = sext <2 x i1> %obit to <2 x i32>
  store <2 x i32> %val, ptr %p2
  ret <2 x i32> %res
}

define <3 x i32> @umulo_v3i32(<3 x i32> %a0, <3 x i32> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v3i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmuludq %xmm1, %xmm0, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xf4,0xd1]
; CHECK-NEXT:    vpshufd $245, %xmm1, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xd9,0xf5]
; CHECK-NEXT:    # xmm3 = xmm1[1,1,3,3]
; CHECK-NEXT:    vpshufd $245, %xmm0, %xmm4 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xe0,0xf5]
; CHECK-NEXT:    # xmm4 = xmm0[1,1,3,3]
; CHECK-NEXT:    vpmuludq %xmm3, %xmm4, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xd9,0xf4,0xdb]
; CHECK-NEXT:    vmovdqa {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm4 # EVEX TO VEX Compression xmm4 = [1,5,3,7]
; CHECK-NEXT:    # encoding: [0xc5,0xf9,0x6f,0x25,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 4, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpermi2d %xmm3, %xmm2, %xmm4 # encoding: [0x62,0xf2,0x6d,0x08,0x76,0xe3]
; CHECK-NEXT:    vptestmd %xmm4, %xmm4, %k0 # encoding: [0x62,0xf2,0x5d,0x08,0x27,0xc4]
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x40,0xc9]
; CHECK-NEXT:    vpmovm2d %k0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x38,0xc0]
; CHECK-NEXT:    vpextrd $2, %xmm1, 8(%rdi) # EVEX TO VEX Compression encoding: [0xc4,0xe3,0x79,0x16,0x4f,0x08,0x02]
; CHECK-NEXT:    vmovq %xmm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xd6,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<3 x i32>, <3 x i1>} @llvm.umul.with.overflow.v3i32(<3 x i32> %a0, <3 x i32> %a1)
  %val = extractvalue {<3 x i32>, <3 x i1>} %t, 0
  %obit = extractvalue {<3 x i32>, <3 x i1>} %t, 1
  %res = sext <3 x i1> %obit to <3 x i32>
  store <3 x i32> %val, ptr %p2
  ret <3 x i32> %res
}

define <4 x i32> @umulo_v4i32(<4 x i32> %a0, <4 x i32> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v4i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmuludq %xmm1, %xmm0, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xf4,0xd1]
; CHECK-NEXT:    vpshufd $245, %xmm1, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xd9,0xf5]
; CHECK-NEXT:    # xmm3 = xmm1[1,1,3,3]
; CHECK-NEXT:    vpshufd $245, %xmm0, %xmm4 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xe0,0xf5]
; CHECK-NEXT:    # xmm4 = xmm0[1,1,3,3]
; CHECK-NEXT:    vpmuludq %xmm3, %xmm4, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xd9,0xf4,0xdb]
; CHECK-NEXT:    vmovdqa {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm4 # EVEX TO VEX Compression xmm4 = [1,5,3,7]
; CHECK-NEXT:    # encoding: [0xc5,0xf9,0x6f,0x25,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 4, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpermi2d %xmm3, %xmm2, %xmm4 # encoding: [0x62,0xf2,0x6d,0x08,0x76,0xe3]
; CHECK-NEXT:    vptestmd %xmm4, %xmm4, %k0 # encoding: [0x62,0xf2,0x5d,0x08,0x27,0xc4]
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x40,0xc9]
; CHECK-NEXT:    vpmovm2d %k0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x38,0xc0]
; CHECK-NEXT:    vmovdqa %xmm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x7f,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<4 x i32>, <4 x i1>} @llvm.umul.with.overflow.v4i32(<4 x i32> %a0, <4 x i32> %a1)
  %val = extractvalue {<4 x i32>, <4 x i1>} %t, 0
  %obit = extractvalue {<4 x i32>, <4 x i1>} %t, 1
  %res = sext <4 x i1> %obit to <4 x i32>
  store <4 x i32> %val, ptr %p2
  ret <4 x i32> %res
}

define <6 x i32> @umulo_v6i32(<6 x i32> %a0, <6 x i32> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v6i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmuludq %ymm1, %ymm0, %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0xf4,0xd1]
; CHECK-NEXT:    vpshufd $245, %ymm1, %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x70,0xd9,0xf5]
; CHECK-NEXT:    # ymm3 = ymm1[1,1,3,3,5,5,7,7]
; CHECK-NEXT:    vpshufd $245, %ymm0, %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x70,0xe0,0xf5]
; CHECK-NEXT:    # ymm4 = ymm0[1,1,3,3,5,5,7,7]
; CHECK-NEXT:    vpmuludq %ymm3, %ymm4, %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xdd,0xf4,0xdb]
; CHECK-NEXT:    vmovdqa {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm4 # EVEX TO VEX Compression ymm4 = [1,9,3,11,5,13,7,15]
; CHECK-NEXT:    # encoding: [0xc5,0xfd,0x6f,0x25,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 4, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpermi2d %ymm3, %ymm2, %ymm4 # encoding: [0x62,0xf2,0x6d,0x28,0x76,0xe3]
; CHECK-NEXT:    vptestmd %ymm4, %ymm4, %k0 # encoding: [0x62,0xf2,0x5d,0x28,0x27,0xc4]
; CHECK-NEXT:    vpmulld %ymm1, %ymm0, %ymm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7d,0x40,0xc9]
; CHECK-NEXT:    vpmovm2d %k0, %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0x38,0xc0]
; CHECK-NEXT:    vextracti128 $1, %ymm1, %xmm2 # EVEX TO VEX Compression encoding: [0xc4,0xe3,0x7d,0x39,0xca,0x01]
; CHECK-NEXT:    vmovq %xmm2, 16(%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xd6,0x57,0x10]
; CHECK-NEXT:    vmovdqa %xmm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x7f,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<6 x i32>, <6 x i1>} @llvm.umul.with.overflow.v6i32(<6 x i32> %a0, <6 x i32> %a1)
  %val = extractvalue {<6 x i32>, <6 x i1>} %t, 0
  %obit = extractvalue {<6 x i32>, <6 x i1>} %t, 1
  %res = sext <6 x i1> %obit to <6 x i32>
  store <6 x i32> %val, ptr %p2
  ret <6 x i32> %res
}

define <8 x i32> @umulo_v8i32(<8 x i32> %a0, <8 x i32> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmuludq %ymm1, %ymm0, %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0xf4,0xd1]
; CHECK-NEXT:    vpshufd $245, %ymm1, %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x70,0xd9,0xf5]
; CHECK-NEXT:    # ymm3 = ymm1[1,1,3,3,5,5,7,7]
; CHECK-NEXT:    vpshufd $245, %ymm0, %ymm4 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x70,0xe0,0xf5]
; CHECK-NEXT:    # ymm4 = ymm0[1,1,3,3,5,5,7,7]
; CHECK-NEXT:    vpmuludq %ymm3, %ymm4, %ymm3 # EVEX TO VEX Compression encoding: [0xc5,0xdd,0xf4,0xdb]
; CHECK-NEXT:    vmovdqa {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm4 # EVEX TO VEX Compression ymm4 = [1,9,3,11,5,13,7,15]
; CHECK-NEXT:    # encoding: [0xc5,0xfd,0x6f,0x25,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 4, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpermi2d %ymm3, %ymm2, %ymm4 # encoding: [0x62,0xf2,0x6d,0x28,0x76,0xe3]
; CHECK-NEXT:    vptestmd %ymm4, %ymm4, %k0 # encoding: [0x62,0xf2,0x5d,0x28,0x27,0xc4]
; CHECK-NEXT:    vpmulld %ymm1, %ymm0, %ymm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x7d,0x40,0xc9]
; CHECK-NEXT:    vpmovm2d %k0, %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0x38,0xc0]
; CHECK-NEXT:    vmovdqa %ymm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x7f,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<8 x i32>, <8 x i1>} @llvm.umul.with.overflow.v8i32(<8 x i32> %a0, <8 x i32> %a1)
  %val = extractvalue {<8 x i32>, <8 x i1>} %t, 0
  %obit = extractvalue {<8 x i32>, <8 x i1>} %t, 1
  %res = sext <8 x i1> %obit to <8 x i32>
  store <8 x i32> %val, ptr %p2
  ret <8 x i32> %res
}

define <8 x i32> @umulo_v8i16(<8 x i16> %a0, <8 x i16> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v8i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpmullw %xmm1, %xmm0, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xd5,0xd1]
; CHECK-NEXT:    vpmulhuw %xmm1, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xe4,0xc1]
; CHECK-NEXT:    vptestmw %xmm0, %xmm0, %k0 # encoding: [0x62,0xf2,0xfd,0x08,0x26,0xc0]
; CHECK-NEXT:    vpmovm2d %k0, %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0x38,0xc0]
; CHECK-NEXT:    vmovdqa %xmm2, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x7f,0x17]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<8 x i16>, <8 x i1>} @llvm.umul.with.overflow.v8i16(<8 x i16> %a0, <8 x i16> %a1)
  %val = extractvalue {<8 x i16>, <8 x i1>} %t, 0
  %obit = extractvalue {<8 x i16>, <8 x i1>} %t, 1
  %res = sext <8 x i1> %obit to <8 x i32>
  store <8 x i16> %val, ptr %p2
  ret <8 x i32> %res
}

define <2 x i32> @umulo_v2i64(<2 x i64> %a0, <2 x i64> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpextrq $1, %xmm0, %rcx # EVEX TO VEX Compression encoding: [0xc4,0xe3,0xf9,0x16,0xc1,0x01]
; CHECK-NEXT:    vpextrq $1, %xmm1, %r8 # EVEX TO VEX Compression encoding: [0xc4,0xc3,0xf9,0x16,0xc8,0x01]
; CHECK-NEXT:    vmovq %xmm0, %rax # EVEX TO VEX Compression encoding: [0xc4,0xe1,0xf9,0x7e,0xc0]
; CHECK-NEXT:    vmovq %xmm1, %rdx # EVEX TO VEX Compression encoding: [0xc4,0xe1,0xf9,0x7e,0xca]
; CHECK-NEXT:    mulq %rdx # encoding: [0x48,0xf7,0xe2]
; CHECK-NEXT:    movq %rax, %rsi # encoding: [0x48,0x89,0xc6]
; CHECK-NEXT:    seto %r9b # encoding: [0x41,0x0f,0x90,0xc1]
; CHECK-NEXT:    movq %rcx, %rax # encoding: [0x48,0x89,0xc8]
; CHECK-NEXT:    mulq %r8 # encoding: [0x49,0xf7,0xe0]
; CHECK-NEXT:    vmovq %rax, %xmm0 # EVEX TO VEX Compression encoding: [0xc4,0xe1,0xf9,0x6e,0xc0]
; CHECK-NEXT:    vmovq %rsi, %xmm1 # EVEX TO VEX Compression encoding: [0xc4,0xe1,0xf9,0x6e,0xce]
; CHECK-NEXT:    vpunpcklqdq %xmm0, %xmm1, %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf1,0x6c,0xc8]
; CHECK-NEXT:    # xmm1 = xmm1[0],xmm0[0]
; CHECK-NEXT:    seto %al # encoding: [0x0f,0x90,0xc0]
; CHECK-NEXT:    kmovd %eax, %k0 # encoding: [0xc5,0xfb,0x92,0xc0]
; CHECK-NEXT:    kshiftlb $7, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x07]
; CHECK-NEXT:    kshiftrb $6, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x30,0xc0,0x06]
; CHECK-NEXT:    kmovd %r9d, %k1 # encoding: [0xc4,0xc1,0x7b,0x92,0xc9]
; CHECK-NEXT:    kshiftlb $7, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x07]
; CHECK-NEXT:    kshiftrb $7, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x30,0xc9,0x07]
; CHECK-NEXT:    korw %k0, %k1, %k0 # encoding: [0xc5,0xf4,0x45,0xc0]
; CHECK-NEXT:    vpmovm2d %k0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x38,0xc0]
; CHECK-NEXT:    vmovdqa %xmm1, (%rdi) # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x7f,0x0f]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<2 x i64>, <2 x i1>} @llvm.umul.with.overflow.v2i64(<2 x i64> %a0, <2 x i64> %a1)
  %val = extractvalue {<2 x i64>, <2 x i1>} %t, 0
  %obit = extractvalue {<2 x i64>, <2 x i1>} %t, 1
  %res = sext <2 x i1> %obit to <2 x i32>
  store <2 x i64> %val, ptr %p2
  ret <2 x i32> %res
}

define <4 x i32> @umulo_v4i24(<4 x i24> %a0, <4 x i24> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v4i24:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpbroadcastd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm2 # EVEX TO VEX Compression xmm2 = [255,255,255,0,255,255,255,0,255,255,255,0,255,255,255,0]
; CHECK-NEXT:    # encoding: [0xc4,0xe2,0x79,0x58,0x15,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 5, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpand %xmm2, %xmm1, %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf1,0xdb,0xca]
; CHECK-NEXT:    vpand %xmm2, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xdb,0xc2]
; CHECK-NEXT:    vpmuludq %xmm1, %xmm0, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xf4,0xd1]
; CHECK-NEXT:    vpshufd $245, %xmm1, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xd9,0xf5]
; CHECK-NEXT:    # xmm3 = xmm1[1,1,3,3]
; CHECK-NEXT:    vpshufd $245, %xmm0, %xmm4 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x70,0xe0,0xf5]
; CHECK-NEXT:    # xmm4 = xmm0[1,1,3,3]
; CHECK-NEXT:    vpmuludq %xmm3, %xmm4, %xmm3 # EVEX TO VEX Compression encoding: [0xc5,0xd9,0xf4,0xdb]
; CHECK-NEXT:    vmovdqa {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm4 # EVEX TO VEX Compression xmm4 = [1,5,3,7]
; CHECK-NEXT:    # encoding: [0xc5,0xf9,0x6f,0x25,A,A,A,A]
; CHECK-NEXT:    # fixup A - offset: 4, value: {{\.?LCPI[0-9]+_[0-9]+}}-4, kind: reloc_riprel_4byte
; CHECK-NEXT:    vpermi2d %xmm3, %xmm2, %xmm4 # encoding: [0x62,0xf2,0x6d,0x08,0x76,0xe3]
; CHECK-NEXT:    vpmulld %xmm1, %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc4,0xe2,0x79,0x40,0xc9]
; CHECK-NEXT:    vpsrld $24, %xmm1, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x72,0xd1,0x18]
; CHECK-NEXT:    vpor %xmm4, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xeb,0xc4]
; CHECK-NEXT:    vptestmd %xmm0, %xmm0, %k0 # encoding: [0x62,0xf2,0x7d,0x08,0x27,0xc0]
; CHECK-NEXT:    vpmovm2d %k0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x38,0xc0]
; CHECK-NEXT:    vpextrd $3, %xmm1, %eax # EVEX TO VEX Compression encoding: [0xc4,0xe3,0x79,0x16,0xc8,0x03]
; CHECK-NEXT:    movw %ax, 9(%rdi) # encoding: [0x66,0x89,0x47,0x09]
; CHECK-NEXT:    vpextrd $2, %xmm1, %ecx # EVEX TO VEX Compression encoding: [0xc4,0xe3,0x79,0x16,0xc9,0x02]
; CHECK-NEXT:    movw %cx, 6(%rdi) # encoding: [0x66,0x89,0x4f,0x06]
; CHECK-NEXT:    vpextrd $1, %xmm1, %edx # EVEX TO VEX Compression encoding: [0xc4,0xe3,0x79,0x16,0xca,0x01]
; CHECK-NEXT:    movw %dx, 3(%rdi) # encoding: [0x66,0x89,0x57,0x03]
; CHECK-NEXT:    vmovd %xmm1, %esi # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x7e,0xce]
; CHECK-NEXT:    movw %si, (%rdi) # encoding: [0x66,0x89,0x37]
; CHECK-NEXT:    shrl $16, %eax # encoding: [0xc1,0xe8,0x10]
; CHECK-NEXT:    movb %al, 11(%rdi) # encoding: [0x88,0x47,0x0b]
; CHECK-NEXT:    shrl $16, %ecx # encoding: [0xc1,0xe9,0x10]
; CHECK-NEXT:    movb %cl, 8(%rdi) # encoding: [0x88,0x4f,0x08]
; CHECK-NEXT:    shrl $16, %edx # encoding: [0xc1,0xea,0x10]
; CHECK-NEXT:    movb %dl, 5(%rdi) # encoding: [0x88,0x57,0x05]
; CHECK-NEXT:    shrl $16, %esi # encoding: [0xc1,0xee,0x10]
; CHECK-NEXT:    movb %sil, 2(%rdi) # encoding: [0x40,0x88,0x77,0x02]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<4 x i24>, <4 x i1>} @llvm.umul.with.overflow.v4i24(<4 x i24> %a0, <4 x i24> %a1)
  %val = extractvalue {<4 x i24>, <4 x i1>} %t, 0
  %obit = extractvalue {<4 x i24>, <4 x i1>} %t, 1
  %res = sext <4 x i1> %obit to <4 x i32>
  store <4 x i24> %val, ptr %p2
  ret <4 x i32> %res
}

define <4 x i32> @umulo_v4i1(<4 x i1> %a0, <4 x i1> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v4i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpand %xmm1, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xdb,0xc1]
; CHECK-NEXT:    vpslld $31, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x72,0xf0,0x1f]
; CHECK-NEXT:    vpmovd2m %xmm0, %k0 # encoding: [0x62,0xf2,0x7e,0x08,0x39,0xc0]
; CHECK-NEXT:    kmovb %k0, (%rdi) # encoding: [0xc5,0xf9,0x91,0x07]
; CHECK-NEXT:    vpxor %xmm0, %xmm0, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0xef,0xc0]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<4 x i1>, <4 x i1>} @llvm.umul.with.overflow.v4i1(<4 x i1> %a0, <4 x i1> %a1)
  %val = extractvalue {<4 x i1>, <4 x i1>} %t, 0
  %obit = extractvalue {<4 x i1>, <4 x i1>} %t, 1
  %res = sext <4 x i1> %obit to <4 x i32>
  store <4 x i1> %val, ptr %p2
  ret <4 x i32> %res
}

define <2 x i32> @umulo_v2i128(<2 x i128> %a0, <2 x i128> %a1, ptr %p2) nounwind {
; CHECK-LABEL: umulo_v2i128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %rbp # encoding: [0x55]
; CHECK-NEXT:    pushq %r15 # encoding: [0x41,0x57]
; CHECK-NEXT:    pushq %r14 # encoding: [0x41,0x56]
; CHECK-NEXT:    pushq %rbx # encoding: [0x53]
; CHECK-NEXT:    movq %r9, %r11 # encoding: [0x4d,0x89,0xcb]
; CHECK-NEXT:    movq %rcx, %r10 # encoding: [0x49,0x89,0xca]
; CHECK-NEXT:    movq %rdx, %rcx # encoding: [0x48,0x89,0xd1]
; CHECK-NEXT:    movq %rsi, %rax # encoding: [0x48,0x89,0xf0]
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rbx # encoding: [0x48,0x8b,0x5c,0x24,0x38]
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r14 # encoding: [0x4c,0x8b,0x74,0x24,0x28]
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r9 # encoding: [0x4c,0x8b,0x4c,0x24,0x30]
; CHECK-NEXT:    testq %r11, %r11 # encoding: [0x4d,0x85,0xdb]
; CHECK-NEXT:    setne %dl # encoding: [0x0f,0x95,0xc2]
; CHECK-NEXT:    testq %rsi, %rsi # encoding: [0x48,0x85,0xf6]
; CHECK-NEXT:    setne %bpl # encoding: [0x40,0x0f,0x95,0xc5]
; CHECK-NEXT:    andb %dl, %bpl # encoding: [0x40,0x20,0xd5]
; CHECK-NEXT:    mulq %r8 # encoding: [0x49,0xf7,0xe0]
; CHECK-NEXT:    movq %rax, %rsi # encoding: [0x48,0x89,0xc6]
; CHECK-NEXT:    seto %r15b # encoding: [0x41,0x0f,0x90,0xc7]
; CHECK-NEXT:    movq %r11, %rax # encoding: [0x4c,0x89,0xd8]
; CHECK-NEXT:    mulq %rdi # encoding: [0x48,0xf7,0xe7]
; CHECK-NEXT:    seto %r11b # encoding: [0x41,0x0f,0x90,0xc3]
; CHECK-NEXT:    orb %r15b, %r11b # encoding: [0x45,0x08,0xfb]
; CHECK-NEXT:    orb %bpl, %r11b # encoding: [0x41,0x08,0xeb]
; CHECK-NEXT:    leaq (%rsi,%rax), %r15 # encoding: [0x4c,0x8d,0x3c,0x06]
; CHECK-NEXT:    movq %rdi, %rax # encoding: [0x48,0x89,0xf8]
; CHECK-NEXT:    mulq %r8 # encoding: [0x49,0xf7,0xe0]
; CHECK-NEXT:    movq %rax, %rdi # encoding: [0x48,0x89,0xc7]
; CHECK-NEXT:    movq %rdx, %rsi # encoding: [0x48,0x89,0xd6]
; CHECK-NEXT:    addq %r15, %rsi # encoding: [0x4c,0x01,0xfe]
; CHECK-NEXT:    setb %al # encoding: [0x0f,0x92,0xc0]
; CHECK-NEXT:    orb %r11b, %al # encoding: [0x44,0x08,0xd8]
; CHECK-NEXT:    kmovd %eax, %k0 # encoding: [0xc5,0xfb,0x92,0xc0]
; CHECK-NEXT:    testq %r9, %r9 # encoding: [0x4d,0x85,0xc9]
; CHECK-NEXT:    setne %al # encoding: [0x0f,0x95,0xc0]
; CHECK-NEXT:    testq %r10, %r10 # encoding: [0x4d,0x85,0xd2]
; CHECK-NEXT:    setne %r11b # encoding: [0x41,0x0f,0x95,0xc3]
; CHECK-NEXT:    andb %al, %r11b # encoding: [0x41,0x20,0xc3]
; CHECK-NEXT:    movq %r10, %rax # encoding: [0x4c,0x89,0xd0]
; CHECK-NEXT:    mulq %r14 # encoding: [0x49,0xf7,0xe6]
; CHECK-NEXT:    movq %rax, %r8 # encoding: [0x49,0x89,0xc0]
; CHECK-NEXT:    seto %r10b # encoding: [0x41,0x0f,0x90,0xc2]
; CHECK-NEXT:    movq %r9, %rax # encoding: [0x4c,0x89,0xc8]
; CHECK-NEXT:    mulq %rcx # encoding: [0x48,0xf7,0xe1]
; CHECK-NEXT:    seto %r9b # encoding: [0x41,0x0f,0x90,0xc1]
; CHECK-NEXT:    orb %r10b, %r9b # encoding: [0x45,0x08,0xd1]
; CHECK-NEXT:    orb %r11b, %r9b # encoding: [0x45,0x08,0xd9]
; CHECK-NEXT:    addq %rax, %r8 # encoding: [0x49,0x01,0xc0]
; CHECK-NEXT:    movq %rcx, %rax # encoding: [0x48,0x89,0xc8]
; CHECK-NEXT:    mulq %r14 # encoding: [0x49,0xf7,0xe6]
; CHECK-NEXT:    addq %r8, %rdx # encoding: [0x4c,0x01,0xc2]
; CHECK-NEXT:    setb %cl # encoding: [0x0f,0x92,0xc1]
; CHECK-NEXT:    orb %r9b, %cl # encoding: [0x44,0x08,0xc9]
; CHECK-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; CHECK-NEXT:    kshiftlb $1, %k1, %k1 # encoding: [0xc4,0xe3,0x79,0x32,0xc9,0x01]
; CHECK-NEXT:    kshiftlb $7, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x32,0xc0,0x07]
; CHECK-NEXT:    kshiftrb $7, %k0, %k0 # encoding: [0xc4,0xe3,0x79,0x30,0xc0,0x07]
; CHECK-NEXT:    korw %k1, %k0, %k0 # encoding: [0xc5,0xfc,0x45,0xc1]
; CHECK-NEXT:    vpmovm2d %k0, %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0x38,0xc0]
; CHECK-NEXT:    movq %rax, 16(%rbx) # encoding: [0x48,0x89,0x43,0x10]
; CHECK-NEXT:    movq %rdi, (%rbx) # encoding: [0x48,0x89,0x3b]
; CHECK-NEXT:    movq %rdx, 24(%rbx) # encoding: [0x48,0x89,0x53,0x18]
; CHECK-NEXT:    movq %rsi, 8(%rbx) # encoding: [0x48,0x89,0x73,0x08]
; CHECK-NEXT:    popq %rbx # encoding: [0x5b]
; CHECK-NEXT:    popq %r14 # encoding: [0x41,0x5e]
; CHECK-NEXT:    popq %r15 # encoding: [0x41,0x5f]
; CHECK-NEXT:    popq %rbp # encoding: [0x5d]
; CHECK-NEXT:    retq # encoding: [0xc3]
  %t = call {<2 x i128>, <2 x i1>} @llvm.umul.with.overflow.v2i128(<2 x i128> %a0, <2 x i128> %a1)
  %val = extractvalue {<2 x i128>, <2 x i1>} %t, 0
  %obit = extractvalue {<2 x i128>, <2 x i1>} %t, 1
  %res = sext <2 x i1> %obit to <2 x i32>
  store <2 x i128> %val, ptr %p2
  ret <2 x i32> %res
}
declare {<2 x i32>, <2 x i1>} @llvm.umul.with.overflow.v2i32(<2 x i32>, <2 x i32>)
declare {<3 x i32>, <3 x i1>} @llvm.umul.with.overflow.v3i32(<3 x i32>, <3 x i32>)
declare {<4 x i32>, <4 x i1>} @llvm.umul.with.overflow.v4i32(<4 x i32>, <4 x i32>)
declare {<6 x i32>, <6 x i1>} @llvm.umul.with.overflow.v6i32(<6 x i32>, <6 x i32>)
declare {<8 x i32>, <8 x i1>} @llvm.umul.with.overflow.v8i32(<8 x i32>, <8 x i32>)
declare {<8 x i16>, <8 x i1>} @llvm.umul.with.overflow.v8i16(<8 x i16>, <8 x i16>)
declare {<2 x i64>, <2 x i1>} @llvm.umul.with.overflow.v2i64(<2 x i64>, <2 x i64>)
declare {<4 x i24>, <4 x i1>} @llvm.umul.with.overflow.v4i24(<4 x i24>, <4 x i24>)
declare {<4 x i1>, <4 x i1>} @llvm.umul.with.overflow.v4i1(<4 x i1>, <4 x i1>)
declare {<2 x i128>, <2 x i1>} @llvm.umul.with.overflow.v2i128(<2 x i128>, <2 x i128>)
