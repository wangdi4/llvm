; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx256p
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+avx256p --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx256p --show-mc-encoding | FileCheck %s --check-prefixes=CHECK,X64
declare <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half>, <16 x half>, <16 x half>)

define <16 x half> @stack_fold_fmsubadd123ph(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2) {
; X86-LABEL: stack_fold_fmsubadd123ph:
; X86:       # %bb.0:
; X86-NEXT:    vfmsubadd213ph %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf6,0x75,0x28,0xa7,0xc2]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd123ph:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0x28,0xa7,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = fneg <16 x half> %a2
  %3 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a1, <16 x half> %2)
  ret <16 x half> %3
}

define <16 x half> @stack_fold_fmsubadd213ph(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2) {
; X86-LABEL: stack_fold_fmsubadd213ph:
; X86:       # %bb.0:
; X86-NEXT:    vfmsubadd213ph %ymm2, %ymm1, %ymm0 # encoding: [0x62,0xf6,0x75,0x28,0xa7,0xc2]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd213ph:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0x28,0xa7,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = fneg <16 x half> %a2
  %3 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a1, <16 x half> %a0, <16 x half> %2)
  ret <16 x half> %3
}

define <16 x half> @stack_fold_fmsubadd123ph_mask(ptr %p, <16 x half> %a1, <16 x half> %a2, i16 %mask) {
; X86-LABEL: stack_fold_fmsubadd123ph_mask:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm1, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x0c,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    vmovaps (%eax), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x10]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x28]
; X86-NEXT:    vfmsubadd213ph (%esp), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0xa7,0x14,0x24]
; X86-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd123ph_mask:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x4c,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vmovaps (%rdi), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x17]
; X64-NEXT:    kmovd %esi, %k1 # encoding: [0xc5,0xfb,0x92,0xce]
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0xa7,0x94,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %a0 = load <16 x half>, ptr %p
  %neg = fneg <16 x half> %a2
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a1, <16 x half> %neg)
  %3 = bitcast i16 %mask to <16 x i1>
  %4 = select <16 x i1> %3, <16 x half> %2, <16 x half> %a0
  ret <16 x half> %4
}

define <16 x half> @stack_fold_fmsubadd213ph_mask(ptr %p, <16 x half> %a1, <16 x half> %a2, i16 %mask) {
; X86-LABEL: stack_fold_fmsubadd213ph_mask:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm1, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x0c,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    vmovaps (%eax), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x10]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x28]
; X86-NEXT:    vfmsubadd213ph (%esp), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0xa7,0x14,0x24]
; X86-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd213ph_mask:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x4c,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vmovaps (%rdi), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x17]
; X64-NEXT:    kmovd %esi, %k1 # encoding: [0xc5,0xfb,0x92,0xce]
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0xa7,0x94,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %a0 = load <16 x half>, ptr %p
  %neg = fneg <16 x half> %a2
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a1, <16 x half> %a0, <16 x half> %neg)
  %3 = bitcast i16 %mask to <16 x i1>
  %4 = select <16 x i1> %3, <16 x half> %2, <16 x half> %a0
  ret <16 x half> %4
}

define <16 x half> @stack_fold_fmsubadd123ph_maskz(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2, ptr %mask) {
; X86-LABEL: stack_fold_fmsubadd123ph_maskz:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm2, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x14,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    kmovw (%eax), %k1 # encoding: [0xc5,0xf8,0x90,0x08]
; X86-NEXT:    vfmsubadd213ph (%esp), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0xa7,0x04,0x24]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd123ph_maskz:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    kmovw (%rdi), %k1 # encoding: [0xc5,0xf8,0x90,0x0f]
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0xa7,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %neg = fneg <16 x half> %a2
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a1, <16 x half> %neg)
  %3 = load i16, ptr %mask
  %4 = bitcast i16 %3 to <16 x i1>
  %5 = select <16 x i1> %4, <16 x half> %2, <16 x half> zeroinitializer
  ret <16 x half> %5
}

define <16 x half> @stack_fold_fmsubadd213ph_maskz(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2, ptr %mask) {
; X86-LABEL: stack_fold_fmsubadd213ph_maskz:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm2, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x14,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    kmovw (%eax), %k1 # encoding: [0xc5,0xf8,0x90,0x08]
; X86-NEXT:    vfmsubadd213ph (%esp), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0xa7,0x04,0x24]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd213ph_maskz:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    kmovw (%rdi), %k1 # encoding: [0xc5,0xf8,0x90,0x0f]
; X64-NEXT:    vfmsubadd213ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0xa7,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %neg = fneg <16 x half> %a2
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a1, <16 x half> %a0, <16 x half> %neg)
  %3 = load i16, ptr %mask
  %4 = bitcast i16 %3 to <16 x i1>
  %5 = select <16 x i1> %4, <16 x half> %2, <16 x half> zeroinitializer
  ret <16 x half> %5
}

define <16 x half> @stack_fold_fmsubadd132ph(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2) {
; X86-LABEL: stack_fold_fmsubadd132ph:
; X86:       # %bb.0:
; X86-NEXT:    vfmsubadd213ph %ymm1, %ymm2, %ymm0 # encoding: [0x62,0xf6,0x6d,0x28,0xa7,0xc1]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd132ph:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0x28,0x97,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = fneg <16 x half> %a1
  %3 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a2, <16 x half> %2)
  ret <16 x half> %3
}

define <16 x half> @stack_fold_fmsubadd312ph(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2) {
; X86-LABEL: stack_fold_fmsubadd312ph:
; X86:       # %bb.0:
; X86-NEXT:    vfmsubadd213ph %ymm1, %ymm2, %ymm0 # encoding: [0x62,0xf6,0x6d,0x28,0xa7,0xc1]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd312ph:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0x28,0x97,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %2 = fneg <16 x half> %a1
  %3 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a2, <16 x half> %a0, <16 x half> %2)
  ret <16 x half> %3
}

define <16 x half> @stack_fold_fmsubadd132ph_mask(ptr %p, <16 x half> %a1, <16 x half> %a2, i16 %mask) {
; X86-LABEL: stack_fold_fmsubadd132ph_mask:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm1, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x0c,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    vmovaps (%eax), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x10]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x28]
; X86-NEXT:    vfmsubadd132ph (%esp), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0x97,0x14,0x24]
; X86-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd132ph_mask:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x4c,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vmovaps (%rdi), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x17]
; X64-NEXT:    kmovd %esi, %k1 # encoding: [0xc5,0xfb,0x92,0xce]
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0x97,0x94,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %a0 = load <16 x half>, ptr %p
  %neg = fneg <16 x half> %a1
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a2, <16 x half> %neg)
  %3 = bitcast i16 %mask to <16 x i1>
  %4 = select <16 x i1> %3, <16 x half> %2, <16 x half> %a0
  ret <16 x half> %4
}

define <16 x half> @stack_fold_fmsubadd312ph_mask(ptr %p, <16 x half> %a1, <16 x half> %a2, i16 %mask) {
; X86-LABEL: stack_fold_fmsubadd312ph_mask:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm1, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x0c,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    vmovaps (%eax), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x10]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x28]
; X86-NEXT:    vfmsubadd132ph (%esp), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0x97,0x14,0x24]
; X86-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd312ph_mask:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm1, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x4c,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    vmovaps (%rdi), %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0x17]
; X64-NEXT:    kmovd %esi, %k1 # encoding: [0xc5,0xfb,0x92,0xce]
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %ymm2 {%k1} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x7d,0x29,0x97,0x94,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm2},~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %a0 = load <16 x half>, ptr %p
  %neg = fneg <16 x half> %a1
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a2, <16 x half> %a0, <16 x half> %neg)
  %3 = bitcast i16 %mask to <16 x i1>
  %4 = select <16 x i1> %3, <16 x half> %2, <16 x half> %a0
  ret <16 x half> %4
}

define <16 x half> @stack_fold_fmsubadd132ph_maskz(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2, ptr %mask) {
; X86-LABEL: stack_fold_fmsubadd132ph_maskz:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm2, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x14,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    kmovw (%eax), %k1 # encoding: [0xc5,0xf8,0x90,0x08]
; X86-NEXT:    vfmsubadd132ph (%esp), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0x97,0x04,0x24]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd132ph_maskz:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    kmovw (%rdi), %k1 # encoding: [0xc5,0xf8,0x90,0x0f]
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0x97,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %neg = fneg <16 x half> %a1
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a0, <16 x half> %a2, <16 x half> %neg)
  %3 = load i16, ptr %mask
  %4 = bitcast i16 %3 to <16 x i1>
  %5 = select <16 x i1> %4, <16 x half> %2, <16 x half> zeroinitializer
  ret <16 x half> %5
}

define <16 x half> @stack_fold_fmsubadd312ph_maskz(<16 x half> %a0, <16 x half> %a1, <16 x half> %a2, ptr %mask) {
; X86-LABEL: stack_fold_fmsubadd312ph_maskz:
; X86:       # %bb.0:
; X86-NEXT:    subl $32, %esp # encoding: [0x83,0xec,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 36
; X86-NEXT:    vmovups %ymm2, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x14,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x24]
; X86-NEXT:    #APP
; X86-NEXT:    nop # encoding: [0x90]
; X86-NEXT:    #NO_APP
; X86-NEXT:    kmovw (%eax), %k1 # encoding: [0xc5,0xf8,0x90,0x08]
; X86-NEXT:    vfmsubadd132ph (%esp), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X86-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0x97,0x04,0x24]
; X86-NEXT:    addl $32, %esp # encoding: [0x83,0xc4,0x20]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
;
; X64-LABEL: stack_fold_fmsubadd312ph_maskz:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xd8]
; X64-NEXT:    #APP
; X64-NEXT:    nop # encoding: [0x90]
; X64-NEXT:    #NO_APP
; X64-NEXT:    kmovw (%rdi), %k1 # encoding: [0xc5,0xf8,0x90,0x0f]
; X64-NEXT:    vfmsubadd132ph {{[-0-9]+}}(%r{{[sb]}}p), %ymm1, %ymm0 {%k1} {z} # 32-byte Folded Reload
; X64-NEXT:    # encoding: [0x62,0xf6,0x75,0xa9,0x97,0x84,0x24,0xd8,0xff,0xff,0xff]
; X64-NEXT:    retq # encoding: [0xc3]
  %1 = tail call <2 x i64> asm sideeffect "nop", "=x,~{xmm3},~{xmm4},~{xmm5},~{xmm6},~{xmm7},~{xmm8},~{xmm9},~{xmm10},~{xmm11},~{xmm12},~{xmm13},~{xmm14},~{xmm15},~{xmm16},~{xmm17},~{xmm18},~{xmm19},~{xmm20},~{xmm21},~{xmm22},~{xmm23},~{xmm24},~{xmm25},~{xmm26},~{xmm27},~{xmm28},~{xmm29},~{xmm30},~{xmm31},~{flags}"()
  %neg = fneg <16 x half> %a1
  %2 = call <16 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.256(<16 x half> %a2, <16 x half> %a0, <16 x half> %neg)
  %3 = load i16, ptr %mask
  %4 = bitcast i16 %3 to <16 x i1>
  %5 = select <16 x i1> %4, <16 x half> %2, <16 x half> zeroinitializer
  ret <16 x half> %5
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
