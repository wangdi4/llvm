# RUN: llc -o - %s -run-pass=machine-scheduler | FileCheck %s
--- |
  ; ModuleID = 'main.ll'
  source_filename = "main.cpp"
  target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
  target triple = "x86_64-unknown-linux-gnu"

  ; Function Attrs: nofree noinline nosync nounwind uwtable
  define dso_local void @_Z4testPDv8_dmPKS_mS2_mm(ptr noalias nocapture noundef writeonly %O, i64 noundef %strideO, ptr noalias nocapture noundef readonly %I, i64 noundef %strideI, ptr noalias nocapture noundef readonly %P, i64 noundef %strideP, i64 noundef %length) local_unnamed_addr #0 {
  entry:
    %mul.i1021 = shl i64 %strideI, 6
    %mul.i1027 = shl i64 %strideO, 6
    %mul.i1036 = shl i64 %strideP, 6
    %mul.i = shl i64 %strideI, 8
    %mul.i291 = shl i64 %strideP, 8
    %mul.i307 = shl i64 %strideI, 7
    %mul.i563 = shl i64 %strideP, 7
    %0 = add i64 %mul.i563, 192
    %uglygep = getelementptr i8, ptr %P, i64 %0
    %1 = add i64 %mul.i1036, 192
    %uglygep8 = getelementptr i8, ptr %P, i64 %1
    %2 = add nuw nsw i64 %mul.i291, 192
    %uglygep16 = getelementptr i8, ptr %P, i64 %2
    %uglygep24 = getelementptr i8, ptr %P, i64 192
    %3 = mul i64 %strideP, 448
    %4 = add i64 %3, 192
    %uglygep32 = getelementptr i8, ptr %P, i64 %4
    %5 = mul i64 %strideP, 192
    %6 = add i64 %5, 192
    %uglygep40 = getelementptr i8, ptr %P, i64 %6
    %7 = mul i64 %strideP, 384
    %8 = add i64 %7, 192
    %uglygep48 = getelementptr i8, ptr %P, i64 %8
    %9 = mul i64 %strideP, 320
    %10 = add i64 %9, 192
    %uglygep56 = getelementptr i8, ptr %P, i64 %10
    %uglygep64 = getelementptr i8, ptr %O, i64 192
    %11 = add i64 %mul.i307, 192
    %uglygep72 = getelementptr i8, ptr %I, i64 %11
    %12 = add i64 %mul.i1021, 192
    %uglygep80 = getelementptr i8, ptr %I, i64 %12
    %13 = add nuw nsw i64 %mul.i, 192
    %uglygep88 = getelementptr i8, ptr %I, i64 %13
    %14 = mul i64 %strideI, 448
    %15 = add i64 %14, 192
    %uglygep104 = getelementptr i8, ptr %I, i64 %15
    %16 = mul i64 %strideI, 192
    %17 = add i64 %16, 192
    %uglygep112 = getelementptr i8, ptr %I, i64 %17
    %18 = mul i64 %strideI, 384
    %19 = add i64 %18, 192
    %uglygep120 = getelementptr i8, ptr %I, i64 %19
    %20 = mul i64 %strideI, 320
    %21 = add i64 %20, 192
    %uglygep128 = getelementptr i8, ptr %I, i64 %21
    %22 = mul i64 %strideO, 384
    %23 = sub i64 64, %22
    br label %do.body

  do.body:                                          ; preds = %do.body, %entry
    %lsr.iv = phi i64 [ %lsr.iv.next, %do.body ], [ 0, %entry ]
    %length.addr.0 = phi i64 [ %length, %entry ], [ %dec, %do.body ]
    %uglygep30 = getelementptr i8, ptr %uglygep24, i64 %lsr.iv
    %uglygep31 = getelementptr i8, ptr %uglygep30, i64 -192
    %uglygep38 = getelementptr i8, ptr %uglygep32, i64 %lsr.iv
    %uglygep39 = getelementptr i8, ptr %uglygep38, i64 -192
    %uglygep70 = getelementptr i8, ptr %uglygep64, i64 %lsr.iv
    %uglygep71 = getelementptr i8, ptr %uglygep70, i64 -192
    %uglygep110 = getelementptr i8, ptr %uglygep104, i64 %lsr.iv
    %uglygep111 = getelementptr i8, ptr %uglygep110, i64 -192
    %sunkaddr = getelementptr i8, ptr %I, i64 %lsr.iv
    %24 = load <8 x double>, ptr %sunkaddr, align 64, !tbaa !5
    %sunkaddr165 = getelementptr i8, ptr %I, i64 %lsr.iv
    %sunkaddr166 = getelementptr i8, ptr %sunkaddr165, i64 64
    %25 = load <8 x double>, ptr %sunkaddr166, align 64, !tbaa !5
    %sunkaddr167 = getelementptr i8, ptr %I, i64 %lsr.iv
    %sunkaddr168 = getelementptr i8, ptr %sunkaddr167, i64 128
    %26 = load <8 x double>, ptr %sunkaddr168, align 64, !tbaa !5
    %sunkaddr169 = getelementptr i8, ptr %I, i64 %lsr.iv
    %sunkaddr170 = getelementptr i8, ptr %sunkaddr169, i64 192
    %27 = load <8 x double>, ptr %sunkaddr170, align 64, !tbaa !5
    %add.i.i = fadd <8 x double> %24, %25
    %sub.i.i = fsub <8 x double> %24, %25
    %shuffle.i.i = shufflevector <8 x double> %add.i.i, <8 x double> %sub.i.i, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i = shufflevector <8 x double> %add.i.i, <8 x double> %sub.i.i, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i = fmul <8 x double> %shuffle.i25.i, %shuffle.i.i
    %28 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i, <8 x double> %shuffle.i25.i, <8 x double> %mul.i.i)
    %add.i26.i = fadd <8 x double> %mul.i.i, %28
    %sub.i27.i = fsub <8 x double> %mul.i.i, %28
    %add.i.i132 = fadd <8 x double> %26, %27
    %sub.i.i133 = fsub <8 x double> %26, %27
    %shuffle.i.i134 = shufflevector <8 x double> %add.i.i132, <8 x double> %sub.i.i133, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i135 = shufflevector <8 x double> %add.i.i132, <8 x double> %sub.i.i133, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i136 = fmul <8 x double> %shuffle.i25.i135, %shuffle.i.i134
    %29 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i134, <8 x double> %shuffle.i25.i135, <8 x double> %mul.i.i136)
    %add.i26.i137 = fadd <8 x double> %mul.i.i136, %29
    %sub.i27.i138 = fsub <8 x double> %mul.i.i136, %29
    %add.i.i139 = fadd <8 x double> %add.i26.i, %add.i26.i137
    %sub.i.i140 = fsub <8 x double> %add.i26.i, %add.i26.i137
    %shuffle.i.i141 = shufflevector <8 x double> %add.i.i139, <8 x double> %sub.i.i140, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i142 = shufflevector <8 x double> %add.i.i139, <8 x double> %sub.i.i140, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i143 = fmul <8 x double> %shuffle.i25.i142, %shuffle.i.i141
    %30 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i141, <8 x double> %shuffle.i25.i142, <8 x double> %mul.i.i143)
    %add.i26.i144 = fadd <8 x double> %mul.i.i143, %30
    %sub.i27.i145 = fsub <8 x double> %mul.i.i143, %30
    %add.i.i146 = fadd <8 x double> %sub.i27.i138, %sub.i27.i138
    %sub.i.i147 = fsub <8 x double> %sub.i27.i138, %sub.i27.i138
    %shuffle.i25.i149 = shufflevector <8 x double> %add.i.i146, <8 x double> %sub.i.i147, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i150 = fmul <8 x double> %shuffle.i25.i149, %shuffle.i25.i149
    %31 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i149, <8 x double> %shuffle.i25.i149, <8 x double> %mul.i.i150)
    %sub.i27.i152 = fsub <8 x double> %mul.i.i150, %31
    %uglygep94 = getelementptr i8, ptr %uglygep88, i64 %lsr.iv
    %uglygep95 = getelementptr i8, ptr %uglygep94, i64 -192
    %32 = load <8 x double>, ptr %uglygep95, align 64, !tbaa !5
    %uglygep92 = getelementptr i8, ptr %uglygep88, i64 %lsr.iv
    %uglygep93 = getelementptr i8, ptr %uglygep92, i64 -128
    %33 = load <8 x double>, ptr %uglygep93, align 64, !tbaa !5
    %uglygep90 = getelementptr i8, ptr %uglygep88, i64 %lsr.iv
    %uglygep91 = getelementptr i8, ptr %uglygep90, i64 -64
    %34 = load <8 x double>, ptr %uglygep91, align 64, !tbaa !5
    %uglygep89 = getelementptr i8, ptr %uglygep88, i64 %lsr.iv
    %35 = load <8 x double>, ptr %uglygep89, align 64, !tbaa !5
    %add.i.i169 = fadd <8 x double> %32, %33
    %sub.i.i170 = fsub <8 x double> %32, %33
    %shuffle.i.i171 = shufflevector <8 x double> %add.i.i169, <8 x double> %sub.i.i170, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i172 = shufflevector <8 x double> %add.i.i169, <8 x double> %sub.i.i170, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i173 = fmul <8 x double> %shuffle.i25.i172, %shuffle.i.i171
    %36 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i171, <8 x double> %shuffle.i25.i172, <8 x double> %mul.i.i173)
    %add.i26.i174 = fadd <8 x double> %mul.i.i173, %36
    %sub.i27.i175 = fsub <8 x double> %mul.i.i173, %36
    %add.i.i176 = fadd <8 x double> %34, %35
    %sub.i.i177 = fsub <8 x double> %34, %35
    %shuffle.i.i178 = shufflevector <8 x double> %add.i.i176, <8 x double> %sub.i.i177, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i179 = shufflevector <8 x double> %add.i.i176, <8 x double> %sub.i.i177, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i180 = fmul <8 x double> %shuffle.i25.i179, %shuffle.i.i178
    %37 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i178, <8 x double> %shuffle.i25.i179, <8 x double> %mul.i.i180)
    %add.i26.i181 = fadd <8 x double> %mul.i.i180, %37
    %sub.i27.i182 = fsub <8 x double> %mul.i.i180, %37
    %add.i.i183 = fadd <8 x double> %add.i26.i174, %add.i26.i181
    %sub.i.i184 = fsub <8 x double> %add.i26.i174, %add.i26.i181
    %shuffle.i.i185 = shufflevector <8 x double> %add.i.i183, <8 x double> %sub.i.i184, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i186 = shufflevector <8 x double> %add.i.i183, <8 x double> %sub.i.i184, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i187 = fmul <8 x double> %shuffle.i25.i186, %shuffle.i.i185
    %38 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i185, <8 x double> %shuffle.i25.i186, <8 x double> %mul.i.i187)
    %add.i26.i188 = fadd <8 x double> %mul.i.i187, %38
    %sub.i27.i189 = fsub <8 x double> %mul.i.i187, %38
    %add.i.i190 = fadd <8 x double> %sub.i27.i182, %sub.i27.i182
    %sub.i.i191 = fsub <8 x double> %sub.i27.i182, %sub.i27.i182
    %shuffle.i25.i193 = shufflevector <8 x double> %add.i.i190, <8 x double> %sub.i.i191, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i194 = fmul <8 x double> %shuffle.i25.i193, %shuffle.i25.i193
    %39 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i193, <8 x double> %shuffle.i25.i193, <8 x double> %mul.i.i194)
    %sub.i27.i196 = fsub <8 x double> %mul.i.i194, %39
    tail call void @llvm.prefetch.p0(ptr %uglygep31, i32 0, i32 3, i32 1)
    %uglygep28 = getelementptr i8, ptr %uglygep24, i64 %lsr.iv
    %uglygep29 = getelementptr i8, ptr %uglygep28, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep29, i32 0, i32 3, i32 1)
    %uglygep26 = getelementptr i8, ptr %uglygep24, i64 %lsr.iv
    %uglygep27 = getelementptr i8, ptr %uglygep26, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep27, i32 0, i32 3, i32 1)
    %uglygep25 = getelementptr i8, ptr %uglygep24, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep25, i32 0, i32 3, i32 1)
    %uglygep86 = getelementptr i8, ptr %uglygep80, i64 %lsr.iv
    %uglygep87 = getelementptr i8, ptr %uglygep86, i64 -192
    %40 = load <8 x double>, ptr %uglygep87, align 64, !tbaa !5
    %uglygep84 = getelementptr i8, ptr %uglygep80, i64 %lsr.iv
    %uglygep85 = getelementptr i8, ptr %uglygep84, i64 -128
    %41 = load <8 x double>, ptr %uglygep85, align 64, !tbaa !5
    %uglygep82 = getelementptr i8, ptr %uglygep80, i64 %lsr.iv
    %uglygep83 = getelementptr i8, ptr %uglygep82, i64 -64
    %42 = load <8 x double>, ptr %uglygep83, align 64, !tbaa !5
    %uglygep81 = getelementptr i8, ptr %uglygep80, i64 %lsr.iv
    %43 = load <8 x double>, ptr %uglygep81, align 64, !tbaa !5
    %add.i.i216 = fadd <8 x double> %40, %41
    %sub.i.i217 = fsub <8 x double> %40, %41
    %shuffle.i.i218 = shufflevector <8 x double> %add.i.i216, <8 x double> %sub.i.i217, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i219 = shufflevector <8 x double> %add.i.i216, <8 x double> %sub.i.i217, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i220 = fmul <8 x double> %shuffle.i25.i219, %shuffle.i.i218
    %44 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i218, <8 x double> %shuffle.i25.i219, <8 x double> %mul.i.i220)
    %add.i26.i221 = fadd <8 x double> %mul.i.i220, %44
    %sub.i27.i222 = fsub <8 x double> %mul.i.i220, %44
    %add.i.i223 = fadd <8 x double> %42, %43
    %sub.i.i224 = fsub <8 x double> %42, %43
    %shuffle.i.i225 = shufflevector <8 x double> %add.i.i223, <8 x double> %sub.i.i224, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i226 = shufflevector <8 x double> %add.i.i223, <8 x double> %sub.i.i224, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i227 = fmul <8 x double> %shuffle.i25.i226, %shuffle.i.i225
    %45 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i225, <8 x double> %shuffle.i25.i226, <8 x double> %mul.i.i227)
    %add.i26.i228 = fadd <8 x double> %mul.i.i227, %45
    %sub.i27.i229 = fsub <8 x double> %mul.i.i227, %45
    %add.i.i230 = fadd <8 x double> %add.i26.i221, %add.i26.i228
    %sub.i.i231 = fsub <8 x double> %add.i26.i221, %add.i26.i228
    %shuffle.i.i232 = shufflevector <8 x double> %add.i.i230, <8 x double> %sub.i.i231, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i233 = shufflevector <8 x double> %add.i.i230, <8 x double> %sub.i.i231, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i234 = fmul <8 x double> %shuffle.i25.i233, %shuffle.i.i232
    %46 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i232, <8 x double> %shuffle.i25.i233, <8 x double> %mul.i.i234)
    %add.i26.i235 = fadd <8 x double> %mul.i.i234, %46
    %sub.i27.i236 = fsub <8 x double> %mul.i.i234, %46
    %add.i.i237 = fadd <8 x double> %sub.i27.i229, %sub.i27.i229
    %sub.i.i238 = fsub <8 x double> %sub.i27.i229, %sub.i27.i229
    %shuffle.i25.i240 = shufflevector <8 x double> %add.i.i237, <8 x double> %sub.i.i238, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i241 = fmul <8 x double> %shuffle.i25.i240, %shuffle.i25.i240
    %47 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i240, <8 x double> %shuffle.i25.i240, <8 x double> %mul.i.i241)
    %sub.i27.i243 = fsub <8 x double> %mul.i.i241, %47
    %uglygep134 = getelementptr i8, ptr %uglygep128, i64 %lsr.iv
    %uglygep135 = getelementptr i8, ptr %uglygep134, i64 -192
    %48 = load <8 x double>, ptr %uglygep135, align 64, !tbaa !5
    %uglygep132 = getelementptr i8, ptr %uglygep128, i64 %lsr.iv
    %uglygep133 = getelementptr i8, ptr %uglygep132, i64 -128
    %49 = load <8 x double>, ptr %uglygep133, align 64, !tbaa !5
    %uglygep130 = getelementptr i8, ptr %uglygep128, i64 %lsr.iv
    %uglygep131 = getelementptr i8, ptr %uglygep130, i64 -64
    %50 = load <8 x double>, ptr %uglygep131, align 64, !tbaa !5
    %uglygep129 = getelementptr i8, ptr %uglygep128, i64 %lsr.iv
    %51 = load <8 x double>, ptr %uglygep129, align 64, !tbaa !5
    %add.i.i261 = fadd <8 x double> %48, %49
    %sub.i.i262 = fsub <8 x double> %48, %49
    %shuffle.i.i263 = shufflevector <8 x double> %add.i.i261, <8 x double> %sub.i.i262, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i264 = shufflevector <8 x double> %add.i.i261, <8 x double> %sub.i.i262, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i265 = fmul <8 x double> %shuffle.i25.i264, %shuffle.i.i263
    %52 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i263, <8 x double> %shuffle.i25.i264, <8 x double> %mul.i.i265)
    %add.i26.i266 = fadd <8 x double> %mul.i.i265, %52
    %sub.i27.i267 = fsub <8 x double> %mul.i.i265, %52
    %add.i.i268 = fadd <8 x double> %50, %51
    %sub.i.i269 = fsub <8 x double> %50, %51
    %shuffle.i.i270 = shufflevector <8 x double> %add.i.i268, <8 x double> %sub.i.i269, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i271 = shufflevector <8 x double> %add.i.i268, <8 x double> %sub.i.i269, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i272 = fmul <8 x double> %shuffle.i25.i271, %shuffle.i.i270
    %53 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i270, <8 x double> %shuffle.i25.i271, <8 x double> %mul.i.i272)
    %add.i26.i273 = fadd <8 x double> %mul.i.i272, %53
    %sub.i27.i274 = fsub <8 x double> %mul.i.i272, %53
    %add.i.i275 = fadd <8 x double> %add.i26.i266, %add.i26.i273
    %sub.i.i276 = fsub <8 x double> %add.i26.i266, %add.i26.i273
    %shuffle.i.i277 = shufflevector <8 x double> %add.i.i275, <8 x double> %sub.i.i276, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i278 = shufflevector <8 x double> %add.i.i275, <8 x double> %sub.i.i276, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i279 = fmul <8 x double> %shuffle.i25.i278, %shuffle.i.i277
    %54 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i277, <8 x double> %shuffle.i25.i278, <8 x double> %mul.i.i279)
    %add.i26.i280 = fadd <8 x double> %mul.i.i279, %54
    %sub.i27.i281 = fsub <8 x double> %mul.i.i279, %54
    %add.i.i282 = fadd <8 x double> %sub.i27.i274, %sub.i27.i274
    %sub.i.i283 = fsub <8 x double> %sub.i27.i274, %sub.i27.i274
    %shuffle.i25.i285 = shufflevector <8 x double> %add.i.i282, <8 x double> %sub.i.i283, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i286 = fmul <8 x double> %shuffle.i25.i285, %shuffle.i25.i285
    %55 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i285, <8 x double> %shuffle.i25.i285, <8 x double> %mul.i.i286)
    %sub.i27.i288 = fsub <8 x double> %mul.i.i286, %55
    %uglygep22 = getelementptr i8, ptr %uglygep16, i64 %lsr.iv
    %uglygep23 = getelementptr i8, ptr %uglygep22, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep23, i32 0, i32 3, i32 1)
    %uglygep20 = getelementptr i8, ptr %uglygep16, i64 %lsr.iv
    %uglygep21 = getelementptr i8, ptr %uglygep20, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep21, i32 0, i32 3, i32 1)
    %uglygep18 = getelementptr i8, ptr %uglygep16, i64 %lsr.iv
    %uglygep19 = getelementptr i8, ptr %uglygep18, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep19, i32 0, i32 3, i32 1)
    %uglygep17 = getelementptr i8, ptr %uglygep16, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep17, i32 0, i32 3, i32 1)
    %uglygep78 = getelementptr i8, ptr %uglygep72, i64 %lsr.iv
    %uglygep79 = getelementptr i8, ptr %uglygep78, i64 -192
    %56 = load <8 x double>, ptr %uglygep79, align 64, !tbaa !5
    %uglygep76 = getelementptr i8, ptr %uglygep72, i64 %lsr.iv
    %uglygep77 = getelementptr i8, ptr %uglygep76, i64 -128
    %57 = load <8 x double>, ptr %uglygep77, align 64, !tbaa !5
    %uglygep74 = getelementptr i8, ptr %uglygep72, i64 %lsr.iv
    %uglygep75 = getelementptr i8, ptr %uglygep74, i64 -64
    %58 = load <8 x double>, ptr %uglygep75, align 64, !tbaa !5
    %uglygep73 = getelementptr i8, ptr %uglygep72, i64 %lsr.iv
    %59 = load <8 x double>, ptr %uglygep73, align 64, !tbaa !5
    %add.i.i324 = fadd <8 x double> %56, %57
    %sub.i.i325 = fsub <8 x double> %56, %57
    %shuffle.i.i326 = shufflevector <8 x double> %add.i.i324, <8 x double> %sub.i.i325, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i327 = shufflevector <8 x double> %add.i.i324, <8 x double> %sub.i.i325, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i328 = fmul <8 x double> %shuffle.i25.i327, %shuffle.i.i326
    %60 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i326, <8 x double> %shuffle.i25.i327, <8 x double> %mul.i.i328)
    %add.i26.i329 = fadd <8 x double> %mul.i.i328, %60
    %sub.i27.i330 = fsub <8 x double> %mul.i.i328, %60
    %add.i.i331 = fadd <8 x double> %58, %59
    %sub.i.i332 = fsub <8 x double> %58, %59
    %shuffle.i.i333 = shufflevector <8 x double> %add.i.i331, <8 x double> %sub.i.i332, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i334 = shufflevector <8 x double> %add.i.i331, <8 x double> %sub.i.i332, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i335 = fmul <8 x double> %shuffle.i25.i334, %shuffle.i.i333
    %61 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i333, <8 x double> %shuffle.i25.i334, <8 x double> %mul.i.i335)
    %add.i26.i336 = fadd <8 x double> %mul.i.i335, %61
    %sub.i27.i337 = fsub <8 x double> %mul.i.i335, %61
    %add.i.i338 = fadd <8 x double> %add.i26.i329, %add.i26.i336
    %sub.i.i339 = fsub <8 x double> %add.i26.i329, %add.i26.i336
    %shuffle.i.i340 = shufflevector <8 x double> %add.i.i338, <8 x double> %sub.i.i339, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i341 = shufflevector <8 x double> %add.i.i338, <8 x double> %sub.i.i339, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i342 = fmul <8 x double> %shuffle.i25.i341, %shuffle.i.i340
    %62 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i340, <8 x double> %shuffle.i25.i341, <8 x double> %mul.i.i342)
    %add.i26.i343 = fadd <8 x double> %mul.i.i342, %62
    %sub.i27.i344 = fsub <8 x double> %mul.i.i342, %62
    %add.i.i345 = fadd <8 x double> %sub.i27.i337, %sub.i27.i337
    %sub.i.i346 = fsub <8 x double> %sub.i27.i337, %sub.i27.i337
    %shuffle.i25.i348 = shufflevector <8 x double> %add.i.i345, <8 x double> %sub.i.i346, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i349 = fmul <8 x double> %shuffle.i25.i348, %shuffle.i25.i348
    %63 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i348, <8 x double> %shuffle.i25.i348, <8 x double> %mul.i.i349)
    %sub.i27.i351 = fsub <8 x double> %mul.i.i349, %63
    %uglygep126 = getelementptr i8, ptr %uglygep120, i64 %lsr.iv
    %uglygep127 = getelementptr i8, ptr %uglygep126, i64 -192
    %64 = load <8 x double>, ptr %uglygep127, align 64, !tbaa !5
    %uglygep124 = getelementptr i8, ptr %uglygep120, i64 %lsr.iv
    %uglygep125 = getelementptr i8, ptr %uglygep124, i64 -128
    %65 = load <8 x double>, ptr %uglygep125, align 64, !tbaa !5
    %uglygep122 = getelementptr i8, ptr %uglygep120, i64 %lsr.iv
    %uglygep123 = getelementptr i8, ptr %uglygep122, i64 -64
    %66 = load <8 x double>, ptr %uglygep123, align 64, !tbaa !5
    %uglygep121 = getelementptr i8, ptr %uglygep120, i64 %lsr.iv
    %67 = load <8 x double>, ptr %uglygep121, align 64, !tbaa !5
    %add.i.i367 = fadd <8 x double> %64, %65
    %sub.i.i368 = fsub <8 x double> %64, %65
    %shuffle.i.i369 = shufflevector <8 x double> %add.i.i367, <8 x double> %sub.i.i368, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i370 = shufflevector <8 x double> %add.i.i367, <8 x double> %sub.i.i368, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i371 = fmul <8 x double> %shuffle.i25.i370, %shuffle.i.i369
    %68 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i369, <8 x double> %shuffle.i25.i370, <8 x double> %mul.i.i371)
    %add.i26.i372 = fadd <8 x double> %mul.i.i371, %68
    %sub.i27.i373 = fsub <8 x double> %mul.i.i371, %68
    %add.i.i374 = fadd <8 x double> %66, %67
    %sub.i.i375 = fsub <8 x double> %66, %67
    %shuffle.i.i376 = shufflevector <8 x double> %add.i.i374, <8 x double> %sub.i.i375, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i377 = shufflevector <8 x double> %add.i.i374, <8 x double> %sub.i.i375, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i378 = fmul <8 x double> %shuffle.i25.i377, %shuffle.i.i376
    %69 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i376, <8 x double> %shuffle.i25.i377, <8 x double> %mul.i.i378)
    %add.i26.i379 = fadd <8 x double> %mul.i.i378, %69
    %sub.i27.i380 = fsub <8 x double> %mul.i.i378, %69
    %add.i.i381 = fadd <8 x double> %add.i26.i372, %add.i26.i379
    %sub.i.i382 = fsub <8 x double> %add.i26.i372, %add.i26.i379
    %shuffle.i.i383 = shufflevector <8 x double> %add.i.i381, <8 x double> %sub.i.i382, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i384 = shufflevector <8 x double> %add.i.i381, <8 x double> %sub.i.i382, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i385 = fmul <8 x double> %shuffle.i25.i384, %shuffle.i.i383
    %70 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i383, <8 x double> %shuffle.i25.i384, <8 x double> %mul.i.i385)
    %add.i26.i386 = fadd <8 x double> %mul.i.i385, %70
    %sub.i27.i387 = fsub <8 x double> %mul.i.i385, %70
    %add.i.i388 = fadd <8 x double> %sub.i27.i380, %sub.i27.i380
    %sub.i.i389 = fsub <8 x double> %sub.i27.i380, %sub.i27.i380
    %shuffle.i25.i391 = shufflevector <8 x double> %add.i.i388, <8 x double> %sub.i.i389, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i392 = fmul <8 x double> %shuffle.i25.i391, %shuffle.i25.i391
    %71 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i391, <8 x double> %shuffle.i25.i391, <8 x double> %mul.i.i392)
    %sub.i27.i394 = fsub <8 x double> %mul.i.i392, %71
    %uglygep14 = getelementptr i8, ptr %uglygep8, i64 %lsr.iv
    %uglygep15 = getelementptr i8, ptr %uglygep14, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep15, i32 0, i32 3, i32 1)
    %uglygep12 = getelementptr i8, ptr %uglygep8, i64 %lsr.iv
    %uglygep13 = getelementptr i8, ptr %uglygep12, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep13, i32 0, i32 3, i32 1)
    %uglygep10 = getelementptr i8, ptr %uglygep8, i64 %lsr.iv
    %uglygep11 = getelementptr i8, ptr %uglygep10, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep11, i32 0, i32 3, i32 1)
    %uglygep9 = getelementptr i8, ptr %uglygep8, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep9, i32 0, i32 3, i32 1)
    %uglygep118 = getelementptr i8, ptr %uglygep112, i64 %lsr.iv
    %uglygep119 = getelementptr i8, ptr %uglygep118, i64 -192
    %72 = load <8 x double>, ptr %uglygep119, align 64, !tbaa !5
    %uglygep116 = getelementptr i8, ptr %uglygep112, i64 %lsr.iv
    %uglygep117 = getelementptr i8, ptr %uglygep116, i64 -128
    %73 = load <8 x double>, ptr %uglygep117, align 64, !tbaa !5
    %uglygep114 = getelementptr i8, ptr %uglygep112, i64 %lsr.iv
    %uglygep115 = getelementptr i8, ptr %uglygep114, i64 -64
    %74 = load <8 x double>, ptr %uglygep115, align 64, !tbaa !5
    %uglygep113 = getelementptr i8, ptr %uglygep112, i64 %lsr.iv
    %75 = load <8 x double>, ptr %uglygep113, align 64, !tbaa !5
    %add.i.i426 = fadd <8 x double> %72, %73
    %sub.i.i427 = fsub <8 x double> %72, %73
    %shuffle.i.i428 = shufflevector <8 x double> %add.i.i426, <8 x double> %sub.i.i427, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i429 = shufflevector <8 x double> %add.i.i426, <8 x double> %sub.i.i427, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i430 = fmul <8 x double> %shuffle.i25.i429, %shuffle.i.i428
    %76 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i428, <8 x double> %shuffle.i25.i429, <8 x double> %mul.i.i430)
    %add.i26.i431 = fadd <8 x double> %mul.i.i430, %76
    %sub.i27.i432 = fsub <8 x double> %mul.i.i430, %76
    %add.i.i433 = fadd <8 x double> %74, %75
    %sub.i.i434 = fsub <8 x double> %74, %75
    %shuffle.i.i435 = shufflevector <8 x double> %add.i.i433, <8 x double> %sub.i.i434, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i436 = shufflevector <8 x double> %add.i.i433, <8 x double> %sub.i.i434, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i437 = fmul <8 x double> %shuffle.i25.i436, %shuffle.i.i435
    %77 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i435, <8 x double> %shuffle.i25.i436, <8 x double> %mul.i.i437)
    %add.i26.i438 = fadd <8 x double> %mul.i.i437, %77
    %sub.i27.i439 = fsub <8 x double> %mul.i.i437, %77
    %add.i.i440 = fadd <8 x double> %add.i26.i431, %add.i26.i438
    %sub.i.i441 = fsub <8 x double> %add.i26.i431, %add.i26.i438
    %shuffle.i.i442 = shufflevector <8 x double> %add.i.i440, <8 x double> %sub.i.i441, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i443 = shufflevector <8 x double> %add.i.i440, <8 x double> %sub.i.i441, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i444 = fmul <8 x double> %shuffle.i25.i443, %shuffle.i.i442
    %78 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i442, <8 x double> %shuffle.i25.i443, <8 x double> %mul.i.i444)
    %add.i26.i445 = fadd <8 x double> %mul.i.i444, %78
    %sub.i27.i446 = fsub <8 x double> %mul.i.i444, %78
    %add.i.i447 = fadd <8 x double> %sub.i27.i439, %sub.i27.i439
    %sub.i.i448 = fsub <8 x double> %sub.i27.i439, %sub.i27.i439
    %shuffle.i25.i450 = shufflevector <8 x double> %add.i.i447, <8 x double> %sub.i.i448, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i451 = fmul <8 x double> %shuffle.i25.i450, %shuffle.i25.i450
    %79 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i450, <8 x double> %shuffle.i25.i450, <8 x double> %mul.i.i451)
    %sub.i27.i453 = fsub <8 x double> %mul.i.i451, %79
    %80 = load <8 x double>, ptr %uglygep111, align 64, !tbaa !5
    %uglygep108 = getelementptr i8, ptr %uglygep104, i64 %lsr.iv
    %uglygep109 = getelementptr i8, ptr %uglygep108, i64 -128
    %81 = load <8 x double>, ptr %uglygep109, align 64, !tbaa !5
    %uglygep106 = getelementptr i8, ptr %uglygep104, i64 %lsr.iv
    %uglygep107 = getelementptr i8, ptr %uglygep106, i64 -64
    %82 = load <8 x double>, ptr %uglygep107, align 64, !tbaa !5
    %uglygep105 = getelementptr i8, ptr %uglygep104, i64 %lsr.iv
    %83 = load <8 x double>, ptr %uglygep105, align 64, !tbaa !5
    %add.i.i461 = fadd <8 x double> %80, %81
    %sub.i.i462 = fsub <8 x double> %80, %81
    %shuffle.i.i463 = shufflevector <8 x double> %add.i.i461, <8 x double> %sub.i.i462, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i464 = shufflevector <8 x double> %add.i.i461, <8 x double> %sub.i.i462, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i465 = fmul <8 x double> %shuffle.i25.i464, %shuffle.i.i463
    %84 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i463, <8 x double> %shuffle.i25.i464, <8 x double> %mul.i.i465)
    %add.i26.i466 = fadd <8 x double> %mul.i.i465, %84
    %sub.i27.i467 = fsub <8 x double> %mul.i.i465, %84
    %add.i.i468 = fadd <8 x double> %82, %83
    %sub.i.i469 = fsub <8 x double> %82, %83
    %shuffle.i.i470 = shufflevector <8 x double> %add.i.i468, <8 x double> %sub.i.i469, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i471 = shufflevector <8 x double> %add.i.i468, <8 x double> %sub.i.i469, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i472 = fmul <8 x double> %shuffle.i25.i471, %shuffle.i.i470
    %85 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i470, <8 x double> %shuffle.i25.i471, <8 x double> %mul.i.i472)
    %add.i26.i473 = fadd <8 x double> %mul.i.i472, %85
    %sub.i27.i474 = fsub <8 x double> %mul.i.i472, %85
    %add.i.i475 = fadd <8 x double> %add.i26.i466, %add.i26.i473
    %sub.i.i476 = fsub <8 x double> %add.i26.i466, %add.i26.i473
    %shuffle.i.i477 = shufflevector <8 x double> %add.i.i475, <8 x double> %sub.i.i476, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i478 = shufflevector <8 x double> %add.i.i475, <8 x double> %sub.i.i476, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i479 = fmul <8 x double> %shuffle.i25.i478, %shuffle.i.i477
    %86 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i477, <8 x double> %shuffle.i25.i478, <8 x double> %mul.i.i479)
    %add.i26.i480 = fadd <8 x double> %mul.i.i479, %86
    %sub.i27.i481 = fsub <8 x double> %mul.i.i479, %86
    %add.i.i482 = fadd <8 x double> %sub.i27.i474, %sub.i27.i474
    %sub.i.i483 = fsub <8 x double> %sub.i27.i474, %sub.i27.i474
    %shuffle.i25.i485 = shufflevector <8 x double> %add.i.i482, <8 x double> %sub.i.i483, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i486 = fmul <8 x double> %shuffle.i25.i485, %shuffle.i25.i485
    %87 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i25.i485, <8 x double> %shuffle.i25.i485, <8 x double> %mul.i.i486)
    %sub.i27.i488 = fsub <8 x double> %mul.i.i486, %87
    %uglygep62 = getelementptr i8, ptr %uglygep56, i64 %lsr.iv
    %uglygep63 = getelementptr i8, ptr %uglygep62, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep63, i32 0, i32 3, i32 1)
    %uglygep60 = getelementptr i8, ptr %uglygep56, i64 %lsr.iv
    %uglygep61 = getelementptr i8, ptr %uglygep60, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep61, i32 0, i32 3, i32 1)
    %uglygep58 = getelementptr i8, ptr %uglygep56, i64 %lsr.iv
    %uglygep59 = getelementptr i8, ptr %uglygep58, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep59, i32 0, i32 3, i32 1)
    %uglygep57 = getelementptr i8, ptr %uglygep56, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep57, i32 0, i32 3, i32 1)
    %add.i.i505 = fadd <8 x double> %add.i26.i144, %add.i26.i188
    %sub.i.i506 = fsub <8 x double> %add.i26.i144, %add.i26.i188
    %shuffle.i.i507 = shufflevector <8 x double> %add.i.i505, <8 x double> %sub.i.i506, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i508 = shufflevector <8 x double> %add.i.i505, <8 x double> %sub.i.i506, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i509 = fmul <8 x double> %shuffle.i25.i508, %shuffle.i.i507
    %88 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i507, <8 x double> %shuffle.i25.i508, <8 x double> %mul.i.i509)
    %add.i26.i510 = fadd <8 x double> %mul.i.i509, %88
    %sub.i27.i511 = fsub <8 x double> %mul.i.i509, %88
    %add.i.i512 = fadd <8 x double> %add.i26.i235, %add.i26.i280
    %sub.i.i513 = fsub <8 x double> %add.i26.i235, %add.i26.i280
    %shuffle.i.i514 = shufflevector <8 x double> %add.i.i512, <8 x double> %sub.i.i513, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i515 = shufflevector <8 x double> %add.i.i512, <8 x double> %sub.i.i513, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i516 = fmul <8 x double> %shuffle.i25.i515, %shuffle.i.i514
    %89 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i514, <8 x double> %shuffle.i25.i515, <8 x double> %mul.i.i516)
    %add.i26.i517 = fadd <8 x double> %mul.i.i516, %89
    %sub.i27.i518 = fsub <8 x double> %mul.i.i516, %89
    %add.i.i519 = fadd <8 x double> %add.i26.i343, %add.i26.i386
    %sub.i.i520 = fsub <8 x double> %add.i26.i343, %add.i26.i386
    %shuffle.i.i521 = shufflevector <8 x double> %add.i.i519, <8 x double> %sub.i.i520, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i522 = shufflevector <8 x double> %add.i.i519, <8 x double> %sub.i.i520, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i523 = fmul <8 x double> %shuffle.i25.i522, %shuffle.i.i521
    %90 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i521, <8 x double> %shuffle.i25.i522, <8 x double> %mul.i.i523)
    %add.i26.i524 = fadd <8 x double> %mul.i.i523, %90
    %sub.i27.i525 = fsub <8 x double> %mul.i.i523, %90
    %add.i.i526 = fadd <8 x double> %add.i26.i445, %add.i26.i480
    %sub.i.i527 = fsub <8 x double> %add.i26.i445, %add.i26.i480
    %shuffle.i.i528 = shufflevector <8 x double> %add.i.i526, <8 x double> %sub.i.i527, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i529 = shufflevector <8 x double> %add.i.i526, <8 x double> %sub.i.i527, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i530 = fmul <8 x double> %shuffle.i25.i529, %shuffle.i.i528
    %91 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i528, <8 x double> %shuffle.i25.i529, <8 x double> %mul.i.i530)
    %add.i26.i531 = fadd <8 x double> %mul.i.i530, %91
    %sub.i27.i532 = fsub <8 x double> %mul.i.i530, %91
    %add.i.i533 = fadd <8 x double> %add.i26.i510, %add.i26.i524
    %sub.i.i534 = fsub <8 x double> %add.i26.i510, %add.i26.i524
    %shuffle.i.i535 = shufflevector <8 x double> %add.i.i533, <8 x double> %sub.i.i534, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i536 = shufflevector <8 x double> %add.i.i533, <8 x double> %sub.i.i534, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i537 = fmul <8 x double> %shuffle.i25.i536, %shuffle.i.i535
    %92 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i535, <8 x double> %shuffle.i25.i536, <8 x double> %mul.i.i537)
    %add.i26.i538 = fadd <8 x double> %mul.i.i537, %92
    %sub.i27.i539 = fsub <8 x double> %mul.i.i537, %92
    %add.i.i540 = fadd <8 x double> %add.i26.i517, %add.i26.i531
    %sub.i.i541 = fsub <8 x double> %add.i26.i517, %add.i26.i531
    %shuffle.i.i542 = shufflevector <8 x double> %add.i.i540, <8 x double> %sub.i.i541, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i543 = shufflevector <8 x double> %add.i.i540, <8 x double> %sub.i.i541, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i544 = fmul <8 x double> %shuffle.i25.i543, %shuffle.i.i542
    %93 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i542, <8 x double> %shuffle.i25.i543, <8 x double> %mul.i.i544)
    %add.i26.i545 = fadd <8 x double> %mul.i.i544, %93
    %sub.i27.i546 = fsub <8 x double> %mul.i.i544, %93
    %add.i.i547 = fadd <8 x double> %sub.i27.i511, %sub.i27.i525
    %sub.i.i548 = fsub <8 x double> %sub.i27.i511, %sub.i27.i525
    %shuffle.i.i549 = shufflevector <8 x double> %add.i.i547, <8 x double> %sub.i.i548, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i550 = shufflevector <8 x double> %add.i.i547, <8 x double> %sub.i.i548, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i551 = fmul <8 x double> %shuffle.i25.i550, %shuffle.i.i549
    %94 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i549, <8 x double> %shuffle.i25.i550, <8 x double> %mul.i.i551)
    %add.i26.i552 = fadd <8 x double> %mul.i.i551, %94
    %sub.i27.i553 = fsub <8 x double> %mul.i.i551, %94
    %add.i.i554 = fadd <8 x double> %sub.i27.i518, %sub.i27.i532
    %sub.i.i555 = fsub <8 x double> %sub.i27.i518, %sub.i27.i532
    %shuffle.i.i556 = shufflevector <8 x double> %add.i.i554, <8 x double> %sub.i.i555, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i557 = shufflevector <8 x double> %add.i.i554, <8 x double> %sub.i.i555, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i558 = fmul <8 x double> %shuffle.i25.i557, %shuffle.i.i556
    %95 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i556, <8 x double> %shuffle.i25.i557, <8 x double> %mul.i.i558)
    %add.i26.i559 = fadd <8 x double> %mul.i.i558, %95
    %sub.i27.i560 = fsub <8 x double> %mul.i.i558, %95
    %uglygep6 = getelementptr i8, ptr %uglygep, i64 %lsr.iv
    %uglygep7 = getelementptr i8, ptr %uglygep6, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep7, i32 0, i32 3, i32 1)
    %uglygep4 = getelementptr i8, ptr %uglygep, i64 %lsr.iv
    %uglygep5 = getelementptr i8, ptr %uglygep4, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep5, i32 0, i32 3, i32 1)
    %uglygep2 = getelementptr i8, ptr %uglygep, i64 %lsr.iv
    %uglygep3 = getelementptr i8, ptr %uglygep2, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep3, i32 0, i32 3, i32 1)
    %uglygep1 = getelementptr i8, ptr %uglygep, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep1, i32 0, i32 3, i32 1)
    %add.i.i577 = fadd <8 x double> %add.i26.i538, %add.i26.i545
    %sub.i.i578 = fsub <8 x double> %add.i26.i538, %add.i26.i545
    %shuffle.i.i579 = shufflevector <8 x double> %add.i.i577, <8 x double> %sub.i.i578, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i580 = shufflevector <8 x double> %add.i.i577, <8 x double> %sub.i.i578, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i581 = fmul <8 x double> %shuffle.i25.i580, %shuffle.i.i579
    %96 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i579, <8 x double> %shuffle.i25.i580, <8 x double> %mul.i.i581)
    %add.i26.i582 = fadd <8 x double> %mul.i.i581, %96
    %sub.i27.i583 = fsub <8 x double> %mul.i.i581, %96
    store <8 x double> %add.i26.i582, ptr %uglygep71, align 64, !tbaa !5
    %uglygep136 = getelementptr i8, ptr %uglygep71, i64 %mul.i1027
    store <8 x double> %sub.i27.i583, ptr %uglygep136, align 64, !tbaa !5
    %add.i.i588 = fadd <8 x double> %sub.i27.i539, %sub.i27.i546
    %sub.i.i589 = fsub <8 x double> %sub.i27.i539, %sub.i27.i546
    %shuffle.i.i590 = shufflevector <8 x double> %add.i.i588, <8 x double> %sub.i.i589, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i591 = shufflevector <8 x double> %add.i.i588, <8 x double> %sub.i.i589, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i592 = fmul <8 x double> %shuffle.i25.i591, %shuffle.i.i590
    %97 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i590, <8 x double> %shuffle.i25.i591, <8 x double> %mul.i.i592)
    %add.i26.i593 = fadd <8 x double> %mul.i.i592, %97
    %sub.i27.i594 = fsub <8 x double> %mul.i.i592, %97
    %uglygep137 = getelementptr i8, ptr %uglygep136, i64 %mul.i1027
    store <8 x double> %add.i26.i593, ptr %uglygep137, align 64, !tbaa !5
    %uglygep138 = getelementptr i8, ptr %uglygep137, i64 %mul.i1027
    store <8 x double> %sub.i27.i594, ptr %uglygep138, align 64, !tbaa !5
    %add.i.i603 = fadd <8 x double> %add.i26.i552, %add.i26.i559
    %sub.i.i604 = fsub <8 x double> %add.i26.i552, %add.i26.i559
    %shuffle.i.i605 = shufflevector <8 x double> %add.i.i603, <8 x double> %sub.i.i604, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i606 = shufflevector <8 x double> %add.i.i603, <8 x double> %sub.i.i604, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i607 = fmul <8 x double> %shuffle.i25.i606, %shuffle.i.i605
    %98 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i605, <8 x double> %shuffle.i25.i606, <8 x double> %mul.i.i607)
    %add.i26.i608 = fadd <8 x double> %mul.i.i607, %98
    %sub.i27.i609 = fsub <8 x double> %mul.i.i607, %98
    %uglygep139 = getelementptr i8, ptr %uglygep138, i64 %mul.i1027
    store <8 x double> %add.i26.i608, ptr %uglygep139, align 64, !tbaa !5
    %uglygep140 = getelementptr i8, ptr %uglygep139, i64 %mul.i1027
    store <8 x double> %sub.i27.i609, ptr %uglygep140, align 64, !tbaa !5
    %add.i.i618 = fadd <8 x double> %sub.i27.i553, %sub.i27.i560
    %sub.i.i619 = fsub <8 x double> %sub.i27.i553, %sub.i27.i560
    %shuffle.i.i620 = shufflevector <8 x double> %add.i.i618, <8 x double> %sub.i.i619, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i621 = shufflevector <8 x double> %add.i.i618, <8 x double> %sub.i.i619, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i622 = fmul <8 x double> %shuffle.i25.i621, %shuffle.i.i620
    %99 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i620, <8 x double> %shuffle.i25.i621, <8 x double> %mul.i.i622)
    %add.i26.i623 = fadd <8 x double> %mul.i.i622, %99
    %sub.i27.i624 = fsub <8 x double> %mul.i.i622, %99
    %uglygep141 = getelementptr i8, ptr %uglygep140, i64 %mul.i1027
    store <8 x double> %add.i26.i623, ptr %uglygep141, align 64, !tbaa !5
    %uglygep142 = getelementptr i8, ptr %uglygep141, i64 %mul.i1027
    store <8 x double> %sub.i27.i624, ptr %uglygep142, align 64, !tbaa !5
    %uglygep54 = getelementptr i8, ptr %uglygep48, i64 %lsr.iv
    %uglygep55 = getelementptr i8, ptr %uglygep54, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep55, i32 0, i32 3, i32 1)
    %uglygep52 = getelementptr i8, ptr %uglygep48, i64 %lsr.iv
    %uglygep53 = getelementptr i8, ptr %uglygep52, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep53, i32 0, i32 3, i32 1)
    %uglygep50 = getelementptr i8, ptr %uglygep48, i64 %lsr.iv
    %uglygep51 = getelementptr i8, ptr %uglygep50, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep51, i32 0, i32 3, i32 1)
    %uglygep49 = getelementptr i8, ptr %uglygep48, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep49, i32 0, i32 3, i32 1)
    %add.i.i641 = fadd <8 x double> %sub.i27.i, %sub.i27.i175
    %sub.i.i642 = fsub <8 x double> %sub.i27.i, %sub.i27.i175
    %shuffle.i.i643 = shufflevector <8 x double> %add.i.i641, <8 x double> %sub.i.i642, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i644 = shufflevector <8 x double> %add.i.i641, <8 x double> %sub.i.i642, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i645 = fmul <8 x double> %shuffle.i25.i644, %shuffle.i.i643
    %100 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i643, <8 x double> %shuffle.i25.i644, <8 x double> %mul.i.i645)
    %add.i26.i646 = fadd <8 x double> %mul.i.i645, %100
    %sub.i27.i647 = fsub <8 x double> %mul.i.i645, %100
    %add.i.i648 = fadd <8 x double> %sub.i27.i222, %sub.i27.i267
    %sub.i.i649 = fsub <8 x double> %sub.i27.i222, %sub.i27.i267
    %shuffle.i.i650 = shufflevector <8 x double> %add.i.i648, <8 x double> %sub.i.i649, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i651 = shufflevector <8 x double> %add.i.i648, <8 x double> %sub.i.i649, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i652 = fmul <8 x double> %shuffle.i25.i651, %shuffle.i.i650
    %101 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i650, <8 x double> %shuffle.i25.i651, <8 x double> %mul.i.i652)
    %add.i26.i653 = fadd <8 x double> %mul.i.i652, %101
    %sub.i27.i654 = fsub <8 x double> %mul.i.i652, %101
    %add.i.i655 = fadd <8 x double> %sub.i27.i330, %sub.i27.i373
    %sub.i.i656 = fsub <8 x double> %sub.i27.i330, %sub.i27.i373
    %shuffle.i.i657 = shufflevector <8 x double> %add.i.i655, <8 x double> %sub.i.i656, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i658 = shufflevector <8 x double> %add.i.i655, <8 x double> %sub.i.i656, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i659 = fmul <8 x double> %shuffle.i25.i658, %shuffle.i.i657
    %102 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i657, <8 x double> %shuffle.i25.i658, <8 x double> %mul.i.i659)
    %add.i26.i660 = fadd <8 x double> %mul.i.i659, %102
    %sub.i27.i661 = fsub <8 x double> %mul.i.i659, %102
    %add.i.i662 = fadd <8 x double> %sub.i27.i432, %sub.i27.i467
    %sub.i.i663 = fsub <8 x double> %sub.i27.i432, %sub.i27.i467
    %shuffle.i.i664 = shufflevector <8 x double> %add.i.i662, <8 x double> %sub.i.i663, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i665 = shufflevector <8 x double> %add.i.i662, <8 x double> %sub.i.i663, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i666 = fmul <8 x double> %shuffle.i25.i665, %shuffle.i.i664
    %103 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i664, <8 x double> %shuffle.i25.i665, <8 x double> %mul.i.i666)
    %add.i26.i667 = fadd <8 x double> %mul.i.i666, %103
    %sub.i27.i668 = fsub <8 x double> %mul.i.i666, %103
    %add.i.i669 = fadd <8 x double> %add.i26.i646, %add.i26.i660
    %sub.i.i670 = fsub <8 x double> %add.i26.i646, %add.i26.i660
    %shuffle.i.i671 = shufflevector <8 x double> %add.i.i669, <8 x double> %sub.i.i670, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i672 = shufflevector <8 x double> %add.i.i669, <8 x double> %sub.i.i670, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i673 = fmul <8 x double> %shuffle.i25.i672, %shuffle.i.i671
    %104 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i671, <8 x double> %shuffle.i25.i672, <8 x double> %mul.i.i673)
    %add.i26.i674 = fadd <8 x double> %mul.i.i673, %104
    %sub.i27.i675 = fsub <8 x double> %mul.i.i673, %104
    %add.i.i676 = fadd <8 x double> %add.i26.i653, %add.i26.i667
    %sub.i.i677 = fsub <8 x double> %add.i26.i653, %add.i26.i667
    %shuffle.i.i678 = shufflevector <8 x double> %add.i.i676, <8 x double> %sub.i.i677, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i679 = shufflevector <8 x double> %add.i.i676, <8 x double> %sub.i.i677, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i680 = fmul <8 x double> %shuffle.i25.i679, %shuffle.i.i678
    %105 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i678, <8 x double> %shuffle.i25.i679, <8 x double> %mul.i.i680)
    %add.i26.i681 = fadd <8 x double> %mul.i.i680, %105
    %sub.i27.i682 = fsub <8 x double> %mul.i.i680, %105
    %add.i.i683 = fadd <8 x double> %sub.i27.i647, %sub.i27.i661
    %sub.i.i684 = fsub <8 x double> %sub.i27.i647, %sub.i27.i661
    %shuffle.i.i685 = shufflevector <8 x double> %add.i.i683, <8 x double> %sub.i.i684, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i686 = shufflevector <8 x double> %add.i.i683, <8 x double> %sub.i.i684, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i687 = fmul <8 x double> %shuffle.i25.i686, %shuffle.i.i685
    %106 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i685, <8 x double> %shuffle.i25.i686, <8 x double> %mul.i.i687)
    %add.i26.i688 = fadd <8 x double> %mul.i.i687, %106
    %sub.i27.i689 = fsub <8 x double> %mul.i.i687, %106
    %add.i.i690 = fadd <8 x double> %sub.i27.i654, %sub.i27.i668
    %sub.i.i691 = fsub <8 x double> %sub.i27.i654, %sub.i27.i668
    %shuffle.i.i692 = shufflevector <8 x double> %add.i.i690, <8 x double> %sub.i.i691, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i693 = shufflevector <8 x double> %add.i.i690, <8 x double> %sub.i.i691, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i694 = fmul <8 x double> %shuffle.i25.i693, %shuffle.i.i692
    %107 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i692, <8 x double> %shuffle.i25.i693, <8 x double> %mul.i.i694)
    %add.i26.i695 = fadd <8 x double> %mul.i.i694, %107
    %sub.i27.i696 = fsub <8 x double> %mul.i.i694, %107
    %add.i.i697 = fadd <8 x double> %add.i26.i674, %add.i26.i681
    %sub.i.i698 = fsub <8 x double> %add.i26.i674, %add.i26.i681
    %shuffle.i.i699 = shufflevector <8 x double> %add.i.i697, <8 x double> %sub.i.i698, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i700 = shufflevector <8 x double> %add.i.i697, <8 x double> %sub.i.i698, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i701 = fmul <8 x double> %shuffle.i25.i700, %shuffle.i.i699
    %108 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i699, <8 x double> %shuffle.i25.i700, <8 x double> %mul.i.i701)
    %add.i26.i702 = fadd <8 x double> %mul.i.i701, %108
    %sub.i27.i703 = fsub <8 x double> %mul.i.i701, %108
    %uglygep68 = getelementptr i8, ptr %uglygep64, i64 %lsr.iv
    %uglygep69 = getelementptr i8, ptr %uglygep68, i64 -128
    store <8 x double> %add.i26.i702, ptr %uglygep69, align 64, !tbaa !5
    %uglygep143 = getelementptr i8, ptr %uglygep142, i64 %23
    store <8 x double> %sub.i27.i703, ptr %uglygep143, align 64, !tbaa !5
    %add.i.i710 = fadd <8 x double> %sub.i27.i675, %sub.i27.i682
    %sub.i.i711 = fsub <8 x double> %sub.i27.i675, %sub.i27.i682
    %shuffle.i.i712 = shufflevector <8 x double> %add.i.i710, <8 x double> %sub.i.i711, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i713 = shufflevector <8 x double> %add.i.i710, <8 x double> %sub.i.i711, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i714 = fmul <8 x double> %shuffle.i25.i713, %shuffle.i.i712
    %109 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i712, <8 x double> %shuffle.i25.i713, <8 x double> %mul.i.i714)
    %add.i26.i715 = fadd <8 x double> %mul.i.i714, %109
    %sub.i27.i716 = fsub <8 x double> %mul.i.i714, %109
    %uglygep144 = getelementptr i8, ptr %uglygep143, i64 %mul.i1027
    store <8 x double> %add.i26.i715, ptr %uglygep144, align 64, !tbaa !5
    %uglygep145 = getelementptr i8, ptr %uglygep144, i64 %mul.i1027
    store <8 x double> %sub.i27.i716, ptr %uglygep145, align 64, !tbaa !5
    %add.i.i727 = fadd <8 x double> %add.i26.i688, %add.i26.i695
    %sub.i.i728 = fsub <8 x double> %add.i26.i688, %add.i26.i695
    %shuffle.i.i729 = shufflevector <8 x double> %add.i.i727, <8 x double> %sub.i.i728, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i730 = shufflevector <8 x double> %add.i.i727, <8 x double> %sub.i.i728, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i731 = fmul <8 x double> %shuffle.i25.i730, %shuffle.i.i729
    %110 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i729, <8 x double> %shuffle.i25.i730, <8 x double> %mul.i.i731)
    %add.i26.i732 = fadd <8 x double> %mul.i.i731, %110
    %sub.i27.i733 = fsub <8 x double> %mul.i.i731, %110
    %uglygep146 = getelementptr i8, ptr %uglygep145, i64 %mul.i1027
    store <8 x double> %add.i26.i732, ptr %uglygep146, align 64, !tbaa !5
    %uglygep147 = getelementptr i8, ptr %uglygep146, i64 %mul.i1027
    store <8 x double> %sub.i27.i733, ptr %uglygep147, align 64, !tbaa !5
    %add.i.i744 = fadd <8 x double> %sub.i27.i689, %sub.i27.i696
    %sub.i.i745 = fsub <8 x double> %sub.i27.i689, %sub.i27.i696
    %shuffle.i.i746 = shufflevector <8 x double> %add.i.i744, <8 x double> %sub.i.i745, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i747 = shufflevector <8 x double> %add.i.i744, <8 x double> %sub.i.i745, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i748 = fmul <8 x double> %shuffle.i25.i747, %shuffle.i.i746
    %111 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i746, <8 x double> %shuffle.i25.i747, <8 x double> %mul.i.i748)
    %add.i26.i749 = fadd <8 x double> %mul.i.i748, %111
    %sub.i27.i750 = fsub <8 x double> %mul.i.i748, %111
    %uglygep148 = getelementptr i8, ptr %uglygep147, i64 %mul.i1027
    store <8 x double> %add.i26.i749, ptr %uglygep148, align 64, !tbaa !5
    %uglygep149 = getelementptr i8, ptr %uglygep148, i64 %mul.i1027
    store <8 x double> %sub.i27.i750, ptr %uglygep149, align 64, !tbaa !5
    %uglygep46 = getelementptr i8, ptr %uglygep40, i64 %lsr.iv
    %uglygep47 = getelementptr i8, ptr %uglygep46, i64 -192
    tail call void @llvm.prefetch.p0(ptr %uglygep47, i32 0, i32 3, i32 1)
    %uglygep44 = getelementptr i8, ptr %uglygep40, i64 %lsr.iv
    %uglygep45 = getelementptr i8, ptr %uglygep44, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep45, i32 0, i32 3, i32 1)
    %uglygep42 = getelementptr i8, ptr %uglygep40, i64 %lsr.iv
    %uglygep43 = getelementptr i8, ptr %uglygep42, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep43, i32 0, i32 3, i32 1)
    %uglygep41 = getelementptr i8, ptr %uglygep40, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep41, i32 0, i32 3, i32 1)
    %add.i.i773 = fadd <8 x double> %sub.i27.i145, %sub.i27.i189
    %sub.i.i774 = fsub <8 x double> %sub.i27.i145, %sub.i27.i189
    %shuffle.i.i775 = shufflevector <8 x double> %add.i.i773, <8 x double> %sub.i.i774, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i776 = shufflevector <8 x double> %add.i.i773, <8 x double> %sub.i.i774, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i777 = fmul <8 x double> %shuffle.i25.i776, %shuffle.i.i775
    %112 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i775, <8 x double> %shuffle.i25.i776, <8 x double> %mul.i.i777)
    %add.i26.i778 = fadd <8 x double> %mul.i.i777, %112
    %sub.i27.i779 = fsub <8 x double> %mul.i.i777, %112
    %add.i.i780 = fadd <8 x double> %sub.i27.i236, %sub.i27.i281
    %sub.i.i781 = fsub <8 x double> %sub.i27.i236, %sub.i27.i281
    %shuffle.i.i782 = shufflevector <8 x double> %add.i.i780, <8 x double> %sub.i.i781, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i783 = shufflevector <8 x double> %add.i.i780, <8 x double> %sub.i.i781, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i784 = fmul <8 x double> %shuffle.i25.i783, %shuffle.i.i782
    %113 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i782, <8 x double> %shuffle.i25.i783, <8 x double> %mul.i.i784)
    %add.i26.i785 = fadd <8 x double> %mul.i.i784, %113
    %sub.i27.i786 = fsub <8 x double> %mul.i.i784, %113
    %add.i.i787 = fadd <8 x double> %sub.i27.i344, %sub.i27.i387
    %sub.i.i788 = fsub <8 x double> %sub.i27.i344, %sub.i27.i387
    %shuffle.i.i789 = shufflevector <8 x double> %add.i.i787, <8 x double> %sub.i.i788, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i790 = shufflevector <8 x double> %add.i.i787, <8 x double> %sub.i.i788, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i791 = fmul <8 x double> %shuffle.i25.i790, %shuffle.i.i789
    %114 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i789, <8 x double> %shuffle.i25.i790, <8 x double> %mul.i.i791)
    %add.i26.i792 = fadd <8 x double> %mul.i.i791, %114
    %sub.i27.i793 = fsub <8 x double> %mul.i.i791, %114
    %add.i.i794 = fadd <8 x double> %sub.i27.i446, %sub.i27.i481
    %sub.i.i795 = fsub <8 x double> %sub.i27.i446, %sub.i27.i481
    %shuffle.i.i796 = shufflevector <8 x double> %add.i.i794, <8 x double> %sub.i.i795, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i797 = shufflevector <8 x double> %add.i.i794, <8 x double> %sub.i.i795, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i798 = fmul <8 x double> %shuffle.i25.i797, %shuffle.i.i796
    %115 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i796, <8 x double> %shuffle.i25.i797, <8 x double> %mul.i.i798)
    %add.i26.i799 = fadd <8 x double> %mul.i.i798, %115
    %sub.i27.i800 = fsub <8 x double> %mul.i.i798, %115
    %add.i.i801 = fadd <8 x double> %add.i26.i778, %add.i26.i792
    %sub.i.i802 = fsub <8 x double> %add.i26.i778, %add.i26.i792
    %shuffle.i.i803 = shufflevector <8 x double> %add.i.i801, <8 x double> %sub.i.i802, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i804 = shufflevector <8 x double> %add.i.i801, <8 x double> %sub.i.i802, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i805 = fmul <8 x double> %shuffle.i25.i804, %shuffle.i.i803
    %116 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i803, <8 x double> %shuffle.i25.i804, <8 x double> %mul.i.i805)
    %add.i26.i806 = fadd <8 x double> %mul.i.i805, %116
    %sub.i27.i807 = fsub <8 x double> %mul.i.i805, %116
    %add.i.i808 = fadd <8 x double> %add.i26.i785, %add.i26.i799
    %sub.i.i809 = fsub <8 x double> %add.i26.i785, %add.i26.i799
    %shuffle.i.i810 = shufflevector <8 x double> %add.i.i808, <8 x double> %sub.i.i809, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i811 = shufflevector <8 x double> %add.i.i808, <8 x double> %sub.i.i809, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i812 = fmul <8 x double> %shuffle.i25.i811, %shuffle.i.i810
    %117 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i810, <8 x double> %shuffle.i25.i811, <8 x double> %mul.i.i812)
    %add.i26.i813 = fadd <8 x double> %mul.i.i812, %117
    %sub.i27.i814 = fsub <8 x double> %mul.i.i812, %117
    %add.i.i815 = fadd <8 x double> %sub.i27.i779, %sub.i27.i793
    %sub.i.i816 = fsub <8 x double> %sub.i27.i779, %sub.i27.i793
    %shuffle.i.i817 = shufflevector <8 x double> %add.i.i815, <8 x double> %sub.i.i816, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i818 = shufflevector <8 x double> %add.i.i815, <8 x double> %sub.i.i816, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i819 = fmul <8 x double> %shuffle.i25.i818, %shuffle.i.i817
    %118 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i817, <8 x double> %shuffle.i25.i818, <8 x double> %mul.i.i819)
    %add.i26.i820 = fadd <8 x double> %mul.i.i819, %118
    %sub.i27.i821 = fsub <8 x double> %mul.i.i819, %118
    %add.i.i822 = fadd <8 x double> %sub.i27.i786, %sub.i27.i800
    %sub.i.i823 = fsub <8 x double> %sub.i27.i786, %sub.i27.i800
    %shuffle.i.i824 = shufflevector <8 x double> %add.i.i822, <8 x double> %sub.i.i823, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i825 = shufflevector <8 x double> %add.i.i822, <8 x double> %sub.i.i823, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i826 = fmul <8 x double> %shuffle.i25.i825, %shuffle.i.i824
    %119 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i824, <8 x double> %shuffle.i25.i825, <8 x double> %mul.i.i826)
    %add.i26.i827 = fadd <8 x double> %mul.i.i826, %119
    %sub.i27.i828 = fsub <8 x double> %mul.i.i826, %119
    %add.i.i829 = fadd <8 x double> %add.i26.i806, %add.i26.i813
    %sub.i.i830 = fsub <8 x double> %add.i26.i806, %add.i26.i813
    %shuffle.i.i831 = shufflevector <8 x double> %add.i.i829, <8 x double> %sub.i.i830, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i832 = shufflevector <8 x double> %add.i.i829, <8 x double> %sub.i.i830, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i833 = fmul <8 x double> %shuffle.i25.i832, %shuffle.i.i831
    %120 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i831, <8 x double> %shuffle.i25.i832, <8 x double> %mul.i.i833)
    %add.i26.i834 = fadd <8 x double> %mul.i.i833, %120
    %sub.i27.i835 = fsub <8 x double> %mul.i.i833, %120
    %uglygep66 = getelementptr i8, ptr %uglygep64, i64 %lsr.iv
    %uglygep67 = getelementptr i8, ptr %uglygep66, i64 -64
    store <8 x double> %add.i26.i834, ptr %uglygep67, align 64, !tbaa !5
    %uglygep150 = getelementptr i8, ptr %uglygep149, i64 %23
    store <8 x double> %sub.i27.i835, ptr %uglygep150, align 64, !tbaa !5
    %add.i.i842 = fadd <8 x double> %sub.i27.i807, %sub.i27.i814
    %sub.i.i843 = fsub <8 x double> %sub.i27.i807, %sub.i27.i814
    %shuffle.i.i844 = shufflevector <8 x double> %add.i.i842, <8 x double> %sub.i.i843, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i845 = shufflevector <8 x double> %add.i.i842, <8 x double> %sub.i.i843, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i846 = fmul <8 x double> %shuffle.i25.i845, %shuffle.i.i844
    %121 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i844, <8 x double> %shuffle.i25.i845, <8 x double> %mul.i.i846)
    %add.i26.i847 = fadd <8 x double> %mul.i.i846, %121
    %sub.i27.i848 = fsub <8 x double> %mul.i.i846, %121
    %uglygep151 = getelementptr i8, ptr %uglygep150, i64 %mul.i1027
    store <8 x double> %add.i26.i847, ptr %uglygep151, align 64, !tbaa !5
    %uglygep152 = getelementptr i8, ptr %uglygep151, i64 %mul.i1027
    store <8 x double> %sub.i27.i848, ptr %uglygep152, align 64, !tbaa !5
    %add.i.i859 = fadd <8 x double> %add.i26.i820, %add.i26.i827
    %sub.i.i860 = fsub <8 x double> %add.i26.i820, %add.i26.i827
    %shuffle.i.i861 = shufflevector <8 x double> %add.i.i859, <8 x double> %sub.i.i860, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i862 = shufflevector <8 x double> %add.i.i859, <8 x double> %sub.i.i860, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i863 = fmul <8 x double> %shuffle.i25.i862, %shuffle.i.i861
    %122 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i861, <8 x double> %shuffle.i25.i862, <8 x double> %mul.i.i863)
    %add.i26.i864 = fadd <8 x double> %mul.i.i863, %122
    %sub.i27.i865 = fsub <8 x double> %mul.i.i863, %122
    %uglygep153 = getelementptr i8, ptr %uglygep152, i64 %mul.i1027
    store <8 x double> %add.i26.i864, ptr %uglygep153, align 64, !tbaa !5
    %uglygep154 = getelementptr i8, ptr %uglygep153, i64 %mul.i1027
    store <8 x double> %sub.i27.i865, ptr %uglygep154, align 64, !tbaa !5
    %add.i.i876 = fadd <8 x double> %sub.i27.i821, %sub.i27.i828
    %sub.i.i877 = fsub <8 x double> %sub.i27.i821, %sub.i27.i828
    %shuffle.i.i878 = shufflevector <8 x double> %add.i.i876, <8 x double> %sub.i.i877, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i879 = shufflevector <8 x double> %add.i.i876, <8 x double> %sub.i.i877, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i880 = fmul <8 x double> %shuffle.i25.i879, %shuffle.i.i878
    %123 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i878, <8 x double> %shuffle.i25.i879, <8 x double> %mul.i.i880)
    %add.i26.i881 = fadd <8 x double> %mul.i.i880, %123
    %sub.i27.i882 = fsub <8 x double> %mul.i.i880, %123
    %uglygep155 = getelementptr i8, ptr %uglygep154, i64 %mul.i1027
    store <8 x double> %add.i26.i881, ptr %uglygep155, align 64, !tbaa !5
    %uglygep156 = getelementptr i8, ptr %uglygep155, i64 %mul.i1027
    store <8 x double> %sub.i27.i882, ptr %uglygep156, align 64, !tbaa !5
    tail call void @llvm.prefetch.p0(ptr %uglygep39, i32 0, i32 3, i32 1)
    %uglygep36 = getelementptr i8, ptr %uglygep32, i64 %lsr.iv
    %uglygep37 = getelementptr i8, ptr %uglygep36, i64 -128
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep37, i32 0, i32 3, i32 1)
    %uglygep34 = getelementptr i8, ptr %uglygep32, i64 %lsr.iv
    %uglygep35 = getelementptr i8, ptr %uglygep34, i64 -64
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep35, i32 0, i32 3, i32 1)
    %uglygep33 = getelementptr i8, ptr %uglygep32, i64 %lsr.iv
    tail call void @llvm.prefetch.p0(ptr nonnull %uglygep33, i32 0, i32 3, i32 1)
    %add.i.i893 = fadd <8 x double> %sub.i27.i152, %sub.i27.i196
    %sub.i.i894 = fsub <8 x double> %sub.i27.i152, %sub.i27.i196
    %shuffle.i.i895 = shufflevector <8 x double> %add.i.i893, <8 x double> %sub.i.i894, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i896 = shufflevector <8 x double> %add.i.i893, <8 x double> %sub.i.i894, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i897 = fmul <8 x double> %shuffle.i25.i896, %shuffle.i.i895
    %124 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i895, <8 x double> %shuffle.i25.i896, <8 x double> %mul.i.i897)
    %add.i26.i898 = fadd <8 x double> %mul.i.i897, %124
    %sub.i27.i899 = fsub <8 x double> %mul.i.i897, %124
    %add.i.i900 = fadd <8 x double> %sub.i27.i243, %sub.i27.i288
    %sub.i.i901 = fsub <8 x double> %sub.i27.i243, %sub.i27.i288
    %shuffle.i.i902 = shufflevector <8 x double> %add.i.i900, <8 x double> %sub.i.i901, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i903 = shufflevector <8 x double> %add.i.i900, <8 x double> %sub.i.i901, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i904 = fmul <8 x double> %shuffle.i25.i903, %shuffle.i.i902
    %125 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i902, <8 x double> %shuffle.i25.i903, <8 x double> %mul.i.i904)
    %add.i26.i905 = fadd <8 x double> %mul.i.i904, %125
    %sub.i27.i906 = fsub <8 x double> %mul.i.i904, %125
    %add.i.i907 = fadd <8 x double> %sub.i27.i351, %sub.i27.i394
    %sub.i.i908 = fsub <8 x double> %sub.i27.i351, %sub.i27.i394
    %shuffle.i.i909 = shufflevector <8 x double> %add.i.i907, <8 x double> %sub.i.i908, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i910 = shufflevector <8 x double> %add.i.i907, <8 x double> %sub.i.i908, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i911 = fmul <8 x double> %shuffle.i25.i910, %shuffle.i.i909
    %126 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i909, <8 x double> %shuffle.i25.i910, <8 x double> %mul.i.i911)
    %add.i26.i912 = fadd <8 x double> %mul.i.i911, %126
    %sub.i27.i913 = fsub <8 x double> %mul.i.i911, %126
    %add.i.i914 = fadd <8 x double> %sub.i27.i453, %sub.i27.i488
    %sub.i.i915 = fsub <8 x double> %sub.i27.i453, %sub.i27.i488
    %shuffle.i.i916 = shufflevector <8 x double> %add.i.i914, <8 x double> %sub.i.i915, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i917 = shufflevector <8 x double> %add.i.i914, <8 x double> %sub.i.i915, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i918 = fmul <8 x double> %shuffle.i25.i917, %shuffle.i.i916
    %127 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i916, <8 x double> %shuffle.i25.i917, <8 x double> %mul.i.i918)
    %add.i26.i919 = fadd <8 x double> %mul.i.i918, %127
    %sub.i27.i920 = fsub <8 x double> %mul.i.i918, %127
    %add.i.i921 = fadd <8 x double> %add.i26.i898, %add.i26.i912
    %sub.i.i922 = fsub <8 x double> %add.i26.i898, %add.i26.i912
    %shuffle.i.i923 = shufflevector <8 x double> %add.i.i921, <8 x double> %sub.i.i922, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i924 = shufflevector <8 x double> %add.i.i921, <8 x double> %sub.i.i922, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i925 = fmul <8 x double> %shuffle.i25.i924, %shuffle.i.i923
    %128 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i923, <8 x double> %shuffle.i25.i924, <8 x double> %mul.i.i925)
    %add.i26.i926 = fadd <8 x double> %mul.i.i925, %128
    %sub.i27.i927 = fsub <8 x double> %mul.i.i925, %128
    %add.i.i928 = fadd <8 x double> %add.i26.i905, %add.i26.i919
    %sub.i.i929 = fsub <8 x double> %add.i26.i905, %add.i26.i919
    %shuffle.i.i930 = shufflevector <8 x double> %add.i.i928, <8 x double> %sub.i.i929, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i931 = shufflevector <8 x double> %add.i.i928, <8 x double> %sub.i.i929, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i932 = fmul <8 x double> %shuffle.i25.i931, %shuffle.i.i930
    %129 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i930, <8 x double> %shuffle.i25.i931, <8 x double> %mul.i.i932)
    %add.i26.i933 = fadd <8 x double> %mul.i.i932, %129
    %sub.i27.i934 = fsub <8 x double> %mul.i.i932, %129
    %add.i.i935 = fadd <8 x double> %sub.i27.i899, %sub.i27.i913
    %sub.i.i936 = fsub <8 x double> %sub.i27.i899, %sub.i27.i913
    %shuffle.i.i937 = shufflevector <8 x double> %add.i.i935, <8 x double> %sub.i.i936, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i938 = shufflevector <8 x double> %add.i.i935, <8 x double> %sub.i.i936, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i939 = fmul <8 x double> %shuffle.i25.i938, %shuffle.i.i937
    %130 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i937, <8 x double> %shuffle.i25.i938, <8 x double> %mul.i.i939)
    %add.i26.i940 = fadd <8 x double> %mul.i.i939, %130
    %sub.i27.i941 = fsub <8 x double> %mul.i.i939, %130
    %add.i.i942 = fadd <8 x double> %sub.i27.i906, %sub.i27.i920
    %sub.i.i943 = fsub <8 x double> %sub.i27.i906, %sub.i27.i920
    %shuffle.i.i944 = shufflevector <8 x double> %add.i.i942, <8 x double> %sub.i.i943, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i945 = shufflevector <8 x double> %add.i.i942, <8 x double> %sub.i.i943, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i946 = fmul <8 x double> %shuffle.i25.i945, %shuffle.i.i944
    %131 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i944, <8 x double> %shuffle.i25.i945, <8 x double> %mul.i.i946)
    %add.i26.i947 = fadd <8 x double> %mul.i.i946, %131
    %sub.i27.i948 = fsub <8 x double> %mul.i.i946, %131
    %add.i.i949 = fadd <8 x double> %add.i26.i926, %add.i26.i933
    %sub.i.i950 = fsub <8 x double> %add.i26.i926, %add.i26.i933
    %shuffle.i.i951 = shufflevector <8 x double> %add.i.i949, <8 x double> %sub.i.i950, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i952 = shufflevector <8 x double> %add.i.i949, <8 x double> %sub.i.i950, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i953 = fmul <8 x double> %shuffle.i25.i952, %shuffle.i.i951
    %132 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i951, <8 x double> %shuffle.i25.i952, <8 x double> %mul.i.i953)
    %add.i26.i954 = fadd <8 x double> %mul.i.i953, %132
    %sub.i27.i955 = fsub <8 x double> %mul.i.i953, %132
    %uglygep65 = getelementptr i8, ptr %uglygep64, i64 %lsr.iv
    store <8 x double> %add.i26.i954, ptr %uglygep65, align 64, !tbaa !5
    %uglygep157 = getelementptr i8, ptr %uglygep156, i64 %23
    store <8 x double> %sub.i27.i955, ptr %uglygep157, align 64, !tbaa !5
    %add.i.i962 = fadd <8 x double> %sub.i27.i927, %sub.i27.i934
    %sub.i.i963 = fsub <8 x double> %sub.i27.i927, %sub.i27.i934
    %shuffle.i.i964 = shufflevector <8 x double> %add.i.i962, <8 x double> %sub.i.i963, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i965 = shufflevector <8 x double> %add.i.i962, <8 x double> %sub.i.i963, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i966 = fmul <8 x double> %shuffle.i25.i965, %shuffle.i.i964
    %133 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i964, <8 x double> %shuffle.i25.i965, <8 x double> %mul.i.i966)
    %add.i26.i967 = fadd <8 x double> %mul.i.i966, %133
    %sub.i27.i968 = fsub <8 x double> %mul.i.i966, %133
    %uglygep158 = getelementptr i8, ptr %uglygep157, i64 %mul.i1027
    store <8 x double> %add.i26.i967, ptr %uglygep158, align 64, !tbaa !5
    %uglygep159 = getelementptr i8, ptr %uglygep158, i64 %mul.i1027
    store <8 x double> %sub.i27.i968, ptr %uglygep159, align 64, !tbaa !5
    %add.i.i979 = fadd <8 x double> %add.i26.i940, %add.i26.i947
    %sub.i.i980 = fsub <8 x double> %add.i26.i940, %add.i26.i947
    %shuffle.i.i981 = shufflevector <8 x double> %add.i.i979, <8 x double> %sub.i.i980, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i982 = shufflevector <8 x double> %add.i.i979, <8 x double> %sub.i.i980, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i983 = fmul <8 x double> %shuffle.i25.i982, %shuffle.i.i981
    %134 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i981, <8 x double> %shuffle.i25.i982, <8 x double> %mul.i.i983)
    %add.i26.i984 = fadd <8 x double> %mul.i.i983, %134
    %sub.i27.i985 = fsub <8 x double> %mul.i.i983, %134
    %uglygep160 = getelementptr i8, ptr %uglygep159, i64 %mul.i1027
    store <8 x double> %add.i26.i984, ptr %uglygep160, align 64, !tbaa !5
    %uglygep161 = getelementptr i8, ptr %uglygep160, i64 %mul.i1027
    store <8 x double> %sub.i27.i985, ptr %uglygep161, align 64, !tbaa !5
    %add.i.i996 = fadd <8 x double> %sub.i27.i941, %sub.i27.i948
    %sub.i.i997 = fsub <8 x double> %sub.i27.i941, %sub.i27.i948
    %shuffle.i.i998 = shufflevector <8 x double> %add.i.i996, <8 x double> %sub.i.i997, <8 x i32> <i32 0, i32 8, i32 2, i32 10, i32 4, i32 12, i32 6, i32 14>
    %shuffle.i25.i999 = shufflevector <8 x double> %add.i.i996, <8 x double> %sub.i.i997, <8 x i32> <i32 1, i32 9, i32 3, i32 11, i32 5, i32 13, i32 7, i32 15>
    %mul.i.i1000 = fmul <8 x double> %shuffle.i25.i999, %shuffle.i.i998
    %135 = tail call <8 x double> @llvm.fma.v8f64(<8 x double> %shuffle.i.i998, <8 x double> %shuffle.i25.i999, <8 x double> %mul.i.i1000)
    %add.i26.i1001 = fadd <8 x double> %mul.i.i1000, %135
    %sub.i27.i1002 = fsub <8 x double> %mul.i.i1000, %135
    %uglygep162 = getelementptr i8, ptr %uglygep161, i64 %mul.i1027
    store <8 x double> %add.i26.i1001, ptr %uglygep162, align 64, !tbaa !5
    %uglygep163 = getelementptr i8, ptr %uglygep162, i64 %mul.i1027
    store <8 x double> %sub.i27.i1002, ptr %uglygep163, align 64, !tbaa !5
    %dec = add i64 %length.addr.0, -1
    %lsr.iv.next = add i64 %lsr.iv, 64
    %tobool.not = icmp eq i64 %dec, 0
    br i1 %tobool.not, label %do.end, label %do.body, !llvm.loop !8

  do.end:                                           ; preds = %do.body
    ret void
  }

  ; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite, inaccessiblemem: readwrite)
  declare void @llvm.prefetch.p0(ptr nocapture readonly, i32 immarg, i32 immarg, i32 immarg) #1

  ; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
  declare <8 x double> @llvm.fma.v8f64(<8 x double>, <8 x double>, <8 x double>) #2

  attributes #0 = { nofree noinline nosync nounwind uwtable "approx-func-fp-math"="true" "denormal-fp-math"="preserve-sign,preserve-sign" "loopopt-pipeline"="full" "min-legal-vector-width"="512" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="true" "pre_loopopt" "stack-protector-buffer-size"="8" "target-cpu"="skylake-avx512" "target-features"="+adx,+aes,+avx,+avx2,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512vl,+bmi,+bmi2,+clflushopt,+clwb,+crc32,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+mmx,+movbe,+pclmul,+pku,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "unsafe-fp-math"="false" }
  attributes #1 = { nocallback nofree nosync nounwind willreturn memory(argmem: readwrite, inaccessiblemem: readwrite) }
  attributes #2 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }

  !llvm.module.flags = !{!0, !1, !2, !3}
  !llvm.ident = !{!4}

  !0 = !{i32 1, !"wchar_size", i32 4}
  !1 = !{i32 7, !"uwtable", i32 2}
  !2 = !{i32 1, !"ThinLTO", i32 0}
  !3 = !{i32 1, !"EnableSplitLTOUnit", i32 1}
  !4 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2023.1.0 (2023.x.0.YYYYMMDD)"}
  !5 = !{!6, !6, i64 0}
  !6 = !{!"omnipotent char", !7, i64 0}
  !7 = !{!"Simple C++ TBAA"}
  !8 = distinct !{!8, !9}
  !9 = !{!"llvm.loop.mustprogress"}

...
---
name:            _Z4testPDv8_dmPKS_mS2_mm
alignment:       16
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
debugInstrRef:   true
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: gr64, preferred-register: '' }
  - { id: 1, class: gr64, preferred-register: '' }
  - { id: 2, class: gr64, preferred-register: '' }
  - { id: 3, class: gr64, preferred-register: '' }
  - { id: 4, class: gr64, preferred-register: '' }
  - { id: 5, class: gr64, preferred-register: '' }
  - { id: 6, class: gr64, preferred-register: '' }
  - { id: 7, class: gr64, preferred-register: '' }
  - { id: 8, class: gr64, preferred-register: '' }
  - { id: 9, class: gr64, preferred-register: '' }
  - { id: 10, class: gr64, preferred-register: '' }
  - { id: 11, class: gr64, preferred-register: '' }
  - { id: 12, class: gr64, preferred-register: '' }
  - { id: 13, class: gr64, preferred-register: '' }
  - { id: 14, class: gr64, preferred-register: '' }
  - { id: 15, class: gr64, preferred-register: '' }
  - { id: 16, class: gr64, preferred-register: '' }
  - { id: 17, class: gr64, preferred-register: '' }
  - { id: 18, class: gr64_nosp, preferred-register: '' }
  - { id: 19, class: gr64, preferred-register: '' }
  - { id: 20, class: gr64, preferred-register: '' }
  - { id: 21, class: gr64_nosp, preferred-register: '' }
  - { id: 22, class: gr64, preferred-register: '' }
  - { id: 23, class: gr64, preferred-register: '' }
  - { id: 24, class: gr64_nosp, preferred-register: '' }
  - { id: 25, class: gr64_nosp, preferred-register: '' }
  - { id: 26, class: gr64, preferred-register: '' }
  - { id: 27, class: gr64_nosp, preferred-register: '' }
  - { id: 28, class: gr64, preferred-register: '' }
  - { id: 29, class: gr64_with_sub_8bit, preferred-register: '' }
  - { id: 30, class: gr64_nosp, preferred-register: '' }
  - { id: 31, class: gr64_nosp, preferred-register: '' }
  - { id: 32, class: gr64_nosp, preferred-register: '' }
  - { id: 33, class: gr64_nosp, preferred-register: '' }
  - { id: 34, class: gr64_nosp, preferred-register: '' }
  - { id: 35, class: gr64_nosp, preferred-register: '' }
  - { id: 36, class: gr64_nosp, preferred-register: '' }
  - { id: 37, class: gr64, preferred-register: '' }
  - { id: 38, class: gr64_nosp, preferred-register: '' }
  - { id: 39, class: gr64_nosp, preferred-register: '' }
  - { id: 40, class: gr64, preferred-register: '' }
  - { id: 41, class: gr64_nosp, preferred-register: '' }
  - { id: 42, class: gr64, preferred-register: '' }
  - { id: 43, class: gr64, preferred-register: '' }
  - { id: 44, class: gr64, preferred-register: '' }
  - { id: 45, class: gr64, preferred-register: '' }
  - { id: 46, class: gr64, preferred-register: '' }
  - { id: 47, class: gr64, preferred-register: '' }
  - { id: 48, class: gr64_nosp, preferred-register: '' }
  - { id: 49, class: gr64, preferred-register: '' }
  - { id: 50, class: gr64, preferred-register: '' }
  - { id: 51, class: gr32, preferred-register: '' }
  - { id: 52, class: gr64_nosp, preferred-register: '' }
  - { id: 53, class: vr512, preferred-register: '' }
  - { id: 54, class: vr512, preferred-register: '' }
  - { id: 55, class: vr512, preferred-register: '' }
  - { id: 56, class: vr512, preferred-register: '' }
  - { id: 57, class: vr512, preferred-register: '' }
  - { id: 58, class: vr512, preferred-register: '' }
  - { id: 59, class: vr512, preferred-register: '' }
  - { id: 60, class: vr512, preferred-register: '' }
  - { id: 61, class: vr512, preferred-register: '' }
  - { id: 62, class: vr512, preferred-register: '' }
  - { id: 63, class: vr512, preferred-register: '' }
  - { id: 64, class: vr512, preferred-register: '' }
  - { id: 65, class: vr512, preferred-register: '' }
  - { id: 66, class: vr512, preferred-register: '' }
  - { id: 67, class: vr512, preferred-register: '' }
  - { id: 68, class: vr512, preferred-register: '' }
  - { id: 69, class: vr512, preferred-register: '' }
  - { id: 70, class: vr512, preferred-register: '' }
  - { id: 71, class: vr512, preferred-register: '' }
  - { id: 72, class: vr512, preferred-register: '' }
  - { id: 73, class: vr512, preferred-register: '' }
  - { id: 74, class: vr512, preferred-register: '' }
  - { id: 75, class: vr512, preferred-register: '' }
  - { id: 76, class: vr512, preferred-register: '' }
  - { id: 77, class: vr512, preferred-register: '' }
  - { id: 78, class: vr512, preferred-register: '' }
  - { id: 79, class: vr512, preferred-register: '' }
  - { id: 80, class: vr512, preferred-register: '' }
  - { id: 81, class: vr512, preferred-register: '' }
  - { id: 82, class: vr512, preferred-register: '' }
  - { id: 83, class: vr512, preferred-register: '' }
  - { id: 84, class: vr512, preferred-register: '' }
  - { id: 85, class: vr512, preferred-register: '' }
  - { id: 86, class: vr512, preferred-register: '' }
  - { id: 87, class: vr512, preferred-register: '' }
  - { id: 88, class: vr512, preferred-register: '' }
  - { id: 89, class: vr512, preferred-register: '' }
  - { id: 90, class: vr512, preferred-register: '' }
  - { id: 91, class: vr512, preferred-register: '' }
  - { id: 92, class: vr512, preferred-register: '' }
  - { id: 93, class: vr512, preferred-register: '' }
  - { id: 94, class: vr512, preferred-register: '' }
  - { id: 95, class: vr512, preferred-register: '' }
  - { id: 96, class: vr512, preferred-register: '' }
  - { id: 97, class: vr512, preferred-register: '' }
  - { id: 98, class: vr512, preferred-register: '' }
  - { id: 99, class: vr512, preferred-register: '' }
  - { id: 100, class: vr512, preferred-register: '' }
  - { id: 101, class: vr512, preferred-register: '' }
  - { id: 102, class: vr512, preferred-register: '' }
  - { id: 103, class: vr512, preferred-register: '' }
  - { id: 104, class: vr512, preferred-register: '' }
  - { id: 105, class: vr512, preferred-register: '' }
  - { id: 106, class: vr512, preferred-register: '' }
  - { id: 107, class: vr512, preferred-register: '' }
  - { id: 108, class: vr512, preferred-register: '' }
  - { id: 109, class: vr512, preferred-register: '' }
  - { id: 110, class: vr512, preferred-register: '' }
  - { id: 111, class: vr512, preferred-register: '' }
  - { id: 112, class: vr512, preferred-register: '' }
  - { id: 113, class: vr512, preferred-register: '' }
  - { id: 114, class: vr512, preferred-register: '' }
  - { id: 115, class: vr512, preferred-register: '' }
  - { id: 116, class: vr512, preferred-register: '' }
  - { id: 117, class: vr512, preferred-register: '' }
  - { id: 118, class: vr512, preferred-register: '' }
  - { id: 119, class: vr512, preferred-register: '' }
  - { id: 120, class: vr512, preferred-register: '' }
  - { id: 121, class: vr512, preferred-register: '' }
  - { id: 122, class: vr512, preferred-register: '' }
  - { id: 123, class: vr512, preferred-register: '' }
  - { id: 124, class: vr512, preferred-register: '' }
  - { id: 125, class: vr512, preferred-register: '' }
  - { id: 126, class: vr512, preferred-register: '' }
  - { id: 127, class: vr512, preferred-register: '' }
  - { id: 128, class: vr512, preferred-register: '' }
  - { id: 129, class: vr512, preferred-register: '' }
  - { id: 130, class: vr512, preferred-register: '' }
  - { id: 131, class: vr512, preferred-register: '' }
  - { id: 132, class: vr512, preferred-register: '' }
  - { id: 133, class: vr512, preferred-register: '' }
  - { id: 134, class: vr512, preferred-register: '' }
  - { id: 135, class: vr512, preferred-register: '' }
  - { id: 136, class: vr512, preferred-register: '' }
  - { id: 137, class: vr512, preferred-register: '' }
  - { id: 138, class: vr512, preferred-register: '' }
  - { id: 139, class: vr512, preferred-register: '' }
  - { id: 140, class: vr512, preferred-register: '' }
  - { id: 141, class: vr512, preferred-register: '' }
  - { id: 142, class: vr512, preferred-register: '' }
  - { id: 143, class: vr512, preferred-register: '' }
  - { id: 144, class: vr512, preferred-register: '' }
  - { id: 145, class: vr512, preferred-register: '' }
  - { id: 146, class: vr512, preferred-register: '' }
  - { id: 147, class: vr512, preferred-register: '' }
  - { id: 148, class: vr512, preferred-register: '' }
  - { id: 149, class: vr512, preferred-register: '' }
  - { id: 150, class: vr512, preferred-register: '' }
  - { id: 151, class: vr512, preferred-register: '' }
  - { id: 152, class: vr512, preferred-register: '' }
  - { id: 153, class: vr512, preferred-register: '' }
  - { id: 154, class: vr512, preferred-register: '' }
  - { id: 155, class: vr512, preferred-register: '' }
  - { id: 156, class: vr512, preferred-register: '' }
  - { id: 157, class: vr512, preferred-register: '' }
  - { id: 158, class: vr512, preferred-register: '' }
  - { id: 159, class: vr512, preferred-register: '' }
  - { id: 160, class: vr512, preferred-register: '' }
  - { id: 161, class: vr512, preferred-register: '' }
  - { id: 162, class: vr512, preferred-register: '' }
  - { id: 163, class: vr512, preferred-register: '' }
  - { id: 164, class: vr512, preferred-register: '' }
  - { id: 165, class: vr512, preferred-register: '' }
  - { id: 166, class: vr512, preferred-register: '' }
  - { id: 167, class: vr512, preferred-register: '' }
  - { id: 168, class: vr512, preferred-register: '' }
  - { id: 169, class: vr512, preferred-register: '' }
  - { id: 170, class: vr512, preferred-register: '' }
  - { id: 171, class: vr512, preferred-register: '' }
  - { id: 172, class: vr512, preferred-register: '' }
  - { id: 173, class: vr512, preferred-register: '' }
  - { id: 174, class: vr512, preferred-register: '' }
  - { id: 175, class: vr512, preferred-register: '' }
  - { id: 176, class: vr512, preferred-register: '' }
  - { id: 177, class: vr512, preferred-register: '' }
  - { id: 178, class: vr512, preferred-register: '' }
  - { id: 179, class: vr512, preferred-register: '' }
  - { id: 180, class: vr512, preferred-register: '' }
  - { id: 181, class: vr512, preferred-register: '' }
  - { id: 182, class: vr512, preferred-register: '' }
  - { id: 183, class: vr512, preferred-register: '' }
  - { id: 184, class: vr512, preferred-register: '' }
  - { id: 185, class: vr512, preferred-register: '' }
  - { id: 186, class: vr512, preferred-register: '' }
  - { id: 187, class: vr512, preferred-register: '' }
  - { id: 188, class: vr512, preferred-register: '' }
  - { id: 189, class: vr512, preferred-register: '' }
  - { id: 190, class: vr512, preferred-register: '' }
  - { id: 191, class: vr512, preferred-register: '' }
  - { id: 192, class: vr512, preferred-register: '' }
  - { id: 193, class: vr512, preferred-register: '' }
  - { id: 194, class: vr512, preferred-register: '' }
  - { id: 195, class: vr512, preferred-register: '' }
  - { id: 196, class: vr512, preferred-register: '' }
  - { id: 197, class: vr512, preferred-register: '' }
  - { id: 198, class: vr512, preferred-register: '' }
  - { id: 199, class: vr512, preferred-register: '' }
  - { id: 200, class: vr512, preferred-register: '' }
  - { id: 201, class: vr512, preferred-register: '' }
  - { id: 202, class: vr512, preferred-register: '' }
  - { id: 203, class: vr512, preferred-register: '' }
  - { id: 204, class: vr512, preferred-register: '' }
  - { id: 205, class: vr512, preferred-register: '' }
  - { id: 206, class: vr512, preferred-register: '' }
  - { id: 207, class: vr512, preferred-register: '' }
  - { id: 208, class: vr512, preferred-register: '' }
  - { id: 209, class: vr512, preferred-register: '' }
  - { id: 210, class: vr512, preferred-register: '' }
  - { id: 211, class: vr512, preferred-register: '' }
  - { id: 212, class: vr512, preferred-register: '' }
  - { id: 213, class: vr512, preferred-register: '' }
  - { id: 214, class: vr512, preferred-register: '' }
  - { id: 215, class: vr512, preferred-register: '' }
  - { id: 216, class: vr512, preferred-register: '' }
  - { id: 217, class: vr512, preferred-register: '' }
  - { id: 218, class: vr512, preferred-register: '' }
  - { id: 219, class: vr512, preferred-register: '' }
  - { id: 220, class: vr512, preferred-register: '' }
  - { id: 221, class: vr512, preferred-register: '' }
  - { id: 222, class: vr512, preferred-register: '' }
  - { id: 223, class: vr512, preferred-register: '' }
  - { id: 224, class: vr512, preferred-register: '' }
  - { id: 225, class: vr512, preferred-register: '' }
  - { id: 226, class: vr512, preferred-register: '' }
  - { id: 227, class: vr512, preferred-register: '' }
  - { id: 228, class: vr512, preferred-register: '' }
  - { id: 229, class: vr512, preferred-register: '' }
  - { id: 230, class: vr512, preferred-register: '' }
  - { id: 231, class: vr512, preferred-register: '' }
  - { id: 232, class: vr512, preferred-register: '' }
  - { id: 233, class: vr512, preferred-register: '' }
  - { id: 234, class: vr512, preferred-register: '' }
  - { id: 235, class: vr512, preferred-register: '' }
  - { id: 236, class: vr512, preferred-register: '' }
  - { id: 237, class: vr512, preferred-register: '' }
  - { id: 238, class: vr512, preferred-register: '' }
  - { id: 239, class: vr512, preferred-register: '' }
  - { id: 240, class: vr512, preferred-register: '' }
  - { id: 241, class: vr512, preferred-register: '' }
  - { id: 242, class: vr512, preferred-register: '' }
  - { id: 243, class: vr512, preferred-register: '' }
  - { id: 244, class: vr512, preferred-register: '' }
  - { id: 245, class: vr512, preferred-register: '' }
  - { id: 246, class: vr512, preferred-register: '' }
  - { id: 247, class: vr512, preferred-register: '' }
  - { id: 248, class: vr512, preferred-register: '' }
  - { id: 249, class: vr512, preferred-register: '' }
  - { id: 250, class: vr512, preferred-register: '' }
  - { id: 251, class: vr512, preferred-register: '' }
  - { id: 252, class: vr512, preferred-register: '' }
  - { id: 253, class: vr512, preferred-register: '' }
  - { id: 254, class: vr512, preferred-register: '' }
  - { id: 255, class: vr512, preferred-register: '' }
  - { id: 256, class: vr512, preferred-register: '' }
  - { id: 257, class: vr512, preferred-register: '' }
  - { id: 258, class: vr512, preferred-register: '' }
  - { id: 259, class: vr512, preferred-register: '' }
  - { id: 260, class: vr512, preferred-register: '' }
  - { id: 261, class: vr512, preferred-register: '' }
  - { id: 262, class: vr512, preferred-register: '' }
  - { id: 263, class: vr512, preferred-register: '' }
  - { id: 264, class: vr512, preferred-register: '' }
  - { id: 265, class: vr512, preferred-register: '' }
  - { id: 266, class: vr512, preferred-register: '' }
  - { id: 267, class: vr512, preferred-register: '' }
  - { id: 268, class: vr512, preferred-register: '' }
  - { id: 269, class: vr512, preferred-register: '' }
  - { id: 270, class: vr512, preferred-register: '' }
  - { id: 271, class: vr512, preferred-register: '' }
  - { id: 272, class: vr512, preferred-register: '' }
  - { id: 273, class: vr512, preferred-register: '' }
  - { id: 274, class: vr512, preferred-register: '' }
  - { id: 275, class: vr512, preferred-register: '' }
  - { id: 276, class: vr512, preferred-register: '' }
  - { id: 277, class: vr512, preferred-register: '' }
  - { id: 278, class: vr512, preferred-register: '' }
  - { id: 279, class: vr512, preferred-register: '' }
  - { id: 280, class: vr512, preferred-register: '' }
  - { id: 281, class: vr512, preferred-register: '' }
  - { id: 282, class: vr512, preferred-register: '' }
  - { id: 283, class: vr512, preferred-register: '' }
  - { id: 284, class: vr512, preferred-register: '' }
  - { id: 285, class: vr512, preferred-register: '' }
  - { id: 286, class: vr512, preferred-register: '' }
  - { id: 287, class: vr512, preferred-register: '' }
  - { id: 288, class: vr512, preferred-register: '' }
  - { id: 289, class: vr512, preferred-register: '' }
  - { id: 290, class: vr512, preferred-register: '' }
  - { id: 291, class: vr512, preferred-register: '' }
  - { id: 292, class: vr512, preferred-register: '' }
  - { id: 293, class: vr512, preferred-register: '' }
  - { id: 294, class: vr512, preferred-register: '' }
  - { id: 295, class: vr512, preferred-register: '' }
  - { id: 296, class: vr512, preferred-register: '' }
  - { id: 297, class: vr512, preferred-register: '' }
  - { id: 298, class: vr512, preferred-register: '' }
  - { id: 299, class: vr512, preferred-register: '' }
  - { id: 300, class: vr512, preferred-register: '' }
  - { id: 301, class: vr512, preferred-register: '' }
  - { id: 302, class: vr512, preferred-register: '' }
  - { id: 303, class: vr512, preferred-register: '' }
  - { id: 304, class: vr512, preferred-register: '' }
  - { id: 305, class: vr512, preferred-register: '' }
  - { id: 306, class: vr512, preferred-register: '' }
  - { id: 307, class: vr512, preferred-register: '' }
  - { id: 308, class: vr512, preferred-register: '' }
  - { id: 309, class: vr512, preferred-register: '' }
  - { id: 310, class: vr512, preferred-register: '' }
  - { id: 311, class: vr512, preferred-register: '' }
  - { id: 312, class: vr512, preferred-register: '' }
  - { id: 313, class: vr512, preferred-register: '' }
  - { id: 314, class: vr512, preferred-register: '' }
  - { id: 315, class: vr512, preferred-register: '' }
  - { id: 316, class: vr512, preferred-register: '' }
  - { id: 317, class: vr512, preferred-register: '' }
  - { id: 318, class: vr512, preferred-register: '' }
  - { id: 319, class: vr512, preferred-register: '' }
  - { id: 320, class: vr512, preferred-register: '' }
  - { id: 321, class: vr512, preferred-register: '' }
  - { id: 322, class: vr512, preferred-register: '' }
  - { id: 323, class: vr512, preferred-register: '' }
  - { id: 324, class: vr512, preferred-register: '' }
  - { id: 325, class: vr512, preferred-register: '' }
  - { id: 326, class: vr512, preferred-register: '' }
  - { id: 327, class: vr512, preferred-register: '' }
  - { id: 328, class: vr512, preferred-register: '' }
  - { id: 329, class: vr512, preferred-register: '' }
  - { id: 330, class: vr512, preferred-register: '' }
  - { id: 331, class: vr512, preferred-register: '' }
  - { id: 332, class: vr512, preferred-register: '' }
  - { id: 333, class: vr512, preferred-register: '' }
  - { id: 334, class: vr512, preferred-register: '' }
  - { id: 335, class: vr512, preferred-register: '' }
  - { id: 336, class: vr512, preferred-register: '' }
  - { id: 337, class: vr512, preferred-register: '' }
  - { id: 338, class: vr512, preferred-register: '' }
  - { id: 339, class: vr512, preferred-register: '' }
  - { id: 340, class: vr512, preferred-register: '' }
  - { id: 341, class: vr512, preferred-register: '' }
  - { id: 342, class: vr512, preferred-register: '' }
  - { id: 343, class: vr512, preferred-register: '' }
  - { id: 344, class: vr512, preferred-register: '' }
  - { id: 345, class: vr512, preferred-register: '' }
  - { id: 346, class: vr512, preferred-register: '' }
  - { id: 347, class: vr512, preferred-register: '' }
  - { id: 348, class: vr512, preferred-register: '' }
  - { id: 349, class: vr512, preferred-register: '' }
  - { id: 350, class: vr512, preferred-register: '' }
  - { id: 351, class: vr512, preferred-register: '' }
  - { id: 352, class: vr512, preferred-register: '' }
  - { id: 353, class: vr512, preferred-register: '' }
  - { id: 354, class: vr512, preferred-register: '' }
  - { id: 355, class: vr512, preferred-register: '' }
  - { id: 356, class: vr512, preferred-register: '' }
  - { id: 357, class: vr512, preferred-register: '' }
  - { id: 358, class: vr512, preferred-register: '' }
  - { id: 359, class: vr512, preferred-register: '' }
  - { id: 360, class: vr512, preferred-register: '' }
  - { id: 361, class: vr512, preferred-register: '' }
  - { id: 362, class: vr512, preferred-register: '' }
  - { id: 363, class: vr512, preferred-register: '' }
  - { id: 364, class: vr512, preferred-register: '' }
  - { id: 365, class: vr512, preferred-register: '' }
  - { id: 366, class: vr512, preferred-register: '' }
  - { id: 367, class: vr512, preferred-register: '' }
  - { id: 368, class: vr512, preferred-register: '' }
  - { id: 369, class: vr512, preferred-register: '' }
  - { id: 370, class: vr512, preferred-register: '' }
  - { id: 371, class: vr512, preferred-register: '' }
  - { id: 372, class: vr512, preferred-register: '' }
  - { id: 373, class: vr512, preferred-register: '' }
  - { id: 374, class: vr512, preferred-register: '' }
  - { id: 375, class: vr512, preferred-register: '' }
  - { id: 376, class: vr512, preferred-register: '' }
  - { id: 377, class: vr512, preferred-register: '' }
  - { id: 378, class: vr512, preferred-register: '' }
  - { id: 379, class: vr512, preferred-register: '' }
  - { id: 380, class: vr512, preferred-register: '' }
  - { id: 381, class: vr512, preferred-register: '' }
  - { id: 382, class: vr512, preferred-register: '' }
  - { id: 383, class: vr512, preferred-register: '' }
  - { id: 384, class: vr512, preferred-register: '' }
  - { id: 385, class: vr512, preferred-register: '' }
  - { id: 386, class: vr512, preferred-register: '' }
  - { id: 387, class: vr512, preferred-register: '' }
  - { id: 388, class: vr512, preferred-register: '' }
  - { id: 389, class: vr512, preferred-register: '' }
  - { id: 390, class: vr512, preferred-register: '' }
  - { id: 391, class: vr512, preferred-register: '' }
  - { id: 392, class: vr512, preferred-register: '' }
  - { id: 393, class: vr512, preferred-register: '' }
  - { id: 394, class: vr512, preferred-register: '' }
  - { id: 395, class: vr512, preferred-register: '' }
  - { id: 396, class: vr512, preferred-register: '' }
  - { id: 397, class: gr64_nosp, preferred-register: '' }
  - { id: 398, class: vr512, preferred-register: '' }
  - { id: 399, class: vr512, preferred-register: '' }
  - { id: 400, class: vr512, preferred-register: '' }
  - { id: 401, class: vr512, preferred-register: '' }
  - { id: 402, class: vr512, preferred-register: '' }
  - { id: 403, class: vr512, preferred-register: '' }
  - { id: 404, class: vr512, preferred-register: '' }
  - { id: 405, class: vr512, preferred-register: '' }
  - { id: 406, class: gr64_nosp, preferred-register: '' }
  - { id: 407, class: gr64_nosp, preferred-register: '' }
  - { id: 408, class: vr512, preferred-register: '' }
  - { id: 409, class: vr512, preferred-register: '' }
  - { id: 410, class: vr512, preferred-register: '' }
  - { id: 411, class: vr512, preferred-register: '' }
  - { id: 412, class: vr512, preferred-register: '' }
  - { id: 413, class: vr512, preferred-register: '' }
  - { id: 414, class: vr512, preferred-register: '' }
  - { id: 415, class: vr512, preferred-register: '' }
  - { id: 416, class: gr64_nosp, preferred-register: '' }
  - { id: 417, class: gr64_nosp, preferred-register: '' }
  - { id: 418, class: vr512, preferred-register: '' }
  - { id: 419, class: vr512, preferred-register: '' }
  - { id: 420, class: vr512, preferred-register: '' }
  - { id: 421, class: vr512, preferred-register: '' }
  - { id: 422, class: vr512, preferred-register: '' }
  - { id: 423, class: vr512, preferred-register: '' }
  - { id: 424, class: vr512, preferred-register: '' }
  - { id: 425, class: vr512, preferred-register: '' }
  - { id: 426, class: gr64_nosp, preferred-register: '' }
  - { id: 427, class: gr64_nosp, preferred-register: '' }
  - { id: 428, class: vr512, preferred-register: '' }
  - { id: 429, class: vr512, preferred-register: '' }
  - { id: 430, class: vr512, preferred-register: '' }
  - { id: 431, class: vr512, preferred-register: '' }
  - { id: 432, class: vr512, preferred-register: '' }
  - { id: 433, class: vr512, preferred-register: '' }
  - { id: 434, class: vr512, preferred-register: '' }
  - { id: 435, class: vr512, preferred-register: '' }
  - { id: 436, class: vr512, preferred-register: '' }
  - { id: 437, class: vr512, preferred-register: '' }
  - { id: 438, class: vr512, preferred-register: '' }
  - { id: 439, class: vr512, preferred-register: '' }
  - { id: 440, class: vr512, preferred-register: '' }
  - { id: 441, class: vr512, preferred-register: '' }
  - { id: 442, class: vr512, preferred-register: '' }
  - { id: 443, class: vr512, preferred-register: '' }
  - { id: 444, class: vr512, preferred-register: '' }
  - { id: 445, class: vr512, preferred-register: '' }
  - { id: 446, class: vr512, preferred-register: '' }
  - { id: 447, class: vr512, preferred-register: '' }
  - { id: 448, class: vr512, preferred-register: '' }
  - { id: 449, class: vr512, preferred-register: '' }
  - { id: 450, class: vr512, preferred-register: '' }
  - { id: 451, class: vr512, preferred-register: '' }
  - { id: 452, class: vr512, preferred-register: '' }
  - { id: 453, class: vr512, preferred-register: '' }
  - { id: 454, class: vr512, preferred-register: '' }
  - { id: 455, class: vr512, preferred-register: '' }
  - { id: 456, class: vr512, preferred-register: '' }
  - { id: 457, class: vr512, preferred-register: '' }
  - { id: 458, class: vr512, preferred-register: '' }
  - { id: 459, class: vr512, preferred-register: '' }
  - { id: 460, class: vr512, preferred-register: '' }
  - { id: 461, class: vr512, preferred-register: '' }
  - { id: 462, class: vr512, preferred-register: '' }
  - { id: 463, class: vr512, preferred-register: '' }
  - { id: 464, class: vr512, preferred-register: '' }
  - { id: 465, class: vr512, preferred-register: '' }
  - { id: 466, class: vr512, preferred-register: '' }
  - { id: 467, class: vr512, preferred-register: '' }
  - { id: 468, class: vr512, preferred-register: '' }
  - { id: 469, class: vr512, preferred-register: '' }
  - { id: 470, class: vr512, preferred-register: '' }
  - { id: 471, class: vr512, preferred-register: '' }
  - { id: 472, class: vr512, preferred-register: '' }
  - { id: 473, class: vr512, preferred-register: '' }
  - { id: 474, class: vr512, preferred-register: '' }
  - { id: 475, class: vr512, preferred-register: '' }
  - { id: 476, class: vr512, preferred-register: '' }
  - { id: 477, class: vr512, preferred-register: '' }
  - { id: 478, class: vr512, preferred-register: '' }
  - { id: 479, class: vr512, preferred-register: '' }
  - { id: 480, class: vr512, preferred-register: '' }
  - { id: 481, class: vr512, preferred-register: '' }
  - { id: 482, class: vr512, preferred-register: '' }
  - { id: 483, class: vr512, preferred-register: '' }
  - { id: 484, class: vr512, preferred-register: '' }
  - { id: 485, class: vr512, preferred-register: '' }
  - { id: 486, class: vr512, preferred-register: '' }
  - { id: 487, class: vr512, preferred-register: '' }
  - { id: 488, class: vr512, preferred-register: '' }
  - { id: 489, class: vr512, preferred-register: '' }
  - { id: 490, class: vr512, preferred-register: '' }
  - { id: 491, class: vr512, preferred-register: '' }
  - { id: 492, class: vr512, preferred-register: '' }
  - { id: 493, class: vr512, preferred-register: '' }
  - { id: 494, class: vr512, preferred-register: '' }
  - { id: 495, class: vr512, preferred-register: '' }
  - { id: 496, class: vr512, preferred-register: '' }
  - { id: 497, class: vr512, preferred-register: '' }
  - { id: 498, class: vr512, preferred-register: '' }
  - { id: 499, class: vr512, preferred-register: '' }
  - { id: 500, class: gr64_nosp, preferred-register: '' }
  - { id: 501, class: vr512, preferred-register: '' }
  - { id: 502, class: vr512, preferred-register: '' }
  - { id: 503, class: vr512, preferred-register: '' }
  - { id: 504, class: vr512, preferred-register: '' }
  - { id: 505, class: vr512, preferred-register: '' }
  - { id: 506, class: vr512, preferred-register: '' }
  - { id: 507, class: vr512, preferred-register: '' }
  - { id: 508, class: vr512, preferred-register: '' }
  - { id: 509, class: gr64_nosp, preferred-register: '' }
  - { id: 510, class: gr64_nosp, preferred-register: '' }
  - { id: 511, class: vr512, preferred-register: '' }
  - { id: 512, class: vr512, preferred-register: '' }
  - { id: 513, class: vr512, preferred-register: '' }
  - { id: 514, class: vr512, preferred-register: '' }
  - { id: 515, class: vr512, preferred-register: '' }
  - { id: 516, class: vr512, preferred-register: '' }
  - { id: 517, class: vr512, preferred-register: '' }
  - { id: 518, class: vr512, preferred-register: '' }
  - { id: 519, class: gr64_nosp, preferred-register: '' }
  - { id: 520, class: gr64_nosp, preferred-register: '' }
  - { id: 521, class: vr512, preferred-register: '' }
  - { id: 522, class: vr512, preferred-register: '' }
  - { id: 523, class: vr512, preferred-register: '' }
  - { id: 524, class: vr512, preferred-register: '' }
  - { id: 525, class: vr512, preferred-register: '' }
  - { id: 526, class: vr512, preferred-register: '' }
  - { id: 527, class: vr512, preferred-register: '' }
  - { id: 528, class: vr512, preferred-register: '' }
  - { id: 529, class: gr64_nosp, preferred-register: '' }
  - { id: 530, class: gr64_nosp, preferred-register: '' }
  - { id: 531, class: vr512, preferred-register: '' }
  - { id: 532, class: vr512, preferred-register: '' }
  - { id: 533, class: vr512, preferred-register: '' }
  - { id: 534, class: vr512, preferred-register: '' }
  - { id: 535, class: vr512, preferred-register: '' }
  - { id: 536, class: vr512, preferred-register: '' }
  - { id: 537, class: vr512, preferred-register: '' }
  - { id: 538, class: vr512, preferred-register: '' }
  - { id: 539, class: vr512, preferred-register: '' }
  - { id: 540, class: vr512, preferred-register: '' }
  - { id: 541, class: vr512, preferred-register: '' }
  - { id: 542, class: vr512, preferred-register: '' }
  - { id: 543, class: vr512, preferred-register: '' }
  - { id: 544, class: vr512, preferred-register: '' }
  - { id: 545, class: vr512, preferred-register: '' }
  - { id: 546, class: vr512, preferred-register: '' }
  - { id: 547, class: vr512, preferred-register: '' }
  - { id: 548, class: vr512, preferred-register: '' }
  - { id: 549, class: vr512, preferred-register: '' }
  - { id: 550, class: vr512, preferred-register: '' }
  - { id: 551, class: vr512, preferred-register: '' }
  - { id: 552, class: vr512, preferred-register: '' }
  - { id: 553, class: vr512, preferred-register: '' }
  - { id: 554, class: vr512, preferred-register: '' }
  - { id: 555, class: vr512, preferred-register: '' }
  - { id: 556, class: vr512, preferred-register: '' }
  - { id: 557, class: vr512, preferred-register: '' }
  - { id: 558, class: vr512, preferred-register: '' }
  - { id: 559, class: vr512, preferred-register: '' }
  - { id: 560, class: vr512, preferred-register: '' }
  - { id: 561, class: vr512, preferred-register: '' }
  - { id: 562, class: vr512, preferred-register: '' }
  - { id: 563, class: vr512, preferred-register: '' }
  - { id: 564, class: vr512, preferred-register: '' }
  - { id: 565, class: vr512, preferred-register: '' }
  - { id: 566, class: vr512, preferred-register: '' }
  - { id: 567, class: vr512, preferred-register: '' }
  - { id: 568, class: vr512, preferred-register: '' }
  - { id: 569, class: vr512, preferred-register: '' }
  - { id: 570, class: vr512, preferred-register: '' }
  - { id: 571, class: vr512, preferred-register: '' }
  - { id: 572, class: vr512, preferred-register: '' }
  - { id: 573, class: vr512, preferred-register: '' }
  - { id: 574, class: vr512, preferred-register: '' }
  - { id: 575, class: vr512, preferred-register: '' }
  - { id: 576, class: vr512, preferred-register: '' }
  - { id: 577, class: vr512, preferred-register: '' }
  - { id: 578, class: vr512, preferred-register: '' }
  - { id: 579, class: vr512, preferred-register: '' }
  - { id: 580, class: vr512, preferred-register: '' }
  - { id: 581, class: vr512, preferred-register: '' }
  - { id: 582, class: vr512, preferred-register: '' }
  - { id: 583, class: vr512, preferred-register: '' }
  - { id: 584, class: vr512, preferred-register: '' }
  - { id: 585, class: vr512, preferred-register: '' }
  - { id: 586, class: vr512, preferred-register: '' }
  - { id: 587, class: vr512, preferred-register: '' }
  - { id: 588, class: vr512, preferred-register: '' }
  - { id: 589, class: vr512, preferred-register: '' }
  - { id: 590, class: vr512, preferred-register: '' }
  - { id: 591, class: vr512, preferred-register: '' }
  - { id: 592, class: vr512, preferred-register: '' }
  - { id: 593, class: vr512, preferred-register: '' }
  - { id: 594, class: vr512, preferred-register: '' }
  - { id: 595, class: vr512, preferred-register: '' }
  - { id: 596, class: vr512, preferred-register: '' }
  - { id: 597, class: vr512, preferred-register: '' }
  - { id: 598, class: vr512, preferred-register: '' }
  - { id: 599, class: vr512, preferred-register: '' }
  - { id: 600, class: vr512, preferred-register: '' }
  - { id: 601, class: vr512, preferred-register: '' }
  - { id: 602, class: vr512, preferred-register: '' }
  - { id: 603, class: gr64_nosp, preferred-register: '' }
  - { id: 604, class: vr512, preferred-register: '' }
  - { id: 605, class: vr512, preferred-register: '' }
  - { id: 606, class: vr512, preferred-register: '' }
  - { id: 607, class: vr512, preferred-register: '' }
  - { id: 608, class: vr512, preferred-register: '' }
  - { id: 609, class: vr512, preferred-register: '' }
  - { id: 610, class: vr512, preferred-register: '' }
  - { id: 611, class: vr512, preferred-register: '' }
  - { id: 612, class: gr64_nosp, preferred-register: '' }
  - { id: 613, class: gr64_nosp, preferred-register: '' }
  - { id: 614, class: vr512, preferred-register: '' }
  - { id: 615, class: vr512, preferred-register: '' }
  - { id: 616, class: vr512, preferred-register: '' }
  - { id: 617, class: vr512, preferred-register: '' }
  - { id: 618, class: vr512, preferred-register: '' }
  - { id: 619, class: vr512, preferred-register: '' }
  - { id: 620, class: vr512, preferred-register: '' }
  - { id: 621, class: vr512, preferred-register: '' }
  - { id: 622, class: gr64_nosp, preferred-register: '' }
  - { id: 623, class: gr64_nosp, preferred-register: '' }
  - { id: 624, class: vr512, preferred-register: '' }
  - { id: 625, class: vr512, preferred-register: '' }
  - { id: 626, class: vr512, preferred-register: '' }
  - { id: 627, class: vr512, preferred-register: '' }
  - { id: 628, class: vr512, preferred-register: '' }
  - { id: 629, class: vr512, preferred-register: '' }
  - { id: 630, class: vr512, preferred-register: '' }
  - { id: 631, class: vr512, preferred-register: '' }
  - { id: 632, class: gr64_nosp, preferred-register: '' }
  - { id: 633, class: gr64_nosp, preferred-register: '' }
  - { id: 634, class: vr512, preferred-register: '' }
  - { id: 635, class: vr512, preferred-register: '' }
  - { id: 636, class: vr512, preferred-register: '' }
  - { id: 637, class: vr512, preferred-register: '' }
  - { id: 638, class: vr512, preferred-register: '' }
  - { id: 639, class: vr512, preferred-register: '' }
  - { id: 640, class: vr512, preferred-register: '' }
  - { id: 641, class: vr512, preferred-register: '' }
  - { id: 642, class: vr512, preferred-register: '' }
  - { id: 643, class: vr512, preferred-register: '' }
  - { id: 644, class: vr512, preferred-register: '' }
  - { id: 645, class: vr512, preferred-register: '' }
  - { id: 646, class: vr512, preferred-register: '' }
  - { id: 647, class: vr512, preferred-register: '' }
  - { id: 648, class: vr512, preferred-register: '' }
  - { id: 649, class: vr512, preferred-register: '' }
  - { id: 650, class: vr512, preferred-register: '' }
  - { id: 651, class: vr512, preferred-register: '' }
  - { id: 652, class: vr512, preferred-register: '' }
  - { id: 653, class: vr512, preferred-register: '' }
  - { id: 654, class: vr512, preferred-register: '' }
  - { id: 655, class: vr512, preferred-register: '' }
  - { id: 656, class: vr512, preferred-register: '' }
  - { id: 657, class: vr512, preferred-register: '' }
  - { id: 658, class: vr512, preferred-register: '' }
  - { id: 659, class: vr512, preferred-register: '' }
  - { id: 660, class: vr512, preferred-register: '' }
  - { id: 661, class: vr512, preferred-register: '' }
  - { id: 662, class: vr512, preferred-register: '' }
  - { id: 663, class: vr512, preferred-register: '' }
  - { id: 664, class: vr512, preferred-register: '' }
  - { id: 665, class: vr512, preferred-register: '' }
  - { id: 666, class: vr512, preferred-register: '' }
  - { id: 667, class: vr512, preferred-register: '' }
  - { id: 668, class: vr512, preferred-register: '' }
  - { id: 669, class: vr512, preferred-register: '' }
  - { id: 670, class: vr512, preferred-register: '' }
  - { id: 671, class: vr512, preferred-register: '' }
  - { id: 672, class: vr512, preferred-register: '' }
  - { id: 673, class: vr512, preferred-register: '' }
  - { id: 674, class: vr512, preferred-register: '' }
  - { id: 675, class: vr512, preferred-register: '' }
  - { id: 676, class: vr512, preferred-register: '' }
  - { id: 677, class: vr512, preferred-register: '' }
  - { id: 678, class: vr512, preferred-register: '' }
  - { id: 679, class: vr512, preferred-register: '' }
  - { id: 680, class: vr512, preferred-register: '' }
  - { id: 681, class: vr512, preferred-register: '' }
  - { id: 682, class: vr512, preferred-register: '' }
  - { id: 683, class: vr512, preferred-register: '' }
  - { id: 684, class: vr512, preferred-register: '' }
  - { id: 685, class: vr512, preferred-register: '' }
  - { id: 686, class: vr512, preferred-register: '' }
  - { id: 687, class: vr512, preferred-register: '' }
  - { id: 688, class: vr512, preferred-register: '' }
  - { id: 689, class: vr512, preferred-register: '' }
  - { id: 690, class: vr512, preferred-register: '' }
  - { id: 691, class: vr512, preferred-register: '' }
  - { id: 692, class: vr512, preferred-register: '' }
  - { id: 693, class: vr512, preferred-register: '' }
  - { id: 694, class: vr512, preferred-register: '' }
  - { id: 695, class: vr512, preferred-register: '' }
  - { id: 696, class: vr512, preferred-register: '' }
  - { id: 697, class: vr512, preferred-register: '' }
  - { id: 698, class: vr512, preferred-register: '' }
  - { id: 699, class: vr512, preferred-register: '' }
  - { id: 700, class: vr512, preferred-register: '' }
  - { id: 701, class: vr512, preferred-register: '' }
  - { id: 702, class: vr512, preferred-register: '' }
  - { id: 703, class: vr512, preferred-register: '' }
  - { id: 704, class: vr512, preferred-register: '' }
  - { id: 705, class: vr512, preferred-register: '' }
  - { id: 706, class: gr64_nosp, preferred-register: '' }
  - { id: 707, class: vr512, preferred-register: '' }
  - { id: 708, class: vr512, preferred-register: '' }
  - { id: 709, class: vr512, preferred-register: '' }
  - { id: 710, class: vr512, preferred-register: '' }
  - { id: 711, class: vr512, preferred-register: '' }
  - { id: 712, class: vr512, preferred-register: '' }
  - { id: 713, class: vr512, preferred-register: '' }
  - { id: 714, class: vr512, preferred-register: '' }
  - { id: 715, class: gr64_nosp, preferred-register: '' }
  - { id: 716, class: gr64_nosp, preferred-register: '' }
  - { id: 717, class: vr512, preferred-register: '' }
  - { id: 718, class: vr512, preferred-register: '' }
  - { id: 719, class: vr512, preferred-register: '' }
  - { id: 720, class: vr512, preferred-register: '' }
  - { id: 721, class: vr512, preferred-register: '' }
  - { id: 722, class: vr512, preferred-register: '' }
  - { id: 723, class: vr512, preferred-register: '' }
  - { id: 724, class: vr512, preferred-register: '' }
  - { id: 725, class: gr64_nosp, preferred-register: '' }
  - { id: 726, class: gr64_nosp, preferred-register: '' }
  - { id: 727, class: vr512, preferred-register: '' }
  - { id: 728, class: vr512, preferred-register: '' }
  - { id: 729, class: vr512, preferred-register: '' }
  - { id: 730, class: vr512, preferred-register: '' }
  - { id: 731, class: vr512, preferred-register: '' }
  - { id: 732, class: vr512, preferred-register: '' }
  - { id: 733, class: vr512, preferred-register: '' }
  - { id: 734, class: vr512, preferred-register: '' }
  - { id: 735, class: gr64_nosp, preferred-register: '' }
  - { id: 736, class: gr64_nosp, preferred-register: '' }
  - { id: 737, class: gr64, preferred-register: '' }
liveins:
  - { reg: '$rdi', virtual-reg: '%22' }
  - { reg: '$rsi', virtual-reg: '%23' }
  - { reg: '$rdx', virtual-reg: '%24' }
  - { reg: '$rcx', virtual-reg: '%25' }
  - { reg: '$r8', virtual-reg: '%26' }
  - { reg: '$r9', virtual-reg: '%27' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    8
  adjustsStack:    false
  hasCalls:        false
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 4294967295
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:
  - { id: 0, type: default, offset: 0, size: 8, alignment: 16, stack-id: default,
      isImmutable: true, isAliased: false, callee-saved-register: '', callee-saved-restored: true,
      debug-info-variable: '', debug-info-expression: '', debug-info-location: '' }
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo: {}
body:             |
  ; CHECK-LABEL: name: _Z4testPDv8_dmPKS_mS2_mm
  ; CHECK:       bb.0.entry:
  ; CHECK-NEXT:    successors: %bb.1(0x80000000)
  ; CHECK-NEXT:    liveins: $rdi, $rsi, $rdx, $rcx, $r8, $r9
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:    %27:gr64_nosp = COPY $r9
  ; CHECK-NEXT:    %26:gr64 = COPY $r8
  ; CHECK-NEXT:    %25:gr64_nosp = COPY $rcx
  ; CHECK-NEXT:    %24:gr64_nosp = COPY $rdx
  ; CHECK-NEXT:    %48:gr64_nosp = COPY $rsi
  ; CHECK-NEXT:    %9:gr64 = COPY $rdi
  ; CHECK-NEXT:    %32:gr64_nosp = COPY %25
  ; CHECK-NEXT:    %34:gr64_nosp = COPY %25
  ; CHECK-NEXT:    %42:gr64 = IMUL64rri32 %25, 448, implicit-def dead $eflags
  ; CHECK-NEXT:    %45:gr64 = LEA64r %25, 2, %25, 0, $noreg
  ; CHECK-NEXT:    %47:gr64 = LEA64r %25, 4, %25, 0, $noreg
  ; CHECK-NEXT:    %30:gr64_nosp = COPY %25
  ; CHECK-NEXT:    %30:gr64_nosp = SHL64ri %30, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %0:gr64 = COPY %48
  ; CHECK-NEXT:    %0:gr64 = SHL64ri %0, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %33:gr64_nosp = COPY %27
  ; CHECK-NEXT:    %35:gr64_nosp = COPY %27
  ; CHECK-NEXT:    %48:gr64_nosp = SHL64ri %48, 7, implicit-def dead $eflags
  ; CHECK-NEXT:    %49:gr64 = LEA64r %48, 2, %48, 0, $noreg
  ; CHECK-NEXT:    %17:gr64 = MOV32ri64 64
  ; CHECK-NEXT:    %17:gr64 = SUB64rr %17, %49, implicit-def dead $eflags
  ; CHECK-NEXT:    %36:gr64_nosp = IMUL64rri32 %27, 448, implicit-def dead $eflags
  ; CHECK-NEXT:    %39:gr64_nosp = LEA64r %27, 2, %27, 0, $noreg
  ; CHECK-NEXT:    %41:gr64_nosp = LEA64r %27, 4, %27, 0, $noreg
  ; CHECK-NEXT:    %31:gr64_nosp = COPY %27
  ; CHECK-NEXT:    %31:gr64_nosp = SHL64ri %31, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %32:gr64_nosp = SHL64ri %32, 8, implicit-def dead $eflags
  ; CHECK-NEXT:    %33:gr64_nosp = SHL64ri %33, 8, implicit-def dead $eflags
  ; CHECK-NEXT:    %34:gr64_nosp = SHL64ri %34, 7, implicit-def dead $eflags
  ; CHECK-NEXT:    %35:gr64_nosp = SHL64ri %35, 7, implicit-def dead $eflags
  ; CHECK-NEXT:    %1:gr64 = LEA64r %26, 1, %35, 192, $noreg
  ; CHECK-NEXT:    %2:gr64 = LEA64r %26, 1, %31, 192, $noreg
  ; CHECK-NEXT:    %3:gr64 = LEA64r %26, 1, %33, 192, $noreg
  ; CHECK-NEXT:    %5:gr64 = LEA64r %26, 1, %36, 192, $noreg
  ; CHECK-NEXT:    %38:gr64_nosp = COPY %39
  ; CHECK-NEXT:    %38:gr64_nosp = SHL64ri %38, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %6:gr64 = LEA64r %26, 1, %38, 192, $noreg
  ; CHECK-NEXT:    %39:gr64_nosp = SHL64ri %39, 7, implicit-def dead $eflags
  ; CHECK-NEXT:    %7:gr64 = LEA64r %26, 1, %39, 192, $noreg
  ; CHECK-NEXT:    %41:gr64_nosp = SHL64ri %41, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %8:gr64 = LEA64r %26, 1, %41, 192, $noreg
  ; CHECK-NEXT:    %4:gr64 = LEA64r %26, 1, $noreg, 192, $noreg
  ; CHECK-NEXT:    %9:gr64 = ADD64ri32 %9, 192, implicit-def dead $eflags
  ; CHECK-NEXT:    %10:gr64 = LEA64r %34, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %11:gr64 = LEA64r %30, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %12:gr64 = LEA64r %32, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %13:gr64 = LEA64r %42, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %44:gr64 = COPY %45
  ; CHECK-NEXT:    %44:gr64 = SHL64ri %44, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %14:gr64 = LEA64r %44, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %45:gr64 = SHL64ri %45, 7, implicit-def dead $eflags
  ; CHECK-NEXT:    %15:gr64 = LEA64r %45, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %47:gr64 = SHL64ri %47, 6, implicit-def dead $eflags
  ; CHECK-NEXT:    %16:gr64 = LEA64r %47, 1, %24, 192, $noreg
  ; CHECK-NEXT:    %737:gr64 = MOV64rm %fixed-stack.0, 1, $noreg, 0, $noreg :: (load (s64) from %fixed-stack.0, align 16)
  ; CHECK-NEXT:    undef %736.sub_32bit:gr64_nosp = MOV32r0 implicit-def dead $eflags
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:  bb.1.do.body:
  ; CHECK-NEXT:    successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:    %53:vr512 = VMOVAPDZrm %24, 1, %736, 0, $noreg :: (load (s512) from %ir.sunkaddr, !tbaa !5)
  ; CHECK-NEXT:    %54:vr512 = VMOVAPDZrm %24, 1, %736, 64, $noreg :: (load (s512) from %ir.sunkaddr166, !tbaa !5)
  ; CHECK-NEXT:    PREFETCHT0 %4, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep31)
  ; CHECK-NEXT:    %55:vr512 = VMOVAPDZrm %24, 1, %736, 128, $noreg :: (load (s512) from %ir.sunkaddr168, !tbaa !5)
  ; CHECK-NEXT:    %56:vr512 = VMOVAPDZrm %24, 1, %736, 192, $noreg :: (load (s512) from %ir.sunkaddr170, !tbaa !5)
  ; CHECK-NEXT:    %57:vr512 = nofpexcept VADDPDZrr %53, %54, implicit $mxcsr
  ; CHECK-NEXT:    %58:vr512 = nofpexcept VSUBPDZrr %53, %54, implicit $mxcsr
  ; CHECK-NEXT:    %59:vr512 = VUNPCKLPDZrr %57, %58
  ; CHECK-NEXT:    %62:vr512 = VUNPCKHPDZrr %57, %58
  ; CHECK-NEXT:    %61:vr512 = nofpexcept VMULPDZrr %62, %59, implicit $mxcsr
  ; CHECK-NEXT:    %62:vr512 = nofpexcept VFMADD213PDZr %62, %59, %61, implicit $mxcsr
  ; CHECK-NEXT:    %63:vr512 = nofpexcept VADDPDZrr %61, %62, implicit $mxcsr
  ; CHECK-NEXT:    %65:vr512 = nofpexcept VADDPDZrr %55, %56, implicit $mxcsr
  ; CHECK-NEXT:    %66:vr512 = nofpexcept VSUBPDZrr %55, %56, implicit $mxcsr
  ; CHECK-NEXT:    %67:vr512 = VUNPCKLPDZrr %65, %66
  ; CHECK-NEXT:    %70:vr512 = VUNPCKHPDZrr %65, %66
  ; CHECK-NEXT:    %69:vr512 = nofpexcept VMULPDZrr %70, %67, implicit $mxcsr
  ; CHECK-NEXT:    %64:vr512 = nofpexcept VSUBPDZrr %61, %62, implicit $mxcsr
  ; CHECK-NEXT:    %70:vr512 = nofpexcept VFMADD213PDZr %70, %67, %69, implicit $mxcsr
  ; CHECK-NEXT:    %71:vr512 = nofpexcept VADDPDZrr %69, %70, implicit $mxcsr
  ; CHECK-NEXT:    %72:vr512 = nofpexcept VSUBPDZrr %69, %70, implicit $mxcsr
  ; CHECK-NEXT:    %87:vr512 = VMOVAPDZrm %12, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep95, !tbaa !5)
  ; CHECK-NEXT:    %73:vr512 = nofpexcept VADDPDZrr %63, %71, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %4, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep29)
  ; CHECK-NEXT:    %88:vr512 = VMOVAPDZrm %12, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep93, !tbaa !5)
  ; CHECK-NEXT:    %89:vr512 = VMOVAPDZrm %12, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep91, !tbaa !5)
  ; CHECK-NEXT:    %74:vr512 = nofpexcept VSUBPDZrr %63, %71, implicit $mxcsr
  ; CHECK-NEXT:    %81:vr512 = nofpexcept VADDPDZrr %72, %72, implicit $mxcsr
  ; CHECK-NEXT:    %90:vr512 = VMOVAPDZrm %12, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep89, !tbaa !5)
  ; CHECK-NEXT:    %91:vr512 = nofpexcept VADDPDZrr %87, %88, implicit $mxcsr
  ; CHECK-NEXT:    %92:vr512 = nofpexcept VSUBPDZrr %87, %88, implicit $mxcsr
  ; CHECK-NEXT:    %82:vr512 = nofpexcept VSUBPDZrr %72, %72, implicit $mxcsr
  ; CHECK-NEXT:    %93:vr512 = VUNPCKLPDZrr %91, %92
  ; CHECK-NEXT:    %96:vr512 = VUNPCKHPDZrr %91, %92
  ; CHECK-NEXT:    %95:vr512 = nofpexcept VMULPDZrr %96, %93, implicit $mxcsr
  ; CHECK-NEXT:    %96:vr512 = nofpexcept VFMADD213PDZr %96, %93, %95, implicit $mxcsr
  ; CHECK-NEXT:    %97:vr512 = nofpexcept VADDPDZrr %95, %96, implicit $mxcsr
  ; CHECK-NEXT:    %75:vr512 = VUNPCKLPDZrr %73, %74
  ; CHECK-NEXT:    %98:vr512 = nofpexcept VSUBPDZrr %95, %96, implicit $mxcsr
  ; CHECK-NEXT:    %99:vr512 = nofpexcept VADDPDZrr %89, %90, implicit $mxcsr
  ; CHECK-NEXT:    %100:vr512 = nofpexcept VSUBPDZrr %89, %90, implicit $mxcsr
  ; CHECK-NEXT:    %101:vr512 = VUNPCKLPDZrr %99, %100
  ; CHECK-NEXT:    %104:vr512 = VUNPCKHPDZrr %99, %100
  ; CHECK-NEXT:    %78:vr512 = VUNPCKHPDZrr %73, %74
  ; CHECK-NEXT:    %103:vr512 = nofpexcept VMULPDZrr %104, %101, implicit $mxcsr
  ; CHECK-NEXT:    %104:vr512 = nofpexcept VFMADD213PDZr %104, %101, %103, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %4, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep27)
  ; CHECK-NEXT:    %105:vr512 = nofpexcept VADDPDZrr %103, %104, implicit $mxcsr
  ; CHECK-NEXT:    %106:vr512 = nofpexcept VSUBPDZrr %103, %104, implicit $mxcsr
  ; CHECK-NEXT:    %85:vr512 = VUNPCKHPDZrr %81, %82
  ; CHECK-NEXT:    %107:vr512 = nofpexcept VADDPDZrr %97, %105, implicit $mxcsr
  ; CHECK-NEXT:    %108:vr512 = nofpexcept VSUBPDZrr %97, %105, implicit $mxcsr
  ; CHECK-NEXT:    %115:vr512 = nofpexcept VADDPDZrr %106, %106, implicit $mxcsr
  ; CHECK-NEXT:    %121:vr512 = VMOVAPDZrm %11, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep87, !tbaa !5)
  ; CHECK-NEXT:    %109:vr512 = VUNPCKLPDZrr %107, %108
  ; CHECK-NEXT:    %116:vr512 = nofpexcept VSUBPDZrr %106, %106, implicit $mxcsr
  ; CHECK-NEXT:    %122:vr512 = VMOVAPDZrm %11, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep85, !tbaa !5)
  ; CHECK-NEXT:    %123:vr512 = VMOVAPDZrm %11, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep83, !tbaa !5)
  ; CHECK-NEXT:    %112:vr512 = VUNPCKHPDZrr %107, %108
  ; CHECK-NEXT:    %119:vr512 = VUNPCKHPDZrr %115, %116
  ; CHECK-NEXT:    %124:vr512 = VMOVAPDZrm %11, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep81, !tbaa !5)
  ; CHECK-NEXT:    %125:vr512 = nofpexcept VADDPDZrr %121, %122, implicit $mxcsr
  ; CHECK-NEXT:    %126:vr512 = nofpexcept VSUBPDZrr %121, %122, implicit $mxcsr
  ; CHECK-NEXT:    %77:vr512 = nofpexcept VMULPDZrr %78, %75, implicit $mxcsr
  ; CHECK-NEXT:    %127:vr512 = VUNPCKLPDZrr %125, %126
  ; CHECK-NEXT:    %130:vr512 = VUNPCKHPDZrr %125, %126
  ; CHECK-NEXT:    %129:vr512 = nofpexcept VMULPDZrr %130, %127, implicit $mxcsr
  ; CHECK-NEXT:    %130:vr512 = nofpexcept VFMADD213PDZr %130, %127, %129, implicit $mxcsr
  ; CHECK-NEXT:    %131:vr512 = nofpexcept VADDPDZrr %129, %130, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %4, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep25)
  ; CHECK-NEXT:    %132:vr512 = nofpexcept VSUBPDZrr %129, %130, implicit $mxcsr
  ; CHECK-NEXT:    %133:vr512 = nofpexcept VADDPDZrr %123, %124, implicit $mxcsr
  ; CHECK-NEXT:    %134:vr512 = nofpexcept VSUBPDZrr %123, %124, implicit $mxcsr
  ; CHECK-NEXT:    %135:vr512 = VUNPCKLPDZrr %133, %134
  ; CHECK-NEXT:    %138:vr512 = VUNPCKHPDZrr %133, %134
  ; CHECK-NEXT:    %84:vr512 = nofpexcept VMULPDZrr %85, %85, implicit $mxcsr
  ; CHECK-NEXT:    %137:vr512 = nofpexcept VMULPDZrr %138, %135, implicit $mxcsr
  ; CHECK-NEXT:    %138:vr512 = nofpexcept VFMADD213PDZr %138, %135, %137, implicit $mxcsr
  ; CHECK-NEXT:    %139:vr512 = nofpexcept VADDPDZrr %137, %138, implicit $mxcsr
  ; CHECK-NEXT:    %140:vr512 = nofpexcept VSUBPDZrr %137, %138, implicit $mxcsr
  ; CHECK-NEXT:    %141:vr512 = nofpexcept VADDPDZrr %131, %139, implicit $mxcsr
  ; CHECK-NEXT:    %111:vr512 = nofpexcept VMULPDZrr %112, %109, implicit $mxcsr
  ; CHECK-NEXT:    %142:vr512 = nofpexcept VSUBPDZrr %131, %139, implicit $mxcsr
  ; CHECK-NEXT:    %143:vr512 = VUNPCKLPDZrr %141, %142
  ; CHECK-NEXT:    %146:vr512 = VUNPCKHPDZrr %141, %142
  ; CHECK-NEXT:    %149:vr512 = nofpexcept VADDPDZrr %140, %140, implicit $mxcsr
  ; CHECK-NEXT:    %150:vr512 = nofpexcept VSUBPDZrr %140, %140, implicit $mxcsr
  ; CHECK-NEXT:    %118:vr512 = nofpexcept VMULPDZrr %119, %119, implicit $mxcsr
  ; CHECK-NEXT:    %153:vr512 = VUNPCKHPDZrr %149, %150
  ; CHECK-NEXT:    %155:vr512 = VMOVAPDZrm %16, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep135, !tbaa !5)
  ; CHECK-NEXT:    %156:vr512 = VMOVAPDZrm %16, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep133, !tbaa !5)
  ; CHECK-NEXT:    %145:vr512 = nofpexcept VMULPDZrr %146, %143, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %3, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep23)
  ; CHECK-NEXT:    %157:vr512 = VMOVAPDZrm %16, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep131, !tbaa !5)
  ; CHECK-NEXT:    %158:vr512 = VMOVAPDZrm %16, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep129, !tbaa !5)
  ; CHECK-NEXT:    %152:vr512 = nofpexcept VMULPDZrr %153, %153, implicit $mxcsr
  ; CHECK-NEXT:    %159:vr512 = nofpexcept VADDPDZrr %155, %156, implicit $mxcsr
  ; CHECK-NEXT:    %160:vr512 = nofpexcept VSUBPDZrr %155, %156, implicit $mxcsr
  ; CHECK-NEXT:    %161:vr512 = VUNPCKLPDZrr %159, %160
  ; CHECK-NEXT:    %164:vr512 = VUNPCKHPDZrr %159, %160
  ; CHECK-NEXT:    %163:vr512 = nofpexcept VMULPDZrr %164, %161, implicit $mxcsr
  ; CHECK-NEXT:    %78:vr512 = nofpexcept VFMADD213PDZr %78, %75, %77, implicit $mxcsr
  ; CHECK-NEXT:    %164:vr512 = nofpexcept VFMADD213PDZr %164, %161, %163, implicit $mxcsr
  ; CHECK-NEXT:    %165:vr512 = nofpexcept VADDPDZrr %163, %164, implicit $mxcsr
  ; CHECK-NEXT:    %166:vr512 = nofpexcept VSUBPDZrr %163, %164, implicit $mxcsr
  ; CHECK-NEXT:    %167:vr512 = nofpexcept VADDPDZrr %157, %158, implicit $mxcsr
  ; CHECK-NEXT:    %168:vr512 = nofpexcept VSUBPDZrr %157, %158, implicit $mxcsr
  ; CHECK-NEXT:    %85:vr512 = nofpexcept VFMADD213PDZr %85, %85, %84, implicit $mxcsr
  ; CHECK-NEXT:    %169:vr512 = VUNPCKLPDZrr %167, %168
  ; CHECK-NEXT:    %172:vr512 = VUNPCKHPDZrr %167, %168
  ; CHECK-NEXT:    %171:vr512 = nofpexcept VMULPDZrr %172, %169, implicit $mxcsr
  ; CHECK-NEXT:    %172:vr512 = nofpexcept VFMADD213PDZr %172, %169, %171, implicit $mxcsr
  ; CHECK-NEXT:    %173:vr512 = nofpexcept VADDPDZrr %171, %172, implicit $mxcsr
  ; CHECK-NEXT:    %112:vr512 = nofpexcept VFMADD213PDZr %112, %109, %111, implicit $mxcsr
  ; CHECK-NEXT:    %174:vr512 = nofpexcept VSUBPDZrr %171, %172, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %3, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep21)
  ; CHECK-NEXT:    %175:vr512 = nofpexcept VADDPDZrr %165, %173, implicit $mxcsr
  ; CHECK-NEXT:    %176:vr512 = nofpexcept VSUBPDZrr %165, %173, implicit $mxcsr
  ; CHECK-NEXT:    %177:vr512 = VUNPCKLPDZrr %175, %176
  ; CHECK-NEXT:    %119:vr512 = nofpexcept VFMADD213PDZr %119, %119, %118, implicit $mxcsr
  ; CHECK-NEXT:    %180:vr512 = VUNPCKHPDZrr %175, %176
  ; CHECK-NEXT:    %179:vr512 = nofpexcept VMULPDZrr %180, %177, implicit $mxcsr
  ; CHECK-NEXT:    %183:vr512 = nofpexcept VADDPDZrr %174, %174, implicit $mxcsr
  ; CHECK-NEXT:    %184:vr512 = nofpexcept VSUBPDZrr %174, %174, implicit $mxcsr
  ; CHECK-NEXT:    %187:vr512 = VUNPCKHPDZrr %183, %184
  ; CHECK-NEXT:    %146:vr512 = nofpexcept VFMADD213PDZr %146, %143, %145, implicit $mxcsr
  ; CHECK-NEXT:    %186:vr512 = nofpexcept VMULPDZrr %187, %187, implicit $mxcsr
  ; CHECK-NEXT:    %189:vr512 = VMOVAPDZrm %10, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep79, !tbaa !5)
  ; CHECK-NEXT:    %190:vr512 = VMOVAPDZrm %10, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep77, !tbaa !5)
  ; CHECK-NEXT:    %153:vr512 = nofpexcept VFMADD213PDZr %153, %153, %152, implicit $mxcsr
  ; CHECK-NEXT:    %191:vr512 = VMOVAPDZrm %10, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep75, !tbaa !5)
  ; CHECK-NEXT:    %192:vr512 = VMOVAPDZrm %10, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep73, !tbaa !5)
  ; CHECK-NEXT:    %193:vr512 = nofpexcept VADDPDZrr %189, %190, implicit $mxcsr
  ; CHECK-NEXT:    %180:vr512 = nofpexcept VFMADD213PDZr %180, %177, %179, implicit $mxcsr
  ; CHECK-NEXT:    %194:vr512 = nofpexcept VSUBPDZrr %189, %190, implicit $mxcsr
  ; CHECK-NEXT:    %195:vr512 = VUNPCKLPDZrr %193, %194
  ; CHECK-NEXT:    %198:vr512 = VUNPCKHPDZrr %193, %194
  ; CHECK-NEXT:    %197:vr512 = nofpexcept VMULPDZrr %198, %195, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %3, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep19)
  ; CHECK-NEXT:    %187:vr512 = nofpexcept VFMADD213PDZr %187, %187, %186, implicit $mxcsr
  ; CHECK-NEXT:    %198:vr512 = nofpexcept VFMADD213PDZr %198, %195, %197, implicit $mxcsr
  ; CHECK-NEXT:    %199:vr512 = nofpexcept VADDPDZrr %197, %198, implicit $mxcsr
  ; CHECK-NEXT:    %200:vr512 = nofpexcept VSUBPDZrr %197, %198, implicit $mxcsr
  ; CHECK-NEXT:    %201:vr512 = nofpexcept VADDPDZrr %191, %192, implicit $mxcsr
  ; CHECK-NEXT:    %202:vr512 = nofpexcept VSUBPDZrr %191, %192, implicit $mxcsr
  ; CHECK-NEXT:    %79:vr512 = nofpexcept VADDPDZrr %77, %78, implicit $mxcsr
  ; CHECK-NEXT:    %203:vr512 = VUNPCKLPDZrr %201, %202
  ; CHECK-NEXT:    %206:vr512 = VUNPCKHPDZrr %201, %202
  ; CHECK-NEXT:    %205:vr512 = nofpexcept VMULPDZrr %206, %203, implicit $mxcsr
  ; CHECK-NEXT:    %206:vr512 = nofpexcept VFMADD213PDZr %206, %203, %205, implicit $mxcsr
  ; CHECK-NEXT:    %207:vr512 = nofpexcept VADDPDZrr %205, %206, implicit $mxcsr
  ; CHECK-NEXT:    %80:vr512 = nofpexcept VSUBPDZrr %77, %78, implicit $mxcsr
  ; CHECK-NEXT:    %208:vr512 = nofpexcept VSUBPDZrr %205, %206, implicit $mxcsr
  ; CHECK-NEXT:    %209:vr512 = nofpexcept VADDPDZrr %199, %207, implicit $mxcsr
  ; CHECK-NEXT:    %210:vr512 = nofpexcept VSUBPDZrr %199, %207, implicit $mxcsr
  ; CHECK-NEXT:    %211:vr512 = VUNPCKLPDZrr %209, %210
  ; CHECK-NEXT:    %214:vr512 = VUNPCKHPDZrr %209, %210
  ; CHECK-NEXT:    %86:vr512 = nofpexcept VSUBPDZrr %84, %85, implicit $mxcsr
  ; CHECK-NEXT:    %213:vr512 = nofpexcept VMULPDZrr %214, %211, implicit $mxcsr
  ; CHECK-NEXT:    %214:vr512 = nofpexcept VFMADD213PDZr %214, %211, %213, implicit $mxcsr
  ; CHECK-NEXT:    %217:vr512 = nofpexcept VADDPDZrr %208, %208, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %3, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep17)
  ; CHECK-NEXT:    %218:vr512 = nofpexcept VSUBPDZrr %208, %208, implicit $mxcsr
  ; CHECK-NEXT:    %113:vr512 = nofpexcept VADDPDZrr %111, %112, implicit $mxcsr
  ; CHECK-NEXT:    %221:vr512 = VUNPCKHPDZrr %217, %218
  ; CHECK-NEXT:    %223:vr512 = VMOVAPDZrm %15, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep127, !tbaa !5)
  ; CHECK-NEXT:    %224:vr512 = VMOVAPDZrm %15, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep125, !tbaa !5)
  ; CHECK-NEXT:    %114:vr512 = nofpexcept VSUBPDZrr %111, %112, implicit $mxcsr
  ; CHECK-NEXT:    %220:vr512 = nofpexcept VMULPDZrr %221, %221, implicit $mxcsr
  ; CHECK-NEXT:    %225:vr512 = VMOVAPDZrm %15, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep123, !tbaa !5)
  ; CHECK-NEXT:    %226:vr512 = VMOVAPDZrm %15, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep121, !tbaa !5)
  ; CHECK-NEXT:    %120:vr512 = nofpexcept VSUBPDZrr %118, %119, implicit $mxcsr
  ; CHECK-NEXT:    %221:vr512 = nofpexcept VFMADD213PDZr %221, %221, %220, implicit $mxcsr
  ; CHECK-NEXT:    %227:vr512 = nofpexcept VADDPDZrr %223, %224, implicit $mxcsr
  ; CHECK-NEXT:    %228:vr512 = nofpexcept VSUBPDZrr %223, %224, implicit $mxcsr
  ; CHECK-NEXT:    %229:vr512 = VUNPCKLPDZrr %227, %228
  ; CHECK-NEXT:    %232:vr512 = VUNPCKHPDZrr %227, %228
  ; CHECK-NEXT:    %147:vr512 = nofpexcept VADDPDZrr %145, %146, implicit $mxcsr
  ; CHECK-NEXT:    %231:vr512 = nofpexcept VMULPDZrr %232, %229, implicit $mxcsr
  ; CHECK-NEXT:    %232:vr512 = nofpexcept VFMADD213PDZr %232, %229, %231, implicit $mxcsr
  ; CHECK-NEXT:    %233:vr512 = nofpexcept VADDPDZrr %231, %232, implicit $mxcsr
  ; CHECK-NEXT:    %234:vr512 = nofpexcept VSUBPDZrr %231, %232, implicit $mxcsr
  ; CHECK-NEXT:    %235:vr512 = nofpexcept VADDPDZrr %225, %226, implicit $mxcsr
  ; CHECK-NEXT:    %148:vr512 = nofpexcept VSUBPDZrr %145, %146, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %2, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep15)
  ; CHECK-NEXT:    %236:vr512 = nofpexcept VSUBPDZrr %225, %226, implicit $mxcsr
  ; CHECK-NEXT:    %237:vr512 = VUNPCKLPDZrr %235, %236
  ; CHECK-NEXT:    %240:vr512 = VUNPCKHPDZrr %235, %236
  ; CHECK-NEXT:    %239:vr512 = nofpexcept VMULPDZrr %240, %237, implicit $mxcsr
  ; CHECK-NEXT:    %154:vr512 = nofpexcept VSUBPDZrr %152, %153, implicit $mxcsr
  ; CHECK-NEXT:    %240:vr512 = nofpexcept VFMADD213PDZr %240, %237, %239, implicit $mxcsr
  ; CHECK-NEXT:    %241:vr512 = nofpexcept VADDPDZrr %239, %240, implicit $mxcsr
  ; CHECK-NEXT:    %242:vr512 = nofpexcept VSUBPDZrr %239, %240, implicit $mxcsr
  ; CHECK-NEXT:    %243:vr512 = nofpexcept VADDPDZrr %233, %241, implicit $mxcsr
  ; CHECK-NEXT:    %244:vr512 = nofpexcept VSUBPDZrr %233, %241, implicit $mxcsr
  ; CHECK-NEXT:    %181:vr512 = nofpexcept VADDPDZrr %179, %180, implicit $mxcsr
  ; CHECK-NEXT:    %245:vr512 = VUNPCKLPDZrr %243, %244
  ; CHECK-NEXT:    %248:vr512 = VUNPCKHPDZrr %243, %244
  ; CHECK-NEXT:    %247:vr512 = nofpexcept VMULPDZrr %248, %245, implicit $mxcsr
  ; CHECK-NEXT:    %248:vr512 = nofpexcept VFMADD213PDZr %248, %245, %247, implicit $mxcsr
  ; CHECK-NEXT:    %251:vr512 = nofpexcept VADDPDZrr %242, %242, implicit $mxcsr
  ; CHECK-NEXT:    %182:vr512 = nofpexcept VSUBPDZrr %179, %180, implicit $mxcsr
  ; CHECK-NEXT:    %252:vr512 = nofpexcept VSUBPDZrr %242, %242, implicit $mxcsr
  ; CHECK-NEXT:    %255:vr512 = VUNPCKHPDZrr %251, %252
  ; CHECK-NEXT:    %254:vr512 = nofpexcept VMULPDZrr %255, %255, implicit $mxcsr
  ; CHECK-NEXT:    %257:vr512 = VMOVAPDZrm %14, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep119, !tbaa !5)
  ; CHECK-NEXT:    %188:vr512 = nofpexcept VSUBPDZrr %186, %187, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %2, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep13)
  ; CHECK-NEXT:    %258:vr512 = VMOVAPDZrm %14, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep117, !tbaa !5)
  ; CHECK-NEXT:    %259:vr512 = VMOVAPDZrm %14, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep115, !tbaa !5)
  ; CHECK-NEXT:    %215:vr512 = nofpexcept VADDPDZrr %213, %214, implicit $mxcsr
  ; CHECK-NEXT:    %255:vr512 = nofpexcept VFMADD213PDZr %255, %255, %254, implicit $mxcsr
  ; CHECK-NEXT:    %260:vr512 = VMOVAPDZrm %14, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep113, !tbaa !5)
  ; CHECK-NEXT:    %261:vr512 = nofpexcept VADDPDZrr %257, %258, implicit $mxcsr
  ; CHECK-NEXT:    %262:vr512 = nofpexcept VSUBPDZrr %257, %258, implicit $mxcsr
  ; CHECK-NEXT:    %216:vr512 = nofpexcept VSUBPDZrr %213, %214, implicit $mxcsr
  ; CHECK-NEXT:    %263:vr512 = VUNPCKLPDZrr %261, %262
  ; CHECK-NEXT:    %266:vr512 = VUNPCKHPDZrr %261, %262
  ; CHECK-NEXT:    %265:vr512 = nofpexcept VMULPDZrr %266, %263, implicit $mxcsr
  ; CHECK-NEXT:    %266:vr512 = nofpexcept VFMADD213PDZr %266, %263, %265, implicit $mxcsr
  ; CHECK-NEXT:    %267:vr512 = nofpexcept VADDPDZrr %265, %266, implicit $mxcsr
  ; CHECK-NEXT:    %222:vr512 = nofpexcept VSUBPDZrr %220, %221, implicit $mxcsr
  ; CHECK-NEXT:    %268:vr512 = nofpexcept VSUBPDZrr %265, %266, implicit $mxcsr
  ; CHECK-NEXT:    %269:vr512 = nofpexcept VADDPDZrr %259, %260, implicit $mxcsr
  ; CHECK-NEXT:    %270:vr512 = nofpexcept VSUBPDZrr %259, %260, implicit $mxcsr
  ; CHECK-NEXT:    %271:vr512 = VUNPCKLPDZrr %269, %270
  ; CHECK-NEXT:    %274:vr512 = VUNPCKHPDZrr %269, %270
  ; CHECK-NEXT:    %249:vr512 = nofpexcept VADDPDZrr %247, %248, implicit $mxcsr
  ; CHECK-NEXT:    %273:vr512 = nofpexcept VMULPDZrr %274, %271, implicit $mxcsr
  ; CHECK-NEXT:    %274:vr512 = nofpexcept VFMADD213PDZr %274, %271, %273, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %2, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep11)
  ; CHECK-NEXT:    %275:vr512 = nofpexcept VADDPDZrr %273, %274, implicit $mxcsr
  ; CHECK-NEXT:    %276:vr512 = nofpexcept VSUBPDZrr %273, %274, implicit $mxcsr
  ; CHECK-NEXT:    %250:vr512 = nofpexcept VSUBPDZrr %247, %248, implicit $mxcsr
  ; CHECK-NEXT:    %277:vr512 = nofpexcept VADDPDZrr %267, %275, implicit $mxcsr
  ; CHECK-NEXT:    %278:vr512 = nofpexcept VSUBPDZrr %267, %275, implicit $mxcsr
  ; CHECK-NEXT:    %279:vr512 = VUNPCKLPDZrr %277, %278
  ; CHECK-NEXT:    %282:vr512 = VUNPCKHPDZrr %277, %278
  ; CHECK-NEXT:    %281:vr512 = nofpexcept VMULPDZrr %282, %279, implicit $mxcsr
  ; CHECK-NEXT:    %256:vr512 = nofpexcept VSUBPDZrr %254, %255, implicit $mxcsr
  ; CHECK-NEXT:    %282:vr512 = nofpexcept VFMADD213PDZr %282, %279, %281, implicit $mxcsr
  ; CHECK-NEXT:    %285:vr512 = nofpexcept VADDPDZrr %276, %276, implicit $mxcsr
  ; CHECK-NEXT:    %286:vr512 = nofpexcept VSUBPDZrr %276, %276, implicit $mxcsr
  ; CHECK-NEXT:    %291:vr512 = VMOVAPDZrm %13, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep111, !tbaa !5)
  ; CHECK-NEXT:    %283:vr512 = nofpexcept VADDPDZrr %281, %282, implicit $mxcsr
  ; CHECK-NEXT:    %289:vr512 = VUNPCKHPDZrr %285, %286
  ; CHECK-NEXT:    %292:vr512 = VMOVAPDZrm %13, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep109, !tbaa !5)
  ; CHECK-NEXT:    %293:vr512 = VMOVAPDZrm %13, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep107, !tbaa !5)
  ; CHECK-NEXT:    %284:vr512 = nofpexcept VSUBPDZrr %281, %282, implicit $mxcsr
  ; CHECK-NEXT:    %295:vr512 = nofpexcept VADDPDZrr %291, %292, implicit $mxcsr
  ; CHECK-NEXT:    %296:vr512 = nofpexcept VSUBPDZrr %291, %292, implicit $mxcsr
  ; CHECK-NEXT:    %297:vr512 = VUNPCKLPDZrr %295, %296
  ; CHECK-NEXT:    %300:vr512 = VUNPCKHPDZrr %295, %296
  ; CHECK-NEXT:    PREFETCHT0 %2, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep9)
  ; CHECK-NEXT:    %299:vr512 = nofpexcept VMULPDZrr %300, %297, implicit $mxcsr
  ; CHECK-NEXT:    %300:vr512 = nofpexcept VFMADD213PDZr %300, %297, %299, implicit $mxcsr
  ; CHECK-NEXT:    %301:vr512 = nofpexcept VADDPDZrr %299, %300, implicit $mxcsr
  ; CHECK-NEXT:    %302:vr512 = nofpexcept VSUBPDZrr %299, %300, implicit $mxcsr
  ; CHECK-NEXT:    %294:vr512 = VMOVAPDZrm %13, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep105, !tbaa !5)
  ; CHECK-NEXT:    %303:vr512 = nofpexcept VADDPDZrr %293, %294, implicit $mxcsr
  ; CHECK-NEXT:    %304:vr512 = nofpexcept VSUBPDZrr %293, %294, implicit $mxcsr
  ; CHECK-NEXT:    %305:vr512 = VUNPCKLPDZrr %303, %304
  ; CHECK-NEXT:    %308:vr512 = VUNPCKHPDZrr %303, %304
  ; CHECK-NEXT:    %307:vr512 = nofpexcept VMULPDZrr %308, %305, implicit $mxcsr
  ; CHECK-NEXT:    %308:vr512 = nofpexcept VFMADD213PDZr %308, %305, %307, implicit $mxcsr
  ; CHECK-NEXT:    %309:vr512 = nofpexcept VADDPDZrr %307, %308, implicit $mxcsr
  ; CHECK-NEXT:    %310:vr512 = nofpexcept VSUBPDZrr %307, %308, implicit $mxcsr
  ; CHECK-NEXT:    %311:vr512 = nofpexcept VADDPDZrr %301, %309, implicit $mxcsr
  ; CHECK-NEXT:    %312:vr512 = nofpexcept VSUBPDZrr %301, %309, implicit $mxcsr
  ; CHECK-NEXT:    %288:vr512 = nofpexcept VMULPDZrr %289, %289, implicit $mxcsr
  ; CHECK-NEXT:    %289:vr512 = nofpexcept VFMADD213PDZr %289, %289, %288, implicit $mxcsr
  ; CHECK-NEXT:    %290:vr512 = nofpexcept VSUBPDZrr %288, %289, implicit $mxcsr
  ; CHECK-NEXT:    %313:vr512 = VUNPCKLPDZrr %311, %312
  ; CHECK-NEXT:    %316:vr512 = VUNPCKHPDZrr %311, %312
  ; CHECK-NEXT:    %315:vr512 = nofpexcept VMULPDZrr %316, %313, implicit $mxcsr
  ; CHECK-NEXT:    %316:vr512 = nofpexcept VFMADD213PDZr %316, %313, %315, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %8, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep63)
  ; CHECK-NEXT:    %317:vr512 = nofpexcept VADDPDZrr %315, %316, implicit $mxcsr
  ; CHECK-NEXT:    %318:vr512 = nofpexcept VSUBPDZrr %315, %316, implicit $mxcsr
  ; CHECK-NEXT:    %319:vr512 = nofpexcept VADDPDZrr %310, %310, implicit $mxcsr
  ; CHECK-NEXT:    %320:vr512 = nofpexcept VSUBPDZrr %310, %310, implicit $mxcsr
  ; CHECK-NEXT:    %323:vr512 = VUNPCKHPDZrr %319, %320
  ; CHECK-NEXT:    %322:vr512 = nofpexcept VMULPDZrr %323, %323, implicit $mxcsr
  ; CHECK-NEXT:    %323:vr512 = nofpexcept VFMADD213PDZr %323, %323, %322, implicit $mxcsr
  ; CHECK-NEXT:    %324:vr512 = nofpexcept VSUBPDZrr %322, %323, implicit $mxcsr
  ; CHECK-NEXT:    %325:vr512 = nofpexcept VADDPDZrr %79, %113, implicit $mxcsr
  ; CHECK-NEXT:    %326:vr512 = nofpexcept VSUBPDZrr %79, %113, implicit $mxcsr
  ; CHECK-NEXT:    %327:vr512 = VUNPCKLPDZrr %325, %326
  ; CHECK-NEXT:    %330:vr512 = VUNPCKHPDZrr %325, %326
  ; CHECK-NEXT:    %329:vr512 = nofpexcept VMULPDZrr %330, %327, implicit $mxcsr
  ; CHECK-NEXT:    %330:vr512 = nofpexcept VFMADD213PDZr %330, %327, %329, implicit $mxcsr
  ; CHECK-NEXT:    %333:vr512 = nofpexcept VADDPDZrr %147, %181, implicit $mxcsr
  ; CHECK-NEXT:    %334:vr512 = nofpexcept VSUBPDZrr %147, %181, implicit $mxcsr
  ; CHECK-NEXT:    %335:vr512 = VUNPCKLPDZrr %333, %334
  ; CHECK-NEXT:    %338:vr512 = VUNPCKHPDZrr %333, %334
  ; CHECK-NEXT:    %337:vr512 = nofpexcept VMULPDZrr %338, %335, implicit $mxcsr
  ; CHECK-NEXT:    %338:vr512 = nofpexcept VFMADD213PDZr %338, %335, %337, implicit $mxcsr
  ; CHECK-NEXT:    %341:vr512 = nofpexcept VADDPDZrr %215, %249, implicit $mxcsr
  ; CHECK-NEXT:    %342:vr512 = nofpexcept VSUBPDZrr %215, %249, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %8, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep61)
  ; CHECK-NEXT:    %331:vr512 = nofpexcept VADDPDZrr %329, %330, implicit $mxcsr
  ; CHECK-NEXT:    %332:vr512 = nofpexcept VSUBPDZrr %329, %330, implicit $mxcsr
  ; CHECK-NEXT:    %343:vr512 = VUNPCKLPDZrr %341, %342
  ; CHECK-NEXT:    %346:vr512 = VUNPCKHPDZrr %341, %342
  ; CHECK-NEXT:    %345:vr512 = nofpexcept VMULPDZrr %346, %343, implicit $mxcsr
  ; CHECK-NEXT:    %346:vr512 = nofpexcept VFMADD213PDZr %346, %343, %345, implicit $mxcsr
  ; CHECK-NEXT:    %339:vr512 = nofpexcept VADDPDZrr %337, %338, implicit $mxcsr
  ; CHECK-NEXT:    %340:vr512 = nofpexcept VSUBPDZrr %337, %338, implicit $mxcsr
  ; CHECK-NEXT:    %349:vr512 = nofpexcept VADDPDZrr %283, %317, implicit $mxcsr
  ; CHECK-NEXT:    %350:vr512 = nofpexcept VSUBPDZrr %283, %317, implicit $mxcsr
  ; CHECK-NEXT:    %351:vr512 = VUNPCKLPDZrr %349, %350
  ; CHECK-NEXT:    %354:vr512 = VUNPCKHPDZrr %349, %350
  ; CHECK-NEXT:    %347:vr512 = nofpexcept VADDPDZrr %345, %346, implicit $mxcsr
  ; CHECK-NEXT:    %348:vr512 = nofpexcept VSUBPDZrr %345, %346, implicit $mxcsr
  ; CHECK-NEXT:    %353:vr512 = nofpexcept VMULPDZrr %354, %351, implicit $mxcsr
  ; CHECK-NEXT:    %354:vr512 = nofpexcept VFMADD213PDZr %354, %351, %353, implicit $mxcsr
  ; CHECK-NEXT:    %355:vr512 = nofpexcept VADDPDZrr %353, %354, implicit $mxcsr
  ; CHECK-NEXT:    %356:vr512 = nofpexcept VSUBPDZrr %353, %354, implicit $mxcsr
  ; CHECK-NEXT:    %357:vr512 = nofpexcept VADDPDZrr %331, %347, implicit $mxcsr
  ; CHECK-NEXT:    %358:vr512 = nofpexcept VSUBPDZrr %331, %347, implicit $mxcsr
  ; CHECK-NEXT:    %359:vr512 = VUNPCKLPDZrr %357, %358
  ; CHECK-NEXT:    %362:vr512 = VUNPCKHPDZrr %357, %358
  ; CHECK-NEXT:    PREFETCHT0 %8, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep59)
  ; CHECK-NEXT:    %361:vr512 = nofpexcept VMULPDZrr %362, %359, implicit $mxcsr
  ; CHECK-NEXT:    %362:vr512 = nofpexcept VFMADD213PDZr %362, %359, %361, implicit $mxcsr
  ; CHECK-NEXT:    %363:vr512 = nofpexcept VADDPDZrr %361, %362, implicit $mxcsr
  ; CHECK-NEXT:    %364:vr512 = nofpexcept VSUBPDZrr %361, %362, implicit $mxcsr
  ; CHECK-NEXT:    %365:vr512 = nofpexcept VADDPDZrr %339, %355, implicit $mxcsr
  ; CHECK-NEXT:    %366:vr512 = nofpexcept VSUBPDZrr %339, %355, implicit $mxcsr
  ; CHECK-NEXT:    %367:vr512 = VUNPCKLPDZrr %365, %366
  ; CHECK-NEXT:    %370:vr512 = VUNPCKHPDZrr %365, %366
  ; CHECK-NEXT:    %369:vr512 = nofpexcept VMULPDZrr %370, %367, implicit $mxcsr
  ; CHECK-NEXT:    %370:vr512 = nofpexcept VFMADD213PDZr %370, %367, %369, implicit $mxcsr
  ; CHECK-NEXT:    %371:vr512 = nofpexcept VADDPDZrr %369, %370, implicit $mxcsr
  ; CHECK-NEXT:    %372:vr512 = nofpexcept VSUBPDZrr %369, %370, implicit $mxcsr
  ; CHECK-NEXT:    %373:vr512 = nofpexcept VADDPDZrr %332, %348, implicit $mxcsr
  ; CHECK-NEXT:    %374:vr512 = nofpexcept VSUBPDZrr %332, %348, implicit $mxcsr
  ; CHECK-NEXT:    %375:vr512 = VUNPCKLPDZrr %373, %374
  ; CHECK-NEXT:    %378:vr512 = VUNPCKHPDZrr %373, %374
  ; CHECK-NEXT:    %377:vr512 = nofpexcept VMULPDZrr %378, %375, implicit $mxcsr
  ; CHECK-NEXT:    %378:vr512 = nofpexcept VFMADD213PDZr %378, %375, %377, implicit $mxcsr
  ; CHECK-NEXT:    %381:vr512 = nofpexcept VADDPDZrr %340, %356, implicit $mxcsr
  ; CHECK-NEXT:    %382:vr512 = nofpexcept VSUBPDZrr %340, %356, implicit $mxcsr
  ; CHECK-NEXT:    %383:vr512 = VUNPCKLPDZrr %381, %382
  ; CHECK-NEXT:    %386:vr512 = VUNPCKHPDZrr %381, %382
  ; CHECK-NEXT:    PREFETCHT0 %8, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep57)
  ; CHECK-NEXT:    %385:vr512 = nofpexcept VMULPDZrr %386, %383, implicit $mxcsr
  ; CHECK-NEXT:    %386:vr512 = nofpexcept VFMADD213PDZr %386, %383, %385, implicit $mxcsr
  ; CHECK-NEXT:    %379:vr512 = nofpexcept VADDPDZrr %377, %378, implicit $mxcsr
  ; CHECK-NEXT:    %380:vr512 = nofpexcept VSUBPDZrr %377, %378, implicit $mxcsr
  ; CHECK-NEXT:    %387:vr512 = nofpexcept VADDPDZrr %385, %386, implicit $mxcsr
  ; CHECK-NEXT:    %388:vr512 = nofpexcept VSUBPDZrr %385, %386, implicit $mxcsr
  ; CHECK-NEXT:    %389:vr512 = nofpexcept VADDPDZrr %363, %371, implicit $mxcsr
  ; CHECK-NEXT:    %390:vr512 = nofpexcept VSUBPDZrr %363, %371, implicit $mxcsr
  ; CHECK-NEXT:    %391:vr512 = VUNPCKLPDZrr %389, %390
  ; CHECK-NEXT:    %394:vr512 = VUNPCKHPDZrr %389, %390
  ; CHECK-NEXT:    %393:vr512 = nofpexcept VMULPDZrr %394, %391, implicit $mxcsr
  ; CHECK-NEXT:    %394:vr512 = nofpexcept VFMADD213PDZr %394, %391, %393, implicit $mxcsr
  ; CHECK-NEXT:    %398:vr512 = nofpexcept VADDPDZrr %364, %372, implicit $mxcsr
  ; CHECK-NEXT:    %399:vr512 = nofpexcept VSUBPDZrr %364, %372, implicit $mxcsr
  ; CHECK-NEXT:    %395:vr512 = nofpexcept VADDPDZrr %393, %394, implicit $mxcsr
  ; CHECK-NEXT:    %396:vr512 = nofpexcept VSUBPDZrr %393, %394, implicit $mxcsr
  ; CHECK-NEXT:    %400:vr512 = VUNPCKLPDZrr %398, %399
  ; CHECK-NEXT:    %403:vr512 = VUNPCKHPDZrr %398, %399
  ; CHECK-NEXT:    %402:vr512 = nofpexcept VMULPDZrr %403, %400, implicit $mxcsr
  ; CHECK-NEXT:    %403:vr512 = nofpexcept VFMADD213PDZr %403, %400, %402, implicit $mxcsr
  ; CHECK-NEXT:    VMOVAPDZmr %9, 1, %736, -192, $noreg, %395 :: (store (s512) into %ir.uglygep71, !tbaa !5)
  ; CHECK-NEXT:    %404:vr512 = nofpexcept VADDPDZrr %402, %403, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %1, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep7)
  ; CHECK-NEXT:    %405:vr512 = nofpexcept VSUBPDZrr %402, %403, implicit $mxcsr
  ; CHECK-NEXT:    %52:gr64_nosp = LEA64r %9, 1, %736, 0, $noreg
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %52, -192, $noreg, %396 :: (store (s512) into %ir.uglygep136, !tbaa !5)
  ; CHECK-NEXT:    %407:gr64_nosp = LEA64r %0, 1, %52, -192, $noreg
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %404 :: (store (s512) into %ir.uglygep137, !tbaa !5)
  ; CHECK-NEXT:    %408:vr512 = nofpexcept VADDPDZrr %379, %387, implicit $mxcsr
  ; CHECK-NEXT:    %409:vr512 = nofpexcept VSUBPDZrr %379, %387, implicit $mxcsr
  ; CHECK-NEXT:    %410:vr512 = VUNPCKLPDZrr %408, %409
  ; CHECK-NEXT:    %413:vr512 = VUNPCKHPDZrr %408, %409
  ; CHECK-NEXT:    %412:vr512 = nofpexcept VMULPDZrr %413, %410, implicit $mxcsr
  ; CHECK-NEXT:    %413:vr512 = nofpexcept VFMADD213PDZr %413, %410, %412, implicit $mxcsr
  ; CHECK-NEXT:    %414:vr512 = nofpexcept VADDPDZrr %412, %413, implicit $mxcsr
  ; CHECK-NEXT:    %415:vr512 = nofpexcept VSUBPDZrr %412, %413, implicit $mxcsr
  ; CHECK-NEXT:    %418:vr512 = nofpexcept VADDPDZrr %380, %388, implicit $mxcsr
  ; CHECK-NEXT:    %419:vr512 = nofpexcept VSUBPDZrr %380, %388, implicit $mxcsr
  ; CHECK-NEXT:    %420:vr512 = VUNPCKLPDZrr %418, %419
  ; CHECK-NEXT:    %423:vr512 = VUNPCKHPDZrr %418, %419
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %405 :: (store (s512) into %ir.uglygep138, !tbaa !5)
  ; CHECK-NEXT:    %422:vr512 = nofpexcept VMULPDZrr %423, %420, implicit $mxcsr
  ; CHECK-NEXT:    %423:vr512 = nofpexcept VFMADD213PDZr %423, %420, %422, implicit $mxcsr
  ; CHECK-NEXT:    %424:vr512 = nofpexcept VADDPDZrr %422, %423, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %1, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep5)
  ; CHECK-NEXT:    %425:vr512 = nofpexcept VSUBPDZrr %422, %423, implicit $mxcsr
  ; CHECK-NEXT:    %428:vr512 = nofpexcept VADDPDZrr %64, %98, implicit $mxcsr
  ; CHECK-NEXT:    %429:vr512 = nofpexcept VSUBPDZrr %64, %98, implicit $mxcsr
  ; CHECK-NEXT:    %430:vr512 = VUNPCKLPDZrr %428, %429
  ; CHECK-NEXT:    %433:vr512 = VUNPCKHPDZrr %428, %429
  ; CHECK-NEXT:    %432:vr512 = nofpexcept VMULPDZrr %433, %430, implicit $mxcsr
  ; CHECK-NEXT:    %433:vr512 = nofpexcept VFMADD213PDZr %433, %430, %432, implicit $mxcsr
  ; CHECK-NEXT:    %434:vr512 = nofpexcept VADDPDZrr %432, %433, implicit $mxcsr
  ; CHECK-NEXT:    %435:vr512 = nofpexcept VSUBPDZrr %432, %433, implicit $mxcsr
  ; CHECK-NEXT:    %436:vr512 = nofpexcept VADDPDZrr %132, %166, implicit $mxcsr
  ; CHECK-NEXT:    %437:vr512 = nofpexcept VSUBPDZrr %132, %166, implicit $mxcsr
  ; CHECK-NEXT:    %438:vr512 = VUNPCKLPDZrr %436, %437
  ; CHECK-NEXT:    %441:vr512 = VUNPCKHPDZrr %436, %437
  ; CHECK-NEXT:    %440:vr512 = nofpexcept VMULPDZrr %441, %438, implicit $mxcsr
  ; CHECK-NEXT:    %441:vr512 = nofpexcept VFMADD213PDZr %441, %438, %440, implicit $mxcsr
  ; CHECK-NEXT:    %442:vr512 = nofpexcept VADDPDZrr %440, %441, implicit $mxcsr
  ; CHECK-NEXT:    %443:vr512 = nofpexcept VSUBPDZrr %440, %441, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %414 :: (store (s512) into %ir.uglygep139, !tbaa !5)
  ; CHECK-NEXT:    %444:vr512 = nofpexcept VADDPDZrr %200, %234, implicit $mxcsr
  ; CHECK-NEXT:    %445:vr512 = nofpexcept VSUBPDZrr %200, %234, implicit $mxcsr
  ; CHECK-NEXT:    %446:vr512 = VUNPCKLPDZrr %444, %445
  ; CHECK-NEXT:    PREFETCHT0 %1, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep3)
  ; CHECK-NEXT:    %449:vr512 = VUNPCKHPDZrr %444, %445
  ; CHECK-NEXT:    %448:vr512 = nofpexcept VMULPDZrr %449, %446, implicit $mxcsr
  ; CHECK-NEXT:    %449:vr512 = nofpexcept VFMADD213PDZr %449, %446, %448, implicit $mxcsr
  ; CHECK-NEXT:    %450:vr512 = nofpexcept VADDPDZrr %448, %449, implicit $mxcsr
  ; CHECK-NEXT:    %451:vr512 = nofpexcept VSUBPDZrr %448, %449, implicit $mxcsr
  ; CHECK-NEXT:    %452:vr512 = nofpexcept VADDPDZrr %268, %302, implicit $mxcsr
  ; CHECK-NEXT:    %453:vr512 = nofpexcept VSUBPDZrr %268, %302, implicit $mxcsr
  ; CHECK-NEXT:    %454:vr512 = VUNPCKLPDZrr %452, %453
  ; CHECK-NEXT:    %457:vr512 = VUNPCKHPDZrr %452, %453
  ; CHECK-NEXT:    %456:vr512 = nofpexcept VMULPDZrr %457, %454, implicit $mxcsr
  ; CHECK-NEXT:    %457:vr512 = nofpexcept VFMADD213PDZr %457, %454, %456, implicit $mxcsr
  ; CHECK-NEXT:    %458:vr512 = nofpexcept VADDPDZrr %456, %457, implicit $mxcsr
  ; CHECK-NEXT:    %459:vr512 = nofpexcept VSUBPDZrr %456, %457, implicit $mxcsr
  ; CHECK-NEXT:    %460:vr512 = nofpexcept VADDPDZrr %434, %450, implicit $mxcsr
  ; CHECK-NEXT:    %461:vr512 = nofpexcept VSUBPDZrr %434, %450, implicit $mxcsr
  ; CHECK-NEXT:    %462:vr512 = VUNPCKLPDZrr %460, %461
  ; CHECK-NEXT:    %465:vr512 = VUNPCKHPDZrr %460, %461
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %415 :: (store (s512) into %ir.uglygep140, !tbaa !5)
  ; CHECK-NEXT:    %464:vr512 = nofpexcept VMULPDZrr %465, %462, implicit $mxcsr
  ; CHECK-NEXT:    %465:vr512 = nofpexcept VFMADD213PDZr %465, %462, %464, implicit $mxcsr
  ; CHECK-NEXT:    %466:vr512 = nofpexcept VADDPDZrr %464, %465, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %1, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep1)
  ; CHECK-NEXT:    %467:vr512 = nofpexcept VSUBPDZrr %464, %465, implicit $mxcsr
  ; CHECK-NEXT:    %468:vr512 = nofpexcept VADDPDZrr %442, %458, implicit $mxcsr
  ; CHECK-NEXT:    %469:vr512 = nofpexcept VSUBPDZrr %442, %458, implicit $mxcsr
  ; CHECK-NEXT:    %470:vr512 = VUNPCKLPDZrr %468, %469
  ; CHECK-NEXT:    %473:vr512 = VUNPCKHPDZrr %468, %469
  ; CHECK-NEXT:    %472:vr512 = nofpexcept VMULPDZrr %473, %470, implicit $mxcsr
  ; CHECK-NEXT:    %473:vr512 = nofpexcept VFMADD213PDZr %473, %470, %472, implicit $mxcsr
  ; CHECK-NEXT:    %474:vr512 = nofpexcept VADDPDZrr %472, %473, implicit $mxcsr
  ; CHECK-NEXT:    %475:vr512 = nofpexcept VSUBPDZrr %472, %473, implicit $mxcsr
  ; CHECK-NEXT:    %476:vr512 = nofpexcept VADDPDZrr %435, %451, implicit $mxcsr
  ; CHECK-NEXT:    %477:vr512 = nofpexcept VSUBPDZrr %435, %451, implicit $mxcsr
  ; CHECK-NEXT:    %478:vr512 = VUNPCKLPDZrr %476, %477
  ; CHECK-NEXT:    %481:vr512 = VUNPCKHPDZrr %476, %477
  ; CHECK-NEXT:    %480:vr512 = nofpexcept VMULPDZrr %481, %478, implicit $mxcsr
  ; CHECK-NEXT:    %481:vr512 = nofpexcept VFMADD213PDZr %481, %478, %480, implicit $mxcsr
  ; CHECK-NEXT:    %482:vr512 = nofpexcept VADDPDZrr %480, %481, implicit $mxcsr
  ; CHECK-NEXT:    %483:vr512 = nofpexcept VSUBPDZrr %480, %481, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %424 :: (store (s512) into %ir.uglygep141, !tbaa !5)
  ; CHECK-NEXT:    %484:vr512 = nofpexcept VADDPDZrr %443, %459, implicit $mxcsr
  ; CHECK-NEXT:    %485:vr512 = nofpexcept VSUBPDZrr %443, %459, implicit $mxcsr
  ; CHECK-NEXT:    %486:vr512 = VUNPCKLPDZrr %484, %485
  ; CHECK-NEXT:    PREFETCHT0 %7, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep55)
  ; CHECK-NEXT:    %489:vr512 = VUNPCKHPDZrr %484, %485
  ; CHECK-NEXT:    %488:vr512 = nofpexcept VMULPDZrr %489, %486, implicit $mxcsr
  ; CHECK-NEXT:    %489:vr512 = nofpexcept VFMADD213PDZr %489, %486, %488, implicit $mxcsr
  ; CHECK-NEXT:    %492:vr512 = nofpexcept VADDPDZrr %466, %474, implicit $mxcsr
  ; CHECK-NEXT:    %493:vr512 = nofpexcept VSUBPDZrr %466, %474, implicit $mxcsr
  ; CHECK-NEXT:    %494:vr512 = VUNPCKLPDZrr %492, %493
  ; CHECK-NEXT:    %497:vr512 = VUNPCKHPDZrr %492, %493
  ; CHECK-NEXT:    %496:vr512 = nofpexcept VMULPDZrr %497, %494, implicit $mxcsr
  ; CHECK-NEXT:    %497:vr512 = nofpexcept VFMADD213PDZr %497, %494, %496, implicit $mxcsr
  ; CHECK-NEXT:    %498:vr512 = nofpexcept VADDPDZrr %496, %497, implicit $mxcsr
  ; CHECK-NEXT:    %499:vr512 = nofpexcept VSUBPDZrr %496, %497, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %425 :: (store (s512) into %ir.uglygep142, !tbaa !5)
  ; CHECK-NEXT:    VMOVAPDZmr %9, 1, %736, -128, $noreg, %498 :: (store (s512) into %ir.uglygep69, !tbaa !5)
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %17, 1, %407, 0, $noreg, %499 :: (store (s512) into %ir.uglygep143, !tbaa !5)
  ; CHECK-NEXT:    %490:vr512 = nofpexcept VADDPDZrr %488, %489, implicit $mxcsr
  ; CHECK-NEXT:    %491:vr512 = nofpexcept VSUBPDZrr %488, %489, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
  ; CHECK-NEXT:    %501:vr512 = nofpexcept VADDPDZrr %467, %475, implicit $mxcsr
  ; CHECK-NEXT:    %502:vr512 = nofpexcept VSUBPDZrr %467, %475, implicit $mxcsr
  ; CHECK-NEXT:    %503:vr512 = VUNPCKLPDZrr %501, %502
  ; CHECK-NEXT:    PREFETCHT0 %7, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep53)
  ; CHECK-NEXT:    %506:vr512 = VUNPCKHPDZrr %501, %502
  ; CHECK-NEXT:    %505:vr512 = nofpexcept VMULPDZrr %506, %503, implicit $mxcsr
  ; CHECK-NEXT:    %506:vr512 = nofpexcept VFMADD213PDZr %506, %503, %505, implicit $mxcsr
  ; CHECK-NEXT:    %507:vr512 = nofpexcept VADDPDZrr %505, %506, implicit $mxcsr
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %507 :: (store (s512) into %ir.uglygep144, !tbaa !5)
  ; CHECK-NEXT:    %508:vr512 = nofpexcept VSUBPDZrr %505, %506, implicit $mxcsr
  ; CHECK-NEXT:    %511:vr512 = nofpexcept VADDPDZrr %482, %490, implicit $mxcsr
  ; CHECK-NEXT:    %512:vr512 = nofpexcept VSUBPDZrr %482, %490, implicit $mxcsr
  ; CHECK-NEXT:    %513:vr512 = VUNPCKLPDZrr %511, %512
  ; CHECK-NEXT:    %516:vr512 = VUNPCKHPDZrr %511, %512
  ; CHECK-NEXT:    %515:vr512 = nofpexcept VMULPDZrr %516, %513, implicit $mxcsr
  ; CHECK-NEXT:    %516:vr512 = nofpexcept VFMADD213PDZr %516, %513, %515, implicit $mxcsr
  ; CHECK-NEXT:    %517:vr512 = nofpexcept VADDPDZrr %515, %516, implicit $mxcsr
  ; CHECK-NEXT:    %518:vr512 = nofpexcept VSUBPDZrr %515, %516, implicit $mxcsr
  ; CHECK-NEXT:    %521:vr512 = nofpexcept VADDPDZrr %483, %491, implicit $mxcsr
  ; CHECK-NEXT:    %522:vr512 = nofpexcept VSUBPDZrr %483, %491, implicit $mxcsr
  ; CHECK-NEXT:    %523:vr512 = VUNPCKLPDZrr %521, %522
  ; CHECK-NEXT:    %526:vr512 = VUNPCKHPDZrr %521, %522
  ; CHECK-NEXT:    %525:vr512 = nofpexcept VMULPDZrr %526, %523, implicit $mxcsr
  ; CHECK-NEXT:    %526:vr512 = nofpexcept VFMADD213PDZr %526, %523, %525, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %508 :: (store (s512) into %ir.uglygep145, !tbaa !5)
  ; CHECK-NEXT:    PREFETCHT0 %7, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep51)
  ; CHECK-NEXT:    %527:vr512 = nofpexcept VADDPDZrr %525, %526, implicit $mxcsr
  ; CHECK-NEXT:    %528:vr512 = nofpexcept VSUBPDZrr %525, %526, implicit $mxcsr
  ; CHECK-NEXT:    %531:vr512 = nofpexcept VADDPDZrr %80, %114, implicit $mxcsr
  ; CHECK-NEXT:    %532:vr512 = nofpexcept VSUBPDZrr %80, %114, implicit $mxcsr
  ; CHECK-NEXT:    %533:vr512 = VUNPCKLPDZrr %531, %532
  ; CHECK-NEXT:    %536:vr512 = VUNPCKHPDZrr %531, %532
  ; CHECK-NEXT:    %535:vr512 = nofpexcept VMULPDZrr %536, %533, implicit $mxcsr
  ; CHECK-NEXT:    %536:vr512 = nofpexcept VFMADD213PDZr %536, %533, %535, implicit $mxcsr
  ; CHECK-NEXT:    %537:vr512 = nofpexcept VADDPDZrr %535, %536, implicit $mxcsr
  ; CHECK-NEXT:    %538:vr512 = nofpexcept VSUBPDZrr %535, %536, implicit $mxcsr
  ; CHECK-NEXT:    %539:vr512 = nofpexcept VADDPDZrr %148, %182, implicit $mxcsr
  ; CHECK-NEXT:    %540:vr512 = nofpexcept VSUBPDZrr %148, %182, implicit $mxcsr
  ; CHECK-NEXT:    %541:vr512 = VUNPCKLPDZrr %539, %540
  ; CHECK-NEXT:    %544:vr512 = VUNPCKHPDZrr %539, %540
  ; CHECK-NEXT:    %543:vr512 = nofpexcept VMULPDZrr %544, %541, implicit $mxcsr
  ; CHECK-NEXT:    %544:vr512 = nofpexcept VFMADD213PDZr %544, %541, %543, implicit $mxcsr
  ; CHECK-NEXT:    %545:vr512 = nofpexcept VADDPDZrr %543, %544, implicit $mxcsr
  ; CHECK-NEXT:    %546:vr512 = nofpexcept VSUBPDZrr %543, %544, implicit $mxcsr
  ; CHECK-NEXT:    %547:vr512 = nofpexcept VADDPDZrr %216, %250, implicit $mxcsr
  ; CHECK-NEXT:    %548:vr512 = nofpexcept VSUBPDZrr %216, %250, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %517 :: (store (s512) into %ir.uglygep146, !tbaa !5)
  ; CHECK-NEXT:    PREFETCHT0 %7, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep49)
  ; CHECK-NEXT:    %549:vr512 = VUNPCKLPDZrr %547, %548
  ; CHECK-NEXT:    %552:vr512 = VUNPCKHPDZrr %547, %548
  ; CHECK-NEXT:    %551:vr512 = nofpexcept VMULPDZrr %552, %549, implicit $mxcsr
  ; CHECK-NEXT:    %552:vr512 = nofpexcept VFMADD213PDZr %552, %549, %551, implicit $mxcsr
  ; CHECK-NEXT:    %553:vr512 = nofpexcept VADDPDZrr %551, %552, implicit $mxcsr
  ; CHECK-NEXT:    %554:vr512 = nofpexcept VSUBPDZrr %551, %552, implicit $mxcsr
  ; CHECK-NEXT:    %555:vr512 = nofpexcept VADDPDZrr %284, %318, implicit $mxcsr
  ; CHECK-NEXT:    %556:vr512 = nofpexcept VSUBPDZrr %284, %318, implicit $mxcsr
  ; CHECK-NEXT:    %557:vr512 = VUNPCKLPDZrr %555, %556
  ; CHECK-NEXT:    %560:vr512 = VUNPCKHPDZrr %555, %556
  ; CHECK-NEXT:    %559:vr512 = nofpexcept VMULPDZrr %560, %557, implicit $mxcsr
  ; CHECK-NEXT:    %560:vr512 = nofpexcept VFMADD213PDZr %560, %557, %559, implicit $mxcsr
  ; CHECK-NEXT:    %561:vr512 = nofpexcept VADDPDZrr %559, %560, implicit $mxcsr
  ; CHECK-NEXT:    %562:vr512 = nofpexcept VSUBPDZrr %559, %560, implicit $mxcsr
  ; CHECK-NEXT:    %563:vr512 = nofpexcept VADDPDZrr %537, %553, implicit $mxcsr
  ; CHECK-NEXT:    %564:vr512 = nofpexcept VSUBPDZrr %537, %553, implicit $mxcsr
  ; CHECK-NEXT:    %565:vr512 = VUNPCKLPDZrr %563, %564
  ; CHECK-NEXT:    %568:vr512 = VUNPCKHPDZrr %563, %564
  ; CHECK-NEXT:    %567:vr512 = nofpexcept VMULPDZrr %568, %565, implicit $mxcsr
  ; CHECK-NEXT:    %568:vr512 = nofpexcept VFMADD213PDZr %568, %565, %567, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %518 :: (store (s512) into %ir.uglygep147, !tbaa !5)
  ; CHECK-NEXT:    PREFETCHT0 %6, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep47)
  ; CHECK-NEXT:    %569:vr512 = nofpexcept VADDPDZrr %567, %568, implicit $mxcsr
  ; CHECK-NEXT:    %570:vr512 = nofpexcept VSUBPDZrr %567, %568, implicit $mxcsr
  ; CHECK-NEXT:    %571:vr512 = nofpexcept VADDPDZrr %545, %561, implicit $mxcsr
  ; CHECK-NEXT:    %572:vr512 = nofpexcept VSUBPDZrr %545, %561, implicit $mxcsr
  ; CHECK-NEXT:    %573:vr512 = VUNPCKLPDZrr %571, %572
  ; CHECK-NEXT:    %576:vr512 = VUNPCKHPDZrr %571, %572
  ; CHECK-NEXT:    %575:vr512 = nofpexcept VMULPDZrr %576, %573, implicit $mxcsr
  ; CHECK-NEXT:    %576:vr512 = nofpexcept VFMADD213PDZr %576, %573, %575, implicit $mxcsr
  ; CHECK-NEXT:    %577:vr512 = nofpexcept VADDPDZrr %575, %576, implicit $mxcsr
  ; CHECK-NEXT:    %578:vr512 = nofpexcept VSUBPDZrr %575, %576, implicit $mxcsr
  ; CHECK-NEXT:    %579:vr512 = nofpexcept VADDPDZrr %538, %554, implicit $mxcsr
  ; CHECK-NEXT:    %580:vr512 = nofpexcept VSUBPDZrr %538, %554, implicit $mxcsr
  ; CHECK-NEXT:    %581:vr512 = VUNPCKLPDZrr %579, %580
  ; CHECK-NEXT:    %584:vr512 = VUNPCKHPDZrr %579, %580
  ; CHECK-NEXT:    %583:vr512 = nofpexcept VMULPDZrr %584, %581, implicit $mxcsr
  ; CHECK-NEXT:    %584:vr512 = nofpexcept VFMADD213PDZr %584, %581, %583, implicit $mxcsr
  ; CHECK-NEXT:    %585:vr512 = nofpexcept VADDPDZrr %583, %584, implicit $mxcsr
  ; CHECK-NEXT:    %586:vr512 = nofpexcept VSUBPDZrr %583, %584, implicit $mxcsr
  ; CHECK-NEXT:    %587:vr512 = nofpexcept VADDPDZrr %546, %562, implicit $mxcsr
  ; CHECK-NEXT:    %588:vr512 = nofpexcept VSUBPDZrr %546, %562, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %527 :: (store (s512) into %ir.uglygep148, !tbaa !5)
  ; CHECK-NEXT:    PREFETCHT0 %6, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep45)
  ; CHECK-NEXT:    %589:vr512 = VUNPCKLPDZrr %587, %588
  ; CHECK-NEXT:    %592:vr512 = VUNPCKHPDZrr %587, %588
  ; CHECK-NEXT:    %591:vr512 = nofpexcept VMULPDZrr %592, %589, implicit $mxcsr
  ; CHECK-NEXT:    %592:vr512 = nofpexcept VFMADD213PDZr %592, %589, %591, implicit $mxcsr
  ; CHECK-NEXT:    %593:vr512 = nofpexcept VADDPDZrr %591, %592, implicit $mxcsr
  ; CHECK-NEXT:    %594:vr512 = nofpexcept VSUBPDZrr %591, %592, implicit $mxcsr
  ; CHECK-NEXT:    %595:vr512 = nofpexcept VADDPDZrr %569, %577, implicit $mxcsr
  ; CHECK-NEXT:    %596:vr512 = nofpexcept VSUBPDZrr %569, %577, implicit $mxcsr
  ; CHECK-NEXT:    %597:vr512 = VUNPCKLPDZrr %595, %596
  ; CHECK-NEXT:    %600:vr512 = VUNPCKHPDZrr %595, %596
  ; CHECK-NEXT:    %599:vr512 = nofpexcept VMULPDZrr %600, %597, implicit $mxcsr
  ; CHECK-NEXT:    %600:vr512 = nofpexcept VFMADD213PDZr %600, %597, %599, implicit $mxcsr
  ; CHECK-NEXT:    %601:vr512 = nofpexcept VADDPDZrr %599, %600, implicit $mxcsr
  ; CHECK-NEXT:    %602:vr512 = nofpexcept VSUBPDZrr %599, %600, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %528 :: (store (s512) into %ir.uglygep149, !tbaa !5)
  ; CHECK-NEXT:    %604:vr512 = nofpexcept VADDPDZrr %570, %578, implicit $mxcsr
  ; CHECK-NEXT:    %605:vr512 = nofpexcept VSUBPDZrr %570, %578, implicit $mxcsr
  ; CHECK-NEXT:    %606:vr512 = VUNPCKLPDZrr %604, %605
  ; CHECK-NEXT:    %609:vr512 = VUNPCKHPDZrr %604, %605
  ; CHECK-NEXT:    VMOVAPDZmr %9, 1, %736, -64, $noreg, %601 :: (store (s512) into %ir.uglygep67, !tbaa !5)
  ; CHECK-NEXT:    %608:vr512 = nofpexcept VMULPDZrr %609, %606, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %6, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep43)
  ; CHECK-NEXT:    %609:vr512 = nofpexcept VFMADD213PDZr %609, %606, %608, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %17, 1, %407, 0, $noreg, %602 :: (store (s512) into %ir.uglygep150, !tbaa !5)
  ; CHECK-NEXT:    %610:vr512 = nofpexcept VADDPDZrr %608, %609, implicit $mxcsr
  ; CHECK-NEXT:    %611:vr512 = nofpexcept VSUBPDZrr %608, %609, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %610 :: (store (s512) into %ir.uglygep151, !tbaa !5)
  ; CHECK-NEXT:    %614:vr512 = nofpexcept VADDPDZrr %585, %593, implicit $mxcsr
  ; CHECK-NEXT:    %615:vr512 = nofpexcept VSUBPDZrr %585, %593, implicit $mxcsr
  ; CHECK-NEXT:    %616:vr512 = VUNPCKLPDZrr %614, %615
  ; CHECK-NEXT:    %619:vr512 = VUNPCKHPDZrr %614, %615
  ; CHECK-NEXT:    %618:vr512 = nofpexcept VMULPDZrr %619, %616, implicit $mxcsr
  ; CHECK-NEXT:    %619:vr512 = nofpexcept VFMADD213PDZr %619, %616, %618, implicit $mxcsr
  ; CHECK-NEXT:    %620:vr512 = nofpexcept VADDPDZrr %618, %619, implicit $mxcsr
  ; CHECK-NEXT:    %621:vr512 = nofpexcept VSUBPDZrr %618, %619, implicit $mxcsr
  ; CHECK-NEXT:    %624:vr512 = nofpexcept VADDPDZrr %586, %594, implicit $mxcsr
  ; CHECK-NEXT:    %625:vr512 = nofpexcept VSUBPDZrr %586, %594, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %611 :: (store (s512) into %ir.uglygep152, !tbaa !5)
  ; CHECK-NEXT:    %626:vr512 = VUNPCKLPDZrr %624, %625
  ; CHECK-NEXT:    %629:vr512 = VUNPCKHPDZrr %624, %625
  ; CHECK-NEXT:    %628:vr512 = nofpexcept VMULPDZrr %629, %626, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %6, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep41)
  ; CHECK-NEXT:    %629:vr512 = nofpexcept VFMADD213PDZr %629, %626, %628, implicit $mxcsr
  ; CHECK-NEXT:    %630:vr512 = nofpexcept VADDPDZrr %628, %629, implicit $mxcsr
  ; CHECK-NEXT:    %631:vr512 = nofpexcept VSUBPDZrr %628, %629, implicit $mxcsr
  ; CHECK-NEXT:    %634:vr512 = nofpexcept VADDPDZrr %86, %120, implicit $mxcsr
  ; CHECK-NEXT:    %635:vr512 = nofpexcept VSUBPDZrr %86, %120, implicit $mxcsr
  ; CHECK-NEXT:    %636:vr512 = VUNPCKLPDZrr %634, %635
  ; CHECK-NEXT:    %639:vr512 = VUNPCKHPDZrr %634, %635
  ; CHECK-NEXT:    %638:vr512 = nofpexcept VMULPDZrr %639, %636, implicit $mxcsr
  ; CHECK-NEXT:    %639:vr512 = nofpexcept VFMADD213PDZr %639, %636, %638, implicit $mxcsr
  ; CHECK-NEXT:    %640:vr512 = nofpexcept VADDPDZrr %638, %639, implicit $mxcsr
  ; CHECK-NEXT:    %641:vr512 = nofpexcept VSUBPDZrr %638, %639, implicit $mxcsr
  ; CHECK-NEXT:    %642:vr512 = nofpexcept VADDPDZrr %154, %188, implicit $mxcsr
  ; CHECK-NEXT:    %643:vr512 = nofpexcept VSUBPDZrr %154, %188, implicit $mxcsr
  ; CHECK-NEXT:    %644:vr512 = VUNPCKLPDZrr %642, %643
  ; CHECK-NEXT:    %647:vr512 = VUNPCKHPDZrr %642, %643
  ; CHECK-NEXT:    %646:vr512 = nofpexcept VMULPDZrr %647, %644, implicit $mxcsr
  ; CHECK-NEXT:    %647:vr512 = nofpexcept VFMADD213PDZr %647, %644, %646, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %620 :: (store (s512) into %ir.uglygep153, !tbaa !5)
  ; CHECK-NEXT:    %648:vr512 = nofpexcept VADDPDZrr %646, %647, implicit $mxcsr
  ; CHECK-NEXT:    %649:vr512 = nofpexcept VSUBPDZrr %646, %647, implicit $mxcsr
  ; CHECK-NEXT:    %650:vr512 = nofpexcept VADDPDZrr %222, %256, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %5, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep39)
  ; CHECK-NEXT:    %651:vr512 = nofpexcept VSUBPDZrr %222, %256, implicit $mxcsr
  ; CHECK-NEXT:    %652:vr512 = VUNPCKLPDZrr %650, %651
  ; CHECK-NEXT:    %655:vr512 = VUNPCKHPDZrr %650, %651
  ; CHECK-NEXT:    %654:vr512 = nofpexcept VMULPDZrr %655, %652, implicit $mxcsr
  ; CHECK-NEXT:    %655:vr512 = nofpexcept VFMADD213PDZr %655, %652, %654, implicit $mxcsr
  ; CHECK-NEXT:    %656:vr512 = nofpexcept VADDPDZrr %654, %655, implicit $mxcsr
  ; CHECK-NEXT:    %657:vr512 = nofpexcept VSUBPDZrr %654, %655, implicit $mxcsr
  ; CHECK-NEXT:    %658:vr512 = nofpexcept VADDPDZrr %290, %324, implicit $mxcsr
  ; CHECK-NEXT:    %659:vr512 = nofpexcept VSUBPDZrr %290, %324, implicit $mxcsr
  ; CHECK-NEXT:    %660:vr512 = VUNPCKLPDZrr %658, %659
  ; CHECK-NEXT:    %663:vr512 = VUNPCKHPDZrr %658, %659
  ; CHECK-NEXT:    %662:vr512 = nofpexcept VMULPDZrr %663, %660, implicit $mxcsr
  ; CHECK-NEXT:    %663:vr512 = nofpexcept VFMADD213PDZr %663, %660, %662, implicit $mxcsr
  ; CHECK-NEXT:    %664:vr512 = nofpexcept VADDPDZrr %662, %663, implicit $mxcsr
  ; CHECK-NEXT:    %665:vr512 = nofpexcept VSUBPDZrr %662, %663, implicit $mxcsr
  ; CHECK-NEXT:    %666:vr512 = nofpexcept VADDPDZrr %640, %656, implicit $mxcsr
  ; CHECK-NEXT:    %667:vr512 = nofpexcept VSUBPDZrr %640, %656, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %621 :: (store (s512) into %ir.uglygep154, !tbaa !5)
  ; CHECK-NEXT:    %668:vr512 = VUNPCKLPDZrr %666, %667
  ; CHECK-NEXT:    %671:vr512 = VUNPCKHPDZrr %666, %667
  ; CHECK-NEXT:    %670:vr512 = nofpexcept VMULPDZrr %671, %668, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %5, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep37)
  ; CHECK-NEXT:    %671:vr512 = nofpexcept VFMADD213PDZr %671, %668, %670, implicit $mxcsr
  ; CHECK-NEXT:    %672:vr512 = nofpexcept VADDPDZrr %670, %671, implicit $mxcsr
  ; CHECK-NEXT:    %673:vr512 = nofpexcept VSUBPDZrr %670, %671, implicit $mxcsr
  ; CHECK-NEXT:    %674:vr512 = nofpexcept VADDPDZrr %648, %664, implicit $mxcsr
  ; CHECK-NEXT:    %675:vr512 = nofpexcept VSUBPDZrr %648, %664, implicit $mxcsr
  ; CHECK-NEXT:    %676:vr512 = VUNPCKLPDZrr %674, %675
  ; CHECK-NEXT:    %679:vr512 = VUNPCKHPDZrr %674, %675
  ; CHECK-NEXT:    %678:vr512 = nofpexcept VMULPDZrr %679, %676, implicit $mxcsr
  ; CHECK-NEXT:    %679:vr512 = nofpexcept VFMADD213PDZr %679, %676, %678, implicit $mxcsr
  ; CHECK-NEXT:    %680:vr512 = nofpexcept VADDPDZrr %678, %679, implicit $mxcsr
  ; CHECK-NEXT:    %681:vr512 = nofpexcept VSUBPDZrr %678, %679, implicit $mxcsr
  ; CHECK-NEXT:    %682:vr512 = nofpexcept VADDPDZrr %641, %657, implicit $mxcsr
  ; CHECK-NEXT:    %683:vr512 = nofpexcept VSUBPDZrr %641, %657, implicit $mxcsr
  ; CHECK-NEXT:    %684:vr512 = VUNPCKLPDZrr %682, %683
  ; CHECK-NEXT:    %687:vr512 = VUNPCKHPDZrr %682, %683
  ; CHECK-NEXT:    %686:vr512 = nofpexcept VMULPDZrr %687, %684, implicit $mxcsr
  ; CHECK-NEXT:    %687:vr512 = nofpexcept VFMADD213PDZr %687, %684, %686, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %630 :: (store (s512) into %ir.uglygep155, !tbaa !5)
  ; CHECK-NEXT:    %688:vr512 = nofpexcept VADDPDZrr %686, %687, implicit $mxcsr
  ; CHECK-NEXT:    %689:vr512 = nofpexcept VSUBPDZrr %686, %687, implicit $mxcsr
  ; CHECK-NEXT:    %690:vr512 = nofpexcept VADDPDZrr %649, %665, implicit $mxcsr
  ; CHECK-NEXT:    PREFETCHT0 %5, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep35)
  ; CHECK-NEXT:    %691:vr512 = nofpexcept VSUBPDZrr %649, %665, implicit $mxcsr
  ; CHECK-NEXT:    %692:vr512 = VUNPCKLPDZrr %690, %691
  ; CHECK-NEXT:    %695:vr512 = VUNPCKHPDZrr %690, %691
  ; CHECK-NEXT:    %694:vr512 = nofpexcept VMULPDZrr %695, %692, implicit $mxcsr
  ; CHECK-NEXT:    %695:vr512 = nofpexcept VFMADD213PDZr %695, %692, %694, implicit $mxcsr
  ; CHECK-NEXT:    %696:vr512 = nofpexcept VADDPDZrr %694, %695, implicit $mxcsr
  ; CHECK-NEXT:    %697:vr512 = nofpexcept VSUBPDZrr %694, %695, implicit $mxcsr
  ; CHECK-NEXT:    %698:vr512 = nofpexcept VADDPDZrr %672, %680, implicit $mxcsr
  ; CHECK-NEXT:    %699:vr512 = nofpexcept VSUBPDZrr %672, %680, implicit $mxcsr
  ; CHECK-NEXT:    %700:vr512 = VUNPCKLPDZrr %698, %699
  ; CHECK-NEXT:    %703:vr512 = VUNPCKHPDZrr %698, %699
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %631 :: (store (s512) into %ir.uglygep156, !tbaa !5)
  ; CHECK-NEXT:    %702:vr512 = nofpexcept VMULPDZrr %703, %700, implicit $mxcsr
  ; CHECK-NEXT:    %703:vr512 = nofpexcept VFMADD213PDZr %703, %700, %702, implicit $mxcsr
  ; CHECK-NEXT:    %704:vr512 = nofpexcept VADDPDZrr %702, %703, implicit $mxcsr
  ; CHECK-NEXT:    %705:vr512 = nofpexcept VSUBPDZrr %702, %703, implicit $mxcsr
  ; CHECK-NEXT:    VMOVAPDZmr %9, 1, %736, 0, $noreg, %704 :: (store (s512) into %ir.uglygep65, !tbaa !5)
  ; CHECK-NEXT:    %707:vr512 = nofpexcept VADDPDZrr %673, %681, implicit $mxcsr
  ; CHECK-NEXT:    %708:vr512 = nofpexcept VSUBPDZrr %673, %681, implicit $mxcsr
  ; CHECK-NEXT:    %709:vr512 = VUNPCKLPDZrr %707, %708
  ; CHECK-NEXT:    %712:vr512 = VUNPCKHPDZrr %707, %708
  ; CHECK-NEXT:    PREFETCHT0 %5, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep33)
  ; CHECK-NEXT:    %711:vr512 = nofpexcept VMULPDZrr %712, %709, implicit $mxcsr
  ; CHECK-NEXT:    %712:vr512 = nofpexcept VFMADD213PDZr %712, %709, %711, implicit $mxcsr
  ; CHECK-NEXT:    %713:vr512 = nofpexcept VADDPDZrr %711, %712, implicit $mxcsr
  ; CHECK-NEXT:    %714:vr512 = nofpexcept VSUBPDZrr %711, %712, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %17, 1, %407, 0, $noreg, %705 :: (store (s512) into %ir.uglygep157, !tbaa !5)
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %713 :: (store (s512) into %ir.uglygep158, !tbaa !5)
  ; CHECK-NEXT:    %717:vr512 = nofpexcept VADDPDZrr %688, %696, implicit $mxcsr
  ; CHECK-NEXT:    %718:vr512 = nofpexcept VSUBPDZrr %688, %696, implicit $mxcsr
  ; CHECK-NEXT:    %719:vr512 = VUNPCKLPDZrr %717, %718
  ; CHECK-NEXT:    %722:vr512 = VUNPCKHPDZrr %717, %718
  ; CHECK-NEXT:    %721:vr512 = nofpexcept VMULPDZrr %722, %719, implicit $mxcsr
  ; CHECK-NEXT:    %722:vr512 = nofpexcept VFMADD213PDZr %722, %719, %721, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %714 :: (store (s512) into %ir.uglygep159, !tbaa !5)
  ; CHECK-NEXT:    %723:vr512 = nofpexcept VADDPDZrr %721, %722, implicit $mxcsr
  ; CHECK-NEXT:    %724:vr512 = nofpexcept VSUBPDZrr %721, %722, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %723 :: (store (s512) into %ir.uglygep160, !tbaa !5)
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %724 :: (store (s512) into %ir.uglygep161, !tbaa !5)
  ; CHECK-NEXT:    %727:vr512 = nofpexcept VADDPDZrr %689, %697, implicit $mxcsr
  ; CHECK-NEXT:    %728:vr512 = nofpexcept VSUBPDZrr %689, %697, implicit $mxcsr
  ; CHECK-NEXT:    %729:vr512 = VUNPCKLPDZrr %727, %728
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    %732:vr512 = VUNPCKHPDZrr %727, %728
  ; CHECK-NEXT:    %731:vr512 = nofpexcept VMULPDZrr %732, %729, implicit $mxcsr
  ; CHECK-NEXT:    %732:vr512 = nofpexcept VFMADD213PDZr %732, %729, %731, implicit $mxcsr
  ; CHECK-NEXT:    %733:vr512 = nofpexcept VADDPDZrr %731, %732, implicit $mxcsr
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %733 :: (store (s512) into %ir.uglygep162, !tbaa !5)
  ; CHECK-NEXT:    %734:vr512 = nofpexcept VSUBPDZrr %731, %732, implicit $mxcsr
  ; CHECK-NEXT:    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
  ; CHECK-NEXT:    VMOVAPDZmr %0, 1, %407, 0, $noreg, %734 :: (store (s512) into %ir.uglygep163, !tbaa !5)
  ; CHECK-NEXT:    %736:gr64_nosp = ADD64ri8 %736, 64, implicit-def dead $eflags
  ; CHECK-NEXT:    %737:gr64 = DEC64r %737, implicit-def $eflags
  ; CHECK-NEXT:    JCC_1 %bb.1, 5, implicit $eflags
  ; CHECK-NEXT:    JMP_1 %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:  bb.2.do.end:
  ; CHECK-NEXT:    RET 0
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $rdi, $rsi, $rdx, $rcx, $r8, $r9

    %27:gr64_nosp = COPY $r9
    %26:gr64 = COPY $r8
    %25:gr64_nosp = COPY $rcx
    %24:gr64_nosp = COPY $rdx
    %48:gr64_nosp = COPY $rsi
    %9:gr64 = COPY $rdi
    %737:gr64 = MOV64rm %fixed-stack.0, 1, $noreg, 0, $noreg :: (load (s64) from %fixed-stack.0, align 16)
    %30:gr64_nosp = COPY %25
    %30:gr64_nosp = SHL64ri %30, 6, implicit-def dead $eflags
    %0:gr64 = COPY %48
    %0:gr64 = SHL64ri %0, 6, implicit-def dead $eflags
    %31:gr64_nosp = COPY %27
    %31:gr64_nosp = SHL64ri %31, 6, implicit-def dead $eflags
    %32:gr64_nosp = COPY %25
    %32:gr64_nosp = SHL64ri %32, 8, implicit-def dead $eflags
    %33:gr64_nosp = COPY %27
    %33:gr64_nosp = SHL64ri %33, 8, implicit-def dead $eflags
    %34:gr64_nosp = COPY %25
    %34:gr64_nosp = SHL64ri %34, 7, implicit-def dead $eflags
    %35:gr64_nosp = COPY %27
    %35:gr64_nosp = SHL64ri %35, 7, implicit-def dead $eflags
    %1:gr64 = LEA64r %26, 1, %35, 192, $noreg
    %2:gr64 = LEA64r %26, 1, %31, 192, $noreg
    %3:gr64 = LEA64r %26, 1, %33, 192, $noreg
    %4:gr64 = LEA64r %26, 1, $noreg, 192, $noreg
    %36:gr64_nosp = IMUL64rri32 %27, 448, implicit-def dead $eflags
    %5:gr64 = LEA64r %26, 1, %36, 192, $noreg
    %39:gr64_nosp = LEA64r %27, 2, %27, 0, $noreg
    %38:gr64_nosp = COPY %39
    %38:gr64_nosp = SHL64ri %38, 6, implicit-def dead $eflags
    %6:gr64 = LEA64r %26, 1, %38, 192, $noreg
    %39:gr64_nosp = SHL64ri %39, 7, implicit-def dead $eflags
    %7:gr64 = LEA64r %26, 1, %39, 192, $noreg
    %41:gr64_nosp = LEA64r %27, 4, %27, 0, $noreg
    %41:gr64_nosp = SHL64ri %41, 6, implicit-def dead $eflags
    %8:gr64 = LEA64r %26, 1, %41, 192, $noreg
    %9:gr64 = ADD64ri32 %9, 192, implicit-def dead $eflags
    %10:gr64 = LEA64r %34, 1, %24, 192, $noreg
    %11:gr64 = LEA64r %30, 1, %24, 192, $noreg
    %12:gr64 = LEA64r %32, 1, %24, 192, $noreg
    %42:gr64 = IMUL64rri32 %25, 448, implicit-def dead $eflags
    %13:gr64 = LEA64r %42, 1, %24, 192, $noreg
    %45:gr64 = LEA64r %25, 2, %25, 0, $noreg
    %44:gr64 = COPY %45
    %44:gr64 = SHL64ri %44, 6, implicit-def dead $eflags
    %14:gr64 = LEA64r %44, 1, %24, 192, $noreg
    %45:gr64 = SHL64ri %45, 7, implicit-def dead $eflags
    %15:gr64 = LEA64r %45, 1, %24, 192, $noreg
    %47:gr64 = LEA64r %25, 4, %25, 0, $noreg
    %47:gr64 = SHL64ri %47, 6, implicit-def dead $eflags
    %16:gr64 = LEA64r %47, 1, %24, 192, $noreg
    %48:gr64_nosp = SHL64ri %48, 7, implicit-def dead $eflags
    %49:gr64 = LEA64r %48, 2, %48, 0, $noreg
    %17:gr64 = MOV32ri64 64
    %17:gr64 = SUB64rr %17, %49, implicit-def dead $eflags
    undef %736.sub_32bit:gr64_nosp = MOV32r0 implicit-def dead $eflags

  bb.1.do.body:
    successors: %bb.2(0x04000000), %bb.1(0x7c000000)

    %52:gr64_nosp = LEA64r %9, 1, %736, 0, $noreg
    %53:vr512 = VMOVAPDZrm %24, 1, %736, 0, $noreg :: (load (s512) from %ir.sunkaddr, !tbaa !5)
    %54:vr512 = VMOVAPDZrm %24, 1, %736, 64, $noreg :: (load (s512) from %ir.sunkaddr166, !tbaa !5)
    %55:vr512 = VMOVAPDZrm %24, 1, %736, 128, $noreg :: (load (s512) from %ir.sunkaddr168, !tbaa !5)
    %56:vr512 = VMOVAPDZrm %24, 1, %736, 192, $noreg :: (load (s512) from %ir.sunkaddr170, !tbaa !5)
    %57:vr512 = nofpexcept VADDPDZrr %53, %54, implicit $mxcsr
    %58:vr512 = nofpexcept VSUBPDZrr %53, %54, implicit $mxcsr
    %59:vr512 = VUNPCKLPDZrr %57, %58
    %62:vr512 = VUNPCKHPDZrr %57, %58
    %61:vr512 = nofpexcept VMULPDZrr %62, %59, implicit $mxcsr
    %62:vr512 = nofpexcept VFMADD213PDZr %62, %59, %61, implicit $mxcsr
    %63:vr512 = nofpexcept VADDPDZrr %61, %62, implicit $mxcsr
    %64:vr512 = nofpexcept VSUBPDZrr %61, %62, implicit $mxcsr
    %65:vr512 = nofpexcept VADDPDZrr %55, %56, implicit $mxcsr
    %66:vr512 = nofpexcept VSUBPDZrr %55, %56, implicit $mxcsr
    %67:vr512 = VUNPCKLPDZrr %65, %66
    %70:vr512 = VUNPCKHPDZrr %65, %66
    %69:vr512 = nofpexcept VMULPDZrr %70, %67, implicit $mxcsr
    %70:vr512 = nofpexcept VFMADD213PDZr %70, %67, %69, implicit $mxcsr
    %71:vr512 = nofpexcept VADDPDZrr %69, %70, implicit $mxcsr
    %72:vr512 = nofpexcept VSUBPDZrr %69, %70, implicit $mxcsr
    %73:vr512 = nofpexcept VADDPDZrr %63, %71, implicit $mxcsr
    %74:vr512 = nofpexcept VSUBPDZrr %63, %71, implicit $mxcsr
    %75:vr512 = VUNPCKLPDZrr %73, %74
    %78:vr512 = VUNPCKHPDZrr %73, %74
    %77:vr512 = nofpexcept VMULPDZrr %78, %75, implicit $mxcsr
    %78:vr512 = nofpexcept VFMADD213PDZr %78, %75, %77, implicit $mxcsr
    %79:vr512 = nofpexcept VADDPDZrr %77, %78, implicit $mxcsr
    %80:vr512 = nofpexcept VSUBPDZrr %77, %78, implicit $mxcsr
    %81:vr512 = nofpexcept VADDPDZrr %72, %72, implicit $mxcsr
    %82:vr512 = nofpexcept VSUBPDZrr %72, %72, implicit $mxcsr
    %85:vr512 = VUNPCKHPDZrr %81, %82
    %84:vr512 = nofpexcept VMULPDZrr %85, %85, implicit $mxcsr
    %85:vr512 = nofpexcept VFMADD213PDZr %85, %85, %84, implicit $mxcsr
    %86:vr512 = nofpexcept VSUBPDZrr %84, %85, implicit $mxcsr
    %87:vr512 = VMOVAPDZrm %12, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep95, !tbaa !5)
    %88:vr512 = VMOVAPDZrm %12, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep93, !tbaa !5)
    %89:vr512 = VMOVAPDZrm %12, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep91, !tbaa !5)
    %90:vr512 = VMOVAPDZrm %12, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep89, !tbaa !5)
    %91:vr512 = nofpexcept VADDPDZrr %87, %88, implicit $mxcsr
    %92:vr512 = nofpexcept VSUBPDZrr %87, %88, implicit $mxcsr
    %93:vr512 = VUNPCKLPDZrr %91, %92
    %96:vr512 = VUNPCKHPDZrr %91, %92
    %95:vr512 = nofpexcept VMULPDZrr %96, %93, implicit $mxcsr
    %96:vr512 = nofpexcept VFMADD213PDZr %96, %93, %95, implicit $mxcsr
    %97:vr512 = nofpexcept VADDPDZrr %95, %96, implicit $mxcsr
    %98:vr512 = nofpexcept VSUBPDZrr %95, %96, implicit $mxcsr
    %99:vr512 = nofpexcept VADDPDZrr %89, %90, implicit $mxcsr
    %100:vr512 = nofpexcept VSUBPDZrr %89, %90, implicit $mxcsr
    %101:vr512 = VUNPCKLPDZrr %99, %100
    %104:vr512 = VUNPCKHPDZrr %99, %100
    %103:vr512 = nofpexcept VMULPDZrr %104, %101, implicit $mxcsr
    %104:vr512 = nofpexcept VFMADD213PDZr %104, %101, %103, implicit $mxcsr
    %105:vr512 = nofpexcept VADDPDZrr %103, %104, implicit $mxcsr
    %106:vr512 = nofpexcept VSUBPDZrr %103, %104, implicit $mxcsr
    %107:vr512 = nofpexcept VADDPDZrr %97, %105, implicit $mxcsr
    %108:vr512 = nofpexcept VSUBPDZrr %97, %105, implicit $mxcsr
    %109:vr512 = VUNPCKLPDZrr %107, %108
    %112:vr512 = VUNPCKHPDZrr %107, %108
    %111:vr512 = nofpexcept VMULPDZrr %112, %109, implicit $mxcsr
    %112:vr512 = nofpexcept VFMADD213PDZr %112, %109, %111, implicit $mxcsr
    %113:vr512 = nofpexcept VADDPDZrr %111, %112, implicit $mxcsr
    %114:vr512 = nofpexcept VSUBPDZrr %111, %112, implicit $mxcsr
    %115:vr512 = nofpexcept VADDPDZrr %106, %106, implicit $mxcsr
    %116:vr512 = nofpexcept VSUBPDZrr %106, %106, implicit $mxcsr
    %119:vr512 = VUNPCKHPDZrr %115, %116
    %118:vr512 = nofpexcept VMULPDZrr %119, %119, implicit $mxcsr
    %119:vr512 = nofpexcept VFMADD213PDZr %119, %119, %118, implicit $mxcsr
    %120:vr512 = nofpexcept VSUBPDZrr %118, %119, implicit $mxcsr
    PREFETCHT0 %4, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep31)
    PREFETCHT0 %4, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep29)
    PREFETCHT0 %4, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep27)
    PREFETCHT0 %4, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep25)
    %121:vr512 = VMOVAPDZrm %11, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep87, !tbaa !5)
    %122:vr512 = VMOVAPDZrm %11, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep85, !tbaa !5)
    %123:vr512 = VMOVAPDZrm %11, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep83, !tbaa !5)
    %124:vr512 = VMOVAPDZrm %11, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep81, !tbaa !5)
    %125:vr512 = nofpexcept VADDPDZrr %121, %122, implicit $mxcsr
    %126:vr512 = nofpexcept VSUBPDZrr %121, %122, implicit $mxcsr
    %127:vr512 = VUNPCKLPDZrr %125, %126
    %130:vr512 = VUNPCKHPDZrr %125, %126
    %129:vr512 = nofpexcept VMULPDZrr %130, %127, implicit $mxcsr
    %130:vr512 = nofpexcept VFMADD213PDZr %130, %127, %129, implicit $mxcsr
    %131:vr512 = nofpexcept VADDPDZrr %129, %130, implicit $mxcsr
    %132:vr512 = nofpexcept VSUBPDZrr %129, %130, implicit $mxcsr
    %133:vr512 = nofpexcept VADDPDZrr %123, %124, implicit $mxcsr
    %134:vr512 = nofpexcept VSUBPDZrr %123, %124, implicit $mxcsr
    %135:vr512 = VUNPCKLPDZrr %133, %134
    %138:vr512 = VUNPCKHPDZrr %133, %134
    %137:vr512 = nofpexcept VMULPDZrr %138, %135, implicit $mxcsr
    %138:vr512 = nofpexcept VFMADD213PDZr %138, %135, %137, implicit $mxcsr
    %139:vr512 = nofpexcept VADDPDZrr %137, %138, implicit $mxcsr
    %140:vr512 = nofpexcept VSUBPDZrr %137, %138, implicit $mxcsr
    %141:vr512 = nofpexcept VADDPDZrr %131, %139, implicit $mxcsr
    %142:vr512 = nofpexcept VSUBPDZrr %131, %139, implicit $mxcsr
    %143:vr512 = VUNPCKLPDZrr %141, %142
    %146:vr512 = VUNPCKHPDZrr %141, %142
    %145:vr512 = nofpexcept VMULPDZrr %146, %143, implicit $mxcsr
    %146:vr512 = nofpexcept VFMADD213PDZr %146, %143, %145, implicit $mxcsr
    %147:vr512 = nofpexcept VADDPDZrr %145, %146, implicit $mxcsr
    %148:vr512 = nofpexcept VSUBPDZrr %145, %146, implicit $mxcsr
    %149:vr512 = nofpexcept VADDPDZrr %140, %140, implicit $mxcsr
    %150:vr512 = nofpexcept VSUBPDZrr %140, %140, implicit $mxcsr
    %153:vr512 = VUNPCKHPDZrr %149, %150
    %152:vr512 = nofpexcept VMULPDZrr %153, %153, implicit $mxcsr
    %153:vr512 = nofpexcept VFMADD213PDZr %153, %153, %152, implicit $mxcsr
    %154:vr512 = nofpexcept VSUBPDZrr %152, %153, implicit $mxcsr
    %155:vr512 = VMOVAPDZrm %16, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep135, !tbaa !5)
    %156:vr512 = VMOVAPDZrm %16, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep133, !tbaa !5)
    %157:vr512 = VMOVAPDZrm %16, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep131, !tbaa !5)
    %158:vr512 = VMOVAPDZrm %16, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep129, !tbaa !5)
    %159:vr512 = nofpexcept VADDPDZrr %155, %156, implicit $mxcsr
    %160:vr512 = nofpexcept VSUBPDZrr %155, %156, implicit $mxcsr
    %161:vr512 = VUNPCKLPDZrr %159, %160
    %164:vr512 = VUNPCKHPDZrr %159, %160
    %163:vr512 = nofpexcept VMULPDZrr %164, %161, implicit $mxcsr
    %164:vr512 = nofpexcept VFMADD213PDZr %164, %161, %163, implicit $mxcsr
    %165:vr512 = nofpexcept VADDPDZrr %163, %164, implicit $mxcsr
    %166:vr512 = nofpexcept VSUBPDZrr %163, %164, implicit $mxcsr
    %167:vr512 = nofpexcept VADDPDZrr %157, %158, implicit $mxcsr
    %168:vr512 = nofpexcept VSUBPDZrr %157, %158, implicit $mxcsr
    %169:vr512 = VUNPCKLPDZrr %167, %168
    %172:vr512 = VUNPCKHPDZrr %167, %168
    %171:vr512 = nofpexcept VMULPDZrr %172, %169, implicit $mxcsr
    %172:vr512 = nofpexcept VFMADD213PDZr %172, %169, %171, implicit $mxcsr
    %173:vr512 = nofpexcept VADDPDZrr %171, %172, implicit $mxcsr
    %174:vr512 = nofpexcept VSUBPDZrr %171, %172, implicit $mxcsr
    %175:vr512 = nofpexcept VADDPDZrr %165, %173, implicit $mxcsr
    %176:vr512 = nofpexcept VSUBPDZrr %165, %173, implicit $mxcsr
    %177:vr512 = VUNPCKLPDZrr %175, %176
    %180:vr512 = VUNPCKHPDZrr %175, %176
    %179:vr512 = nofpexcept VMULPDZrr %180, %177, implicit $mxcsr
    %180:vr512 = nofpexcept VFMADD213PDZr %180, %177, %179, implicit $mxcsr
    %181:vr512 = nofpexcept VADDPDZrr %179, %180, implicit $mxcsr
    %182:vr512 = nofpexcept VSUBPDZrr %179, %180, implicit $mxcsr
    %183:vr512 = nofpexcept VADDPDZrr %174, %174, implicit $mxcsr
    %184:vr512 = nofpexcept VSUBPDZrr %174, %174, implicit $mxcsr
    %187:vr512 = VUNPCKHPDZrr %183, %184
    %186:vr512 = nofpexcept VMULPDZrr %187, %187, implicit $mxcsr
    %187:vr512 = nofpexcept VFMADD213PDZr %187, %187, %186, implicit $mxcsr
    %188:vr512 = nofpexcept VSUBPDZrr %186, %187, implicit $mxcsr
    PREFETCHT0 %3, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep23)
    PREFETCHT0 %3, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep21)
    PREFETCHT0 %3, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep19)
    PREFETCHT0 %3, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep17)
    %189:vr512 = VMOVAPDZrm %10, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep79, !tbaa !5)
    %190:vr512 = VMOVAPDZrm %10, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep77, !tbaa !5)
    %191:vr512 = VMOVAPDZrm %10, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep75, !tbaa !5)
    %192:vr512 = VMOVAPDZrm %10, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep73, !tbaa !5)
    %193:vr512 = nofpexcept VADDPDZrr %189, %190, implicit $mxcsr
    %194:vr512 = nofpexcept VSUBPDZrr %189, %190, implicit $mxcsr
    %195:vr512 = VUNPCKLPDZrr %193, %194
    %198:vr512 = VUNPCKHPDZrr %193, %194
    %197:vr512 = nofpexcept VMULPDZrr %198, %195, implicit $mxcsr
    %198:vr512 = nofpexcept VFMADD213PDZr %198, %195, %197, implicit $mxcsr
    %199:vr512 = nofpexcept VADDPDZrr %197, %198, implicit $mxcsr
    %200:vr512 = nofpexcept VSUBPDZrr %197, %198, implicit $mxcsr
    %201:vr512 = nofpexcept VADDPDZrr %191, %192, implicit $mxcsr
    %202:vr512 = nofpexcept VSUBPDZrr %191, %192, implicit $mxcsr
    %203:vr512 = VUNPCKLPDZrr %201, %202
    %206:vr512 = VUNPCKHPDZrr %201, %202
    %205:vr512 = nofpexcept VMULPDZrr %206, %203, implicit $mxcsr
    %206:vr512 = nofpexcept VFMADD213PDZr %206, %203, %205, implicit $mxcsr
    %207:vr512 = nofpexcept VADDPDZrr %205, %206, implicit $mxcsr
    %208:vr512 = nofpexcept VSUBPDZrr %205, %206, implicit $mxcsr
    %209:vr512 = nofpexcept VADDPDZrr %199, %207, implicit $mxcsr
    %210:vr512 = nofpexcept VSUBPDZrr %199, %207, implicit $mxcsr
    %211:vr512 = VUNPCKLPDZrr %209, %210
    %214:vr512 = VUNPCKHPDZrr %209, %210
    %213:vr512 = nofpexcept VMULPDZrr %214, %211, implicit $mxcsr
    %214:vr512 = nofpexcept VFMADD213PDZr %214, %211, %213, implicit $mxcsr
    %215:vr512 = nofpexcept VADDPDZrr %213, %214, implicit $mxcsr
    %216:vr512 = nofpexcept VSUBPDZrr %213, %214, implicit $mxcsr
    %217:vr512 = nofpexcept VADDPDZrr %208, %208, implicit $mxcsr
    %218:vr512 = nofpexcept VSUBPDZrr %208, %208, implicit $mxcsr
    %221:vr512 = VUNPCKHPDZrr %217, %218
    %220:vr512 = nofpexcept VMULPDZrr %221, %221, implicit $mxcsr
    %221:vr512 = nofpexcept VFMADD213PDZr %221, %221, %220, implicit $mxcsr
    %222:vr512 = nofpexcept VSUBPDZrr %220, %221, implicit $mxcsr
    %223:vr512 = VMOVAPDZrm %15, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep127, !tbaa !5)
    %224:vr512 = VMOVAPDZrm %15, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep125, !tbaa !5)
    %225:vr512 = VMOVAPDZrm %15, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep123, !tbaa !5)
    %226:vr512 = VMOVAPDZrm %15, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep121, !tbaa !5)
    %227:vr512 = nofpexcept VADDPDZrr %223, %224, implicit $mxcsr
    %228:vr512 = nofpexcept VSUBPDZrr %223, %224, implicit $mxcsr
    %229:vr512 = VUNPCKLPDZrr %227, %228
    %232:vr512 = VUNPCKHPDZrr %227, %228
    %231:vr512 = nofpexcept VMULPDZrr %232, %229, implicit $mxcsr
    %232:vr512 = nofpexcept VFMADD213PDZr %232, %229, %231, implicit $mxcsr
    %233:vr512 = nofpexcept VADDPDZrr %231, %232, implicit $mxcsr
    %234:vr512 = nofpexcept VSUBPDZrr %231, %232, implicit $mxcsr
    %235:vr512 = nofpexcept VADDPDZrr %225, %226, implicit $mxcsr
    %236:vr512 = nofpexcept VSUBPDZrr %225, %226, implicit $mxcsr
    %237:vr512 = VUNPCKLPDZrr %235, %236
    %240:vr512 = VUNPCKHPDZrr %235, %236
    %239:vr512 = nofpexcept VMULPDZrr %240, %237, implicit $mxcsr
    %240:vr512 = nofpexcept VFMADD213PDZr %240, %237, %239, implicit $mxcsr
    %241:vr512 = nofpexcept VADDPDZrr %239, %240, implicit $mxcsr
    %242:vr512 = nofpexcept VSUBPDZrr %239, %240, implicit $mxcsr
    %243:vr512 = nofpexcept VADDPDZrr %233, %241, implicit $mxcsr
    %244:vr512 = nofpexcept VSUBPDZrr %233, %241, implicit $mxcsr
    %245:vr512 = VUNPCKLPDZrr %243, %244
    %248:vr512 = VUNPCKHPDZrr %243, %244
    %247:vr512 = nofpexcept VMULPDZrr %248, %245, implicit $mxcsr
    %248:vr512 = nofpexcept VFMADD213PDZr %248, %245, %247, implicit $mxcsr
    %249:vr512 = nofpexcept VADDPDZrr %247, %248, implicit $mxcsr
    %250:vr512 = nofpexcept VSUBPDZrr %247, %248, implicit $mxcsr
    %251:vr512 = nofpexcept VADDPDZrr %242, %242, implicit $mxcsr
    %252:vr512 = nofpexcept VSUBPDZrr %242, %242, implicit $mxcsr
    %255:vr512 = VUNPCKHPDZrr %251, %252
    %254:vr512 = nofpexcept VMULPDZrr %255, %255, implicit $mxcsr
    %255:vr512 = nofpexcept VFMADD213PDZr %255, %255, %254, implicit $mxcsr
    %256:vr512 = nofpexcept VSUBPDZrr %254, %255, implicit $mxcsr
    PREFETCHT0 %2, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep15)
    PREFETCHT0 %2, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep13)
    PREFETCHT0 %2, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep11)
    PREFETCHT0 %2, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep9)
    %257:vr512 = VMOVAPDZrm %14, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep119, !tbaa !5)
    %258:vr512 = VMOVAPDZrm %14, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep117, !tbaa !5)
    %259:vr512 = VMOVAPDZrm %14, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep115, !tbaa !5)
    %260:vr512 = VMOVAPDZrm %14, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep113, !tbaa !5)
    %261:vr512 = nofpexcept VADDPDZrr %257, %258, implicit $mxcsr
    %262:vr512 = nofpexcept VSUBPDZrr %257, %258, implicit $mxcsr
    %263:vr512 = VUNPCKLPDZrr %261, %262
    %266:vr512 = VUNPCKHPDZrr %261, %262
    %265:vr512 = nofpexcept VMULPDZrr %266, %263, implicit $mxcsr
    %266:vr512 = nofpexcept VFMADD213PDZr %266, %263, %265, implicit $mxcsr
    %267:vr512 = nofpexcept VADDPDZrr %265, %266, implicit $mxcsr
    %268:vr512 = nofpexcept VSUBPDZrr %265, %266, implicit $mxcsr
    %269:vr512 = nofpexcept VADDPDZrr %259, %260, implicit $mxcsr
    %270:vr512 = nofpexcept VSUBPDZrr %259, %260, implicit $mxcsr
    %271:vr512 = VUNPCKLPDZrr %269, %270
    %274:vr512 = VUNPCKHPDZrr %269, %270
    %273:vr512 = nofpexcept VMULPDZrr %274, %271, implicit $mxcsr
    %274:vr512 = nofpexcept VFMADD213PDZr %274, %271, %273, implicit $mxcsr
    %275:vr512 = nofpexcept VADDPDZrr %273, %274, implicit $mxcsr
    %276:vr512 = nofpexcept VSUBPDZrr %273, %274, implicit $mxcsr
    %277:vr512 = nofpexcept VADDPDZrr %267, %275, implicit $mxcsr
    %278:vr512 = nofpexcept VSUBPDZrr %267, %275, implicit $mxcsr
    %279:vr512 = VUNPCKLPDZrr %277, %278
    %282:vr512 = VUNPCKHPDZrr %277, %278
    %281:vr512 = nofpexcept VMULPDZrr %282, %279, implicit $mxcsr
    %282:vr512 = nofpexcept VFMADD213PDZr %282, %279, %281, implicit $mxcsr
    %283:vr512 = nofpexcept VADDPDZrr %281, %282, implicit $mxcsr
    %284:vr512 = nofpexcept VSUBPDZrr %281, %282, implicit $mxcsr
    %285:vr512 = nofpexcept VADDPDZrr %276, %276, implicit $mxcsr
    %286:vr512 = nofpexcept VSUBPDZrr %276, %276, implicit $mxcsr
    %289:vr512 = VUNPCKHPDZrr %285, %286
    %288:vr512 = nofpexcept VMULPDZrr %289, %289, implicit $mxcsr
    %289:vr512 = nofpexcept VFMADD213PDZr %289, %289, %288, implicit $mxcsr
    %290:vr512 = nofpexcept VSUBPDZrr %288, %289, implicit $mxcsr
    %291:vr512 = VMOVAPDZrm %13, 1, %736, -192, $noreg :: (load (s512) from %ir.uglygep111, !tbaa !5)
    %292:vr512 = VMOVAPDZrm %13, 1, %736, -128, $noreg :: (load (s512) from %ir.uglygep109, !tbaa !5)
    %293:vr512 = VMOVAPDZrm %13, 1, %736, -64, $noreg :: (load (s512) from %ir.uglygep107, !tbaa !5)
    %294:vr512 = VMOVAPDZrm %13, 1, %736, 0, $noreg :: (load (s512) from %ir.uglygep105, !tbaa !5)
    %295:vr512 = nofpexcept VADDPDZrr %291, %292, implicit $mxcsr
    %296:vr512 = nofpexcept VSUBPDZrr %291, %292, implicit $mxcsr
    %297:vr512 = VUNPCKLPDZrr %295, %296
    %300:vr512 = VUNPCKHPDZrr %295, %296
    %299:vr512 = nofpexcept VMULPDZrr %300, %297, implicit $mxcsr
    %300:vr512 = nofpexcept VFMADD213PDZr %300, %297, %299, implicit $mxcsr
    %301:vr512 = nofpexcept VADDPDZrr %299, %300, implicit $mxcsr
    %302:vr512 = nofpexcept VSUBPDZrr %299, %300, implicit $mxcsr
    %303:vr512 = nofpexcept VADDPDZrr %293, %294, implicit $mxcsr
    %304:vr512 = nofpexcept VSUBPDZrr %293, %294, implicit $mxcsr
    %305:vr512 = VUNPCKLPDZrr %303, %304
    %308:vr512 = VUNPCKHPDZrr %303, %304
    %307:vr512 = nofpexcept VMULPDZrr %308, %305, implicit $mxcsr
    %308:vr512 = nofpexcept VFMADD213PDZr %308, %305, %307, implicit $mxcsr
    %309:vr512 = nofpexcept VADDPDZrr %307, %308, implicit $mxcsr
    %310:vr512 = nofpexcept VSUBPDZrr %307, %308, implicit $mxcsr
    %311:vr512 = nofpexcept VADDPDZrr %301, %309, implicit $mxcsr
    %312:vr512 = nofpexcept VSUBPDZrr %301, %309, implicit $mxcsr
    %313:vr512 = VUNPCKLPDZrr %311, %312
    %316:vr512 = VUNPCKHPDZrr %311, %312
    %315:vr512 = nofpexcept VMULPDZrr %316, %313, implicit $mxcsr
    %316:vr512 = nofpexcept VFMADD213PDZr %316, %313, %315, implicit $mxcsr
    %317:vr512 = nofpexcept VADDPDZrr %315, %316, implicit $mxcsr
    %318:vr512 = nofpexcept VSUBPDZrr %315, %316, implicit $mxcsr
    %319:vr512 = nofpexcept VADDPDZrr %310, %310, implicit $mxcsr
    %320:vr512 = nofpexcept VSUBPDZrr %310, %310, implicit $mxcsr
    %323:vr512 = VUNPCKHPDZrr %319, %320
    %322:vr512 = nofpexcept VMULPDZrr %323, %323, implicit $mxcsr
    %323:vr512 = nofpexcept VFMADD213PDZr %323, %323, %322, implicit $mxcsr
    %324:vr512 = nofpexcept VSUBPDZrr %322, %323, implicit $mxcsr
    PREFETCHT0 %8, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep63)
    PREFETCHT0 %8, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep61)
    PREFETCHT0 %8, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep59)
    PREFETCHT0 %8, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep57)
    %325:vr512 = nofpexcept VADDPDZrr %79, %113, implicit $mxcsr
    %326:vr512 = nofpexcept VSUBPDZrr %79, %113, implicit $mxcsr
    %327:vr512 = VUNPCKLPDZrr %325, %326
    %330:vr512 = VUNPCKHPDZrr %325, %326
    %329:vr512 = nofpexcept VMULPDZrr %330, %327, implicit $mxcsr
    %330:vr512 = nofpexcept VFMADD213PDZr %330, %327, %329, implicit $mxcsr
    %331:vr512 = nofpexcept VADDPDZrr %329, %330, implicit $mxcsr
    %332:vr512 = nofpexcept VSUBPDZrr %329, %330, implicit $mxcsr
    %333:vr512 = nofpexcept VADDPDZrr %147, %181, implicit $mxcsr
    %334:vr512 = nofpexcept VSUBPDZrr %147, %181, implicit $mxcsr
    %335:vr512 = VUNPCKLPDZrr %333, %334
    %338:vr512 = VUNPCKHPDZrr %333, %334
    %337:vr512 = nofpexcept VMULPDZrr %338, %335, implicit $mxcsr
    %338:vr512 = nofpexcept VFMADD213PDZr %338, %335, %337, implicit $mxcsr
    %339:vr512 = nofpexcept VADDPDZrr %337, %338, implicit $mxcsr
    %340:vr512 = nofpexcept VSUBPDZrr %337, %338, implicit $mxcsr
    %341:vr512 = nofpexcept VADDPDZrr %215, %249, implicit $mxcsr
    %342:vr512 = nofpexcept VSUBPDZrr %215, %249, implicit $mxcsr
    %343:vr512 = VUNPCKLPDZrr %341, %342
    %346:vr512 = VUNPCKHPDZrr %341, %342
    %345:vr512 = nofpexcept VMULPDZrr %346, %343, implicit $mxcsr
    %346:vr512 = nofpexcept VFMADD213PDZr %346, %343, %345, implicit $mxcsr
    %347:vr512 = nofpexcept VADDPDZrr %345, %346, implicit $mxcsr
    %348:vr512 = nofpexcept VSUBPDZrr %345, %346, implicit $mxcsr
    %349:vr512 = nofpexcept VADDPDZrr %283, %317, implicit $mxcsr
    %350:vr512 = nofpexcept VSUBPDZrr %283, %317, implicit $mxcsr
    %351:vr512 = VUNPCKLPDZrr %349, %350
    %354:vr512 = VUNPCKHPDZrr %349, %350
    %353:vr512 = nofpexcept VMULPDZrr %354, %351, implicit $mxcsr
    %354:vr512 = nofpexcept VFMADD213PDZr %354, %351, %353, implicit $mxcsr
    %355:vr512 = nofpexcept VADDPDZrr %353, %354, implicit $mxcsr
    %356:vr512 = nofpexcept VSUBPDZrr %353, %354, implicit $mxcsr
    %357:vr512 = nofpexcept VADDPDZrr %331, %347, implicit $mxcsr
    %358:vr512 = nofpexcept VSUBPDZrr %331, %347, implicit $mxcsr
    %359:vr512 = VUNPCKLPDZrr %357, %358
    %362:vr512 = VUNPCKHPDZrr %357, %358
    %361:vr512 = nofpexcept VMULPDZrr %362, %359, implicit $mxcsr
    %362:vr512 = nofpexcept VFMADD213PDZr %362, %359, %361, implicit $mxcsr
    %363:vr512 = nofpexcept VADDPDZrr %361, %362, implicit $mxcsr
    %364:vr512 = nofpexcept VSUBPDZrr %361, %362, implicit $mxcsr
    %365:vr512 = nofpexcept VADDPDZrr %339, %355, implicit $mxcsr
    %366:vr512 = nofpexcept VSUBPDZrr %339, %355, implicit $mxcsr
    %367:vr512 = VUNPCKLPDZrr %365, %366
    %370:vr512 = VUNPCKHPDZrr %365, %366
    %369:vr512 = nofpexcept VMULPDZrr %370, %367, implicit $mxcsr
    %370:vr512 = nofpexcept VFMADD213PDZr %370, %367, %369, implicit $mxcsr
    %371:vr512 = nofpexcept VADDPDZrr %369, %370, implicit $mxcsr
    %372:vr512 = nofpexcept VSUBPDZrr %369, %370, implicit $mxcsr
    %373:vr512 = nofpexcept VADDPDZrr %332, %348, implicit $mxcsr
    %374:vr512 = nofpexcept VSUBPDZrr %332, %348, implicit $mxcsr
    %375:vr512 = VUNPCKLPDZrr %373, %374
    %378:vr512 = VUNPCKHPDZrr %373, %374
    %377:vr512 = nofpexcept VMULPDZrr %378, %375, implicit $mxcsr
    %378:vr512 = nofpexcept VFMADD213PDZr %378, %375, %377, implicit $mxcsr
    %379:vr512 = nofpexcept VADDPDZrr %377, %378, implicit $mxcsr
    %380:vr512 = nofpexcept VSUBPDZrr %377, %378, implicit $mxcsr
    %381:vr512 = nofpexcept VADDPDZrr %340, %356, implicit $mxcsr
    %382:vr512 = nofpexcept VSUBPDZrr %340, %356, implicit $mxcsr
    %383:vr512 = VUNPCKLPDZrr %381, %382
    %386:vr512 = VUNPCKHPDZrr %381, %382
    %385:vr512 = nofpexcept VMULPDZrr %386, %383, implicit $mxcsr
    %386:vr512 = nofpexcept VFMADD213PDZr %386, %383, %385, implicit $mxcsr
    %387:vr512 = nofpexcept VADDPDZrr %385, %386, implicit $mxcsr
    %388:vr512 = nofpexcept VSUBPDZrr %385, %386, implicit $mxcsr
    PREFETCHT0 %1, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep7)
    PREFETCHT0 %1, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep5)
    PREFETCHT0 %1, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep3)
    PREFETCHT0 %1, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep1)
    %389:vr512 = nofpexcept VADDPDZrr %363, %371, implicit $mxcsr
    %390:vr512 = nofpexcept VSUBPDZrr %363, %371, implicit $mxcsr
    %391:vr512 = VUNPCKLPDZrr %389, %390
    %394:vr512 = VUNPCKHPDZrr %389, %390
    %393:vr512 = nofpexcept VMULPDZrr %394, %391, implicit $mxcsr
    %394:vr512 = nofpexcept VFMADD213PDZr %394, %391, %393, implicit $mxcsr
    %395:vr512 = nofpexcept VADDPDZrr %393, %394, implicit $mxcsr
    %396:vr512 = nofpexcept VSUBPDZrr %393, %394, implicit $mxcsr
    VMOVAPDZmr %9, 1, %736, -192, $noreg, %395 :: (store (s512) into %ir.uglygep71, !tbaa !5)
    %407:gr64_nosp = LEA64r %0, 1, %52, -192, $noreg
    VMOVAPDZmr %0, 1, %52, -192, $noreg, %396 :: (store (s512) into %ir.uglygep136, !tbaa !5)
    %398:vr512 = nofpexcept VADDPDZrr %364, %372, implicit $mxcsr
    %399:vr512 = nofpexcept VSUBPDZrr %364, %372, implicit $mxcsr
    %400:vr512 = VUNPCKLPDZrr %398, %399
    %403:vr512 = VUNPCKHPDZrr %398, %399
    %402:vr512 = nofpexcept VMULPDZrr %403, %400, implicit $mxcsr
    %403:vr512 = nofpexcept VFMADD213PDZr %403, %400, %402, implicit $mxcsr
    %404:vr512 = nofpexcept VADDPDZrr %402, %403, implicit $mxcsr
    %405:vr512 = nofpexcept VSUBPDZrr %402, %403, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %404 :: (store (s512) into %ir.uglygep137, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %405 :: (store (s512) into %ir.uglygep138, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %408:vr512 = nofpexcept VADDPDZrr %379, %387, implicit $mxcsr
    %409:vr512 = nofpexcept VSUBPDZrr %379, %387, implicit $mxcsr
    %410:vr512 = VUNPCKLPDZrr %408, %409
    %413:vr512 = VUNPCKHPDZrr %408, %409
    %412:vr512 = nofpexcept VMULPDZrr %413, %410, implicit $mxcsr
    %413:vr512 = nofpexcept VFMADD213PDZr %413, %410, %412, implicit $mxcsr
    %414:vr512 = nofpexcept VADDPDZrr %412, %413, implicit $mxcsr
    %415:vr512 = nofpexcept VSUBPDZrr %412, %413, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %414 :: (store (s512) into %ir.uglygep139, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %415 :: (store (s512) into %ir.uglygep140, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %418:vr512 = nofpexcept VADDPDZrr %380, %388, implicit $mxcsr
    %419:vr512 = nofpexcept VSUBPDZrr %380, %388, implicit $mxcsr
    %420:vr512 = VUNPCKLPDZrr %418, %419
    %423:vr512 = VUNPCKHPDZrr %418, %419
    %422:vr512 = nofpexcept VMULPDZrr %423, %420, implicit $mxcsr
    %423:vr512 = nofpexcept VFMADD213PDZr %423, %420, %422, implicit $mxcsr
    %424:vr512 = nofpexcept VADDPDZrr %422, %423, implicit $mxcsr
    %425:vr512 = nofpexcept VSUBPDZrr %422, %423, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %424 :: (store (s512) into %ir.uglygep141, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %425 :: (store (s512) into %ir.uglygep142, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    PREFETCHT0 %7, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep55)
    PREFETCHT0 %7, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep53)
    PREFETCHT0 %7, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep51)
    PREFETCHT0 %7, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep49)
    %428:vr512 = nofpexcept VADDPDZrr %64, %98, implicit $mxcsr
    %429:vr512 = nofpexcept VSUBPDZrr %64, %98, implicit $mxcsr
    %430:vr512 = VUNPCKLPDZrr %428, %429
    %433:vr512 = VUNPCKHPDZrr %428, %429
    %432:vr512 = nofpexcept VMULPDZrr %433, %430, implicit $mxcsr
    %433:vr512 = nofpexcept VFMADD213PDZr %433, %430, %432, implicit $mxcsr
    %434:vr512 = nofpexcept VADDPDZrr %432, %433, implicit $mxcsr
    %435:vr512 = nofpexcept VSUBPDZrr %432, %433, implicit $mxcsr
    %436:vr512 = nofpexcept VADDPDZrr %132, %166, implicit $mxcsr
    %437:vr512 = nofpexcept VSUBPDZrr %132, %166, implicit $mxcsr
    %438:vr512 = VUNPCKLPDZrr %436, %437
    %441:vr512 = VUNPCKHPDZrr %436, %437
    %440:vr512 = nofpexcept VMULPDZrr %441, %438, implicit $mxcsr
    %441:vr512 = nofpexcept VFMADD213PDZr %441, %438, %440, implicit $mxcsr
    %442:vr512 = nofpexcept VADDPDZrr %440, %441, implicit $mxcsr
    %443:vr512 = nofpexcept VSUBPDZrr %440, %441, implicit $mxcsr
    %444:vr512 = nofpexcept VADDPDZrr %200, %234, implicit $mxcsr
    %445:vr512 = nofpexcept VSUBPDZrr %200, %234, implicit $mxcsr
    %446:vr512 = VUNPCKLPDZrr %444, %445
    %449:vr512 = VUNPCKHPDZrr %444, %445
    %448:vr512 = nofpexcept VMULPDZrr %449, %446, implicit $mxcsr
    %449:vr512 = nofpexcept VFMADD213PDZr %449, %446, %448, implicit $mxcsr
    %450:vr512 = nofpexcept VADDPDZrr %448, %449, implicit $mxcsr
    %451:vr512 = nofpexcept VSUBPDZrr %448, %449, implicit $mxcsr
    %452:vr512 = nofpexcept VADDPDZrr %268, %302, implicit $mxcsr
    %453:vr512 = nofpexcept VSUBPDZrr %268, %302, implicit $mxcsr
    %454:vr512 = VUNPCKLPDZrr %452, %453
    %457:vr512 = VUNPCKHPDZrr %452, %453
    %456:vr512 = nofpexcept VMULPDZrr %457, %454, implicit $mxcsr
    %457:vr512 = nofpexcept VFMADD213PDZr %457, %454, %456, implicit $mxcsr
    %458:vr512 = nofpexcept VADDPDZrr %456, %457, implicit $mxcsr
    %459:vr512 = nofpexcept VSUBPDZrr %456, %457, implicit $mxcsr
    %460:vr512 = nofpexcept VADDPDZrr %434, %450, implicit $mxcsr
    %461:vr512 = nofpexcept VSUBPDZrr %434, %450, implicit $mxcsr
    %462:vr512 = VUNPCKLPDZrr %460, %461
    %465:vr512 = VUNPCKHPDZrr %460, %461
    %464:vr512 = nofpexcept VMULPDZrr %465, %462, implicit $mxcsr
    %465:vr512 = nofpexcept VFMADD213PDZr %465, %462, %464, implicit $mxcsr
    %466:vr512 = nofpexcept VADDPDZrr %464, %465, implicit $mxcsr
    %467:vr512 = nofpexcept VSUBPDZrr %464, %465, implicit $mxcsr
    %468:vr512 = nofpexcept VADDPDZrr %442, %458, implicit $mxcsr
    %469:vr512 = nofpexcept VSUBPDZrr %442, %458, implicit $mxcsr
    %470:vr512 = VUNPCKLPDZrr %468, %469
    %473:vr512 = VUNPCKHPDZrr %468, %469
    %472:vr512 = nofpexcept VMULPDZrr %473, %470, implicit $mxcsr
    %473:vr512 = nofpexcept VFMADD213PDZr %473, %470, %472, implicit $mxcsr
    %474:vr512 = nofpexcept VADDPDZrr %472, %473, implicit $mxcsr
    %475:vr512 = nofpexcept VSUBPDZrr %472, %473, implicit $mxcsr
    %476:vr512 = nofpexcept VADDPDZrr %435, %451, implicit $mxcsr
    %477:vr512 = nofpexcept VSUBPDZrr %435, %451, implicit $mxcsr
    %478:vr512 = VUNPCKLPDZrr %476, %477
    %481:vr512 = VUNPCKHPDZrr %476, %477
    %480:vr512 = nofpexcept VMULPDZrr %481, %478, implicit $mxcsr
    %481:vr512 = nofpexcept VFMADD213PDZr %481, %478, %480, implicit $mxcsr
    %482:vr512 = nofpexcept VADDPDZrr %480, %481, implicit $mxcsr
    %483:vr512 = nofpexcept VSUBPDZrr %480, %481, implicit $mxcsr
    %484:vr512 = nofpexcept VADDPDZrr %443, %459, implicit $mxcsr
    %485:vr512 = nofpexcept VSUBPDZrr %443, %459, implicit $mxcsr
    %486:vr512 = VUNPCKLPDZrr %484, %485
    %489:vr512 = VUNPCKHPDZrr %484, %485
    %488:vr512 = nofpexcept VMULPDZrr %489, %486, implicit $mxcsr
    %489:vr512 = nofpexcept VFMADD213PDZr %489, %486, %488, implicit $mxcsr
    %490:vr512 = nofpexcept VADDPDZrr %488, %489, implicit $mxcsr
    %491:vr512 = nofpexcept VSUBPDZrr %488, %489, implicit $mxcsr
    %492:vr512 = nofpexcept VADDPDZrr %466, %474, implicit $mxcsr
    %493:vr512 = nofpexcept VSUBPDZrr %466, %474, implicit $mxcsr
    %494:vr512 = VUNPCKLPDZrr %492, %493
    %497:vr512 = VUNPCKHPDZrr %492, %493
    %496:vr512 = nofpexcept VMULPDZrr %497, %494, implicit $mxcsr
    %497:vr512 = nofpexcept VFMADD213PDZr %497, %494, %496, implicit $mxcsr
    %498:vr512 = nofpexcept VADDPDZrr %496, %497, implicit $mxcsr
    %499:vr512 = nofpexcept VSUBPDZrr %496, %497, implicit $mxcsr
    VMOVAPDZmr %9, 1, %736, -128, $noreg, %498 :: (store (s512) into %ir.uglygep69, !tbaa !5)
    VMOVAPDZmr %17, 1, %407, 0, $noreg, %499 :: (store (s512) into %ir.uglygep143, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
    %501:vr512 = nofpexcept VADDPDZrr %467, %475, implicit $mxcsr
    %502:vr512 = nofpexcept VSUBPDZrr %467, %475, implicit $mxcsr
    %503:vr512 = VUNPCKLPDZrr %501, %502
    %506:vr512 = VUNPCKHPDZrr %501, %502
    %505:vr512 = nofpexcept VMULPDZrr %506, %503, implicit $mxcsr
    %506:vr512 = nofpexcept VFMADD213PDZr %506, %503, %505, implicit $mxcsr
    %507:vr512 = nofpexcept VADDPDZrr %505, %506, implicit $mxcsr
    %508:vr512 = nofpexcept VSUBPDZrr %505, %506, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %507 :: (store (s512) into %ir.uglygep144, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %508 :: (store (s512) into %ir.uglygep145, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %511:vr512 = nofpexcept VADDPDZrr %482, %490, implicit $mxcsr
    %512:vr512 = nofpexcept VSUBPDZrr %482, %490, implicit $mxcsr
    %513:vr512 = VUNPCKLPDZrr %511, %512
    %516:vr512 = VUNPCKHPDZrr %511, %512
    %515:vr512 = nofpexcept VMULPDZrr %516, %513, implicit $mxcsr
    %516:vr512 = nofpexcept VFMADD213PDZr %516, %513, %515, implicit $mxcsr
    %517:vr512 = nofpexcept VADDPDZrr %515, %516, implicit $mxcsr
    %518:vr512 = nofpexcept VSUBPDZrr %515, %516, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %517 :: (store (s512) into %ir.uglygep146, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %518 :: (store (s512) into %ir.uglygep147, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %521:vr512 = nofpexcept VADDPDZrr %483, %491, implicit $mxcsr
    %522:vr512 = nofpexcept VSUBPDZrr %483, %491, implicit $mxcsr
    %523:vr512 = VUNPCKLPDZrr %521, %522
    %526:vr512 = VUNPCKHPDZrr %521, %522
    %525:vr512 = nofpexcept VMULPDZrr %526, %523, implicit $mxcsr
    %526:vr512 = nofpexcept VFMADD213PDZr %526, %523, %525, implicit $mxcsr
    %527:vr512 = nofpexcept VADDPDZrr %525, %526, implicit $mxcsr
    %528:vr512 = nofpexcept VSUBPDZrr %525, %526, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %527 :: (store (s512) into %ir.uglygep148, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %528 :: (store (s512) into %ir.uglygep149, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    PREFETCHT0 %6, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep47)
    PREFETCHT0 %6, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep45)
    PREFETCHT0 %6, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep43)
    PREFETCHT0 %6, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep41)
    %531:vr512 = nofpexcept VADDPDZrr %80, %114, implicit $mxcsr
    %532:vr512 = nofpexcept VSUBPDZrr %80, %114, implicit $mxcsr
    %533:vr512 = VUNPCKLPDZrr %531, %532
    %536:vr512 = VUNPCKHPDZrr %531, %532
    %535:vr512 = nofpexcept VMULPDZrr %536, %533, implicit $mxcsr
    %536:vr512 = nofpexcept VFMADD213PDZr %536, %533, %535, implicit $mxcsr
    %537:vr512 = nofpexcept VADDPDZrr %535, %536, implicit $mxcsr
    %538:vr512 = nofpexcept VSUBPDZrr %535, %536, implicit $mxcsr
    %539:vr512 = nofpexcept VADDPDZrr %148, %182, implicit $mxcsr
    %540:vr512 = nofpexcept VSUBPDZrr %148, %182, implicit $mxcsr
    %541:vr512 = VUNPCKLPDZrr %539, %540
    %544:vr512 = VUNPCKHPDZrr %539, %540
    %543:vr512 = nofpexcept VMULPDZrr %544, %541, implicit $mxcsr
    %544:vr512 = nofpexcept VFMADD213PDZr %544, %541, %543, implicit $mxcsr
    %545:vr512 = nofpexcept VADDPDZrr %543, %544, implicit $mxcsr
    %546:vr512 = nofpexcept VSUBPDZrr %543, %544, implicit $mxcsr
    %547:vr512 = nofpexcept VADDPDZrr %216, %250, implicit $mxcsr
    %548:vr512 = nofpexcept VSUBPDZrr %216, %250, implicit $mxcsr
    %549:vr512 = VUNPCKLPDZrr %547, %548
    %552:vr512 = VUNPCKHPDZrr %547, %548
    %551:vr512 = nofpexcept VMULPDZrr %552, %549, implicit $mxcsr
    %552:vr512 = nofpexcept VFMADD213PDZr %552, %549, %551, implicit $mxcsr
    %553:vr512 = nofpexcept VADDPDZrr %551, %552, implicit $mxcsr
    %554:vr512 = nofpexcept VSUBPDZrr %551, %552, implicit $mxcsr
    %555:vr512 = nofpexcept VADDPDZrr %284, %318, implicit $mxcsr
    %556:vr512 = nofpexcept VSUBPDZrr %284, %318, implicit $mxcsr
    %557:vr512 = VUNPCKLPDZrr %555, %556
    %560:vr512 = VUNPCKHPDZrr %555, %556
    %559:vr512 = nofpexcept VMULPDZrr %560, %557, implicit $mxcsr
    %560:vr512 = nofpexcept VFMADD213PDZr %560, %557, %559, implicit $mxcsr
    %561:vr512 = nofpexcept VADDPDZrr %559, %560, implicit $mxcsr
    %562:vr512 = nofpexcept VSUBPDZrr %559, %560, implicit $mxcsr
    %563:vr512 = nofpexcept VADDPDZrr %537, %553, implicit $mxcsr
    %564:vr512 = nofpexcept VSUBPDZrr %537, %553, implicit $mxcsr
    %565:vr512 = VUNPCKLPDZrr %563, %564
    %568:vr512 = VUNPCKHPDZrr %563, %564
    %567:vr512 = nofpexcept VMULPDZrr %568, %565, implicit $mxcsr
    %568:vr512 = nofpexcept VFMADD213PDZr %568, %565, %567, implicit $mxcsr
    %569:vr512 = nofpexcept VADDPDZrr %567, %568, implicit $mxcsr
    %570:vr512 = nofpexcept VSUBPDZrr %567, %568, implicit $mxcsr
    %571:vr512 = nofpexcept VADDPDZrr %545, %561, implicit $mxcsr
    %572:vr512 = nofpexcept VSUBPDZrr %545, %561, implicit $mxcsr
    %573:vr512 = VUNPCKLPDZrr %571, %572
    %576:vr512 = VUNPCKHPDZrr %571, %572
    %575:vr512 = nofpexcept VMULPDZrr %576, %573, implicit $mxcsr
    %576:vr512 = nofpexcept VFMADD213PDZr %576, %573, %575, implicit $mxcsr
    %577:vr512 = nofpexcept VADDPDZrr %575, %576, implicit $mxcsr
    %578:vr512 = nofpexcept VSUBPDZrr %575, %576, implicit $mxcsr
    %579:vr512 = nofpexcept VADDPDZrr %538, %554, implicit $mxcsr
    %580:vr512 = nofpexcept VSUBPDZrr %538, %554, implicit $mxcsr
    %581:vr512 = VUNPCKLPDZrr %579, %580
    %584:vr512 = VUNPCKHPDZrr %579, %580
    %583:vr512 = nofpexcept VMULPDZrr %584, %581, implicit $mxcsr
    %584:vr512 = nofpexcept VFMADD213PDZr %584, %581, %583, implicit $mxcsr
    %585:vr512 = nofpexcept VADDPDZrr %583, %584, implicit $mxcsr
    %586:vr512 = nofpexcept VSUBPDZrr %583, %584, implicit $mxcsr
    %587:vr512 = nofpexcept VADDPDZrr %546, %562, implicit $mxcsr
    %588:vr512 = nofpexcept VSUBPDZrr %546, %562, implicit $mxcsr
    %589:vr512 = VUNPCKLPDZrr %587, %588
    %592:vr512 = VUNPCKHPDZrr %587, %588
    %591:vr512 = nofpexcept VMULPDZrr %592, %589, implicit $mxcsr
    %592:vr512 = nofpexcept VFMADD213PDZr %592, %589, %591, implicit $mxcsr
    %593:vr512 = nofpexcept VADDPDZrr %591, %592, implicit $mxcsr
    %594:vr512 = nofpexcept VSUBPDZrr %591, %592, implicit $mxcsr
    %595:vr512 = nofpexcept VADDPDZrr %569, %577, implicit $mxcsr
    %596:vr512 = nofpexcept VSUBPDZrr %569, %577, implicit $mxcsr
    %597:vr512 = VUNPCKLPDZrr %595, %596
    %600:vr512 = VUNPCKHPDZrr %595, %596
    %599:vr512 = nofpexcept VMULPDZrr %600, %597, implicit $mxcsr
    %600:vr512 = nofpexcept VFMADD213PDZr %600, %597, %599, implicit $mxcsr
    %601:vr512 = nofpexcept VADDPDZrr %599, %600, implicit $mxcsr
    %602:vr512 = nofpexcept VSUBPDZrr %599, %600, implicit $mxcsr
    VMOVAPDZmr %9, 1, %736, -64, $noreg, %601 :: (store (s512) into %ir.uglygep67, !tbaa !5)
    VMOVAPDZmr %17, 1, %407, 0, $noreg, %602 :: (store (s512) into %ir.uglygep150, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
    %604:vr512 = nofpexcept VADDPDZrr %570, %578, implicit $mxcsr
    %605:vr512 = nofpexcept VSUBPDZrr %570, %578, implicit $mxcsr
    %606:vr512 = VUNPCKLPDZrr %604, %605
    %609:vr512 = VUNPCKHPDZrr %604, %605
    %608:vr512 = nofpexcept VMULPDZrr %609, %606, implicit $mxcsr
    %609:vr512 = nofpexcept VFMADD213PDZr %609, %606, %608, implicit $mxcsr
    %610:vr512 = nofpexcept VADDPDZrr %608, %609, implicit $mxcsr
    %611:vr512 = nofpexcept VSUBPDZrr %608, %609, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %610 :: (store (s512) into %ir.uglygep151, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %611 :: (store (s512) into %ir.uglygep152, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %614:vr512 = nofpexcept VADDPDZrr %585, %593, implicit $mxcsr
    %615:vr512 = nofpexcept VSUBPDZrr %585, %593, implicit $mxcsr
    %616:vr512 = VUNPCKLPDZrr %614, %615
    %619:vr512 = VUNPCKHPDZrr %614, %615
    %618:vr512 = nofpexcept VMULPDZrr %619, %616, implicit $mxcsr
    %619:vr512 = nofpexcept VFMADD213PDZr %619, %616, %618, implicit $mxcsr
    %620:vr512 = nofpexcept VADDPDZrr %618, %619, implicit $mxcsr
    %621:vr512 = nofpexcept VSUBPDZrr %618, %619, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %620 :: (store (s512) into %ir.uglygep153, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %621 :: (store (s512) into %ir.uglygep154, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %624:vr512 = nofpexcept VADDPDZrr %586, %594, implicit $mxcsr
    %625:vr512 = nofpexcept VSUBPDZrr %586, %594, implicit $mxcsr
    %626:vr512 = VUNPCKLPDZrr %624, %625
    %629:vr512 = VUNPCKHPDZrr %624, %625
    %628:vr512 = nofpexcept VMULPDZrr %629, %626, implicit $mxcsr
    %629:vr512 = nofpexcept VFMADD213PDZr %629, %626, %628, implicit $mxcsr
    %630:vr512 = nofpexcept VADDPDZrr %628, %629, implicit $mxcsr
    %631:vr512 = nofpexcept VSUBPDZrr %628, %629, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %630 :: (store (s512) into %ir.uglygep155, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %631 :: (store (s512) into %ir.uglygep156, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    PREFETCHT0 %5, 1, %736, -192, $noreg :: (load (s8) from %ir.uglygep39)
    PREFETCHT0 %5, 1, %736, -128, $noreg :: (load (s8) from %ir.uglygep37)
    PREFETCHT0 %5, 1, %736, -64, $noreg :: (load (s8) from %ir.uglygep35)
    PREFETCHT0 %5, 1, %736, 0, $noreg :: (load (s8) from %ir.uglygep33)
    %634:vr512 = nofpexcept VADDPDZrr %86, %120, implicit $mxcsr
    %635:vr512 = nofpexcept VSUBPDZrr %86, %120, implicit $mxcsr
    %636:vr512 = VUNPCKLPDZrr %634, %635
    %639:vr512 = VUNPCKHPDZrr %634, %635
    %638:vr512 = nofpexcept VMULPDZrr %639, %636, implicit $mxcsr
    %639:vr512 = nofpexcept VFMADD213PDZr %639, %636, %638, implicit $mxcsr
    %640:vr512 = nofpexcept VADDPDZrr %638, %639, implicit $mxcsr
    %641:vr512 = nofpexcept VSUBPDZrr %638, %639, implicit $mxcsr
    %642:vr512 = nofpexcept VADDPDZrr %154, %188, implicit $mxcsr
    %643:vr512 = nofpexcept VSUBPDZrr %154, %188, implicit $mxcsr
    %644:vr512 = VUNPCKLPDZrr %642, %643
    %647:vr512 = VUNPCKHPDZrr %642, %643
    %646:vr512 = nofpexcept VMULPDZrr %647, %644, implicit $mxcsr
    %647:vr512 = nofpexcept VFMADD213PDZr %647, %644, %646, implicit $mxcsr
    %648:vr512 = nofpexcept VADDPDZrr %646, %647, implicit $mxcsr
    %649:vr512 = nofpexcept VSUBPDZrr %646, %647, implicit $mxcsr
    %650:vr512 = nofpexcept VADDPDZrr %222, %256, implicit $mxcsr
    %651:vr512 = nofpexcept VSUBPDZrr %222, %256, implicit $mxcsr
    %652:vr512 = VUNPCKLPDZrr %650, %651
    %655:vr512 = VUNPCKHPDZrr %650, %651
    %654:vr512 = nofpexcept VMULPDZrr %655, %652, implicit $mxcsr
    %655:vr512 = nofpexcept VFMADD213PDZr %655, %652, %654, implicit $mxcsr
    %656:vr512 = nofpexcept VADDPDZrr %654, %655, implicit $mxcsr
    %657:vr512 = nofpexcept VSUBPDZrr %654, %655, implicit $mxcsr
    %658:vr512 = nofpexcept VADDPDZrr %290, %324, implicit $mxcsr
    %659:vr512 = nofpexcept VSUBPDZrr %290, %324, implicit $mxcsr
    %660:vr512 = VUNPCKLPDZrr %658, %659
    %663:vr512 = VUNPCKHPDZrr %658, %659
    %662:vr512 = nofpexcept VMULPDZrr %663, %660, implicit $mxcsr
    %663:vr512 = nofpexcept VFMADD213PDZr %663, %660, %662, implicit $mxcsr
    %664:vr512 = nofpexcept VADDPDZrr %662, %663, implicit $mxcsr
    %665:vr512 = nofpexcept VSUBPDZrr %662, %663, implicit $mxcsr
    %666:vr512 = nofpexcept VADDPDZrr %640, %656, implicit $mxcsr
    %667:vr512 = nofpexcept VSUBPDZrr %640, %656, implicit $mxcsr
    %668:vr512 = VUNPCKLPDZrr %666, %667
    %671:vr512 = VUNPCKHPDZrr %666, %667
    %670:vr512 = nofpexcept VMULPDZrr %671, %668, implicit $mxcsr
    %671:vr512 = nofpexcept VFMADD213PDZr %671, %668, %670, implicit $mxcsr
    %672:vr512 = nofpexcept VADDPDZrr %670, %671, implicit $mxcsr
    %673:vr512 = nofpexcept VSUBPDZrr %670, %671, implicit $mxcsr
    %674:vr512 = nofpexcept VADDPDZrr %648, %664, implicit $mxcsr
    %675:vr512 = nofpexcept VSUBPDZrr %648, %664, implicit $mxcsr
    %676:vr512 = VUNPCKLPDZrr %674, %675
    %679:vr512 = VUNPCKHPDZrr %674, %675
    %678:vr512 = nofpexcept VMULPDZrr %679, %676, implicit $mxcsr
    %679:vr512 = nofpexcept VFMADD213PDZr %679, %676, %678, implicit $mxcsr
    %680:vr512 = nofpexcept VADDPDZrr %678, %679, implicit $mxcsr
    %681:vr512 = nofpexcept VSUBPDZrr %678, %679, implicit $mxcsr
    %682:vr512 = nofpexcept VADDPDZrr %641, %657, implicit $mxcsr
    %683:vr512 = nofpexcept VSUBPDZrr %641, %657, implicit $mxcsr
    %684:vr512 = VUNPCKLPDZrr %682, %683
    %687:vr512 = VUNPCKHPDZrr %682, %683
    %686:vr512 = nofpexcept VMULPDZrr %687, %684, implicit $mxcsr
    %687:vr512 = nofpexcept VFMADD213PDZr %687, %684, %686, implicit $mxcsr
    %688:vr512 = nofpexcept VADDPDZrr %686, %687, implicit $mxcsr
    %689:vr512 = nofpexcept VSUBPDZrr %686, %687, implicit $mxcsr
    %690:vr512 = nofpexcept VADDPDZrr %649, %665, implicit $mxcsr
    %691:vr512 = nofpexcept VSUBPDZrr %649, %665, implicit $mxcsr
    %692:vr512 = VUNPCKLPDZrr %690, %691
    %695:vr512 = VUNPCKHPDZrr %690, %691
    %694:vr512 = nofpexcept VMULPDZrr %695, %692, implicit $mxcsr
    %695:vr512 = nofpexcept VFMADD213PDZr %695, %692, %694, implicit $mxcsr
    %696:vr512 = nofpexcept VADDPDZrr %694, %695, implicit $mxcsr
    %697:vr512 = nofpexcept VSUBPDZrr %694, %695, implicit $mxcsr
    %698:vr512 = nofpexcept VADDPDZrr %672, %680, implicit $mxcsr
    %699:vr512 = nofpexcept VSUBPDZrr %672, %680, implicit $mxcsr
    %700:vr512 = VUNPCKLPDZrr %698, %699
    %703:vr512 = VUNPCKHPDZrr %698, %699
    %702:vr512 = nofpexcept VMULPDZrr %703, %700, implicit $mxcsr
    %703:vr512 = nofpexcept VFMADD213PDZr %703, %700, %702, implicit $mxcsr
    %704:vr512 = nofpexcept VADDPDZrr %702, %703, implicit $mxcsr
    %705:vr512 = nofpexcept VSUBPDZrr %702, %703, implicit $mxcsr
    VMOVAPDZmr %9, 1, %736, 0, $noreg, %704 :: (store (s512) into %ir.uglygep65, !tbaa !5)
    VMOVAPDZmr %17, 1, %407, 0, $noreg, %705 :: (store (s512) into %ir.uglygep157, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %17, implicit-def dead $eflags
    %707:vr512 = nofpexcept VADDPDZrr %673, %681, implicit $mxcsr
    %708:vr512 = nofpexcept VSUBPDZrr %673, %681, implicit $mxcsr
    %709:vr512 = VUNPCKLPDZrr %707, %708
    %712:vr512 = VUNPCKHPDZrr %707, %708
    %711:vr512 = nofpexcept VMULPDZrr %712, %709, implicit $mxcsr
    %712:vr512 = nofpexcept VFMADD213PDZr %712, %709, %711, implicit $mxcsr
    %713:vr512 = nofpexcept VADDPDZrr %711, %712, implicit $mxcsr
    %714:vr512 = nofpexcept VSUBPDZrr %711, %712, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %713 :: (store (s512) into %ir.uglygep158, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %714 :: (store (s512) into %ir.uglygep159, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %717:vr512 = nofpexcept VADDPDZrr %688, %696, implicit $mxcsr
    %718:vr512 = nofpexcept VSUBPDZrr %688, %696, implicit $mxcsr
    %719:vr512 = VUNPCKLPDZrr %717, %718
    %722:vr512 = VUNPCKHPDZrr %717, %718
    %721:vr512 = nofpexcept VMULPDZrr %722, %719, implicit $mxcsr
    %722:vr512 = nofpexcept VFMADD213PDZr %722, %719, %721, implicit $mxcsr
    %723:vr512 = nofpexcept VADDPDZrr %721, %722, implicit $mxcsr
    %724:vr512 = nofpexcept VSUBPDZrr %721, %722, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %723 :: (store (s512) into %ir.uglygep160, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %724 :: (store (s512) into %ir.uglygep161, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    %727:vr512 = nofpexcept VADDPDZrr %689, %697, implicit $mxcsr
    %728:vr512 = nofpexcept VSUBPDZrr %689, %697, implicit $mxcsr
    %729:vr512 = VUNPCKLPDZrr %727, %728
    %732:vr512 = VUNPCKHPDZrr %727, %728
    %731:vr512 = nofpexcept VMULPDZrr %732, %729, implicit $mxcsr
    %732:vr512 = nofpexcept VFMADD213PDZr %732, %729, %731, implicit $mxcsr
    %733:vr512 = nofpexcept VADDPDZrr %731, %732, implicit $mxcsr
    %734:vr512 = nofpexcept VSUBPDZrr %731, %732, implicit $mxcsr
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %733 :: (store (s512) into %ir.uglygep162, !tbaa !5)
    %407:gr64_nosp = ADD64rr %407, %0, implicit-def dead $eflags
    VMOVAPDZmr %0, 1, %407, 0, $noreg, %734 :: (store (s512) into %ir.uglygep163, !tbaa !5)
    %736:gr64_nosp = ADD64ri8 %736, 64, implicit-def dead $eflags
    %737:gr64 = DEC64r %737, implicit-def $eflags
    JCC_1 %bb.1, 5, implicit killed $eflags
    JMP_1 %bb.2

  bb.2.do.end:
    RET 0

...
