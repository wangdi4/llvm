; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=skylake -mtriple=x86_64-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKL
; RUN: llc -mcpu=skylake -mtriple=i686-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKL-32
; RUN: llc -mcpu=skx -mtriple=x86_64-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKX
; RUN: llc -mcpu=skx -mtriple=i686-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKX-32

define <8 x float> @test_f32v8_1(float *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_1:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, (%rdi,%ymm1,4), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_1:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-32-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, (%eax,%ymm1,4), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_1:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps (%rdi,%ymm1,4), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_1:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps (%eax,%ymm1,4), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff>
  %gep = getelementptr float, float *%base, <8 x i64> %idx
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F0 = type { i16, float }

define <8 x float> @test_f32v8_2(%F0 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_2:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vandps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1,8), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_2:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vandps {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1,8), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_2:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1,8), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_2:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1,8), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F0, %F0 *%base, <8 x i64> %idx, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F1 = type { float, i16, i16, i16 }
define <8 x float> @test_f32v8_3(%F1 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_3:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [12,12,12,12,12,12,12,12]
; SKL-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, (%rdi,%ymm1), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_3:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [12,12,12,12,12,12,12,12]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, (%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_3:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKX-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps (%rdi,%ymm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_3:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps (%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F1, %F1 *%base, <8 x i64> %idx, <8 x i32><i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F20 = type { i16, float, i16, i16 }
%F2 = type { %F20, i16, i16, i16 }
define <8 x float> @test_f32v8_4(%F2 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_4:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_4:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_4:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKX-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_4:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F2, %F2 *%base, <8 x i64> %idx, i32 0, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

define <8 x float> @test_f32v8_5(%F2 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_5:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpbroadcastq {{.*#+}} ymm3 = [20,20,20,20]
; SKL-NEXT:    vpmuldq %ymm3, %ymm0, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; SKL-NEXT:    vpmuldq %ymm3, %ymm1, %ymm1
; SKL-NEXT:    vextracti128 $1, %ymm2, %xmm3
; SKL-NEXT:    vxorps %xmm4, %xmm4, %xmm4
; SKL-NEXT:    vxorps %xmm5, %xmm5, %xmm5
; SKL-NEXT:    vgatherqps %xmm3, 4(%rdi,%ymm1), %xmm5
; SKL-NEXT:    vgatherqps %xmm2, 4(%rdi,%ymm0), %xmm4
; SKL-NEXT:    vinsertf128 $1, %xmm5, %ymm4, %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_5:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_5:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpandq {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %zmm0, %zmm0
; SKX-NEXT:    vpmuldq {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherqps 4(%rdi,%zmm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_5:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0x7fffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F2, %F2 *%base, <8 x i64> %idx, i32 0, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

define <8 x float> @test_f32v8_6(%F0 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_6:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1,8), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_6:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-32-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1,8), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_6:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1,8), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_6:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1,8), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff>
  %vec_base = insertelement <8 x %F0*> poison, %F0* %base, i64 0
  %splat_base = shufflevector <8 x %F0*> %vec_base, <8 x %F0*> poison, <8 x i32> zeroinitializer
  %gep = getelementptr %F0, <8 x %F0*> %splat_base, <8 x i64> %idx, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

define <8 x float> @test_f32v8_7([4000 x float] *%base, <8 x float> %init, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_7:
; SKL:       # %bb.0: # %entry
; SKL-NEXT:    xorl %eax, %eax
; SKL-NEXT:    vpbroadcastd {{.*#+}} ymm2 = [16000,16000,16000,16000,16000,16000,16000,16000]
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; SKL-NEXT:    vpslld $31, %ymm1, %ymm1
; SKL-NEXT:    .p2align 4, 0x90
; SKL-NEXT:  .LBB6_1: # %header
; SKL-NEXT:    # =>This Inner Loop Header: Depth=1
; SKL-NEXT:    vmovd %eax, %xmm3
; SKL-NEXT:    vpbroadcastd %xmm3, %ymm3
; SKL-NEXT:    vpmulld %ymm2, %ymm3, %ymm3
; SKL-NEXT:    vmovdqa %ymm1, %ymm4
; SKL-NEXT:    vxorps %xmm5, %xmm5, %xmm5
; SKL-NEXT:    vgatherdps %ymm4, 4(%rdi,%ymm3), %ymm5
; SKL-NEXT:    vaddps %ymm0, %ymm5, %ymm0
; SKL-NEXT:    incq %rax
; SKL-NEXT:    cmpq $16, %rax
; SKL-NEXT:    jne .LBB6_1
; SKL-NEXT:  # %bb.2: # %exit
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_7:
; SKL-32:       # %bb.0: # %entry
; SKL-32-NEXT:    pushl %esi
; SKL-32-NEXT:    .cfi_def_cfa_offset 8
; SKL-32-NEXT:    .cfi_offset %esi, -8
; SKL-32-NEXT:    xorl %eax, %eax
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm2 = [16000,16000,16000,16000,16000,16000,16000,16000]
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm1, %ymm1
; SKL-32-NEXT:    xorl %edx, %edx
; SKL-32-NEXT:    .p2align 4, 0x90
; SKL-32-NEXT:  .LBB6_1: # %header
; SKL-32-NEXT:    # =>This Inner Loop Header: Depth=1
; SKL-32-NEXT:    vmovd %eax, %xmm3
; SKL-32-NEXT:    vpbroadcastd %xmm3, %ymm3
; SKL-32-NEXT:    vpmulld %ymm2, %ymm3, %ymm3
; SKL-32-NEXT:    vmovdqa %ymm1, %ymm4
; SKL-32-NEXT:    vxorps %xmm5, %xmm5, %xmm5
; SKL-32-NEXT:    vgatherdps %ymm4, 4(%ecx,%ymm3), %ymm5
; SKL-32-NEXT:    vaddps %ymm0, %ymm5, %ymm0
; SKL-32-NEXT:    addl $1, %eax
; SKL-32-NEXT:    adcl $0, %edx
; SKL-32-NEXT:    movl %eax, %esi
; SKL-32-NEXT:    xorl $16, %esi
; SKL-32-NEXT:    orl %edx, %esi
; SKL-32-NEXT:    jne .LBB6_1
; SKL-32-NEXT:  # %bb.2: # %exit
; SKL-32-NEXT:    popl %esi
; SKL-32-NEXT:    .cfi_def_cfa_offset 4
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_7:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    xorl %eax, %eax
; SKX-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [16000,16000,16000,16000,16000,16000,16000,16000]
; SKX-NEXT:    .p2align 4, 0x90
; SKX-NEXT:  .LBB6_1: # %header
; SKX-NEXT:    # =>This Inner Loop Header: Depth=1
; SKX-NEXT:    vpbroadcastd %eax, %ymm2
; SKX-NEXT:    vpmulld %ymm1, %ymm2, %ymm2
; SKX-NEXT:    vxorps %xmm3, %xmm3, %xmm3
; SKX-NEXT:    kmovq %k1, %k2
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm2), %ymm3 {%k2}
; SKX-NEXT:    vaddps %ymm0, %ymm3, %ymm0
; SKX-NEXT:    incq %rax
; SKX-NEXT:    cmpq $16, %rax
; SKX-NEXT:    jne .LBB6_1
; SKX-NEXT:  # %bb.2: # %exit
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_7:
; SKX-32:       # %bb.0: # %entry
; SKX-32-NEXT:    pushl %esi
; SKX-32-NEXT:    .cfi_def_cfa_offset 8
; SKX-32-NEXT:    .cfi_offset %esi, -8
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    xorl %eax, %eax
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; SKX-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [16000,16000,16000,16000,16000,16000,16000,16000]
; SKX-32-NEXT:    xorl %edx, %edx
; SKX-32-NEXT:    .p2align 4, 0x90
; SKX-32-NEXT:  .LBB6_1: # %header
; SKX-32-NEXT:    # =>This Inner Loop Header: Depth=1
; SKX-32-NEXT:    vpbroadcastd %eax, %ymm2
; SKX-32-NEXT:    vpmulld %ymm1, %ymm2, %ymm2
; SKX-32-NEXT:    kmovq %k1, %k2
; SKX-32-NEXT:    vxorps %xmm3, %xmm3, %xmm3
; SKX-32-NEXT:    vgatherdps 4(%ecx,%ymm2), %ymm3 {%k2}
; SKX-32-NEXT:    vaddps %ymm0, %ymm3, %ymm0
; SKX-32-NEXT:    addl $1, %eax
; SKX-32-NEXT:    adcl $0, %edx
; SKX-32-NEXT:    movl %eax, %esi
; SKX-32-NEXT:    xorl $16, %esi
; SKX-32-NEXT:    orl %edx, %esi
; SKX-32-NEXT:    jne .LBB6_1
; SKX-32-NEXT:  # %bb.2: # %exit
; SKX-32-NEXT:    popl %esi
; SKX-32-NEXT:    .cfi_def_cfa_offset 4
; SKX-32-NEXT:    retl
entry:
  br label %header

header:
  %iv = phi i64 [0, %entry], [%iv.next, %header]
  %sum.prev = phi <8 x float> [%init, %entry], [%sum, %header]
  %vec_iv = insertelement <8 x i64> poison, i64 %iv, i64 0
  %splat_iv = shufflevector <8 x i64> %vec_iv, <8 x i64> poison, <8 x i32> zeroinitializer
  %gep = getelementptr [4000 x float], [4000 x float] *%base, <8 x i64> %splat_iv, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %gather = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  %sum = fadd <8 x float> %gather, %sum.prev
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 16
  br i1 %exitcond, label %exit, label %header
exit:
  ret <8 x float> %sum
}

declare <8 x float> @llvm.masked.gather.v8float(<8 x float*> %ptrs, i32 %align, <8 x i1> %masks, <8 x float> %passthru)
