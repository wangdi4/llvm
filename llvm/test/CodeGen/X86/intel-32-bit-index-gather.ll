; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mcpu=skylake -mtriple=x86_64-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKL
; RUN: llc -mcpu=skylake -mtriple=i686-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKL-32
; RUN: llc -mcpu=skx -mtriple=x86_64-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKX
; RUN: llc -mcpu=skx -mtriple=i686-pc-linux-gnu < %s | FileCheck %s --check-prefixes=SKX-32

define <8 x float> @test_f32v8_1(float *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_1:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, (%rdi,%ymm1,4), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_1:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-32-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, (%eax,%ymm1,4), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_1:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps (%rdi,%ymm1,4), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_1:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps (%eax,%ymm1,4), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff>
  %gep = getelementptr float, float *%base, <8 x i64> %idx
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F0 = type { i16, float }

define <8 x float> @test_f32v8_2(%F0 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_2:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vandps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1,8), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_2:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vandps {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1,8), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_2:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1,8), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_2:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1,8), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F0, %F0 *%base, <8 x i64> %idx, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F1 = type { float, i16, i16, i16 }
define <8 x float> @test_f32v8_3(%F1 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_3:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [12,12,12,12,12,12,12,12]
; SKL-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, (%rdi,%ymm1), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_3:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [12,12,12,12,12,12,12,12]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, (%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_3:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKX-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps (%rdi,%ymm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_3:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps (%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F1, %F1 *%base, <8 x i64> %idx, <8 x i32><i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0, i32 0>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

%F20 = type { i16, float, i16, i16 }
%F2 = type { %F20, i16, i16, i16 }
define <8 x float> @test_f32v8_4(%F2 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_4:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_4:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_4:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKX-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_4:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F2, %F2 *%base, <8 x i64> %idx, i32 0, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

define <8 x float> @test_f32v8_5(%F2 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_5:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vpbroadcastq {{.*#+}} ymm3 = [20,20,20,20]
; SKL-NEXT:    vpmuldq %ymm3, %ymm0, %ymm0
; SKL-NEXT:    vmovq %rdi, %xmm4
; SKL-NEXT:    vpbroadcastq %xmm4, %ymm4
; SKL-NEXT:    vpaddq %ymm0, %ymm4, %ymm0
; SKL-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; SKL-NEXT:    vpmuldq %ymm3, %ymm1, %ymm1
; SKL-NEXT:    vpaddq %ymm1, %ymm4, %ymm1
; SKL-NEXT:    vextracti128 $1, %ymm2, %xmm3
; SKL-NEXT:    vpxor %xmm4, %xmm4, %xmm4
; SKL-NEXT:    vxorps %xmm5, %xmm5, %xmm5
; SKL-NEXT:    vgatherqps %xmm3, 4(,%ymm1), %xmm5
; SKL-NEXT:    vgatherqps %xmm2, 4(,%ymm0), %xmm4
; SKL-NEXT:    vinsertf128 $1, %xmm5, %ymm4, %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_5:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovdqa {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermd %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermd %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKL-32-NEXT:    vpbroadcastd {{.*#+}} ymm1 = [20,20,20,20,20,20,20,20]
; SKL-32-NEXT:    vpmulld %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_5:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpandq {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %zmm0, %zmm0
; SKX-NEXT:    vpbroadcastq %rdi, %zmm1
; SKX-NEXT:    vpmuldq {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; SKX-NEXT:    vpaddq %zmm0, %zmm1, %zmm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherqps 4(,%zmm1), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_5:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}, %ymm0, %ymm0
; SKX-32-NEXT:    vpmulld {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0x7fffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff, i64 u0xffffff>
  %gep = getelementptr %F2, %F2 *%base, <8 x i64> %idx, i32 0, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

define <8 x float> @test_f32v8_6(%F0 *%base, <8 x i64> %i, <8 x i1> %mask) {
; SKL-LABEL: test_f32v8_6:
; SKL:       # %bb.0:
; SKL-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-NEXT:    vgatherdps %ymm2, 4(%rdi,%ymm1,8), %ymm0
; SKL-NEXT:    retq
;
; SKL-32-LABEL: test_f32v8_6:
; SKL-32:       # %bb.0:
; SKL-32-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; SKL-32-NEXT:    vpslld $31, %ymm2, %ymm2
; SKL-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKL-32-NEXT:    vmovaps {{.*#+}} ymm3 = [0,2,4,6,4,6,6,7]
; SKL-32-NEXT:    vpermps %ymm0, %ymm3, %ymm0
; SKL-32-NEXT:    vpermps %ymm1, %ymm3, %ymm1
; SKL-32-NEXT:    vinsertf128 $1, %xmm1, %ymm0, %ymm0
; SKL-32-NEXT:    vbroadcastss {{.*#+}} ymm1 = [268435455,268435455,268435455,268435455,268435455,268435455,268435455,268435455]
; SKL-32-NEXT:    vandps %ymm1, %ymm0, %ymm1
; SKL-32-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; SKL-32-NEXT:    vgatherdps %ymm2, 4(%eax,%ymm1,8), %ymm0
; SKL-32-NEXT:    retl
;
; SKX-LABEL: test_f32v8_6:
; SKX:       # %bb.0:
; SKX-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-NEXT:    vpmovw2m %xmm1, %k1
; SKX-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %ymm0, %ymm1
; SKX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-NEXT:    vgatherdps 4(%rdi,%ymm1,8), %ymm0 {%k1}
; SKX-NEXT:    retq
;
; SKX-32-LABEL: test_f32v8_6:
; SKX-32:       # %bb.0:
; SKX-32-NEXT:    vpsllw $15, %xmm1, %xmm1
; SKX-32-NEXT:    vpmovw2m %xmm1, %k1
; SKX-32-NEXT:    movl {{[0-9]+}}(%esp), %eax
; SKX-32-NEXT:    vpmovqd %zmm0, %ymm0
; SKX-32-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}{1to8}, %ymm0, %ymm1
; SKX-32-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; SKX-32-NEXT:    vgatherdps 4(%eax,%ymm1,8), %ymm0 {%k1}
; SKX-32-NEXT:    retl
  %idx = and <8 x i64> %i, <i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff, i64 u0xfffffff>
  %vec_base = insertelement <8 x %F0*> poison, %F0* %base, i64 0
  %splat_base = shufflevector <8 x %F0*> %vec_base, <8 x %F0*> poison, <8 x i32> zeroinitializer
  %gep = getelementptr %F0, <8 x %F0*> %splat_base, <8 x i64> %idx, <8 x i32><i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %res = call <8 x float> @llvm.masked.gather.v8float(<8 x float*> %gep, i32 0, <8 x i1> %mask, <8 x float> undef)
  ret <8 x float> %res
}

declare <8 x float> @llvm.masked.gather.v8float(<8 x float*> %ptrs, i32 %align, <8 x i1> %masks, <8 x float> %passthru)
