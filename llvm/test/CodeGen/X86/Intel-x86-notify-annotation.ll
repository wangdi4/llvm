; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
;RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu | FileCheck %s -check-prefix=X64
;RUN: llc < %s -mtriple=x86_64-windows | FileCheck %s -check-prefix=X64Win
;RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu | FileCheck %s -check-prefix=X64
;RUN: llc < %s -mtriple=x86_64-windows | FileCheck %s -check-prefix=X64Win

@foo.lock2 = internal global i32 2, align 4
@foo.lock3 = internal global i32 3, align 4
@.str = private unnamed_addr constant [18 x i8] c"__itt_sync_create\00", align 1
@.str.1 = private unnamed_addr constant [20 x i8] c"__itt_sync_acquired\00", align 1
@.str.2 = private unnamed_addr constant [19 x i8] c"__itt_sync_destroy\00", align 1

define dso_local i32 @foo(i32 %zc, ptr %lock) {
; X64-LABEL: foo:
; X64:       # %bb.0: # %entry
; X64-NEXT:    subq $24, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 32
; X64-NEXT:    movl %edi, {{[0-9]+}}(%rsp)
; X64-NEXT:    movq %rsi, {{[0-9]+}}(%rsp)
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    je .LBB0_2
; X64-NEXT:  # %bb.1: # %if.then
; X64-NEXT:    #MEMBARRIER
; X64-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; X64-NEXT:  .Lnotify_nzc1:
; X64-NEXT:    #__notify_intrinsic(__itt_sync_create, dwarf::DW_OP_reg0)
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:  .Lnotify_nzc_probe1:
; X64-NEXT:    jmp .LBB0_3
; X64-NEXT:  .LBB0_2: # %if.else
; X64-NEXT:    #MEMBARRIER
; X64-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; X64-NEXT:    addq $4, %rax
; X64-NEXT:  .Lnotify_nzc0:
; X64-NEXT:    #__notify_intrinsic(__itt_sync_acquired, dwarf::DW_OP_reg0)
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:  .Lnotify_nzc_probe0:
; X64-NEXT:  .LBB0_3: # %if.end
; X64-NEXT:    movslq {{[0-9]+}}(%rsp), %rax
; X64-NEXT:    addq {{[0-9]+}}(%rsp), %rax
; X64-NEXT:  .Lnotify_zc0:
; X64-NEXT:    #__notify_zc_intrinsic(__itt_sync_create, dwarf::DW_OP_reg0)
; X64-NEXT:  .Lnotify_zc_probe0:
; X64-NEXT:    movl $foo.lock2, %eax
; X64-NEXT:    movl $foo.lock3, %ecx
; X64-NEXT:    movl foo.lock3(%rip), %edx
; X64-NEXT:    addl %edx, foo.lock2(%rip)
; X64-NEXT:    #MEMBARRIER
; X64-NEXT:  .Lnotify_nzc2:
; X64-NEXT:    #__notify_intrinsic(__itt_sync_destroy, dwarf::DW_OP_reg0)
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:    nop
; X64-NEXT:  .Lnotify_nzc_probe2:
; X64-NEXT:  .Lnotify_zc1:
; X64-NEXT:    #__notify_zc_intrinsic(__itt_sync_destroy, dwarf::DW_OP_reg2)
; X64-NEXT:  .Lnotify_zc_probe1:
; X64-NEXT:    movq {{[0-9]+}}(%rsp), %rdi
; X64-NEXT:    movl $foo.lock2, %esi
; X64-NEXT:    movl $foo.lock3, %edx
; X64-NEXT:    callq bar
; X64-NEXT:    xorl %eax, %eax
; X64-NEXT:    addq $24, %rsp
; X64-NEXT:    .cfi_def_cfa_offset 8
; X64-NEXT:    retq
;
; X64Win-LABEL: foo:
; X64Win:       # %bb.0: # %entry
; X64Win-NEXT:    subq $56, %rsp
; X64Win-NEXT:    .seh_stackalloc 56
; X64Win-NEXT:    .seh_endprologue
; X64Win-NEXT:    movl %ecx, {{[0-9]+}}(%rsp)
; X64Win-NEXT:    movq %rdx, {{[0-9]+}}(%rsp)
; X64Win-NEXT:    testl %ecx, %ecx
; X64Win-NEXT:    je .LBB0_2
; X64Win-NEXT:  # %bb.1: # %if.then
; X64Win-NEXT:    #MEMBARRIER
; X64Win-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; X64Win-NEXT:  .Lnotify_nzc1:
; X64Win-NEXT:    #__notify_intrinsic(__itt_sync_create, dwarf::DW_OP_reg0)
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:  .Lnotify_nzc_probe1:
; X64Win-NEXT:    jmp .LBB0_3
; X64Win-NEXT:  .LBB0_2: # %if.else
; X64Win-NEXT:    #MEMBARRIER
; X64Win-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; X64Win-NEXT:    addq $4, %rax
; X64Win-NEXT:  .Lnotify_nzc0:
; X64Win-NEXT:    #__notify_intrinsic(__itt_sync_acquired, dwarf::DW_OP_reg0)
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:  .Lnotify_nzc_probe0:
; X64Win-NEXT:  .LBB0_3: # %if.end
; X64Win-NEXT:    movslq {{[0-9]+}}(%rsp), %rax
; X64Win-NEXT:    addq {{[0-9]+}}(%rsp), %rax
; X64Win-NEXT:  .Lnotify_zc0:
; X64Win-NEXT:    #__notify_zc_intrinsic(__itt_sync_create, dwarf::DW_OP_reg0)
; X64Win-NEXT:  .Lnotify_zc_probe0:
; X64Win-NEXT:    leaq foo.lock2(%rip), %rdx
; X64Win-NEXT:    leaq foo.lock3(%rip), %r8
; X64Win-NEXT:    movl foo.lock3(%rip), %eax
; X64Win-NEXT:    addl %eax, foo.lock2(%rip)
; X64Win-NEXT:    #MEMBARRIER
; X64Win-NEXT:  .Lnotify_nzc2:
; X64Win-NEXT:    #__notify_intrinsic(__itt_sync_destroy, dwarf::DW_OP_reg1)
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:    nop
; X64Win-NEXT:  .Lnotify_nzc_probe2:
; X64Win-NEXT:  .Lnotify_zc1:
; X64Win-NEXT:    #__notify_zc_intrinsic(__itt_sync_destroy, dwarf::DW_OP_reg8)
; X64Win-NEXT:  .Lnotify_zc_probe1:
; X64Win-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; X64Win-NEXT:    callq bar
; X64Win-NEXT:    xorl %eax, %eax
; X64Win-NEXT:    addq $56, %rsp
; X64Win-NEXT:    retq
; X64Win-NEXT:    .seh_endproc
entry:
  %zc.addr = alloca i32, align 4
  %lock.addr = alloca ptr, align 8
  store i32 %zc, ptr %zc.addr, align 4
  store ptr %lock, ptr %lock.addr, align 8
  %0 = load i32, ptr %zc.addr, align 4
  %tobool = icmp ne i32 %0, 0
  br i1 %tobool, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  fence syncscope("singlethread") seq_cst
  %1 = load ptr, ptr %lock.addr, align 8
  call void @llvm.notify.nzc(ptr @.str, ptr %1)
  br label %if.end

if.else:                                          ; preds = %entry
  fence syncscope("singlethread") seq_cst
  %2 = load ptr, ptr %lock.addr, align 8
  %add.ptr = getelementptr i8, ptr %2, i64 4
  call void @llvm.notify.nzc(ptr @.str.1, ptr %add.ptr)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %3 = load ptr, ptr %lock.addr, align 8
  %4 = load i32, ptr %zc.addr, align 4
  %idx.ext = sext i32 %4 to i64
  %add.ptr1 = getelementptr i8, ptr %3, i64 %idx.ext
  call void @llvm.notify.zc(ptr @.str, ptr %add.ptr1)
  %5 = load i32, ptr @foo.lock2, align 4
  %6 = load i32, ptr @foo.lock3, align 4
  %add = add nsw i32 %5, %6
  store i32 %add, ptr @foo.lock2, align 4
  fence syncscope("singlethread") seq_cst
  call void @llvm.notify.nzc(ptr @.str.2, ptr @foo.lock2)
  call void @llvm.notify.zc(ptr @.str.2, ptr @foo.lock3)
  %7 = load ptr, ptr %lock.addr, align 8
  call void @bar(ptr %7, ptr @foo.lock2, ptr @foo.lock3)
  ret i32 0
}

; Function Attrs: noduplicate nounwind willreturn memory(inaccessiblemem: readwrite)
declare void @llvm.notify.nzc(ptr, ptr) #0

; Function Attrs: noduplicate nounwind willreturn memory(inaccessiblemem: readwrite)
declare void @llvm.notify.zc(ptr, ptr) #0

declare dso_local void @bar(ptr, ptr, ptr)

attributes #0 = { noduplicate nounwind willreturn memory(inaccessiblemem: readwrite) }
