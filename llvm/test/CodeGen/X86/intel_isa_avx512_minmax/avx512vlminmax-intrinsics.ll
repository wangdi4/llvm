; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_minmax
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512minmax | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512vl,+avx512minmax | FileCheck %s --check-prefixes=X86

define <8 x i16> @test_int_x86_avx512minmax_vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxnepbf16128:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7f,0x08,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxnepbf16128:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7f,0x08,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512minmax.vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i32 127)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512minmax.vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i32 %C)

define <8 x i16> @test_int_x86_avx512minmax_mask_vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, <8 x i16> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxnepbf16128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7f,0x09,0x52,0xd1,0x7f]
; X64-NEXT:    vmovdqa %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxnepbf16128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7f,0x09,0x52,0xd1,0x7f]
; X86-NEXT:    vmovdqa %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x6f,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i32 127, <8 x i16> %C, i8 %D)
  ret <8 x i16> %ret
}

declare <8 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i32 %C, <8 x i16> %D, i8 %E)

define <8 x i16> @test_int_x86_avx512minmax_maskz_vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxnepbf16128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7f,0x89,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxnepbf16128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxnepbf16 $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7f,0x89,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16128(<8 x i16> %A, <8 x i16> %B, i32 127, <8 x i16> zeroinitializer, i8 %C)
  ret <8 x i16> %ret
}

define <16 x i16> @test_int_x86_avx512minmax_vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxnepbf16256:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7f,0x28,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxnepbf16256:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7f,0x28,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i16> @llvm.x86.avx512minmax.vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i32 127)
  ret <16 x i16> %ret
}

declare <16 x i16> @llvm.x86.avx512minmax.vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i32 %C)

define <16 x i16> @test_int_x86_avx512minmax_mask_vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, <16 x i16> %C, i16 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxnepbf16256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7f,0x29,0x52,0xd1,0x7f]
; X64-NEXT:    vmovdqa %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxnepbf16256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7f,0x29,0x52,0xd1,0x7f]
; X86-NEXT:    vmovdqa %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x6f,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i32 127, <16 x i16> %C, i16 %D)
  ret <16 x i16> %ret
}

declare <16 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i32 %C, <16 x i16> %D, i16 %E)

define <16 x i16> @test_int_x86_avx512minmax_maskz_vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxnepbf16256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7f,0xa9,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxnepbf16256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxnepbf16 $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7f,0xa9,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x i16> @llvm.x86.avx512minmax.mask.vminmaxnepbf16256(<16 x i16> %A, <16 x i16> %B, i32 127, <16 x i16> zeroinitializer, i16 %C)
  ret <16 x i16> %ret
}

define <2 x double> @test_int_x86_avx512minmax_vminmaxpd128(<2 x double> %A, <2 x double> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxpd128:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0xfd,0x08,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxpd128:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0xfd,0x08,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x double> @llvm.x86.avx512minmax.vminmaxpd128(<2 x double> %A, <2 x double> %B, i32 127)
  ret <2 x double> %ret
}

declare <2 x double> @llvm.x86.avx512minmax.vminmaxpd128(<2 x double> %A, <2 x double> %B, i32 %C)

define <2 x double> @test_int_x86_avx512minmax_mask_vminmaxpd128(<2 x double> %A, <2 x double> %B, <2 x double> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxpd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0xfd,0x09,0x52,0xd1,0x7f]
; X64-NEXT:    vmovapd %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxpd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0xfd,0x09,0x52,0xd1,0x7f]
; X86-NEXT:    vmovapd %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x double> @llvm.x86.avx512minmax.mask.vminmaxpd128(<2 x double> %A, <2 x double> %B, i32 127, <2 x double> %C, i8 %D)
  ret <2 x double> %ret
}

declare <2 x double> @llvm.x86.avx512minmax.mask.vminmaxpd128(<2 x double> %A, <2 x double> %B, i32 %C, <2 x double> %D, i8 %E)

define <2 x double> @test_int_x86_avx512minmax_maskz_vminmaxpd128(<2 x double> %A, <2 x double> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxpd128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0xfd,0x89,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxpd128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxpd $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0xfd,0x89,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <2 x double> @llvm.x86.avx512minmax.mask.vminmaxpd128(<2 x double> %A, <2 x double> %B, i32 127, <2 x double> zeroinitializer, i8 %C)
  ret <2 x double> %ret
}

define <4 x double> @test_int_x86_avx512minmax_vminmaxpd256(<4 x double> %A, <4 x double> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxpd256:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0xfd,0x28,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxpd256:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0xfd,0x28,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x double> @llvm.x86.avx512minmax.vminmaxpd256(<4 x double> %A, <4 x double> %B, i32 127)
  ret <4 x double> %ret
}

declare <4 x double> @llvm.x86.avx512minmax.vminmaxpd256(<4 x double> %A, <4 x double> %B, i32 %C)

define <4 x double> @test_int_x86_avx512minmax_mask_vminmaxpd256(<4 x double> %A, <4 x double> %B, <4 x double> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxpd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0xfd,0x29,0x52,0xd1,0x7f]
; X64-NEXT:    vmovapd %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxpd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0xfd,0x29,0x52,0xd1,0x7f]
; X86-NEXT:    vmovapd %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x double> @llvm.x86.avx512minmax.mask.vminmaxpd256(<4 x double> %A, <4 x double> %B, i32 127, <4 x double> %C, i8 %D)
  ret <4 x double> %ret
}

declare <4 x double> @llvm.x86.avx512minmax.mask.vminmaxpd256(<4 x double> %A, <4 x double> %B, i32 %C, <4 x double> %D, i8 %E)

define <4 x double> @test_int_x86_avx512minmax_maskz_vminmaxpd256(<4 x double> %A, <4 x double> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxpd256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0xfd,0xa9,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxpd256:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxpd $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0xfd,0xa9,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x double> @llvm.x86.avx512minmax.mask.vminmaxpd256(<4 x double> %A, <4 x double> %B, i32 127, <4 x double> zeroinitializer, i8 %C)
  ret <4 x double> %ret
}

define <8 x half> @test_int_x86_avx512minmax_vminmaxph128(<8 x half> %A, <8 x half> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxph128:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7c,0x08,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxph128:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7c,0x08,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512minmax.vminmaxph128(<8 x half> %A, <8 x half> %B, i32 127)
  ret <8 x half> %ret
}

declare <8 x half> @llvm.x86.avx512minmax.vminmaxph128(<8 x half> %A, <8 x half> %B, i32 %C)

define <8 x half> @test_int_x86_avx512minmax_mask_vminmaxph128(<8 x half> %A, <8 x half> %B, <8 x half> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7c,0x09,0x52,0xd1,0x7f]
; X64-NEXT:    vmovaps %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7c,0x09,0x52,0xd1,0x7f]
; X86-NEXT:    vmovaps %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512minmax.mask.vminmaxph128(<8 x half> %A, <8 x half> %B, i32 127, <8 x half> %C, i8 %D)
  ret <8 x half> %ret
}

declare <8 x half> @llvm.x86.avx512minmax.mask.vminmaxph128(<8 x half> %A, <8 x half> %B, i32 %C, <8 x half> %D, i8 %E)

define <8 x half> @test_int_x86_avx512minmax_maskz_vminmaxph128(<8 x half> %A, <8 x half> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxph128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7c,0x89,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxph128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxph $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7c,0x89,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512minmax.mask.vminmaxph128(<8 x half> %A, <8 x half> %B, i32 127, <8 x half> zeroinitializer, i8 %C)
  ret <8 x half> %ret
}

define <16 x half> @test_int_x86_avx512minmax_vminmaxph256(<16 x half> %A, <16 x half> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxph256:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7c,0x28,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxph256:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7c,0x28,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512minmax.vminmaxph256(<16 x half> %A, <16 x half> %B, i32 127)
  ret <16 x half> %ret
}

declare <16 x half> @llvm.x86.avx512minmax.vminmaxph256(<16 x half> %A, <16 x half> %B, i32 %C)

define <16 x half> @test_int_x86_avx512minmax_mask_vminmaxph256(<16 x half> %A, <16 x half> %B, <16 x half> %C, i16 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7c,0x29,0x52,0xd1,0x7f]
; X64-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7c,0x29,0x52,0xd1,0x7f]
; X86-NEXT:    vmovaps %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512minmax.mask.vminmaxph256(<16 x half> %A, <16 x half> %B, i32 127, <16 x half> %C, i16 %D)
  ret <16 x half> %ret
}

declare <16 x half> @llvm.x86.avx512minmax.mask.vminmaxph256(<16 x half> %A, <16 x half> %B, i32 %C, <16 x half> %D, i16 %E)

define <16 x half> @test_int_x86_avx512minmax_maskz_vminmaxph256(<16 x half> %A, <16 x half> %B, i16 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxph256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7c,0xa9,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxph256:
; X86:       # %bb.0:
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxph $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7c,0xa9,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512minmax.mask.vminmaxph256(<16 x half> %A, <16 x half> %B, i32 127, <16 x half> zeroinitializer, i16 %C)
  ret <16 x half> %ret
}

define <4 x float> @test_int_x86_avx512minmax_vminmaxps128(<4 x float> %A, <4 x float> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxps128:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7d,0x08,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxps128:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf3,0x7d,0x08,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512minmax.vminmaxps128(<4 x float> %A, <4 x float> %B, i32 127)
  ret <4 x float> %ret
}

declare <4 x float> @llvm.x86.avx512minmax.vminmaxps128(<4 x float> %A, <4 x float> %B, i32 %C)

define <4 x float> @test_int_x86_avx512minmax_mask_vminmaxps128(<4 x float> %A, <4 x float> %B, <4 x float> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7d,0x09,0x52,0xd1,0x7f]
; X64-NEXT:    vmovapd %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxps128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm2 {%k1} # encoding: [0x62,0xf3,0x7d,0x09,0x52,0xd1,0x7f]
; X86-NEXT:    vmovapd %xmm2, %xmm0 # EVEX TO VEX Compression encoding: [0xc5,0xf9,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512minmax.mask.vminmaxps128(<4 x float> %A, <4 x float> %B, i32 127, <4 x float> %C, i8 %D)
  ret <4 x float> %ret
}

declare <4 x float> @llvm.x86.avx512minmax.mask.vminmaxps128(<4 x float> %A, <4 x float> %B, i32 %C, <4 x float> %D, i8 %E)

define <4 x float> @test_int_x86_avx512minmax_maskz_vminmaxps128(<4 x float> %A, <4 x float> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7d,0x89,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxps128:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxps $127, %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf3,0x7d,0x89,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512minmax.mask.vminmaxps128(<4 x float> %A, <4 x float> %B, i32 127, <4 x float> zeroinitializer, i8 %C)
  ret <4 x float> %ret
}

define <8 x float> @test_int_x86_avx512minmax_vminmaxps256(<8 x float> %A, <8 x float> %B) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_vminmaxps256:
; X64:       # %bb.0:
; X64-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7d,0x28,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_vminmaxps256:
; X86:       # %bb.0:
; X86-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf3,0x7d,0x28,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512minmax.vminmaxps256(<8 x float> %A, <8 x float> %B, i32 127)
  ret <8 x float> %ret
}

declare <8 x float> @llvm.x86.avx512minmax.vminmaxps256(<8 x float> %A, <8 x float> %B, i32 %C)

define <8 x float> @test_int_x86_avx512minmax_mask_vminmaxps256(<8 x float> %A, <8 x float> %B, <8 x float> %C, i8 %D) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_mask_vminmaxps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7d,0x29,0x52,0xd1,0x7f]
; X64-NEXT:    vmovapd %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x28,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_mask_vminmaxps256:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm2 {%k1} # encoding: [0x62,0xf3,0x7d,0x29,0x52,0xd1,0x7f]
; X86-NEXT:    vmovapd %ymm2, %ymm0 # EVEX TO VEX Compression encoding: [0xc5,0xfd,0x28,0xc2]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512minmax.mask.vminmaxps256(<8 x float> %A, <8 x float> %B, i32 127, <8 x float> %C, i8 %D)
  ret <8 x float> %ret
}

declare <8 x float> @llvm.x86.avx512minmax.mask.vminmaxps256(<8 x float> %A, <8 x float> %B, i32 %C, <8 x float> %D, i8 %E)

define <8 x float> @test_int_x86_avx512minmax_maskz_vminmaxps256(<8 x float> %A, <8 x float> %B, i8 %C) nounwind {
; X64-LABEL: test_int_x86_avx512minmax_maskz_vminmaxps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7d,0xa9,0x52,0xc1,0x7f]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512minmax_maskz_vminmaxps256:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vminmaxps $127, %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf3,0x7d,0xa9,0x52,0xc1,0x7f]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512minmax.mask.vminmaxps256(<8 x float> %A, <8 x float> %B, i32 127, <8 x float> zeroinitializer, i8 %C)
  ret <8 x float> %ret
}


