; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512vl,avx512dq | FileCheck %s

define <2 x i64> @bitselectxor_v2i64(<2 x i64> %x, <2 x i64> %y, <2 x i64> %m) {
; CHECK-LABEL: bitselectxor_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa %xmm2, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm0, %xmm1, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm1, %xmm0, %xmm2
; CHECK-NEXT:    vpmullq %xmm2, %xmm3, %xmm0
; CHECK-NEXT:    retq
  %a = xor <2 x i64> %x, %y
  %b = and <2 x i64> %a, %m
  %c = xor <2 x i64> %b, %x
  %d = xor <2 x i64> %b, %y
  %e = mul <2 x i64> %c, %d
  ret <2 x i64> %e
}

define <4 x i64> @bitselectxor_v4i64(<4 x i64> %x, <4 x i64> %y, <4 x i64> %m) {
; CHECK-LABEL: bitselectxor_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa %ymm2, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm0, %ymm1, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm1, %ymm0, %ymm2
; CHECK-NEXT:    vpmullq %ymm2, %ymm3, %ymm0
; CHECK-NEXT:    retq
  %a = xor <4 x i64> %x, %y
  %b = and <4 x i64> %a, %m
  %c = xor <4 x i64> %b, %x
  %d = xor <4 x i64> %b, %y
  %e = mul <4 x i64> %c, %d
  ret <4 x i64> %e
}

define <8 x i64> @bitselectxor_v8i64(<8 x i64> %x, <8 x i64> %y, <8 x i64> %m) {
; CHECK-LABEL: bitselectxor_v8i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa64 %zmm2, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm0, %zmm1, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm1, %zmm0, %zmm2
; CHECK-NEXT:    vpmullq %zmm2, %zmm3, %zmm0
; CHECK-NEXT:    retq
  %a = xor <8 x i64> %x, %y
  %b = and <8 x i64> %a, %m
  %c = xor <8 x i64> %b, %x
  %d = xor <8 x i64> %b, %y
  %e = mul <8 x i64> %c, %d
  ret <8 x i64> %e
}

; These ANDs get converted to blend shuffles then converted back to AND with
; a different type during shuffle lowering.
define <2 x i64> @bitselectxor_v2i64_bitcast(<2 x i64> %x, <2 x i64> %y) {
; CHECK-LABEL: bitselectxor_v2i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa {{.*#+}} xmm2 = [255,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0]
; CHECK-NEXT:    vmovdqa %xmm2, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm0, %xmm1, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm1, %xmm0, %xmm2
; CHECK-NEXT:    vpmullq %xmm2, %xmm3, %xmm0
; CHECK-NEXT:    retq
  %a = xor <2 x i64> %x, %y
  %b = and <2 x i64> %a, <i64 255, i64 255>
  %c = xor <2 x i64> %b, %x
  %d = xor <2 x i64> %b, %y
  %e = mul <2 x i64> %c, %d
  ret <2 x i64> %e
}

define <4 x i64> @bitselectxor_v4i64_bitcast(<4 x i64> %x, <4 x i64> %y) {
; CHECK-LABEL: bitselectxor_v4i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa {{.*#+}} ymm2 = [255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0]
; CHECK-NEXT:    vmovdqa %ymm2, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm0, %ymm1, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm1, %ymm0, %ymm2
; CHECK-NEXT:    vpmullq %ymm2, %ymm3, %ymm0
; CHECK-NEXT:    retq
  %a = xor <4 x i64> %x, %y
  %b = and <4 x i64> %a, <i64 16777215, i64 16777215, i64 16777215, i64 16777215>
  %c = xor <4 x i64> %b, %x
  %d = xor <4 x i64> %b, %y
  %e = mul <4 x i64> %c, %d
  ret <4 x i64> %e
}

define <8 x i64> @bitselectxor_v8i64_bitcast(<8 x i64> %x, <8 x i64> %y) {
; CHECK-LABEL: bitselectxor_v8i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpbroadcastq {{.*#+}} zmm2 = [18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081]
; CHECK-NEXT:    vmovdqa64 %zmm2, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm0, %zmm1, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm1, %zmm0, %zmm2
; CHECK-NEXT:    vpmullq %zmm2, %zmm3, %zmm0
; CHECK-NEXT:    retq
  %a = xor <8 x i64> %x, %y
  %b = and <8 x i64> %a, <i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535>
  %c = xor <8 x i64> %b, %x
  %d = xor <8 x i64> %b, %y
  %e = mul <8 x i64> %c, %d
  ret <8 x i64> %e
}

; make sure we recognize the (y ^ ((x ^ y) & m) and (x ^ ((x ^ y) & m)) as
; two blend instructions. We need to test both at the same time so the AND will
; have two uses. There is a generic DAG combine to rewrite (y ^ ((x ^ y) & m) to
; (x & m) | (y & ~m) which will get turned into a blend shuffle. That combine
; doesn't kick in if the AND has 2 uses so we end up converting just the AND
; to a blend with zero. So we have an X86 DAG combine to fix it.
define <2 x i64> @bitselectblend_v2i64(<2 x i64> %x, <2 x i64> %y, <2 x i64> %m) {
; CHECK-LABEL: bitselectblend_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpblendd {{.*#+}} xmm2 = xmm1[0],xmm0[1],xmm1[2],xmm0[3]
; CHECK-NEXT:    vpblendd {{.*#+}} xmm0 = xmm0[0],xmm1[1],xmm0[2],xmm1[3]
; CHECK-NEXT:    vpmullq %xmm0, %xmm2, %xmm0
; CHECK-NEXT:    retq
  %a = xor <2 x i64> %x, %y
  %b = and <2 x i64> %a, <i64 4294967295, i64 4294967295>
  %c = xor <2 x i64> %b, %x
  %d = xor <2 x i64> %b, %y
  %e = mul <2 x i64> %c, %d ; just to combine the two xors
  ret <2 x i64> %e
}

define <4 x i64> @bitselectblend_v4i64(<4 x i64> %x, <4 x i64> %y, <4 x i64> %m) {
; CHECK-LABEL: bitselectblend_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpblendw {{.*#+}} ymm2 = ymm0[0],ymm1[1,2,3],ymm0[4],ymm1[5,6,7],ymm0[8],ymm1[9,10,11],ymm0[12],ymm1[13,14,15]
; CHECK-NEXT:    vpblendw {{.*#+}} ymm0 = ymm1[0],ymm0[1,2,3],ymm1[4],ymm0[5,6,7],ymm1[8],ymm0[9,10,11],ymm1[12],ymm0[13,14,15]
; CHECK-NEXT:    vpmullq %ymm0, %ymm2, %ymm0
; CHECK-NEXT:    retq
  %a = xor <4 x i64> %x, %y
  %b = and <4 x i64> %a, <i64 -65536, i64 -65536, i64 -65536, i64 -65536>
  %c = xor <4 x i64> %b, %x
  %d = xor <4 x i64> %b, %y
  %e = mul <4 x i64> %c, %d ; just to combine the two xors
  ret <4 x i64> %e
}
