; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512vl,avx512dq | FileCheck %s

define <2 x i64> @bitselectxor_v2i64(<2 x i64> %x, <2 x i64> %y, <2 x i64> %m) {
; CHECK-LABEL: bitselectxor_v2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa %xmm2, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm0, %xmm1, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm1, %xmm0, %xmm2
; CHECK-NEXT:    vpmullq %xmm2, %xmm3, %xmm0
; CHECK-NEXT:    retq
  %a = xor <2 x i64> %x, %y
  %b = and <2 x i64> %a, %m
  %c = xor <2 x i64> %b, %x
  %d = xor <2 x i64> %b, %y
  %e = mul <2 x i64> %c, %d
  ret <2 x i64> %e
}

define <4 x i64> @bitselectxor_v4i64(<4 x i64> %x, <4 x i64> %y, <4 x i64> %m) {
; CHECK-LABEL: bitselectxor_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa %ymm2, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm0, %ymm1, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm1, %ymm0, %ymm2
; CHECK-NEXT:    vpmullq %ymm2, %ymm3, %ymm0
; CHECK-NEXT:    retq
  %a = xor <4 x i64> %x, %y
  %b = and <4 x i64> %a, %m
  %c = xor <4 x i64> %b, %x
  %d = xor <4 x i64> %b, %y
  %e = mul <4 x i64> %c, %d
  ret <4 x i64> %e
}

define <8 x i64> @bitselectxor_v8i64(<8 x i64> %x, <8 x i64> %y, <8 x i64> %m) {
; CHECK-LABEL: bitselectxor_v8i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa64 %zmm2, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm0, %zmm1, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm1, %zmm0, %zmm2
; CHECK-NEXT:    vpmullq %zmm2, %zmm3, %zmm0
; CHECK-NEXT:    retq
  %a = xor <8 x i64> %x, %y
  %b = and <8 x i64> %a, %m
  %c = xor <8 x i64> %b, %x
  %d = xor <8 x i64> %b, %y
  %e = mul <8 x i64> %c, %d
  ret <8 x i64> %e
}

; These ANDs get converted to blend shuffles then converted back to AND with
; a different type during shuffle lowering.
define <2 x i64> @bitselectxor_v2i64_bitcast(<2 x i64> %x, <2 x i64> %y) {
; CHECK-LABEL: bitselectxor_v2i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa {{.*#+}} xmm2 = [255,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0]
; CHECK-NEXT:    vmovdqa %xmm2, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm0, %xmm1, %xmm3
; CHECK-NEXT:    vpternlogq $202, %xmm1, %xmm0, %xmm2
; CHECK-NEXT:    vpmullq %xmm2, %xmm3, %xmm0
; CHECK-NEXT:    retq
  %a = xor <2 x i64> %x, %y
  %b = and <2 x i64> %a, <i64 255, i64 255>
  %c = xor <2 x i64> %b, %x
  %d = xor <2 x i64> %b, %y
  %e = mul <2 x i64> %c, %d
  ret <2 x i64> %e
}

define <4 x i64> @bitselectxor_v4i64_bitcast(<4 x i64> %x, <4 x i64> %y) {
; CHECK-LABEL: bitselectxor_v4i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovdqa {{.*#+}} ymm2 = [255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0,255,255,255,0,0,0,0,0]
; CHECK-NEXT:    vmovdqa %ymm2, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm0, %ymm1, %ymm3
; CHECK-NEXT:    vpternlogq $202, %ymm1, %ymm0, %ymm2
; CHECK-NEXT:    vpmullq %ymm2, %ymm3, %ymm0
; CHECK-NEXT:    retq
  %a = xor <4 x i64> %x, %y
  %b = and <4 x i64> %a, <i64 16777215, i64 16777215, i64 16777215, i64 16777215>
  %c = xor <4 x i64> %b, %x
  %d = xor <4 x i64> %b, %y
  %e = mul <4 x i64> %c, %d
  ret <4 x i64> %e
}

define <8 x i64> @bitselectxor_v8i64_bitcast(<8 x i64> %x, <8 x i64> %y) {
; CHECK-LABEL: bitselectxor_v8i64_bitcast:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vpbroadcastq {{.*#+}} zmm2 = [18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081,18446744073709486081]
; CHECK-NEXT:    vmovdqa64 %zmm2, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm0, %zmm1, %zmm3
; CHECK-NEXT:    vpternlogq $202, %zmm1, %zmm0, %zmm2
; CHECK-NEXT:    vpmullq %zmm2, %zmm3, %zmm0
; CHECK-NEXT:    retq
  %a = xor <8 x i64> %x, %y
  %b = and <8 x i64> %a, <i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535, i64 -65535>
  %c = xor <8 x i64> %b, %x
  %d = xor <8 x i64> %b, %y
  %e = mul <8 x i64> %c, %d
  ret <8 x i64> %e
}
