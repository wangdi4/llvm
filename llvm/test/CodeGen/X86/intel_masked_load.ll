; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse2    | FileCheck %s --check-prefixes=SSE2
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse4.2  | FileCheck %s --check-prefixes=SSE4
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx2    | FileCheck %s --check-prefixes=AVX2

define <12 x i8> @mload_constmask_v12i8_undef_passthrough(<12 x i8>* %addr) {
; SSE2-LABEL: mload_constmask_v12i8_undef_passthrough:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movzbl 7(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    movzbl 6(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm1
; SSE2-NEXT:    punpcklbw {{.*#+}} xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1],xmm1[2],xmm0[2],xmm1[3],xmm0[3],xmm1[4],xmm0[4],xmm1[5],xmm0[5],xmm1[6],xmm0[6],xmm1[7],xmm0[7]
; SSE2-NEXT:    movzbl 5(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    movzbl 4(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    punpcklbw {{.*#+}} xmm2 = xmm2[0],xmm0[0],xmm2[1],xmm0[1],xmm2[2],xmm0[2],xmm2[3],xmm0[3],xmm2[4],xmm0[4],xmm2[5],xmm0[5],xmm2[6],xmm0[6],xmm2[7],xmm0[7]
; SSE2-NEXT:    punpcklwd {{.*#+}} xmm2 = xmm2[0],xmm1[0],xmm2[1],xmm1[1],xmm2[2],xmm1[2],xmm2[3],xmm1[3]
; SSE2-NEXT:    movzbl 3(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    movzbl 2(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm1
; SSE2-NEXT:    punpcklbw {{.*#+}} xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1],xmm1[2],xmm0[2],xmm1[3],xmm0[3],xmm1[4],xmm0[4],xmm1[5],xmm0[5],xmm1[6],xmm0[6],xmm1[7],xmm0[7]
; SSE2-NEXT:    movzbl 1(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    movzbl (%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    punpcklbw {{.*#+}} xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
; SSE2-NEXT:    punpcklwd {{.*#+}} xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1],xmm0[2],xmm1[2],xmm0[3],xmm1[3]
; SSE2-NEXT:    punpckldq {{.*#+}} xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1]
; SSE2-NEXT:    movzbl 9(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm1
; SSE2-NEXT:    movzbl 8(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    punpcklbw {{.*#+}} xmm2 = xmm2[0],xmm1[0],xmm2[1],xmm1[1],xmm2[2],xmm1[2],xmm2[3],xmm1[3],xmm2[4],xmm1[4],xmm2[5],xmm1[5],xmm2[6],xmm1[6],xmm2[7],xmm1[7]
; SSE2-NEXT:    movzbl 10(%rdi), %eax
; SSE2-NEXT:    movd %eax, %xmm1
; SSE2-NEXT:    punpcklwd {{.*#+}} xmm2 = xmm2[0],xmm1[0],xmm2[1],xmm1[1],xmm2[2],xmm1[2],xmm2[3],xmm1[3]
; SSE2-NEXT:    punpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm2[0]
; SSE2-NEXT:    retq
;
; SSE4-LABEL: mload_constmask_v12i8_undef_passthrough:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movzbl (%rdi), %eax
; SSE4-NEXT:    movd %eax, %xmm0
; SSE4-NEXT:    pinsrb $1, 1(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $2, 2(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $3, 3(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $4, 4(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $5, 5(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $6, 6(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $7, 7(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $8, 8(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $9, 9(%rdi), %xmm0
; SSE4-NEXT:    pinsrb $10, 10(%rdi), %xmm0
; SSE4-NEXT:    retq
;
; AVX2-LABEL: mload_constmask_v12i8_undef_passthrough:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    movzbl (%rdi), %eax
; AVX2-NEXT:    vmovd %eax, %xmm0
; AVX2-NEXT:    vpinsrb $1, 1(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $2, 2(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $3, 3(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $4, 4(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $5, 5(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $6, 6(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $7, 7(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $8, 8(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $9, 9(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    vpinsrb $10, 10(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    retq
  %res = call <12 x i8> @llvm.masked.load.v12i8.p0v12i8(<12 x i8>* %addr, i32 4, <12 x i1> <i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 0>, <12 x i8> undef)
  ret <12 x i8> %res
}

declare <12 x i8> @llvm.masked.load.v12i8.p0v12i8(<12 x i8>*, i32, <12 x i1>, <12 x i8>)

