; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse2    | FileCheck %s --check-prefixes=SSE2
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse4.2  | FileCheck %s --check-prefixes=SSE4
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx2    | FileCheck %s --check-prefixes=AVX2

define <12 x i8> @mload_constmask_v12i8_undef_passthrough(<12 x i8>* %addr) {
; SSE2-LABEL: mload_constmask_v12i8_undef_passthrough:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movsd {{.*#+}} xmm0 = mem[0],zero
; SSE2-NEXT:    movss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[0,0],xmm0[3,0]
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[0,1],xmm1[0,2]
; SSE2-NEXT:    retq
;
; SSE4-LABEL: mload_constmask_v12i8_undef_passthrough:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movq {{.*#+}} xmm0 = mem[0],zero
; SSE4-NEXT:    pinsrd $2, 8(%rdi), %xmm0
; SSE4-NEXT:    retq
;
; AVX2-LABEL: mload_constmask_v12i8_undef_passthrough:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; AVX2-NEXT:    vpinsrd $2, 8(%rdi), %xmm0, %xmm0
; AVX2-NEXT:    retq
  %res = call <12 x i8> @llvm.masked.load.v12i8.p0v12i8(<12 x i8>* %addr, i32 4, <12 x i1> <i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 1, i1 0>, <12 x i8> undef)
  ret <12 x i8> %res
}

declare <12 x i8> @llvm.masked.load.v12i8.p0v12i8(<12 x i8>*, i32, <12 x i1>, <12 x i8>)

