; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown | FileCheck %s

; Check the expansion of the complex multiply intrinsic. This only tests
; expansion for 32-bit floats, as the expansion should produce identical IR
; expansions save for ABI of calling __mulsc3, which is tested for each type
; individually in intel-complex-{32,64}bit.ll.

declare <2 x float> @llvm.intel.complex.fmul.v2f32(<2 x float>, <2 x float>)

; Generate a call to __mulsc3
define <2 x float> @intrinsic_slow_f32(<2 x float> %z, <2 x float> %w) {
; CHECK-LABEL: intrinsic_slow_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    movaps %xmm1, %xmm2
; CHECK-NEXT:    movaps %xmm0, %xmm1
; CHECK-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1],xmm0[1,1]
; CHECK-NEXT:    movaps %xmm2, %xmm3
; CHECK-NEXT:    shufps {{.*#+}} xmm3 = xmm3[1,1],xmm2[1,1]
; CHECK-NEXT:    callq __mulsc3@PLT
; CHECK-NEXT:    popq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    retq
  %mul = call <2 x float> @llvm.intel.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

; Do an expansion (because of fast-math flags).
define <2 x float> @intrinsic_implied_limited_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_implied_limited_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm1[1,1,3,3]
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vsubss {{.*}} %xmm4
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vaddss {{.*}} %xmm0
; CHECK-NEXT:    vinsertps {{.*#+}} xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
; CHECK-NEXT:    retq
  %mul = call nnan ninf <2 x float> @llvm.intel.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

; Do an expansion (because of complex-limited-range).
define <2 x float> @intrinsic_limited_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_limited_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm1[1,1,3,3]
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vsubss {{.*}} %xmm4
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vaddss {{.*}} %xmm0
; CHECK-NEXT:    vinsertps {{.*#+}} xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
; CHECK-NEXT:    retq
  %mul = call <2 x float> @llvm.intel.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w) #0
  ret <2 x float> %mul
}

; Do an expansion, and use the FMA (because of fast-math flags).
define <2 x float> @intrinsic_fast_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_fast_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm1[1,1,3,3]
; CHECK-NEXT:    vmulss %xmm3, %xmm2, %xmm4
; CHECK-NEXT:    vfmsub231ss {{.*#+}} xmm4 = (xmm0 * xmm1) - xmm4
; CHECK-NEXT:    vmulss %xmm3, %xmm0, %xmm0
; CHECK-NEXT:    vfmadd231ss {{.*#+}} xmm0 = (xmm2 * xmm1) + xmm0
; CHECK-NEXT:    vinsertps {{.*#+}} xmm0 = xmm4[0],xmm0[0],xmm4[2,3]
; CHECK-NEXT:    retq
  %mul = call fast <2 x float> @llvm.intel.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

attributes #0 = { "complex-limited-range"="true" }
attributes #1 = { "target-features"="+fma" }
