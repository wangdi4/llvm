; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512f -mattr=avx512vl -x86-enable-unaligned-vector-move | FileCheck %s -check-prefix=X64
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=avx512f -mattr=avx512vl -x86-enable-unaligned-vector-move | FileCheck %s -check-prefix=X86

define <8 x i32> @test_256_1(i8 * %addr) {
; CHECK-LABEL: test_256_1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0
; CHECK-NEXT:    retq
; X64-LABEL: test_256_1:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0
; X64-NEXT:    retq
;
; X86-LABEL: test_256_1:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %res = load <8 x i32>, <8 x i32>* %vaddr, align 1
  ret <8 x i32>%res
}

define <8 x i32> @test_256_2(i8 * %addr) {
; CHECK-LABEL: test_256_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_2:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_2:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %res = load <8 x i32>, <8 x i32>* %vaddr, align 32
  ret <8 x i32>%res
}

define void @test_256_3(i8 * %addr, <4 x i64> %data) {
; CHECK-LABEL: test_256_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_3:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_3:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x i64>*
  store <4 x i64>%data, <4 x i64>* %vaddr, align 32
  ret void
}

define void @test_256_4(i8 * %addr, <8 x i32> %data) {
; CHECK-LABEL: test_256_4:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_4:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_4:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i32>*
  store <8 x i32>%data, <8 x i32>* %vaddr, align 1
  ret void
}

define void @test_256_5(i8 * %addr, <8 x i32> %data) {
; CHECK-LABEL: test_256_5:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_5:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_5:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x i32>*
  store <8 x i32>%data, <8 x i32>* %vaddr, align 32
  ret void
}

define  <4 x i64> @test_256_6(i8 * %addr) {
; CHECK-LABEL: test_256_6:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_6:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_6:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %res = load <4 x i64>, <4 x i64>* %vaddr, align 32
  ret <4 x i64>%res
}

define void @test_256_7(i8 * %addr, <4 x i64> %data) {
; CHECK-LABEL: test_256_7:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_7:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_7:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x i64>*
  store <4 x i64>%data, <4 x i64>* %vaddr, align 1
  ret void
}

define <4 x i64> @test_256_8(i8 * %addr) {
; CHECK-LABEL: test_256_8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0
; CHECK-NEXT:    retq
; X64-LABEL: test_256_8:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0
; X64-NEXT:    retq
;
; X86-LABEL: test_256_8:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %res = load <4 x i64>, <4 x i64>* %vaddr, align 1
  ret <4 x i64>%res
}

define void @test_256_9(i8 * %addr, <4 x double> %data) {
; CHECK-LABEL: test_256_9:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_9:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_9:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x double>*
  store <4 x double>%data, <4 x double>* %vaddr, align 32
  ret void
}

define <4 x double> @test_256_10(i8 * %addr) {
; CHECK-LABEL: test_256_10:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_10:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_10:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x double>*
  %res = load <4 x double>, <4 x double>* %vaddr, align 32
  ret <4 x double>%res
}

define void @test_256_11(i8 * %addr, <8 x float> %data) {
; CHECK-LABEL: test_256_11:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_11:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi) # AlignMOV convert to UnAlignMOV
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_11:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax) # AlignMOV convert to UnAlignMOV
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x float>*
  store <8 x float>%data, <8 x float>* %vaddr, align 32
  ret void
}

define <8 x float> @test_256_12(i8 * %addr) {
; CHECK-LABEL: test_256_12:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_12:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0 # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_12:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0 # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x float>*
  %res = load <8 x float>, <8 x float>* %vaddr, align 32
  ret <8 x float>%res
}

define void @test_256_13(i8 * %addr, <4 x double> %data) {
; CHECK-LABEL: test_256_13:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_13:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_13:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x double>*
  store <4 x double>%data, <4 x double>* %vaddr, align 1
  ret void
}

define <4 x double> @test_256_14(i8 * %addr) {
; CHECK-LABEL: test_256_14:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0
; CHECK-NEXT:    retq
; X64-LABEL: test_256_14:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0
; X64-NEXT:    retq
;
; X86-LABEL: test_256_14:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <4 x double>*
  %res = load <4 x double>, <4 x double>* %vaddr, align 1
  ret <4 x double>%res
}

define void @test_256_15(i8 * %addr, <8 x float> %data) {
; CHECK-LABEL: test_256_15:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups %ymm0, (%rdi)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
; X64-LABEL: test_256_15:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm0, (%rdi)
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_256_15:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups %ymm0, (%eax)
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x float>*
  store <8 x float>%data, <8 x float>* %vaddr, align 1
  ret void
}

define <8 x float> @test_256_16(i8 * %addr) {
; CHECK-LABEL: test_256_16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovups (%rdi), %ymm0
; CHECK-NEXT:    retq
; X64-LABEL: test_256_16:
; X64:       # %bb.0:
; X64-NEXT:    vmovups (%rdi), %ymm0
; X64-NEXT:    retq
;
; X86-LABEL: test_256_16:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovups (%eax), %ymm0
; X86-NEXT:    retl
  %vaddr = bitcast i8* %addr to <8 x float>*
  %res = load <8 x float>, <8 x float>* %vaddr, align 1
  ret <8 x float>%res
}

define <8 x i32> @test_256_17(i8 * %addr, <8 x i32> %old, <8 x i32> %mask1) {
; CHECK-LABEL: test_256_17:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %ymm1, %ymm1, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_17:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %ymm1, %ymm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_17:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %ymm1, %ymm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <8 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %r = load <8 x i32>, <8 x i32>* %vaddr, align 32
  %res = select <8 x i1> %mask, <8 x i32> %r, <8 x i32> %old
  ret <8 x i32>%res
}

define <8 x i32> @test_256_18(i8 * %addr, <8 x i32> %old, <8 x i32> %mask1) {
; CHECK-LABEL: test_256_18:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %ymm1, %ymm1, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1}
; CHECK-NEXT:    retq
; X64-LABEL: test_256_18:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %ymm1, %ymm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test_256_18:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %ymm1, %ymm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %ymm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <8 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %r = load <8 x i32>, <8 x i32>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x i32> %r, <8 x i32> %old
  ret <8 x i32>%res
}

define <8 x i32> @test_256_19(i8 * %addr, <8 x i32> %mask1) {
; CHECK-LABEL: test_256_19:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %ymm0, %ymm0, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_19:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %ymm0, %ymm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_19:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %ymm0, %ymm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <8 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %r = load <8 x i32>, <8 x i32>* %vaddr, align 32
  %res = select <8 x i1> %mask, <8 x i32> %r, <8 x i32> zeroinitializer
  ret <8 x i32>%res
}

define <8 x i32> @test_256_20(i8 * %addr, <8 x i32> %mask1) {
; CHECK-LABEL: test_256_20:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %ymm0, %ymm0, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} {z}
; CHECK-NEXT:    retq
; X64-LABEL: test_256_20:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %ymm0, %ymm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %ymm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test_256_20:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %ymm0, %ymm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %ymm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <8 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <8 x i32>*
  %r = load <8 x i32>, <8 x i32>* %vaddr, align 1
  %res = select <8 x i1> %mask, <8 x i32> %r, <8 x i32> zeroinitializer
  ret <8 x i32>%res
}

define <4 x i64> @test_256_21(i8 * %addr, <4 x i64> %old, <4 x i64> %mask1) {
; CHECK-LABEL: test_256_21:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %ymm1, %ymm1, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_21:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %ymm1, %ymm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_21:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %ymm1, %ymm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %ymm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <4 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %r = load <4 x i64>, <4 x i64>* %vaddr, align 32
  %res = select <4 x i1> %mask, <4 x i64> %r, <4 x i64> %old
  ret <4 x i64>%res
}

define <4 x i64> @test_256_22(i8 * %addr, <4 x i64> %old, <4 x i64> %mask1) {
; CHECK-LABEL: test_256_22:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %ymm1, %ymm1, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1}
; CHECK-NEXT:    retq
; X64-LABEL: test_256_22:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %ymm1, %ymm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test_256_22:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %ymm1, %ymm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %ymm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <4 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %r = load <4 x i64>, <4 x i64>* %vaddr, align 1
  %res = select <4 x i1> %mask, <4 x i64> %r, <4 x i64> %old
  ret <4 x i64>%res
}

define <4 x i64> @test_256_23(i8 * %addr, <4 x i64> %mask1) {
; CHECK-LABEL: test_256_23:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %ymm0, %ymm0, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_256_23:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %ymm0, %ymm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_256_23:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %ymm0, %ymm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %ymm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <4 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %r = load <4 x i64>, <4 x i64>* %vaddr, align 32
  %res = select <4 x i1> %mask, <4 x i64> %r, <4 x i64> zeroinitializer
  ret <4 x i64>%res
}

define <4 x i64> @test_256_24(i8 * %addr, <4 x i64> %mask1) {
; CHECK-LABEL: test_256_24:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %ymm0, %ymm0, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} {z}
; CHECK-NEXT:    retq
; X64-LABEL: test_256_24:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %ymm0, %ymm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %ymm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test_256_24:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %ymm0, %ymm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %ymm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <4 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i64>*
  %r = load <4 x i64>, <4 x i64>* %vaddr, align 1
  %res = select <4 x i1> %mask, <4 x i64> %r, <4 x i64> zeroinitializer
  ret <4 x i64>%res
}

define <4 x i32> @test_128_17(i8 * %addr, <4 x i32> %old, <4 x i32> %mask1) {
; CHECK-LABEL: test_128_17:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %xmm1, %xmm1, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_128_17:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %xmm1, %xmm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_128_17:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %xmm1, %xmm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <4 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i32>*
  %r = load <4 x i32>, <4 x i32>* %vaddr, align 16
  %res = select <4 x i1> %mask, <4 x i32> %r, <4 x i32> %old
  ret <4 x i32>%res
}

define <4 x i32> @test_128_18(i8 * %addr, <4 x i32> %old, <4 x i32> %mask1) {
; CHECK-LABEL: test_128_18:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %xmm1, %xmm1, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1}
; CHECK-NEXT:    retq
; X64-LABEL: test_128_18:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %xmm1, %xmm1, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test_128_18:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %xmm1, %xmm1, %k1
; X86-NEXT:    vmovdqu32 (%eax), %xmm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <4 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i32>*
  %r = load <4 x i32>, <4 x i32>* %vaddr, align 1
  %res = select <4 x i1> %mask, <4 x i32> %r, <4 x i32> %old
  ret <4 x i32>%res
}

define <4 x i32> @test_128_19(i8 * %addr, <4 x i32> %mask1) {
; CHECK-LABEL: test_128_19:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_128_19:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %xmm0, %xmm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_128_19:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %xmm0, %xmm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <4 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i32>*
  %r = load <4 x i32>, <4 x i32>* %vaddr, align 16
  %res = select <4 x i1> %mask, <4 x i32> %r, <4 x i32> zeroinitializer
  ret <4 x i32>%res
}

define <4 x i32> @test_128_20(i8 * %addr, <4 x i32> %mask1) {
; CHECK-LABEL: test_128_20:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmd %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} {z}
; CHECK-NEXT:    retq
; X64-LABEL: test_128_20:
; X64:       # %bb.0:
; X64-NEXT:    vptestmd %xmm0, %xmm0, %k1
; X64-NEXT:    vmovdqu32 (%rdi), %xmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test_128_20:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmd %xmm0, %xmm0, %k1
; X86-NEXT:    vmovdqu32 (%eax), %xmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <4 x i32> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <4 x i32>*
  %r = load <4 x i32>, <4 x i32>* %vaddr, align 1
  %res = select <4 x i1> %mask, <4 x i32> %r, <4 x i32> zeroinitializer
  ret <4 x i32>%res
}

define <2 x i64> @test_128_21(i8 * %addr, <2 x i64> %old, <2 x i64> %mask1) {
; CHECK-LABEL: test_128_21:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %xmm1, %xmm1, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_128_21:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %xmm1, %xmm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_128_21:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %xmm1, %xmm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %xmm0 {%k1} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <2 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <2 x i64>*
  %r = load <2 x i64>, <2 x i64>* %vaddr, align 16
  %res = select <2 x i1> %mask, <2 x i64> %r, <2 x i64> %old
  ret <2 x i64>%res
}

define <2 x i64> @test_128_22(i8 * %addr, <2 x i64> %old, <2 x i64> %mask1) {
; CHECK-LABEL: test_128_22:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %xmm1, %xmm1, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1}
; CHECK-NEXT:    retq
; X64-LABEL: test_128_22:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %xmm1, %xmm1, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1}
; X64-NEXT:    retq
;
; X86-LABEL: test_128_22:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %xmm1, %xmm1, %k1
; X86-NEXT:    vmovdqu64 (%eax), %xmm0 {%k1}
; X86-NEXT:    retl
  %mask = icmp ne <2 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <2 x i64>*
  %r = load <2 x i64>, <2 x i64>* %vaddr, align 1
  %res = select <2 x i1> %mask, <2 x i64> %r, <2 x i64> %old
  ret <2 x i64>%res
}

define <2 x i64> @test_128_23(i8 * %addr, <2 x i64> %mask1) {
; CHECK-LABEL: test_128_23:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; CHECK-NEXT:    retq
; X64-LABEL: test_128_23:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %xmm0, %xmm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X64-NEXT:    retq
;
; X86-LABEL: test_128_23:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %xmm0, %xmm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %xmm0 {%k1} {z} # AlignMOV convert to UnAlignMOV
; X86-NEXT:    retl
  %mask = icmp ne <2 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <2 x i64>*
  %r = load <2 x i64>, <2 x i64>* %vaddr, align 16
  %res = select <2 x i1> %mask, <2 x i64> %r, <2 x i64> zeroinitializer
  ret <2 x i64>%res
}

define <2 x i64> @test_128_24(i8 * %addr, <2 x i64> %mask1) {
; CHECK-LABEL: test_128_24:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vptestmq %xmm0, %xmm0, %k1
; CHECK-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} {z}
; CHECK-NEXT:    retq
; X64-LABEL: test_128_24:
; X64:       # %bb.0:
; X64-NEXT:    vptestmq %xmm0, %xmm0, %k1
; X64-NEXT:    vmovdqu64 (%rdi), %xmm0 {%k1} {z}
; X64-NEXT:    retq
;
; X86-LABEL: test_128_24:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vptestmq %xmm0, %xmm0, %k1
; X86-NEXT:    vmovdqu64 (%eax), %xmm0 {%k1} {z}
; X86-NEXT:    retl
  %mask = icmp ne <2 x i64> %mask1, zeroinitializer
  %vaddr = bitcast i8* %addr to <2 x i64>*
  %r = load <2 x i64>, <2 x i64>* %vaddr, align 1
  %res = select <2 x i1> %mask, <2 x i64> %r, <2 x i64> zeroinitializer
  ret <2 x i64>%res
}
