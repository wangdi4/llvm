; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --no_x86_scrub_mem_shuffle --version 3
; RUN: llc < %s -mtriple=i386-unknown-linux-gnu -mattr=+sse3 | FileCheck %s --check-prefixes=X86-SSE3
; RUN: llc < %s -mtriple=i386-unknown-linux-gnu -mattr=+ssse3 | FileCheck %s --check-prefixes=X86-SSSE3
; RUN: llc < %s -mtriple=i386-unknown-linux-gnu -mattr=+avx | FileCheck %s --check-prefixes=X86-AVX-OR-AVX512,X86-AVX
; RUN: llc < %s -mtriple=i386-unknown-linux-gnu -mattr=+avx512f,+avx512bw,+avx512dq,+avx512vl | FileCheck %s --check-prefixes=X86-AVX-OR-AVX512,X86-AVX512
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+sse3 | FileCheck %s --check-prefixes=X64-SSE3
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+ssse3 | FileCheck %s --check-prefixes=X64-SSSE3
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx | FileCheck %s --check-prefixes=X64-AVX-OR-AVX512,X64-AVX
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx512f,+avx512bw,+avx512dq,+avx512vl | FileCheck %s --check-prefixes=X64-AVX-OR-AVX512,X64-AVX512

define void @move(ptr %from, ptr %to) {
; X86-SSE3-LABEL: move:
; X86-SSE3:       # %bb.0: # %entry
; X86-SSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSE3-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-SSE3-NEXT:    lddqu (%ecx), %xmm0
; X86-SSE3-NEXT:    movapd %xmm0, (%eax)
; X86-SSE3-NEXT:    retl
;
; X86-SSSE3-LABEL: move:
; X86-SSSE3:       # %bb.0: # %entry
; X86-SSSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSSE3-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-SSSE3-NEXT:    movups (%ecx), %xmm0
; X86-SSSE3-NEXT:    movaps %xmm0, (%eax)
; X86-SSSE3-NEXT:    retl
;
; X86-AVX-OR-AVX512-LABEL: move:
; X86-AVX-OR-AVX512:       # %bb.0: # %entry
; X86-AVX-OR-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX-OR-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-AVX-OR-AVX512-NEXT:    vmovups (%ecx), %xmm0
; X86-AVX-OR-AVX512-NEXT:    vmovaps %xmm0, (%eax)
; X86-AVX-OR-AVX512-NEXT:    retl
;
; X64-SSE3-LABEL: move:
; X64-SSE3:       # %bb.0: # %entry
; X64-SSE3-NEXT:    lddqu (%rdi), %xmm0
; X64-SSE3-NEXT:    movapd %xmm0, (%rsi)
; X64-SSE3-NEXT:    retq
;
; X64-SSSE3-LABEL: move:
; X64-SSSE3:       # %bb.0: # %entry
; X64-SSSE3-NEXT:    movups (%rdi), %xmm0
; X64-SSSE3-NEXT:    movaps %xmm0, (%rsi)
; X64-SSSE3-NEXT:    retq
;
; X64-AVX-OR-AVX512-LABEL: move:
; X64-AVX-OR-AVX512:       # %bb.0: # %entry
; X64-AVX-OR-AVX512-NEXT:    vmovups (%rdi), %xmm0
; X64-AVX-OR-AVX512-NEXT:    vmovaps %xmm0, (%rsi)
; X64-AVX-OR-AVX512-NEXT:    retq
entry:
  %0 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %from)
  store <16 x i8> %0, ptr %to
  ret void
}

define <4 x i64> @broadcast(ptr %from, ptr %to) {
; X86-SSE3-LABEL: broadcast:
; X86-SSE3:       # %bb.0: # %entry
; X86-SSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSE3-NEXT:    lddqu (%eax), %xmm0
; X86-SSE3-NEXT:    movapd %xmm0, %xmm1
; X86-SSE3-NEXT:    retl
;
; X86-SSSE3-LABEL: broadcast:
; X86-SSSE3:       # %bb.0: # %entry
; X86-SSSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSSE3-NEXT:    movups (%eax), %xmm0
; X86-SSSE3-NEXT:    movaps %xmm0, %xmm1
; X86-SSSE3-NEXT:    retl
;
; X86-AVX-LABEL: broadcast:
; X86-AVX:       # %bb.0: # %entry
; X86-AVX-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX-NEXT:    vbroadcastf128 (%eax), %ymm0 # ymm0 = mem[0,1,0,1]
; X86-AVX-NEXT:    retl
;
; X86-AVX512-LABEL: broadcast:
; X86-AVX512:       # %bb.0: # %entry
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vbroadcasti128 (%eax), %ymm0 # ymm0 = mem[0,1,0,1]
; X86-AVX512-NEXT:    retl
;
; X64-SSE3-LABEL: broadcast:
; X64-SSE3:       # %bb.0: # %entry
; X64-SSE3-NEXT:    lddqu (%rdi), %xmm0
; X64-SSE3-NEXT:    movapd %xmm0, %xmm1
; X64-SSE3-NEXT:    retq
;
; X64-SSSE3-LABEL: broadcast:
; X64-SSSE3:       # %bb.0: # %entry
; X64-SSSE3-NEXT:    movups (%rdi), %xmm0
; X64-SSSE3-NEXT:    movaps %xmm0, %xmm1
; X64-SSSE3-NEXT:    retq
;
; X64-AVX-LABEL: broadcast:
; X64-AVX:       # %bb.0: # %entry
; X64-AVX-NEXT:    vbroadcastf128 (%rdi), %ymm0 # ymm0 = mem[0,1,0,1]
; X64-AVX-NEXT:    retq
;
; X64-AVX512-LABEL: broadcast:
; X64-AVX512:       # %bb.0: # %entry
; X64-AVX512-NEXT:    vbroadcasti128 (%rdi), %ymm0 # ymm0 = mem[0,1,0,1]
; X64-AVX512-NEXT:    retq
entry:
  %0 = tail call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %from)
  %1 = bitcast <16 x i8> %0 to <2 x i64>
  %shuffle.i = shufflevector <2 x i64> %1, <2 x i64> poison, <4 x i32> <i32 0, i32 1, i32 0, i32 1>
  ret <4 x i64> %shuffle.i
}

define <2 x i64> @add(ptr %lhs, ptr %rhs) {
; X86-SSE3-LABEL: add:
; X86-SSE3:       # %bb.0: # %entry
; X86-SSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSE3-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-SSE3-NEXT:    lddqu (%ecx), %xmm1
; X86-SSE3-NEXT:    lddqu (%eax), %xmm0
; X86-SSE3-NEXT:    paddd %xmm1, %xmm0
; X86-SSE3-NEXT:    retl
;
; X86-SSSE3-LABEL: add:
; X86-SSSE3:       # %bb.0: # %entry
; X86-SSSE3-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-SSSE3-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-SSSE3-NEXT:    movdqu (%ecx), %xmm1
; X86-SSSE3-NEXT:    movdqu (%eax), %xmm0
; X86-SSSE3-NEXT:    paddd %xmm1, %xmm0
; X86-SSSE3-NEXT:    retl
;
; X86-AVX-OR-AVX512-LABEL: add:
; X86-AVX-OR-AVX512:       # %bb.0: # %entry
; X86-AVX-OR-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX-OR-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-AVX-OR-AVX512-NEXT:    vmovdqu (%ecx), %xmm0
; X86-AVX-OR-AVX512-NEXT:    vpaddd (%eax), %xmm0, %xmm0
; X86-AVX-OR-AVX512-NEXT:    retl
;
; X64-SSE3-LABEL: add:
; X64-SSE3:       # %bb.0: # %entry
; X64-SSE3-NEXT:    lddqu (%rdi), %xmm1
; X64-SSE3-NEXT:    lddqu (%rsi), %xmm0
; X64-SSE3-NEXT:    paddd %xmm1, %xmm0
; X64-SSE3-NEXT:    retq
;
; X64-SSSE3-LABEL: add:
; X64-SSSE3:       # %bb.0: # %entry
; X64-SSSE3-NEXT:    movdqu (%rdi), %xmm1
; X64-SSSE3-NEXT:    movdqu (%rsi), %xmm0
; X64-SSSE3-NEXT:    paddd %xmm1, %xmm0
; X64-SSSE3-NEXT:    retq
;
; X64-AVX-OR-AVX512-LABEL: add:
; X64-AVX-OR-AVX512:       # %bb.0: # %entry
; X64-AVX-OR-AVX512-NEXT:    vmovdqu (%rdi), %xmm0
; X64-AVX-OR-AVX512-NEXT:    vpaddd (%rsi), %xmm0, %xmm0
; X64-AVX-OR-AVX512-NEXT:    retq
entry:
  %0 = call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %lhs)
  %1 = bitcast <16 x i8> %0 to <4 x i32>
  %2 = call <16 x i8> @llvm.x86.sse3.ldu.dq(ptr %rhs)
  %3 = bitcast <16 x i8> %2 to <4 x i32>
  %4 = add <4 x i32> %1, %3
  %5 = bitcast <4 x i32> %4 to <2 x i64>
  ret <2 x i64> %5
}

declare <16 x i8> @llvm.x86.sse3.ldu.dq(ptr)
