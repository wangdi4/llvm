; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_ne_convert
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512neconvert,+avx512fp16 | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512neconvert,+avx512fp16 | FileCheck %s --check-prefixes=X86

define <16 x float> @test_int_x86_vbcstnebf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vbcstnebf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vbcstnebf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vbcstnebf162ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnebf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vbcstnebf162ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vbcstnebf162ps512(<16 x float> %W, i16 %A, i8* %B)

define <16 x float> @test_int_x86_vbcstnesh2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vbcstnesh2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vbcstnesh2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vbcstnesh2ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnesh2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vbcstnesh2ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vbcstnesh2ps512(<16 x float> %W, i16 %A, i8* %B)

define <32 x half> @test_int_x86_avx512_vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph512:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <32 x half> @llvm.x86.avx512.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, i32 4)
  ret <32 x half> %ret
}

define <32 x half> @test_int_x86_avx512_vcvtne2ps2ph512_round(<16 x float> %A, <16 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph512_round:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph {rz-sae}, %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x78,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph512_round:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph {rz-sae}, %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x78,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <32 x half> @llvm.x86.avx512.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, i32 11)
  ret <32 x half> %ret
}

declare <32 x half> @llvm.x86.avx512.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, i32 immarg)

define <16 x float> @test_int_x86_vcvtneebf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneebf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneebf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneebf162ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneebf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneebf162ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneebf162ps512(<16 x float> %W, i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneeph2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneeph2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneeph2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneeph2ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneeph2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneeph2ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneeph2ps512(<16 x float> %W, i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneobf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneobf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneobf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneobf162ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneobf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneobf162ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneobf162ps512(<16 x float> %W, i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneoph2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7c,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7c,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneoph2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneoph2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneoph2ps512(<16 x float> %W, i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneoph2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovw {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf8,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneoph2ps512(<16 x float> %W, i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneoph2ps512(<16 x float> %W, i16 %A, i8* %B)

