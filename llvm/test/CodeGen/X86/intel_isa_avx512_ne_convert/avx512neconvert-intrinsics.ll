; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_ne_convert
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512neconvert,+avx512fp16 | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512f,+avx512neconvert,+avx512fp16 | FileCheck %s --check-prefixes=X86

define <16 x float> @test_int_x86_vbcstnebf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vbcstnebf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vbcstnebf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vbcstnebf162ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnebf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnebf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vbcstnebf162ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vbcstnebf162ps512(i16 %A, i8* %B)

define <16 x float> @test_int_x86_vbcstnesh2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vbcstnesh2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vbcstnesh2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vbcstnesh2ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnesh2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnesh2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vbcstnesh2ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vbcstnesh2ps512(i16 %A, i8* %B)

define <32 x half> @test_int_x86_avx512_vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph512:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <32 x half> @llvm.x86.avx512.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B)
  ret <32 x half> %ret
}
declare <32 x half> @llvm.x86.avx512.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B)

define <32 x half> @test_int_x86_avx512_mask_vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D) {
; X64-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph512:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %zmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; X64-NEXT:    # encoding: [0x62,0xf1,0x7c,0x48,0x11,0x94,0x24,0x88,0xff,0xff,0xff]
; X64-NEXT:    vmovaps %zmm1, %zmm2 # encoding: [0x62,0xf1,0x7c,0x48,0x28,0xd1]
; X64-NEXT:    vmovaps %zmm0, %zmm1 # encoding: [0x62,0xf1,0x7c,0x48,0x28,0xc8]
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; X64-NEXT:    # encoding: [0x62,0xf1,0x7c,0x48,0x10,0x84,0x24,0x88,0xff,0xff,0xff]
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtne2ps2ph %zmm2, %zmm1, %zmm0 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0x67,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph512:
; X86:       # %bb.0:
; X86-NEXT:    subl $124, %esp # encoding: [0x83,0xec,0x7c]
; X86-NEXT:    .cfi_def_cfa_offset 128
; X86-NEXT:    vmovups %zmm2, (%esp) # 64-byte Spill
; X86-NEXT:    # encoding: [0x62,0xf1,0x7c,0x48,0x11,0x14,0x24]
; X86-NEXT:    vmovaps %zmm1, %zmm2 # encoding: [0x62,0xf1,0x7c,0x48,0x28,0xd1]
; X86-NEXT:    vmovaps %zmm0, %zmm1 # encoding: [0x62,0xf1,0x7c,0x48,0x28,0xc8]
; X86-NEXT:    vmovups (%esp), %zmm0 # 64-byte Reload
; X86-NEXT:    # encoding: [0x62,0xf1,0x7c,0x48,0x10,0x04,0x24]
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x84,0x24,0x80,0x00,0x00,0x00]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %zmm2, %zmm1, %zmm0 {%k1} # encoding: [0x62,0xf2,0x75,0x49,0x67,0xc2]
; X86-NEXT:    addl $124, %esp # encoding: [0x83,0xc4,0x7c]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <32 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D)
  ret <32 x half> %ret
}
declare <32 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D)

define <32 x half> @test_int_x86_avx512_maskz_vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D) {
; X64-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph512:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0xc9,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %zmm1, %zmm0, %zmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0xc9,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <32 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D)
  ret <32 x half> %ret
}
declare <32 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph512(<16 x float> %A, <16 x float> %B, <32 x half> %C, i32 %D)

define <16 x float> @test_int_x86_vcvtneebf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7e,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneebf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneebf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneebf162ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneebf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneebf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneebf162ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneebf162ps512(i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneeph2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7d,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneeph2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneeph2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneeph2ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneeph2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneeph2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneeph2ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneeph2ps512(i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneobf162ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7f,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneobf162ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneobf162ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneobf162ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneobf162ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneobf162ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneobf162ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneobf162ps512(i16 %A, i8* %B)

define <16 x float> @test_int_x86_vcvtneoph2ps512(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %zmm0 # encoding: [0x62,0xf2,0x7c,0x48,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %zmm0 # encoding: [0x62,0xf2,0x7c,0x48,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.vcvtneoph2ps512(i8* %A)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.vcvtneoph2ps512(i8* %A)

define <16 x float> @test_int_x86_mask_vcvtneoph2ps512(i16 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps512:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneoph2ps (%rsi), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps512:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %dx # encoding: [0x66,0x8b,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movw %dx, %cx # encoding: [0x66,0x89,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneoph2ps (%eax), %zmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x49,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x float> @llvm.x86.avx512.mask.vcvtneoph2ps512(i16 %A, i8* %B)
  ret <16 x float> %ret
}
declare <16 x float> @llvm.x86.avx512.mask.vcvtneoph2ps512(i16 %A, i8* %B)

