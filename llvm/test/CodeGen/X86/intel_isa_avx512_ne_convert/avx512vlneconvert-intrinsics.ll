; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_ne_convert
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512fp16,+avx512vl,+avx512neconvert | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -O0 -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512fp16,+avx512vl,+avx512neconvert | FileCheck %s --check-prefixes=X86

define <4 x float> @test_int_x86_vbcstnebf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vbcstnebf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vbcstnebf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vbcstnebf162ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnebf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnebf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vbcstnebf162ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vbcstnebf162ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vbcstnebf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vbcstnebf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vbcstnebf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vbcstnebf162ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnebf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnebf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vbcstnebf162ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vbcstnebf162ps256(i8 %A, i8* %B)

define <4 x float> @test_int_x86_vbcstnesh2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vbcstnesh2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vbcstnesh2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vbcstnesh2ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnesh2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnesh2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vbcstnesh2ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vbcstnesh2ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vbcstnesh2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vbcstnesh2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vbcstnesh2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vbcstnesh2ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vbcstnesh2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vbcstnesh2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vbcstnesh2ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vbcstnesh2ps256(i8 %A, i8* %B)

define <8 x half> @test_int_x86_avx512_vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph128:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B)
  ret <8 x half> %ret
}
declare <8 x half> @llvm.x86.avx512.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B)

define <8 x half> @test_int_x86_avx512_mask_vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D) {
; X64-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph128:
; X64:       # %bb.0:
; X64-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x29,0x54,0x24,0xe8]
; X64-NEXT:    vmovaps %xmm1, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xd1]
; X64-NEXT:    vmovaps %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc8]
; X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0x44,0x24,0xe8]
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtne2ps2ph %xmm2, %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x75,0x09,0x67,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph128:
; X86:       # %bb.0:
; X86-NEXT:    subl $28, %esp # encoding: [0x83,0xec,0x1c]
; X86-NEXT:    .cfi_def_cfa_offset 32
; X86-NEXT:    vmovups %xmm2, (%esp) # 16-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x11,0x14,0x24]
; X86-NEXT:    vmovaps %xmm1, %xmm2 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xd1]
; X86-NEXT:    vmovaps %xmm0, %xmm1 # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x28,0xc8]
; X86-NEXT:    vmovups (%esp), %xmm0 # 16-byte Reload
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xf8,0x10,0x04,0x24]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %cl # encoding: [0x8a,0x4c,0x24,0x20]
; X86-NEXT:    # implicit-def: $eax
; X86-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %xmm2, %xmm1, %xmm0 {%k1} # encoding: [0x62,0xf2,0x75,0x09,0x67,0xc2]
; X86-NEXT:    addl $28, %esp # encoding: [0x83,0xc4,0x1c]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D)
  ret <8 x half> %ret
}
declare <8 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D)

define <8 x half> @test_int_x86_avx512_maskz_vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D) {
; X64-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0x89,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph128:
; X86:       # %bb.0:
; X86-NEXT:    movb {{[0-9]+}}(%esp), %cl # encoding: [0x8a,0x4c,0x24,0x04]
; X86-NEXT:    # implicit-def: $eax
; X86-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0x89,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D)
  ret <8 x half> %ret
}
declare <8 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B, <8 x half> %C, i8 %D)


define <16 x half> @test_int_x86_avx512_vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph256:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B)
  ret <16 x half> %ret
}
declare <16 x half> @llvm.x86.avx512.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B)

define <16 x half> @test_int_x86_avx512_mask_vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D) {
; X64-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph256:
; X64:       # %bb.0:
; X64-NEXT:    vmovups %ymm2, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x54,0x24,0xc8]
; X64-NEXT:    vmovaps %ymm1, %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xd1]
; X64-NEXT:    vmovaps %ymm0, %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc8]
; X64-NEXT:    vmovups {{[-0-9]+}}(%r{{[sb]}}p), %ymm0 # 32-byte Reload
; X64-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x10,0x44,0x24,0xc8]
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtne2ps2ph %ymm2, %ymm1, %ymm0 {%k1} # encoding: [0x62,0xf2,0x75,0x29,0x67,0xc2]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_mask_vcvtne2ps2ph256:
; X86:       # %bb.0:
; X86-NEXT:    subl $60, %esp # encoding: [0x83,0xec,0x3c]
; X86-NEXT:    .cfi_def_cfa_offset 64
; X86-NEXT:    vmovups %ymm2, (%esp) # 32-byte Spill
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x11,0x14,0x24]
; X86-NEXT:    vmovaps %ymm1, %ymm2 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xd1]
; X86-NEXT:    vmovaps %ymm0, %ymm1 # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x28,0xc8]
; X86-NEXT:    vmovups (%esp), %ymm0 # 32-byte Reload
; X86-NEXT:    # EVEX TO VEX Compression encoding: [0xc5,0xfc,0x10,0x04,0x24]
; X86-NEXT:    movw {{[0-9]+}}(%esp), %cx # encoding: [0x66,0x8b,0x4c,0x24,0x40]
; X86-NEXT:    # implicit-def: $eax
; X86-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %ymm2, %ymm1, %ymm0 {%k1} # encoding: [0x62,0xf2,0x75,0x29,0x67,0xc2]
; X86-NEXT:    addl $60, %esp # encoding: [0x83,0xc4,0x3c]
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D)
  ret <16 x half> %ret
}
declare <16 x half> @llvm.x86.avx512.mask.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D)

define <16 x half> @test_int_x86_avx512_maskz_vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D) {
; X64-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph256:
; X64:       # %bb.0:
; X64-NEXT:    movw %di, %cx # encoding: [0x66,0x89,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0xa9,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_maskz_vcvtne2ps2ph256:
; X86:       # %bb.0:
; X86-NEXT:    movw {{[0-9]+}}(%esp), %cx # encoding: [0x66,0x8b,0x4c,0x24,0x04]
; X86-NEXT:    # implicit-def: $eax
; X86-NEXT:    movw %cx, %ax # encoding: [0x66,0x89,0xc8]
; X86-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X86-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 {%k1} {z} # encoding: [0x62,0xf2,0x7d,0xa9,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D)
  ret <16 x half> %ret
}
declare <16 x half> @llvm.x86.avx512.maskz.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B, <16 x half> %C, i16 %D)


define <4 x float> @test_int_x86_vcvtneebf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneebf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneebf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneebf162ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneebf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneebf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneebf162ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneebf162ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneebf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneebf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneebf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneebf162ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneebf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneebf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneebf162ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneebf162ps256(i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneeph2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneeph2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneeph2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneeph2ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneeph2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneeph2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneeph2ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneeph2ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneeph2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneeph2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneeph2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneeph2ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneeph2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneeph2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneeph2ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneeph2ps256(i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneobf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7f,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7f,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneobf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneobf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneobf162ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneobf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneobf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneobf162ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneobf162ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneobf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7f,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7f,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneobf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneobf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneobf162ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneobf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneobf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneobf162ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneobf162ps256(i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneoph2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7c,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7c,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneoph2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneoph2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneoph2ps128(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneoph2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneoph2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneoph2ps128(i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneoph2ps128(i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneoph2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7c,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7c,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneoph2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneoph2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneoph2ps256(i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    movb %dil, %cl # encoding: [0x40,0x88,0xf9]
; X64-NEXT:    # implicit-def: $eax
; X64-NEXT:    movb %cl, %al # encoding: [0x88,0xc8]
; X64-NEXT:    kmovd %eax, %k1 # encoding: [0xc5,0xfb,0x92,0xc8]
; X64-NEXT:    vcvtneoph2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    movb {{[0-9]+}}(%esp), %dl # encoding: [0x8a,0x54,0x24,0x04]
; X86-NEXT:    # implicit-def: $ecx
; X86-NEXT:    movb %dl, %cl # encoding: [0x88,0xd1]
; X86-NEXT:    kmovd %ecx, %k1 # encoding: [0xc5,0xfb,0x92,0xc9]
; X86-NEXT:    vcvtneoph2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneoph2ps256(i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneoph2ps256(i8 %A, i8* %B)

