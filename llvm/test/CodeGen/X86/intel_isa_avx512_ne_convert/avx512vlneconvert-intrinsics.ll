; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_avx512_ne_convert
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown --show-mc-encoding -mattr=+avx512fp16,+avx512vl,+avx512neconvert | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -verify-machineinstrs -mtriple=i686-unknown-unknown --show-mc-encoding -mattr=+avx512fp16,+avx512vl,+avx512neconvert | FileCheck %s --check-prefixes=X86

define <4 x float> @test_int_x86_vbcstnebf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vbcstnebf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vbcstnebf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vbcstnebf162ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnebf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vbcstnebf162ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vbcstnebf162ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vbcstnebf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnebf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vbcstnebf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vbcstnebf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vbcstnebf162ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnebf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnebf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vbcstnebf162ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vbcstnebf162ps256(<8 x float> %W, i8 %A, i8* %B)

define <4 x float> @test_int_x86_vbcstnesh2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vbcstnesh2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vbcstnesh2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vbcstnesh2ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnesh2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vbcstnesh2ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vbcstnesh2ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vbcstnesh2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vbcstnesh2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vbcstnesh2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb1,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vbcstnesh2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vbcstnesh2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vbcstnesh2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vbcstnesh2ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vbcstnesh2ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vbcstnesh2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb1,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vbcstnesh2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vbcstnesh2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb1,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vbcstnesh2ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vbcstnesh2ps256(<8 x float> %W, i8 %A, i8* %B)

define <8 x half> @test_int_x86_avx512_vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph128:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %xmm1, %xmm0, %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x half> @llvm.x86.avx512.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B)
  ret <8 x half> %ret
}
declare <8 x half> @llvm.x86.avx512.vcvtne2ps2ph128(<4 x float> %A, <4 x float> %B)

define <16 x half> @test_int_x86_avx512_vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B) {
; X64-LABEL: test_int_x86_avx512_vcvtne2ps2ph256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0x67,0xc1]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_avx512_vcvtne2ps2ph256:
; X86:       # %bb.0:
; X86-NEXT:    vcvtne2ps2ph %ymm1, %ymm0, %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0x67,0xc1]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <16 x half> @llvm.x86.avx512.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B)
  ret <16 x half> %ret
}
declare <16 x half> @llvm.x86.avx512.vcvtne2ps2ph256(<8 x float> %A, <8 x float> %B)

define <4 x float> @test_int_x86_vcvtneebf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7e,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneebf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneebf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneebf162ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneebf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneebf162ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneebf162ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneebf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneebf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7e,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneebf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneebf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneebf162ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneebf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneebf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneebf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneebf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7e,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneebf162ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneebf162ps256(<8 x float> %W, i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneeph2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7d,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneeph2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneeph2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneeph2ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneeph2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneeph2ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneeph2ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneeph2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneeph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneeph2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneeph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7d,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneeph2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneeph2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneeph2ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneeph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneeph2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneeph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneeph2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7d,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneeph2ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneeph2ps256(<8 x float> %W, i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneobf162ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7f,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7f,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneobf162ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneobf162ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneobf162ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneobf162ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneobf162ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneobf162ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneobf162ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneobf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneobf162ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7f,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneobf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7f,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneobf162ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneobf162ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneobf162ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneobf162ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneobf162ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneobf162ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneobf162ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7f,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneobf162ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneobf162ps256(<8 x float> %W, i8 %A, i8* %B)

define <4 x float> @test_int_x86_vcvtneoph2ps128(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %xmm0 # encoding: [0x62,0xf2,0x7c,0x08,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %xmm0 # encoding: [0x62,0xf2,0x7c,0x08,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.vcvtneoph2ps128(i8* %A)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.vcvtneoph2ps128(i8* %A)

define <4 x float> @test_int_x86_mask_vcvtneoph2ps128(<4 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps128:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneoph2ps (%rsi), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps128:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %xmm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x09,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <4 x float> @llvm.x86.avx512.mask.vcvtneoph2ps128(<4 x float> %W, i8 %A, i8* %B)
  ret <4 x float> %ret
}
declare <4 x float> @llvm.x86.avx512.mask.vcvtneoph2ps128(<4 x float> %W, i8 %A, i8* %B)

define <8 x float> @test_int_x86_vcvtneoph2ps256(i8* %A) {
; X64-LABEL: test_int_x86_vcvtneoph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    vcvtneoph2ps (%rdi), %ymm0 # encoding: [0x62,0xf2,0x7c,0x28,0xb0,0x07]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_vcvtneoph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %ymm0 # encoding: [0x62,0xf2,0x7c,0x28,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.vcvtneoph2ps256(i8* %A)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.vcvtneoph2ps256(i8* %A)

define <8 x float> @test_int_x86_mask_vcvtneoph2ps256(<8 x float> %W, i8 %A, i8* %B) {
; X64-LABEL: test_int_x86_mask_vcvtneoph2ps256:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1 # encoding: [0xc5,0xfb,0x92,0xcf]
; X64-NEXT:    vcvtneoph2ps (%rsi), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0xb0,0x06]
; X64-NEXT:    retq # encoding: [0xc3]
;
; X86-LABEL: test_int_x86_mask_vcvtneoph2ps256:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax # encoding: [0x8b,0x44,0x24,0x08]
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1 # encoding: [0xc5,0xf9,0x90,0x4c,0x24,0x04]
; X86-NEXT:    vcvtneoph2ps (%eax), %ymm0 {%k1} # encoding: [0x62,0xf2,0x7c,0x29,0xb0,0x00]
; X86-NEXT:    retl # encoding: [0xc3]
  %ret = call <8 x float> @llvm.x86.avx512.mask.vcvtneoph2ps256(<8 x float> %W, i8 %A, i8* %B)
  ret <8 x float> %ret
}
declare <8 x float> @llvm.x86.avx512.mask.vcvtneoph2ps256(<8 x float> %W, i8 %A, i8* %B)

