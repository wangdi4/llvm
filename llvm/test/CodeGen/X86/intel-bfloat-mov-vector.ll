; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; INTEL_FEATURE_ISA_BF16_BASE
; REQUIRES: intel_feature_isa_bf16_base
; TODO: Add SSE2 tests when vector emulation bfloat on SSE2 is supported.
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512fp16,+avx512vl | FileCheck %s --check-prefix=AVX512FP16-X64
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+avx512fp16,+avx512vl | FileCheck %s --check-prefix=AVX512FP16-X86

define <8 x bfloat> @broadcastph128(bfloat* %x) {
; AVX512FP16-X64-LABEL: broadcastph128:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    orl %eax, %ecx
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %rcx, %rax
; AVX512FP16-X64-NEXT:    vpbroadcastq %rax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph128:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl (%eax), %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpbroadcastd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    retl
  %l1 = load bfloat, bfloat* %x, align 2
  %vec = insertelement <8 x bfloat> undef, bfloat %l1, i32 0
  %res = shufflevector <8 x bfloat> %vec, <8 x bfloat> undef, <8 x i32> zeroinitializer
  ret <8 x bfloat> %res
}

define <16 x bfloat> @broadcastph256(bfloat* %x) {
; AVX512FP16-X64-LABEL: broadcastph256:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %eax
; AVX512FP16-X64-NEXT:    vpbroadcastw %eax, %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph256:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl (%eax), %eax
; AVX512FP16-X86-NEXT:    vpbroadcastw %eax, %ymm0
; AVX512FP16-X86-NEXT:    retl
  %l1 = load bfloat, bfloat* %x, align 2
  %vec = insertelement <16 x bfloat> undef, bfloat %l1, i32 0
  %res = shufflevector <16 x bfloat> %vec, <16 x bfloat> undef, <16 x i32> zeroinitializer
  ret <16 x bfloat> %res
}

define <32 x bfloat> @broadcastph512(bfloat* %x) {
; AVX512FP16-X64-LABEL: broadcastph512:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %eax
; AVX512FP16-X64-NEXT:    vpbroadcastw %eax, %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph512:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl (%eax), %eax
; AVX512FP16-X86-NEXT:    vpbroadcastw %eax, %zmm0
; AVX512FP16-X86-NEXT:    retl
  %l1 = load bfloat, bfloat* %x, align 2
  %vec = insertelement <32 x bfloat> undef, bfloat %l1, i32 0
  %res = shufflevector <32 x bfloat> %vec, <32 x bfloat> undef, <32 x i32> zeroinitializer
  ret <32 x bfloat> %res
}

define <8 x bfloat> @broadcastph128_scalar(bfloat %x) {
; AVX512FP16-X64-LABEL: broadcastph128_scalar:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    orl %eax, %ecx
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %rcx, %rax
; AVX512FP16-X64-NEXT:    vpbroadcastq %rax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph128_scalar:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpbroadcastd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    retl
  %vec = insertelement <8 x bfloat> undef, bfloat %x, i32 0
  %res = shufflevector <8 x bfloat> %vec, <8 x bfloat> undef, <8 x i32> zeroinitializer
  ret <8 x bfloat> %res
}

define <16 x bfloat> @broadcastph256_scalar(bfloat %x) {
; AVX512FP16-X64-LABEL: broadcastph256_scalar:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vpbroadcastw %eax, %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph256_scalar:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpbroadcastw %eax, %ymm0
; AVX512FP16-X86-NEXT:    retl
  %vec = insertelement <16 x bfloat> undef, bfloat %x, i32 0
  %res = shufflevector <16 x bfloat> %vec, <16 x bfloat> undef, <16 x i32> zeroinitializer
  ret <16 x bfloat> %res
}

define <32 x bfloat> @broadcastph512_scalar(bfloat %x) {
; AVX512FP16-X64-LABEL: broadcastph512_scalar:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vpbroadcastw %eax, %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph512_scalar:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpbroadcastw %eax, %zmm0
; AVX512FP16-X86-NEXT:    retl
  %vec = insertelement <32 x bfloat> undef, bfloat %x, i32 0
  %res = shufflevector <32 x bfloat> %vec, <32 x bfloat> undef, <32 x i32> zeroinitializer
  ret <32 x bfloat> %res
}

define <8 x bfloat> @broadcastph128_reg(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: broadcastph128_reg:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpbroadcastw %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph128_reg:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vpbroadcastw %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    retl
  %res = shufflevector <8 x bfloat> %x, <8 x bfloat> undef, <8 x i32> zeroinitializer
  ret <8 x bfloat> %res
}

define <16 x bfloat> @broadcastph256_reg(<16 x bfloat> %x) {
; AVX512FP16-X64-LABEL: broadcastph256_reg:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpbroadcastw %xmm0, %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph256_reg:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vpbroadcastw %xmm0, %ymm0
; AVX512FP16-X86-NEXT:    retl
  %res = shufflevector <16 x bfloat> %x, <16 x bfloat> undef, <16 x i32> zeroinitializer
  ret <16 x bfloat> %res
}

define <32 x bfloat> @broadcastph512_reg(<32 x bfloat> %x) {
; AVX512FP16-X64-LABEL: broadcastph512_reg:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpbroadcastw %xmm0, %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: broadcastph512_reg:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vpbroadcastw %xmm0, %zmm0
; AVX512FP16-X86-NEXT:    retl
  %res = shufflevector <32 x bfloat> %x, <32 x bfloat> undef, <32 x i32> zeroinitializer
  ret <32 x bfloat> %res
}

define <8 x bfloat> @test11(bfloat* %x) {
; AVX512FP16-X64-LABEL: test11:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovw (%rdi), %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test11:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovw (%eax), %xmm0
; AVX512FP16-X86-NEXT:    retl
   %y = load bfloat, bfloat* %x, align 2
   %res = insertelement <8 x bfloat>zeroinitializer, bfloat %y, i32 0
   ret <8 x bfloat>%res
}

define <16 x bfloat> @test11b(bfloat* %x) {
; AVX512FP16-X64-LABEL: test11b:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %eax
; AVX512FP16-X64-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test11b:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl (%eax), %eax
; AVX512FP16-X86-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X86-NEXT:    retl
   %y = load bfloat, bfloat* %x, align 2
   %res = insertelement <16 x bfloat>zeroinitializer, bfloat %y, i32 0
   ret <16 x bfloat>%res
}

define <32 x bfloat> @test11c(bfloat* %x) {
; AVX512FP16-X64-LABEL: test11c:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %eax
; AVX512FP16-X64-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test11c:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl (%eax), %eax
; AVX512FP16-X86-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X86-NEXT:    retl
   %y = load bfloat, bfloat* %x, align 2
   %res = insertelement <32 x bfloat>zeroinitializer, bfloat %y, i32 0
   ret <32 x bfloat>%res
}

define <8 x bfloat> @test14(bfloat %x) {
; AVX512FP16-X64-LABEL: test14:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test14:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovw {{[0-9]+}}(%esp), %xmm0
; AVX512FP16-X86-NEXT:    retl
   %res = insertelement <8 x bfloat>zeroinitializer, bfloat %x, i32 0
   ret <8 x bfloat>%res
}

define <16 x bfloat> @test14b(bfloat %x) {
; AVX512FP16-X64-LABEL: test14b:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test14b:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X86-NEXT:    retl
   %res = insertelement <16 x bfloat>zeroinitializer, bfloat %x, i32 0
   ret <16 x bfloat>%res
}

define <32 x bfloat> @test14c(bfloat %x) {
; AVX512FP16-X64-LABEL: test14c:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: test14c:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovw %eax, %xmm0
; AVX512FP16-X86-NEXT:    retl
   %res = insertelement <32 x bfloat>zeroinitializer, bfloat %x, i32 0
   ret <32 x bfloat>%res
}

@g8bf16 = external global <8 x bfloat>
@g8bf16u = external global <8 x bfloat>, align 8
@g16bf16 = external global <16 x bfloat>
@g16bf16u = external global <16 x bfloat>, align 8
@g32bf16 = external global <32 x bfloat>
@g32bf16u = external global <32 x bfloat>, align 8

define <32 x bfloat> @load32bf16(<32 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: load32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps (%rdi), %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovaps (%eax), %zmm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <32 x bfloat>, <32 x bfloat>* %a
  ret <32 x bfloat> %res
}

define <32 x bfloat> @load32bf16mask(<32 x bfloat>* %a, <32 x bfloat> %b, i32 %c) {
; AVX512FP16-X64-LABEL: load32bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $1016, %rsp # imm = 0x3F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 1072
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 48(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 50(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 52(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 54(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 56(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 58(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 60(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 62(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 32(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 34(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 36(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 38(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 40(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 42(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 44(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 46(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %r13d
; AVX512FP16-X64-NEXT:    shll $16, %r13d
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %r12d
; AVX512FP16-X64-NEXT:    shll $16, %r12d
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %r15d
; AVX512FP16-X64-NEXT:    shll $16, %r15d
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %r14d
; AVX512FP16-X64-NEXT:    shll $16, %r14d
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %r11d
; AVX512FP16-X64-NEXT:    shll $16, %r11d
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %r10d
; AVX512FP16-X64-NEXT:    shll $16, %r10d
; AVX512FP16-X64-NEXT:    movl (%rdi), %r9d
; AVX512FP16-X64-NEXT:    shll $16, %r9d
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %r8d
; AVX512FP16-X64-NEXT:    shll $16, %r8d
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %esi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %edi
; AVX512FP16-X64-NEXT:    shll $16, %edi
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r13d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r12d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r15d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r14d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r11d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r10d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r9d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r8d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $1016, %rsp # imm = 0x3F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load32bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $1136, %esp # imm = 0x470
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 1156
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %edi
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm2
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm2, %esi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm2, %ebp
; AVX512FP16-X86-NEXT:    vmovd %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm2
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ebx
; AVX512FP16-X86-NEXT:    movzwl 50(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm3
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm3, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm3
; AVX512FP16-X86-NEXT:    vmovdqu %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    vextractf32x4 $2, %zmm0, %xmm0
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, %edi
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 48(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 4(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl (%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 28(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 26(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 24(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 22(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 20(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 18(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 16(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 46(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 44(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 42(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 40(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 38(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 36(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 34(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 32(%ebx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 62(%ebx), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl 60(%ebx), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movzwl 58(%ebx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl 56(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movzwl 54(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 52(%ebx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    vmovss %xmm2, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm1, %xmm1 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $1136, %esp # imm = 0x470
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i32 %c to <32 x i1>
  %res0 = load <32 x bfloat>, <32 x bfloat>* %a
  %res = select <32 x i1> %msk, <32 x bfloat> %res0, <32 x bfloat> %b
  ret <32 x bfloat> %res
}

define <32 x bfloat> @load32bf16maskz(<32 x bfloat>* %a, i32 %c) {
; AVX512FP16-X64-LABEL: load32bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 592
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 48(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 50(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 52(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 54(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 56(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 58(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 60(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 62(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 32(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 34(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 36(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 38(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 40(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 42(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 44(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 46(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load32bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $704, %esp # imm = 0x2C0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 724
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edi
; AVX512FP16-X86-NEXT:    movl 48(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl (%edi), %ecx
; AVX512FP16-X86-NEXT:    movl 4(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 28(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 26(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 50(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    movl 24(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 22(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 20(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 18(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 16(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 46(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 44(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 42(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 40(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 38(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 36(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 34(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 32(%edi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 62(%edi), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 60(%edi), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movzwl 58(%edi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl 56(%edi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movzwl 54(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 52(%edi), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $704, %esp # imm = 0x2C0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i32 %c to <32 x i1>
  %res0 = load <32 x bfloat>, <32 x bfloat>* %a
  %res = select <32 x i1> %msk, <32 x bfloat> %res0, <32 x bfloat> zeroinitializer
  ret <32 x bfloat> %res
}

define <32 x bfloat> @loadu32bf16(<32 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: loadu32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovups (%rdi), %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovups (%eax), %zmm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <32 x bfloat>, <32 x bfloat>* %a, align 8
  ret <32 x bfloat> %res
}

define <32 x bfloat> @loadu32bf16mask(<32 x bfloat>* %a, <32 x bfloat> %b, i32 %c) {
; AVX512FP16-X64-LABEL: loadu32bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $1016, %rsp # imm = 0x3F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 1072
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 48(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 50(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 52(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 54(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 56(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 58(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 60(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 62(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 32(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 34(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 36(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 38(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 40(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 42(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 44(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 46(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %r13d
; AVX512FP16-X64-NEXT:    shll $16, %r13d
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %r12d
; AVX512FP16-X64-NEXT:    shll $16, %r12d
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %r15d
; AVX512FP16-X64-NEXT:    shll $16, %r15d
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %r14d
; AVX512FP16-X64-NEXT:    shll $16, %r14d
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %r11d
; AVX512FP16-X64-NEXT:    shll $16, %r11d
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %r10d
; AVX512FP16-X64-NEXT:    shll $16, %r10d
; AVX512FP16-X64-NEXT:    movl (%rdi), %r9d
; AVX512FP16-X64-NEXT:    shll $16, %r9d
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %r8d
; AVX512FP16-X64-NEXT:    shll $16, %r8d
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %esi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %edi
; AVX512FP16-X64-NEXT:    shll $16, %edi
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r13d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r12d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r15d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r14d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r11d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r10d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r9d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r8d, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $1016, %rsp # imm = 0x3F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu32bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $1136, %esp # imm = 0x470
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 1156
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %edi
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm2
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm2, %esi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm2, %ebp
; AVX512FP16-X86-NEXT:    vmovd %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm2
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ebx
; AVX512FP16-X86-NEXT:    movzwl 50(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm3
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm3, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm3
; AVX512FP16-X86-NEXT:    vmovdqu %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    vextractf32x4 $2, %zmm0, %xmm0
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, %edi
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 48(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 4(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl (%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 28(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 26(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 24(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 22(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 20(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 18(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 16(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 46(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 44(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 42(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 40(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 38(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 36(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 34(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 32(%ebx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 62(%ebx), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl 60(%ebx), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movzwl 58(%ebx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl 56(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movzwl 54(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 52(%ebx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    vmovss %xmm2, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm1, %xmm1 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $1136, %esp # imm = 0x470
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i32 %c to <32 x i1>
  %res0 = load <32 x bfloat>, <32 x bfloat>* %a, align 8
  %res = select <32 x i1> %msk, <32 x bfloat> %res0, <32 x bfloat> %b
  ret <32 x bfloat> %res
}

define <32 x bfloat> @loadu32bf16maskz(<32 x bfloat>* %a, i32 %c) {
; AVX512FP16-X64-LABEL: loadu32bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 592
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl 48(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 50(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 52(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 54(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 56(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 58(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 60(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 62(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 32(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 34(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 36(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 38(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 40(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 42(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 44(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 46(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu32bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $704, %esp # imm = 0x2C0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 724
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edi
; AVX512FP16-X86-NEXT:    movl 48(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl (%edi), %ecx
; AVX512FP16-X86-NEXT:    movl 4(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 28(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 26(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 50(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    movl 24(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 22(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 20(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 18(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 16(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 46(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 44(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 42(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 40(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 38(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 36(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 34(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 32(%edi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 62(%edi), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 60(%edi), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movzwl 58(%edi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl 56(%edi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movzwl 54(%edi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 52(%edi), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $704, %esp # imm = 0x2C0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i32 %c to <32 x i1>
  %res0 = load <32 x bfloat>, <32 x bfloat>* %a, align 8
  %res = select <32 x i1> %msk, <32 x bfloat> %res0, <32 x bfloat> zeroinitializer
  ret <32 x bfloat> %res
}

define void @store32bf16(<32 x bfloat> %a) {
; AVX512FP16-X64-LABEL: store32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g32bf16@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovaps %zmm0, (%rax)
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: store32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %zmm0, g32bf16
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    retl
  store <32 x bfloat> %a, <32 x bfloat>* @g32bf16
  ret void
}

define void @storeu32bf16(<32 x bfloat> %a) {
; AVX512FP16-X64-LABEL: storeu32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g32bf16u@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovups %zmm0, (%rax)
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: storeu32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovups %zmm0, g32bf16u
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    retl
  store <32 x bfloat> %a, <32 x bfloat>* @g32bf16u, align 8
  ret void
}

define <32 x bfloat> @movrr32bf16(<32 x bfloat> %a, <32 x bfloat> %b) {
; AVX512FP16-X64-LABEL: movrr32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrr32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %zmm1, %zmm0
; AVX512FP16-X86-NEXT:    retl
  ret <32 x bfloat> %b
}

define <32 x bfloat> @movrrk32bf16(<32 x bfloat> %a, <32 x bfloat> %b, i32 %msk) {
; AVX512FP16-X64-LABEL: movrrk32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $1032, %rsp # imm = 0x408
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 1088
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti32x4 $3, %zmm1, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm2, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm3
; AVX512FP16-X64-NEXT:    vmovdqa %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $3, %zmm0, %xmm3
; AVX512FP16-X64-NEXT:    vmovq %xmm3, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm3, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $2, %zmm1, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm2, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm3
; AVX512FP16-X64-NEXT:    vmovdqa %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $2, %zmm0, %xmm3
; AVX512FP16-X64-NEXT:    vmovq %xmm3, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %r15
; AVX512FP16-X64-NEXT:    movl %r15d, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm3, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %r15d, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %r15, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm2, %r9
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %r11
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm2, %rbp
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %rsi
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %r10
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rdx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %r13
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %r8
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    shrq $48, %r15
; AVX512FP16-X64-NEXT:    shll $16, %r15d
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm1, %xmm1 {%k1}
; AVX512FP16-X64-NEXT:    vmovd %r15d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    movl %r9d, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movq %rbp, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %r9d, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movq %r9, %r12
; AVX512FP16-X64-NEXT:    shrq $32, %r12
; AVX512FP16-X64-NEXT:    shll $16, %r12d
; AVX512FP16-X64-NEXT:    shrq $32, %rbp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    shrq $48, %r9
; AVX512FP16-X64-NEXT:    shll $16, %r9d
; AVX512FP16-X64-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    movl %r11d, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %esi, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %r11d, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    movl %esi, %r14d
; AVX512FP16-X64-NEXT:    andl $-65536, %r14d # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movq %r11, %rdi
; AVX512FP16-X64-NEXT:    shrq $32, %rdi
; AVX512FP16-X64-NEXT:    shll $16, %edi
; AVX512FP16-X64-NEXT:    movq %rsi, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    shrq $48, %r11
; AVX512FP16-X64-NEXT:    shll $16, %r11d
; AVX512FP16-X64-NEXT:    movq %r11, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rsi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movq %rsi, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; AVX512FP16-X64-NEXT:    movq %r10, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    movl %r8d, %r11d
; AVX512FP16-X64-NEXT:    shll $16, %r11d
; AVX512FP16-X64-NEXT:    andl $-65536, %r10d # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movl %r8d, %r9d
; AVX512FP16-X64-NEXT:    andl $-65536, %r9d # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    movq %rcx, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movq %r8, %rsi
; AVX512FP16-X64-NEXT:    shrq $32, %rsi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    shrq $48, %r8
; AVX512FP16-X64-NEXT:    shll $16, %r8d
; AVX512FP16-X64-NEXT:    shll $16, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    shll $16, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    andl $-65536, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    andl $-65536, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    shrq $32, %r13
; AVX512FP16-X64-NEXT:    shll $16, %r13d
; AVX512FP16-X64-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r15 # 8-byte Reload
; AVX512FP16-X64-NEXT:    shrq $32, %r15
; AVX512FP16-X64-NEXT:    shll $16, %r15d
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r12d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r14d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r11d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r10d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r9d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r8d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r13d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd %r15d, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, %xmm0
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $1032, %rsp # imm = 0x408
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrk32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $1128, %esp # imm = 0x468
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 1148
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti32x4 $3, %zmm1, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm3
; AVX512FP16-X86-NEXT:    vmovdqu %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vextracti32x4 $3, %zmm0, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %edi
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqu %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm1, %xmm4
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm5
; AVX512FP16-X86-NEXT:    vmovd %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm4
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm4, %xmm5, %xmm5 {%k1}
; AVX512FP16-X86-NEXT:    vextractf128 $1, %ymm0, %xmm4
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqu %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %ebx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %edi
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ebp
; AVX512FP16-X86-NEXT:    vextracti32x4 $2, %zmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %esi
; AVX512FP16-X86-NEXT:    vmovd %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractf32x4 $2, %zmm0, %xmm0
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %ebp # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ebx
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %esi
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovss %xmm5, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm1, %xmm1 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $1128, %esp # imm = 0x468
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i32 %msk to <32 x i1>
  %res = select <32 x i1> %mask, <32 x bfloat> %a, <32 x bfloat> %b
  ret <32 x bfloat> %res
}

define <32 x bfloat> @movrrkz32bf16(<32 x bfloat> %a, i32 %msk) {
; AVX512FP16-X64-LABEL: movrrkz32bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 592
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti32x4 $2, %zmm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrd $16, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm2, %xmm2 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm3, %xmm3 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm1
; AVX512FP16-X64-NEXT:    vinserti64x4 $1, %ymm0, %zmm1, %zmm0
; AVX512FP16-X64-NEXT:    addq $536, %rsp # imm = 0x218
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrkz32bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $688, %esp # imm = 0x2B0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 708
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti32x4 $3, %zmm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %esi
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm2
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm2, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm2, %edi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm2, %ebx
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %ebp
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vextracti32x4 $2, %zmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %edx
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X86-NEXT:    movl %eax, %esi
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %ebp # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ebx
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm4 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm4, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm3 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm5 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm5, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm4, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm3, %ymm2, %ymm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm4, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512FP16-X86-NEXT:    addl $688, %esp # imm = 0x2B0
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i32 %msk to <32 x i1>
  %res = select <32 x i1> %mask, <32 x bfloat> %a, <32 x bfloat> zeroinitializer
  ret <32 x bfloat> %res
}

define <16 x bfloat> @load16bf16(<16 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: load16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps (%rdi), %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovaps (%eax), %ymm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <16 x bfloat>, <16 x bfloat>* %a
  ret <16 x bfloat> %res
}

define <16 x bfloat> @load16bf16mask(<16 x bfloat>* %a, <16 x bfloat> %b, i16 %c) {
; AVX512FP16-X64-LABEL: load16bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 560
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load16bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 580
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %esi
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %edx
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %edi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ebx
; AVX512FP16-X86-NEXT:    movzwl 18(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    movl 16(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 4(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl (%ebx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 30(%ebx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 28(%eax), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 26(%eax), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl 24(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movzwl 22(%eax), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl 20(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm2 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i16 %c to <16 x i1>
  %res0 = load <16 x bfloat>, <16 x bfloat>* %a
  %res = select <16 x i1> %msk, <16 x bfloat> %res0, <16 x bfloat> %b
  ret <16 x bfloat> %res
}

define <16 x bfloat> @load16bf16maskz(<16 x bfloat>* %a, i16 %c) {
; AVX512FP16-X64-LABEL: load16bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 320
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load16bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 380
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %esi
; AVX512FP16-X86-NEXT:    movl 16(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl (%esi), %eax
; AVX512FP16-X86-NEXT:    movl 4(%esi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%esi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl 28(%esi), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 26(%esi), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movzwl 18(%esi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    movl 24(%esi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 22(%esi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 20(%esi), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i16 %c to <16 x i1>
  %res0 = load <16 x bfloat>, <16 x bfloat>* %a
  %res = select <16 x i1> %msk, <16 x bfloat> %res0, <16 x bfloat> zeroinitializer
  ret <16 x bfloat> %res
}

define <16 x bfloat> @loadu16bf16(<16 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: loadu16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovups (%rdi), %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovups (%eax), %ymm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <16 x bfloat>, <16 x bfloat>* %a, align 8
  ret <16 x bfloat> %res
}

define <16 x bfloat> @loadu16bf16mask(<16 x bfloat>* %a, <16 x bfloat> %b, i16 %c) {
; AVX512FP16-X64-LABEL: loadu16bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 560
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu16bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 580
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %esi
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %edx
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %edi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ebx
; AVX512FP16-X86-NEXT:    movzwl 18(%ebx), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    movl 16(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl 4(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ebx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl (%ebx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 30(%ebx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 28(%eax), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 26(%eax), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl 24(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movzwl 22(%eax), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl 20(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm2 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i16 %c to <16 x i1>
  %res0 = load <16 x bfloat>, <16 x bfloat>* %a, align 8
  %res = select <16 x i1> %msk, <16 x bfloat> %res0, <16 x bfloat> %b
  ret <16 x bfloat> %res
}

define <16 x bfloat> @loadu16bf16maskz(<16 x bfloat>* %a, i16 %c) {
; AVX512FP16-X64-LABEL: loadu16bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 320
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 16(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 18(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 20(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 22(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 24(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 26(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 28(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 30(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu16bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 380
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %esi
; AVX512FP16-X86-NEXT:    movl 16(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl (%esi), %eax
; AVX512FP16-X86-NEXT:    movl 4(%esi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%esi), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 30(%esi), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl 28(%esi), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 26(%esi), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movzwl 18(%esi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    movl 24(%esi), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 22(%esi), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl 20(%esi), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i16 %c to <16 x i1>
  %res0 = load <16 x bfloat>, <16 x bfloat>* %a, align 8
  %res = select <16 x i1> %msk, <16 x bfloat> %res0, <16 x bfloat> zeroinitializer
  ret <16 x bfloat> %res
}

define void @store16bf16(<16 x bfloat> %a) {
; AVX512FP16-X64-LABEL: store16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g16bf16@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovaps %ymm0, (%rax)
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: store16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %ymm0, g16bf16
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    retl
  store <16 x bfloat> %a, <16 x bfloat>* @g16bf16
  ret void
}

define void @storeu16bf16(<16 x bfloat> %a) {
; AVX512FP16-X64-LABEL: storeu16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g16bf16u@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovups %ymm0, (%rax)
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: storeu16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovups %ymm0, g16bf16u
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    retl
  store <16 x bfloat> %a, <16 x bfloat>* @g16bf16u, align 8
  ret void
}

define <16 x bfloat> @movrr16bf16(<16 x bfloat> %a, <16 x bfloat> %b) {
; AVX512FP16-X64-LABEL: movrr16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrr16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %ymm1, %ymm0
; AVX512FP16-X86-NEXT:    retl
  ret <16 x bfloat> %b
}

define <16 x bfloat> @movrrk16bf16(<16 x bfloat> %a, <16 x bfloat> %b, i16 %msk) {
; AVX512FP16-X64-LABEL: movrrk16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 560
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm2, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm3
; AVX512FP16-X64-NEXT:    vmovdqa %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512FP16-X64-NEXT:    vmovq %xmm3, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm3, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rcx
; AVX512FP16-X64-NEXT:    movl %ecx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $504, %rsp # imm = 0x1F8
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrk16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 580
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm1, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm3
; AVX512FP16-X86-NEXT:    vmovdqu %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %xmm3, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqu %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %ebp
; AVX512FP16-X86-NEXT:    movl %ebp, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqu %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X86-NEXT:    vmovd %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm4, %xmm4 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %edx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %esi
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm3, %ebp
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movl %edi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %edx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %edx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebx, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl %ebx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl %ebp, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ebp # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %ebx
; AVX512FP16-X86-NEXT:    andl $-65536, %ebx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movl %eax, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %ecx # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %ecx, %esi
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl {{[-0-9]+}}(%e{{[sb]}}p), %eax # 4-byte Reload
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovss %xmm4, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm1, %xmm1 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $560, %esp # imm = 0x230
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i16 %msk to <16 x i1>
  %res = select <16 x i1> %mask, <16 x bfloat> %a, <16 x bfloat> %b
  ret <16 x bfloat> %res
}

define <16 x bfloat> @movrrkz16bf16(<16 x bfloat> %a, i16 %msk) {
; AVX512FP16-X64-LABEL: movrrkz16bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    pushq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    subq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 320
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -56
; AVX512FP16-X64-NEXT:    .cfi_offset %r12, -48
; AVX512FP16-X64-NEXT:    .cfi_offset %r13, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    vzeroupper
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrw $8, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r13d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r14d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r15d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %r12d
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r12d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r15d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r14d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %ebp, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ebx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %r13d, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    vpinsrw $1, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $2, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $3, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $4, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $5, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $6, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vpinsrw $7, {{[-0-9]+}}(%r{{[sb]}}p), %xmm1, %xmm1 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    addq $264, %rsp # imm = 0x108
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 56
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 48
; AVX512FP16-X64-NEXT:    popq %r12
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %r13
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrkz16bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 380
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %edi, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ecx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %esi
; AVX512FP16-X86-NEXT:    movl %esi, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %ebx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    movl %ebx, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl %ecx, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %ebp # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovups %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vzeroupper
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm2, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $1, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $3, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $4, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $5, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vpinsrw $6, %eax, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vpinsrw $7, %eax, %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm2, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    addl $360, %esp # imm = 0x168
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i16 %msk to <16 x i1>
  %res = select <16 x i1> %mask, <16 x bfloat> %a, <16 x bfloat> zeroinitializer
  ret <16 x bfloat> %res
}

define <8 x bfloat> @load8bf16(<8 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: load8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps (%rdi), %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovaps (%eax), %xmm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <8 x bfloat>, <8 x bfloat>* %a
  ret <8 x bfloat> %res
}

define <8 x bfloat> @load8bf16mask(<8 x bfloat>* %a, <8 x bfloat> %b, i8 %c) {
; AVX512FP16-X64-LABEL: load8bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 272
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load8bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 280
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ecx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    movl (%ecx), %eax
; AVX512FP16-X86-NEXT:    movl 4(%ecx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movzwl 14(%ecx), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl 12(%ecx), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 10(%ecx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 8(%ecx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 6(%ecx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i8 %c to <8 x i1>
  %res0 = load <8 x bfloat>, <8 x bfloat>* %a
  %res = select <8 x i1> %msk, <8 x bfloat> %res0, <8 x bfloat> %b
  ret <8 x bfloat> %res
}

define <8 x bfloat> @load8bf16maskz(<8 x bfloat>* %a, i8 %c) {
; AVX512FP16-X64-LABEL: load8bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 160
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; AVX512FP16-X64-NEXT:    addq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: load8bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    subl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 200
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    movl (%eax), %edx
; AVX512FP16-X86-NEXT:    movl 4(%eax), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    movzwl 2(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm1 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss %xmm1, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i8 %c to <8 x i1>
  %res0 = load <8 x bfloat>, <8 x bfloat>* %a
  %res = select <8 x i1> %msk, <8 x bfloat> %res0, <8 x bfloat> zeroinitializer
  ret <8 x bfloat> %res
}

define <8 x bfloat> @loadu8bf16(<8 x bfloat>* %a) {
; AVX512FP16-X64-LABEL: loadu8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovups (%rdi), %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovups (%eax), %xmm0
; AVX512FP16-X86-NEXT:    retl
  %res = load <8 x bfloat>, <8 x bfloat>* %a, align 8
  ret <8 x bfloat> %res
}

define <8 x bfloat> @loadu8bf16mask(<8 x bfloat>* %a, <8 x bfloat> %b, i8 %c) {
; AVX512FP16-X64-LABEL: loadu8bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 272
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu8bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 280
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 2(%ecx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X86-NEXT:    movl (%ecx), %eax
; AVX512FP16-X86-NEXT:    movl 4(%ecx), %edx
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    movzwl 14(%ecx), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl 12(%ecx), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    movzwl 10(%ecx), %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    movl 8(%ecx), %ebp
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    movzwl 6(%ecx), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i8 %c to <8 x i1>
  %res0 = load <8 x bfloat>, <8 x bfloat>* %a, align 8
  %res = select <8 x i1> %msk, <8 x bfloat> %res0, <8 x bfloat> %b
  ret <8 x bfloat> %res
}

define <8 x bfloat> @loadu8bf16maskz(<8 x bfloat>* %a, i8 %c) {
; AVX512FP16-X64-LABEL: loadu8bf16maskz:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 160
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    kmovd %esi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    movl (%rdi), %eax
; AVX512FP16-X64-NEXT:    movl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 12(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl 8(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; AVX512FP16-X64-NEXT:    addq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: loadu8bf16maskz:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    subl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 200
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -8
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    movl (%eax), %edx
; AVX512FP16-X86-NEXT:    movl 4(%eax), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 14(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 12(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 10(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl 8(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movzwl 6(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    movzwl 2(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm1 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss %xmm1, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %msk = bitcast i8 %c to <8 x i1>
  %res0 = load <8 x bfloat>, <8 x bfloat>* %a, align 8
  %res = select <8 x i1> %msk, <8 x bfloat> %res0, <8 x bfloat> zeroinitializer
  ret <8 x bfloat> %res
}

define void @store8bf16(<8 x bfloat> %a) {
; AVX512FP16-X64-LABEL: store8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g8bf16@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovaps %xmm0, (%rax)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: store8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %xmm0, g8bf16
; AVX512FP16-X86-NEXT:    retl
  store <8 x bfloat> %a, <8 x bfloat>* @g8bf16
  ret void
}

define void @storeu8bf16(<8 x bfloat> %a) {
; AVX512FP16-X64-LABEL: storeu8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    movq g8bf16u@GOTPCREL(%rip), %rax
; AVX512FP16-X64-NEXT:    vmovups %xmm0, (%rax)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: storeu8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovups %xmm0, g8bf16u
; AVX512FP16-X86-NEXT:    retl
  store <8 x bfloat> %a, <8 x bfloat>* @g8bf16u, align 8
  ret void
}

define <8 x bfloat> @movrr8bf16(<8 x bfloat> %a, <8 x bfloat> %b) {
; AVX512FP16-X64-LABEL: movrr8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovaps %xmm1, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrr8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovaps %xmm1, %xmm0
; AVX512FP16-X86-NEXT:    retl
  ret <8 x bfloat> %b
}

define <8 x bfloat> @movrrk8bf16(<8 x bfloat> %a, <8 x bfloat> %b, i8 %msk) {
; AVX512FP16-X64-LABEL: movrrk8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 272
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm1, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rcx
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:    vmovdqa %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rcx
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rdx
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $240, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrk8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    subl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 280
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -20
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebp, -8
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm1, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:    andl $-65536, %esi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %esi
; AVX512FP16-X86-NEXT:    movl %esi, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqu %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm1, %edx
; AVX512FP16-X86-NEXT:    movl %edx, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %esi
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm1, %ebx
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %ebp
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    movl %esi, %eax
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %ebx, %ecx
; AVX512FP16-X86-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    movl %ebp, %edi
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    shll $16, %ebx
; AVX512FP16-X86-NEXT:    shll $16, %ebp
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %esi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %ebx, %xmm2
; AVX512FP16-X86-NEXT:    vmovd %ebp, %xmm1
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm2, %xmm2 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm0, %xmm0 {%k1}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $260, %esp # imm = 0x104
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 20
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i8 %msk to <8 x i1>
  %res = select <8 x i1> %mask, <8 x bfloat> %a, <8 x bfloat> %b
  ret <8 x bfloat> %res
}

define <8 x bfloat> @movrrkz8bf16(<8 x bfloat> %a, i8 %msk) {
; AVX512FP16-X64-LABEL: movrrkz8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    subq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 160
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movq %rax, %rcx
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    movl %eax, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    kmovd %edi, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %ebx
; AVX512FP16-X64-NEXT:    orl %ebp, %ebx
; AVX512FP16-X64-NEXT:    shlq $32, %rbx
; AVX512FP16-X64-NEXT:    orq %r14, %rbx
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebp, %r14d
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X64-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X64-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebp
; AVX512FP16-X64-NEXT:    shll $16, %ebp
; AVX512FP16-X64-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X64-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebp, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rbx, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $128, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: movrrkz8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    subl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 200
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movl %eax, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm1
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movl %ecx, %edx
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vmovdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm1
; AVX512FP16-X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k2
; AVX512FP16-X86-NEXT:    kmovw %k2, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k2, %k1
; AVX512FP16-X86-NEXT:    vmovss %xmm1, %xmm1, %xmm1 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovss %xmm1, (%esp)
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k2, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $4, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $2, %k1, %k1
; AVX512FP16-X86-NEXT:    kmovw %k1, {{[-0-9]+}}(%e{{[sb]}}p) # 2-byte Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    kshiftrb $1, %k1, %k1
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    kmovw {{[-0-9]+}}(%e{{[sb]}}p), %k1 # 2-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, %xmm0, %xmm0 {%k1} {z}
; AVX512FP16-X86-NEXT:    vmovups %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovups {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movzwl %ax, %eax
; AVX512FP16-X86-NEXT:    orl %ecx, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %edx, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X86-NEXT:    movzwl %si, %esi
; AVX512FP16-X86-NEXT:    orl %edx, %esi
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edi
; AVX512FP16-X86-NEXT:    movzwl %di, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $188, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %mask = bitcast i8 %msk to <8 x i1>
  %res = select <8 x i1> %mask, <8 x bfloat> %a, <8 x bfloat> zeroinitializer
  ret <8 x bfloat> %res
}

define bfloat @extract_f16_0(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_0:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_0:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 0
   ret bfloat %res
}

define bfloat @extract_f16_1(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_1:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    shrl $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_1:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 1
   ret bfloat %res
}

define bfloat @extract_f16_2(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_2:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_2:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 2
   ret bfloat %res
}

define bfloat @extract_f16_3(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_3:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_3:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 3
   ret bfloat %res
}

define bfloat @extract_f16_4(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_4:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_4:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 4
   ret bfloat %res
}

define bfloat @extract_f16_5(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_5:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X64-NEXT:    shrl $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_5:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 5
   ret bfloat %res
}

define bfloat @extract_f16_6(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_6:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_6:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 6
   ret bfloat %res
}

define bfloat @extract_f16_7(<8 x bfloat> %x) {
; AVX512FP16-X64-LABEL: extract_f16_7:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_f16_7:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    flds (%esp)
; AVX512FP16-X86-NEXT:    popl %eax
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 7
   ret bfloat %res
}

define void @extract_store_f16_0(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_0:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrw $0, %xmm0, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_0:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpextrw $0, %xmm0, (%eax)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 0
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_1(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_1:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    shrl $16, %eax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_1:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movw %ax, (%ecx)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 1
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_2(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_2:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_2:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpextrw $2, %xmm0, (%eax)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 2
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_3(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_3:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_3:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vextractps $1, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movw %ax, (%ecx)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 3
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_4(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_4:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrw $4, %xmm0, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_4:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpextrw $4, %xmm0, (%eax)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 4
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_5(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_5:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X64-NEXT:    shrl $16, %eax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_5:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vextractps $2, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movw %ax, (%ecx)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 5
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_6(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_6:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_6:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vpextrw $6, %xmm0, (%eax)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 6
   store bfloat %res, bfloat* %y
   ret void
}

define void @extract_store_f16_7(<8 x bfloat> %x, bfloat* %y) {
; AVX512FP16-X64-LABEL: extract_store_f16_7:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %rax
; AVX512FP16-X64-NEXT:    shrq $48, %rax
; AVX512FP16-X64-NEXT:    movw %ax, (%rdi)
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: extract_store_f16_7:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vextractps $3, %xmm0, %eax
; AVX512FP16-X86-NEXT:    shrl $16, %eax
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movw %ax, (%ecx)
; AVX512FP16-X86-NEXT:    retl
   %res = extractelement <8 x bfloat> %x, i32 7
   store bfloat %res, bfloat* %y
   ret void
}

define <8 x bfloat> @build_vector_xxxxuuuu(bfloat %a0, bfloat %a1, bfloat %a2, bfloat %a3) {
; AVX512FP16-X64-LABEL: build_vector_xxxxuuuu:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X64-NEXT:    vmovd %xmm3, %ecx
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X64-NEXT:    vmovd %xmm1, %esi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movzwl %dx, %edx
; AVX512FP16-X64-NEXT:    orl %esi, %edx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ecx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %rdx, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: build_vector_xxxxuuuu:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    orl %ecx, %edx
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vpunpckldq {{.*#+}} xmm0 = xmm1[0],xmm0[0],xmm1[1],xmm0[1]
; AVX512FP16-X86-NEXT:    vmovq {{.*#+}} xmm0 = xmm0[0],zero
; AVX512FP16-X86-NEXT:    retl
  %a = insertelement <8 x bfloat> undef, bfloat %a0, i32 0
  %b = insertelement <8 x bfloat> %a, bfloat %a1, i32 1
  %c = insertelement <8 x bfloat> %b, bfloat %a2, i32 2
  %d = insertelement <8 x bfloat> %c, bfloat %a3, i32 3
  ret <8 x bfloat> %d
}

define <8 x bfloat> @build_vector_uuuuxxxx(bfloat %a0, bfloat %a1, bfloat %a2, bfloat %a3) {
; AVX512FP16-X64-LABEL: build_vector_uuuuxxxx:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm2, %eax
; AVX512FP16-X64-NEXT:    vmovd %xmm3, %ecx
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X64-NEXT:    vmovd %xmm1, %esi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movzwl %dx, %edx
; AVX512FP16-X64-NEXT:    orl %esi, %edx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ecx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %rdx, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vpslldq {{.*#+}} xmm0 = zero,zero,zero,zero,zero,zero,zero,zero,xmm0[0,1,2,3,4,5,6,7]
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: build_vector_uuuuxxxx:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    orl %ecx, %edx
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    vpunpckldq {{.*#+}} xmm0 = xmm1[0],xmm0[0],xmm1[1],xmm0[1]
; AVX512FP16-X86-NEXT:    vpslldq {{.*#+}} xmm0 = zero,zero,zero,zero,zero,zero,zero,zero,xmm0[0,1,2,3,4,5,6,7]
; AVX512FP16-X86-NEXT:    retl
  %a = insertelement <8 x bfloat> undef, bfloat %a0, i32 4
  %b = insertelement <8 x bfloat> %a, bfloat %a1, i32 5
  %c = insertelement <8 x bfloat> %b, bfloat %a2, i32 6
  %d = insertelement <8 x bfloat> %c, bfloat %a3, i32 7
  ret <8 x bfloat> %d
}

define <8 x bfloat> @build_vector_xxxxxxxx(bfloat %a0, bfloat %a1, bfloat %a2, bfloat %a3, bfloat %a4, bfloat %a5, bfloat %a6, bfloat %a7) {
; AVX512FP16-X64-LABEL: build_vector_xxxxxxxx:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm6, %r8d
; AVX512FP16-X64-NEXT:    vmovd %xmm7, %eax
; AVX512FP16-X64-NEXT:    vmovd %xmm4, %r9d
; AVX512FP16-X64-NEXT:    vmovd %xmm5, %esi
; AVX512FP16-X64-NEXT:    vmovd %xmm2, %r10d
; AVX512FP16-X64-NEXT:    vmovd %xmm3, %ecx
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X64-NEXT:    vmovd %xmm1, %edi
; AVX512FP16-X64-NEXT:    shll $16, %edi
; AVX512FP16-X64-NEXT:    movzwl %dx, %edx
; AVX512FP16-X64-NEXT:    orl %edi, %edx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movzwl %r10w, %edi
; AVX512FP16-X64-NEXT:    orl %ecx, %edi
; AVX512FP16-X64-NEXT:    shlq $32, %rdi
; AVX512FP16-X64-NEXT:    orq %rdx, %rdi
; AVX512FP16-X64-NEXT:    shll $16, %esi
; AVX512FP16-X64-NEXT:    movzwl %r9w, %ecx
; AVX512FP16-X64-NEXT:    orl %esi, %ecx
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movzwl %r8w, %edx
; AVX512FP16-X64-NEXT:    orl %eax, %edx
; AVX512FP16-X64-NEXT:    shlq $32, %rdx
; AVX512FP16-X64-NEXT:    orq %rcx, %rdx
; AVX512FP16-X64-NEXT:    vmovq %rdx, %xmm0
; AVX512FP16-X64-NEXT:    vmovq %rdi, %xmm1
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: build_vector_xxxxxxxx:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -8
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %edx
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %edi
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %esi
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; AVX512FP16-X86-NEXT:    shll $16, %ecx
; AVX512FP16-X86-NEXT:    orl %esi, %ecx
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %esi
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    orl %edi, %esi
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edi
; AVX512FP16-X86-NEXT:    shll $16, %edi
; AVX512FP16-X86-NEXT:    orl %edx, %edi
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    orl %eax, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %edi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $2, %esi, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $3, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %a = insertelement <8 x bfloat> undef, bfloat %a0, i32 0
  %b = insertelement <8 x bfloat> %a, bfloat %a1, i32 1
  %c = insertelement <8 x bfloat> %b, bfloat %a2, i32 2
  %d = insertelement <8 x bfloat> %c, bfloat %a3, i32 3
  %e = insertelement <8 x bfloat> %d, bfloat %a4, i32 4
  %f = insertelement <8 x bfloat> %e, bfloat %a5, i32 5
  %g = insertelement <8 x bfloat> %f, bfloat %a6, i32 6
  %h = insertelement <8 x bfloat> %g, bfloat %a7, i32 7
  ret <8 x bfloat> %h
}

define <16 x bfloat> @build_vector_xxxxuuuuuuuuxxxx(bfloat %a0, bfloat %a1, bfloat %a2, bfloat %a3, bfloat %a4, bfloat %a5, bfloat %a6, bfloat %a7) {
; AVX512FP16-X64-LABEL: build_vector_xxxxuuuuuuuuxxxx:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vmovd %xmm3, %r8d
; AVX512FP16-X64-NEXT:    vmovd %xmm2, %r9d
; AVX512FP16-X64-NEXT:    vmovd %xmm1, %r10d
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %esi
; AVX512FP16-X64-NEXT:    vmovd %xmm7, %edi
; AVX512FP16-X64-NEXT:    vmovd %xmm6, %eax
; AVX512FP16-X64-NEXT:    vmovd %xmm5, %ecx
; AVX512FP16-X64-NEXT:    vmovd %xmm4, %edx
; AVX512FP16-X64-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $4, %edx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $5, %ecx, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $6, %eax, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpinsrw $7, %edi, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vmovw %esi, %xmm1
; AVX512FP16-X64-NEXT:    vpinsrw $1, %r10d, %xmm1, %xmm1
; AVX512FP16-X64-NEXT:    vpinsrw $2, %r9d, %xmm1, %xmm1
; AVX512FP16-X64-NEXT:    vpinsrw $3, %r8d, %xmm1, %xmm1
; AVX512FP16-X64-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: build_vector_xxxxuuuuuuuuxxxx:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    vmovw {{[0-9]+}}(%esp), %xmm0
; AVX512FP16-X86-NEXT:    vpinsrw $1, {{[0-9]+}}(%esp), %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrw $2, %eax, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrw $3, {{[0-9]+}}(%esp), %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpinsrw $4, {{[0-9]+}}(%esp), %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpinsrw $5, {{[0-9]+}}(%esp), %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpinsrw $6, {{[0-9]+}}(%esp), %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpinsrw $7, {{[0-9]+}}(%esp), %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vinserti128 $1, %xmm1, %ymm0, %ymm0
; AVX512FP16-X86-NEXT:    retl
  %a = insertelement <16 x bfloat> undef, bfloat %a0, i32 0
  %b = insertelement <16 x bfloat> %a, bfloat %a1, i32 1
  %c = insertelement <16 x bfloat> %b, bfloat %a2, i32 2
  %d = insertelement <16 x bfloat> %c, bfloat %a3, i32 3
  %e = insertelement <16 x bfloat> %d, bfloat %a4, i32 12
  %f = insertelement <16 x bfloat> %e, bfloat %a5, i32 13
  %g = insertelement <16 x bfloat> %f, bfloat %a6, i32 14
  %h = insertelement <16 x bfloat> %g, bfloat %a7, i32 15
  ret <16 x bfloat> %h
}

define <8 x bfloat> @regression1(<8 x bfloat> %a, <8 x bfloat> %b) {
; AVX512FP16-X64-LABEL: regression1:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    vpshufb {{.*#+}} xmm0 = xmm0[0,1,14,15,0,1,2,3,4,5,6,7,14,15,10,11]
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: regression1:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    vpshufb {{.*#+}} xmm0 = xmm0[0,1,14,15,0,1,2,3,4,5,6,7,14,15,10,11]
; AVX512FP16-X86-NEXT:    retl
  %res = shufflevector <8 x bfloat> %a, <8 x bfloat> %b, <8 x i32> <i32 0, i32 7, i32 0, i32 1, i32 2, i32 3, i32 7, i32 5>
  ret <8 x bfloat> %res
}

declare void @llvm.masked.store.v8bf16.p0v8bf16(<8 x bfloat>, <8 x bfloat>*, i32, <8 x i1>)
declare <8 x bfloat> @llvm.masked.load.v8bf16.p0v8bf16(<8 x bfloat>*, i32,  <8 x i1>, <8 x bfloat>)

define void @storeu8bf16mask(<8 x i1> %mask, <8 x bfloat>* %addr, <8 x bfloat> %val) {
; AVX512FP16-X64-LABEL: storeu8bf16mask:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    subq $24, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 64
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -40
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -32
; AVX512FP16-X64-NEXT:    .cfi_offset %r15, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %rbp, -16
; AVX512FP16-X64-NEXT:    movq %rdi, %r14
; AVX512FP16-X64-NEXT:    vmovdqa %xmm1, (%rsp) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovq %xmm1, %rbx
; AVX512FP16-X64-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X64-NEXT:    kmovd %k0, %ebp
; AVX512FP16-X64-NEXT:    testb $1, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_1
; AVX512FP16-X64-NEXT:  # %bb.2: # %else
; AVX512FP16-X64-NEXT:    testb $2, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_3
; AVX512FP16-X64-NEXT:  .LBB69_4: # %else2
; AVX512FP16-X64-NEXT:    testb $4, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_6
; AVX512FP16-X64-NEXT:  .LBB69_5: # %cond.store3
; AVX512FP16-X64-NEXT:    movq %rbx, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 4(%r14)
; AVX512FP16-X64-NEXT:  .LBB69_6: # %else4
; AVX512FP16-X64-NEXT:    vmovdqa (%rsp), %xmm0 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm0, %r15
; AVX512FP16-X64-NEXT:    testb $8, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_7
; AVX512FP16-X64-NEXT:  # %bb.8: # %else6
; AVX512FP16-X64-NEXT:    testb $16, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_9
; AVX512FP16-X64-NEXT:  .LBB69_10: # %else8
; AVX512FP16-X64-NEXT:    testb $32, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_11
; AVX512FP16-X64-NEXT:  .LBB69_12: # %else10
; AVX512FP16-X64-NEXT:    testb $64, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_13
; AVX512FP16-X64-NEXT:  .LBB69_14: # %else12
; AVX512FP16-X64-NEXT:    testb $-128, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_16
; AVX512FP16-X64-NEXT:  .LBB69_15: # %cond.store13
; AVX512FP16-X64-NEXT:    shrq $48, %r15
; AVX512FP16-X64-NEXT:    shll $16, %r15d
; AVX512FP16-X64-NEXT:    vmovd %r15d, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 14(%r14)
; AVX512FP16-X64-NEXT:  .LBB69_16: # %else14
; AVX512FP16-X64-NEXT:    addq $24, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 40
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 32
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %r15
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %rbp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
; AVX512FP16-X64-NEXT:  .LBB69_1: # %cond.store
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 64
; AVX512FP16-X64-NEXT:    movl %ebx, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, (%r14)
; AVX512FP16-X64-NEXT:    testb $2, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_4
; AVX512FP16-X64-NEXT:  .LBB69_3: # %cond.store1
; AVX512FP16-X64-NEXT:    movl %ebx, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 2(%r14)
; AVX512FP16-X64-NEXT:    testb $4, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_5
; AVX512FP16-X64-NEXT:    jmp .LBB69_6
; AVX512FP16-X64-NEXT:  .LBB69_7: # %cond.store5
; AVX512FP16-X64-NEXT:    shrq $48, %rbx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd %ebx, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 6(%r14)
; AVX512FP16-X64-NEXT:    testb $16, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_10
; AVX512FP16-X64-NEXT:  .LBB69_9: # %cond.store7
; AVX512FP16-X64-NEXT:    movl %r15d, %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 8(%r14)
; AVX512FP16-X64-NEXT:    testb $32, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_12
; AVX512FP16-X64-NEXT:  .LBB69_11: # %cond.store9
; AVX512FP16-X64-NEXT:    movl %r15d, %eax
; AVX512FP16-X64-NEXT:    andl $-65536, %eax # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 10(%r14)
; AVX512FP16-X64-NEXT:    testb $64, %bpl
; AVX512FP16-X64-NEXT:    je .LBB69_14
; AVX512FP16-X64-NEXT:  .LBB69_13: # %cond.store11
; AVX512FP16-X64-NEXT:    movq %r15, %rax
; AVX512FP16-X64-NEXT:    shrq $32, %rax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movw %ax, 12(%r14)
; AVX512FP16-X64-NEXT:    testb $-128, %bpl
; AVX512FP16-X64-NEXT:    jne .LBB69_15
; AVX512FP16-X64-NEXT:    jmp .LBB69_16
;
; AVX512FP16-X86-LABEL: storeu8bf16mask:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    pushl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    subl $52, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 68
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -16
; AVX512FP16-X86-NEXT:    .cfi_offset %edi, -12
; AVX512FP16-X86-NEXT:    .cfi_offset %ebx, -8
; AVX512FP16-X86-NEXT:    vmovdqu %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %edi
; AVX512FP16-X86-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %esi
; AVX512FP16-X86-NEXT:    kmovd %k0, %ebx
; AVX512FP16-X86-NEXT:    testb $1, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_2
; AVX512FP16-X86-NEXT:  # %bb.1: # %cond.store
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, (%esi)
; AVX512FP16-X86-NEXT:  .LBB69_2: # %else
; AVX512FP16-X86-NEXT:    testb $2, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_4
; AVX512FP16-X86-NEXT:  # %bb.3: # %cond.store1
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 2(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_4: # %else2
; AVX512FP16-X86-NEXT:    vmovdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %edi
; AVX512FP16-X86-NEXT:    testb $4, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_6
; AVX512FP16-X86-NEXT:  # %bb.5: # %cond.store3
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 4(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_6: # %else4
; AVX512FP16-X86-NEXT:    testb $8, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_8
; AVX512FP16-X86-NEXT:  # %bb.7: # %cond.store5
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 6(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_8: # %else6
; AVX512FP16-X86-NEXT:    vmovdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %edi
; AVX512FP16-X86-NEXT:    testb $16, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_10
; AVX512FP16-X86-NEXT:  # %bb.9: # %cond.store7
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 8(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_10: # %else8
; AVX512FP16-X86-NEXT:    testb $32, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_12
; AVX512FP16-X86-NEXT:  # %bb.11: # %cond.store9
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 10(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_12: # %else10
; AVX512FP16-X86-NEXT:    vmovdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %edi
; AVX512FP16-X86-NEXT:    testb $64, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_14
; AVX512FP16-X86-NEXT:  # %bb.13: # %cond.store11
; AVX512FP16-X86-NEXT:    movl %edi, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 12(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_14: # %else12
; AVX512FP16-X86-NEXT:    testb $-128, %bl
; AVX512FP16-X86-NEXT:    je .LBB69_16
; AVX512FP16-X86-NEXT:  # %bb.15: # %cond.store13
; AVX512FP16-X86-NEXT:    andl $-65536, %edi # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edi, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    movw %ax, 14(%esi)
; AVX512FP16-X86-NEXT:  .LBB69_16: # %else14
; AVX512FP16-X86-NEXT:    addl $52, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 12
; AVX512FP16-X86-NEXT:    popl %edi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %ebx
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  call void @llvm.masked.store.v8bf16.p0v8bf16(<8 x bfloat> %val, <8 x bfloat>* %addr, i32 4, <8 x i1>%mask)
  ret void
}

define <8 x bfloat> @maskloadu8bf16(<8 x bfloat>* %addr, <8 x bfloat> %val, <8 x i1> %mask) {
; AVX512FP16-X64-LABEL: maskloadu8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    subq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 80
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -16
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm2
; AVX512FP16-X64-NEXT:    vmovq %xmm0, %rdx
; AVX512FP16-X64-NEXT:    vpsllw $15, %xmm1, %xmm0
; AVX512FP16-X64-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X64-NEXT:    kmovd %k0, %eax
; AVX512FP16-X64-NEXT:    testb $1, %al
; AVX512FP16-X64-NEXT:    je .LBB70_1
; AVX512FP16-X64-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %ecx
; AVX512FP16-X64-NEXT:    jmp .LBB70_3
; AVX512FP16-X64-NEXT:  .LBB70_1:
; AVX512FP16-X64-NEXT:    movl %edx, %ecx
; AVX512FP16-X64-NEXT:  .LBB70_3: # %else
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $2, %al
; AVX512FP16-X64-NEXT:    je .LBB70_4
; AVX512FP16-X64-NEXT:  # %bb.5: # %cond.load1
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    jmp .LBB70_6
; AVX512FP16-X64-NEXT:  .LBB70_4: # %else
; AVX512FP16-X64-NEXT:    movl %edx, %ecx
; AVX512FP16-X64-NEXT:    andl $-65536, %ecx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:  .LBB70_6: # %else2
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    testb $4, %al
; AVX512FP16-X64-NEXT:    je .LBB70_7
; AVX512FP16-X64-NEXT:  # %bb.8: # %cond.load4
; AVX512FP16-X64-NEXT:    movzwl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    jmp .LBB70_9
; AVX512FP16-X64-NEXT:  .LBB70_7: # %else2
; AVX512FP16-X64-NEXT:    movq %rdx, %rcx
; AVX512FP16-X64-NEXT:    shrq $32, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:  .LBB70_9: # %else5
; AVX512FP16-X64-NEXT:    vpextrq $1, %xmm2, %rcx
; AVX512FP16-X64-NEXT:    testb $8, %al
; AVX512FP16-X64-NEXT:    je .LBB70_10
; AVX512FP16-X64-NEXT:  # %bb.11: # %cond.load7
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    jne .LBB70_14
; AVX512FP16-X64-NEXT:  .LBB70_13: # %else8
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    jmp .LBB70_15
; AVX512FP16-X64-NEXT:  .LBB70_10: # %else5
; AVX512FP16-X64-NEXT:    shrq $48, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    je .LBB70_13
; AVX512FP16-X64-NEXT:  .LBB70_14: # %cond.load10
; AVX512FP16-X64-NEXT:    movzwl 8(%rdi), %edx
; AVX512FP16-X64-NEXT:  .LBB70_15: # %else11
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $32, %al
; AVX512FP16-X64-NEXT:    je .LBB70_16
; AVX512FP16-X64-NEXT:  # %bb.17: # %cond.load13
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    jmp .LBB70_18
; AVX512FP16-X64-NEXT:  .LBB70_16: # %else11
; AVX512FP16-X64-NEXT:    movl %ecx, %edx
; AVX512FP16-X64-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X64-NEXT:  .LBB70_18: # %else14
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $64, %al
; AVX512FP16-X64-NEXT:    je .LBB70_19
; AVX512FP16-X64-NEXT:  # %bb.20: # %cond.load16
; AVX512FP16-X64-NEXT:    movzwl 12(%rdi), %edx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $-128, %al
; AVX512FP16-X64-NEXT:    jne .LBB70_23
; AVX512FP16-X64-NEXT:  .LBB70_22: # %else17
; AVX512FP16-X64-NEXT:    shrq $48, %rcx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    jmp .LBB70_24
; AVX512FP16-X64-NEXT:  .LBB70_19: # %else14
; AVX512FP16-X64-NEXT:    movq %rcx, %rdx
; AVX512FP16-X64-NEXT:    shrq $32, %rdx
; AVX512FP16-X64-NEXT:    shll $16, %edx
; AVX512FP16-X64-NEXT:    movl %edx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $-128, %al
; AVX512FP16-X64-NEXT:    je .LBB70_22
; AVX512FP16-X64-NEXT:  .LBB70_23: # %cond.load19
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:  .LBB70_24: # %else20
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: maskloadu8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    pushl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    subl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 124
; AVX512FP16-X86-NEXT:    .cfi_offset %esi, -8
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    vpsllw $15, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpmovw2m %xmm1, %k0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    kmovd %k0, %ecx
; AVX512FP16-X86-NEXT:    testb $1, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_1
; AVX512FP16-X86-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X86-NEXT:    movzwl (%eax), %esi
; AVX512FP16-X86-NEXT:    jmp .LBB70_3
; AVX512FP16-X86-NEXT:  .LBB70_1:
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:  .LBB70_3: # %else
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $2, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_4
; AVX512FP16-X86-NEXT:  # %bb.5: # %cond.load1
; AVX512FP16-X86-NEXT:    movzwl 2(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    jmp .LBB70_6
; AVX512FP16-X86-NEXT:  .LBB70_4: # %else
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:  .LBB70_6: # %else2
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $1, %xmm0, %edx
; AVX512FP16-X86-NEXT:    testb $4, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_7
; AVX512FP16-X86-NEXT:  # %bb.8: # %cond.load4
; AVX512FP16-X86-NEXT:    movzwl 4(%eax), %esi
; AVX512FP16-X86-NEXT:    jmp .LBB70_9
; AVX512FP16-X86-NEXT:  .LBB70_7: # %else2
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:  .LBB70_9: # %else5
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $8, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_10
; AVX512FP16-X86-NEXT:  # %bb.11: # %cond.load7
; AVX512FP16-X86-NEXT:    movzwl 6(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    jmp .LBB70_12
; AVX512FP16-X86-NEXT:  .LBB70_10: # %else5
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:  .LBB70_12: # %else8
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $2, %xmm0, %edx
; AVX512FP16-X86-NEXT:    testb $16, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_13
; AVX512FP16-X86-NEXT:  # %bb.14: # %cond.load10
; AVX512FP16-X86-NEXT:    movzwl 8(%eax), %esi
; AVX512FP16-X86-NEXT:    jmp .LBB70_15
; AVX512FP16-X86-NEXT:  .LBB70_13: # %else8
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:  .LBB70_15: # %else11
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $32, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_16
; AVX512FP16-X86-NEXT:  # %bb.17: # %cond.load13
; AVX512FP16-X86-NEXT:    movzwl 10(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    jmp .LBB70_18
; AVX512FP16-X86-NEXT:  .LBB70_16: # %else11
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:  .LBB70_18: # %else14
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    vpextrd $3, %xmm0, %edx
; AVX512FP16-X86-NEXT:    testb $64, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_19
; AVX512FP16-X86-NEXT:  # %bb.20: # %cond.load16
; AVX512FP16-X86-NEXT:    movzwl 12(%eax), %esi
; AVX512FP16-X86-NEXT:    jmp .LBB70_21
; AVX512FP16-X86-NEXT:  .LBB70_19: # %else14
; AVX512FP16-X86-NEXT:    movl %edx, %esi
; AVX512FP16-X86-NEXT:  .LBB70_21: # %else17
; AVX512FP16-X86-NEXT:    shll $16, %esi
; AVX512FP16-X86-NEXT:    movl %esi, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $-128, %cl
; AVX512FP16-X86-NEXT:    je .LBB70_22
; AVX512FP16-X86-NEXT:  # %bb.23: # %cond.load19
; AVX512FP16-X86-NEXT:    movzwl 14(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:    jmp .LBB70_24
; AVX512FP16-X86-NEXT:  .LBB70_22: # %else17
; AVX512FP16-X86-NEXT:    andl $-65536, %edx # imm = 0xFFFF0000
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:  .LBB70_24: # %else20
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    movzwl %dx, %edx
; AVX512FP16-X86-NEXT:    orl %eax, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $3, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X86-NEXT:    popl %esi
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %res = call <8 x bfloat> @llvm.masked.load.v8bf16.p0v8bf16(<8 x bfloat>* %addr, i32 4, <8 x i1> %mask, <8 x bfloat> %val)
  ret <8 x bfloat> %res
}

define <8 x bfloat> @maskuloadu8bf16(<8 x bfloat>* %addr, <8 x i1> %mask) {
; AVX512FP16-X64-LABEL: maskuloadu8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    subq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 80
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -16
; AVX512FP16-X64-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X64-NEXT:    kmovd %k0, %eax
; AVX512FP16-X64-NEXT:    testb $1, %al
; AVX512FP16-X64-NEXT:    je .LBB71_1
; AVX512FP16-X64-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $2, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_5
; AVX512FP16-X64-NEXT:  .LBB71_4: # %else
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X64-NEXT:    testb $4, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_8
; AVX512FP16-X64-NEXT:  .LBB71_7: # %else2
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    testb $8, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_11
; AVX512FP16-X64-NEXT:  .LBB71_10: # %else5
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_14
; AVX512FP16-X64-NEXT:  .LBB71_13: # %else8
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    testb $32, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_17
; AVX512FP16-X64-NEXT:  .LBB71_16: # %else11
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    testb $64, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_20
; AVX512FP16-X64-NEXT:  .LBB71_19: # %else14
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    testb $-128, %al
; AVX512FP16-X64-NEXT:    jne .LBB71_23
; AVX512FP16-X64-NEXT:  .LBB71_22: # %else17
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm1
; AVX512FP16-X64-NEXT:    # kill: killed $xmm1
; AVX512FP16-X64-NEXT:    jmp .LBB71_24
; AVX512FP16-X64-NEXT:  .LBB71_1:
; AVX512FP16-X64-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X64-NEXT:    # kill: killed $xmm0
; AVX512FP16-X64-NEXT:    testb $2, %al
; AVX512FP16-X64-NEXT:    je .LBB71_4
; AVX512FP16-X64-NEXT:  .LBB71_5: # %cond.load1
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    testb $4, %al
; AVX512FP16-X64-NEXT:    je .LBB71_7
; AVX512FP16-X64-NEXT:  .LBB71_8: # %cond.load4
; AVX512FP16-X64-NEXT:    movzwl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $8, %al
; AVX512FP16-X64-NEXT:    je .LBB71_10
; AVX512FP16-X64-NEXT:  .LBB71_11: # %cond.load7
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    je .LBB71_13
; AVX512FP16-X64-NEXT:  .LBB71_14: # %cond.load10
; AVX512FP16-X64-NEXT:    movzwl 8(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $32, %al
; AVX512FP16-X64-NEXT:    je .LBB71_16
; AVX512FP16-X64-NEXT:  .LBB71_17: # %cond.load13
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $64, %al
; AVX512FP16-X64-NEXT:    je .LBB71_19
; AVX512FP16-X64-NEXT:  .LBB71_20: # %cond.load16
; AVX512FP16-X64-NEXT:    movzwl 12(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    testb $-128, %al
; AVX512FP16-X64-NEXT:    je .LBB71_22
; AVX512FP16-X64-NEXT:  .LBB71_23: # %cond.load19
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:  .LBB71_24: # %else20
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: maskuloadu8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    subl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 120
; AVX512FP16-X86-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    kmovd %k0, %ecx
; AVX512FP16-X86-NEXT:    testb $1, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_1
; AVX512FP16-X86-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X86-NEXT:    movzwl (%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $2, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_5
; AVX512FP16-X86-NEXT:  .LBB71_4: # %else
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $4, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_8
; AVX512FP16-X86-NEXT:  .LBB71_7: # %else2
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $8, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_11
; AVX512FP16-X86-NEXT:  .LBB71_10: # %else5
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $16, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_14
; AVX512FP16-X86-NEXT:  .LBB71_13: # %else8
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $32, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_17
; AVX512FP16-X86-NEXT:  .LBB71_16: # %else11
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $64, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_20
; AVX512FP16-X86-NEXT:  .LBB71_19: # %else14
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $-128, %cl
; AVX512FP16-X86-NEXT:    jne .LBB71_23
; AVX512FP16-X86-NEXT:  .LBB71_22: # %else17
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    jmp .LBB71_24
; AVX512FP16-X86-NEXT:  .LBB71_1:
; AVX512FP16-X86-NEXT:    # implicit-def: $xmm0
; AVX512FP16-X86-NEXT:    # kill: killed $xmm0
; AVX512FP16-X86-NEXT:    testb $2, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_4
; AVX512FP16-X86-NEXT:  .LBB71_5: # %cond.load1
; AVX512FP16-X86-NEXT:    movzwl 2(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $4, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_7
; AVX512FP16-X86-NEXT:  .LBB71_8: # %cond.load4
; AVX512FP16-X86-NEXT:    movzwl 4(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $8, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_10
; AVX512FP16-X86-NEXT:  .LBB71_11: # %cond.load7
; AVX512FP16-X86-NEXT:    movzwl 6(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $16, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_13
; AVX512FP16-X86-NEXT:  .LBB71_14: # %cond.load10
; AVX512FP16-X86-NEXT:    movzwl 8(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $32, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_16
; AVX512FP16-X86-NEXT:  .LBB71_17: # %cond.load13
; AVX512FP16-X86-NEXT:    movzwl 10(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $64, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_19
; AVX512FP16-X86-NEXT:  .LBB71_20: # %cond.load16
; AVX512FP16-X86-NEXT:    movzwl 12(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    testb $-128, %cl
; AVX512FP16-X86-NEXT:    je .LBB71_22
; AVX512FP16-X86-NEXT:  .LBB71_23: # %cond.load19
; AVX512FP16-X86-NEXT:    movzwl 14(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:  .LBB71_24: # %else20
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    movzwl %dx, %edx
; AVX512FP16-X86-NEXT:    orl %eax, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $3, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
  %res = call <8 x bfloat> @llvm.masked.load.v8bf16.p0v8bf16(<8 x bfloat>* %addr, i32 4, <8 x i1> %mask, <8 x bfloat> undef)
  ret <8 x bfloat> %res
}

define <8 x bfloat> @maskzloadu8bf16(<8 x bfloat>* %addr, <8 x i1> %mask) {
; AVX512FP16-X64-LABEL: maskzloadu8bf16:
; AVX512FP16-X64:       # %bb.0:
; AVX512FP16-X64-NEXT:    pushq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    pushq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    subq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 80
; AVX512FP16-X64-NEXT:    .cfi_offset %rbx, -24
; AVX512FP16-X64-NEXT:    .cfi_offset %r14, -16
; AVX512FP16-X64-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X64-NEXT:    kmovd %k0, %eax
; AVX512FP16-X64-NEXT:    testb $1, %al
; AVX512FP16-X64-NEXT:    je .LBB72_1
; AVX512FP16-X64-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X64-NEXT:    movzwl (%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    movl %ecx, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    jmp .LBB72_3
; AVX512FP16-X64-NEXT:  .LBB72_1:
; AVX512FP16-X64-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vmovd %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:  .LBB72_3: # %else
; AVX512FP16-X64-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512FP16-X64-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX512FP16-X64-NEXT:    testb $2, %al
; AVX512FP16-X64-NEXT:    je .LBB72_4
; AVX512FP16-X64-NEXT:  # %bb.5: # %cond.load1
; AVX512FP16-X64-NEXT:    movzwl 2(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm5
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm6
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm3
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm2
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm0
; AVX512FP16-X64-NEXT:    testb $4, %al
; AVX512FP16-X64-NEXT:    jne .LBB72_7
; AVX512FP16-X64-NEXT:    jmp .LBB72_8
; AVX512FP16-X64-NEXT:  .LBB72_4: # %else
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm5
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm6
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm3
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm4
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, %xmm2
; AVX512FP16-X64-NEXT:    testb $4, %al
; AVX512FP16-X64-NEXT:    je .LBB72_8
; AVX512FP16-X64-NEXT:  .LBB72_7: # %cond.load4
; AVX512FP16-X64-NEXT:    movzwl 4(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm5
; AVX512FP16-X64-NEXT:  .LBB72_8: # %else5
; AVX512FP16-X64-NEXT:    testb $8, %al
; AVX512FP16-X64-NEXT:    jne .LBB72_9
; AVX512FP16-X64-NEXT:  # %bb.10: # %else8
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    jne .LBB72_11
; AVX512FP16-X64-NEXT:  .LBB72_12: # %else11
; AVX512FP16-X64-NEXT:    testb $32, %al
; AVX512FP16-X64-NEXT:    jne .LBB72_13
; AVX512FP16-X64-NEXT:  .LBB72_14: # %else14
; AVX512FP16-X64-NEXT:    testb $64, %al
; AVX512FP16-X64-NEXT:    je .LBB72_16
; AVX512FP16-X64-NEXT:  .LBB72_15: # %cond.load16
; AVX512FP16-X64-NEXT:    movzwl 12(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm2
; AVX512FP16-X64-NEXT:  .LBB72_16: # %else17
; AVX512FP16-X64-NEXT:    testb $-128, %al
; AVX512FP16-X64-NEXT:    vmovd %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    vmovd %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    vmovd %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    vmovd %xmm5, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    vmovd %xmm6, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X64-NEXT:    je .LBB72_17
; AVX512FP16-X64-NEXT:  # %bb.18: # %cond.load19
; AVX512FP16-X64-NEXT:    movzwl 14(%rdi), %eax
; AVX512FP16-X64-NEXT:    shll $16, %eax
; AVX512FP16-X64-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:    jmp .LBB72_19
; AVX512FP16-X64-NEXT:  .LBB72_9: # %cond.load7
; AVX512FP16-X64-NEXT:    movzwl 6(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm6
; AVX512FP16-X64-NEXT:    testb $16, %al
; AVX512FP16-X64-NEXT:    je .LBB72_12
; AVX512FP16-X64-NEXT:  .LBB72_11: # %cond.load10
; AVX512FP16-X64-NEXT:    movzwl 8(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm3
; AVX512FP16-X64-NEXT:    testb $32, %al
; AVX512FP16-X64-NEXT:    je .LBB72_14
; AVX512FP16-X64-NEXT:  .LBB72_13: # %cond.load13
; AVX512FP16-X64-NEXT:    movzwl 10(%rdi), %ecx
; AVX512FP16-X64-NEXT:    shll $16, %ecx
; AVX512FP16-X64-NEXT:    vmovd %ecx, %xmm4
; AVX512FP16-X64-NEXT:    testb $64, %al
; AVX512FP16-X64-NEXT:    jne .LBB72_15
; AVX512FP16-X64-NEXT:    jmp .LBB72_16
; AVX512FP16-X64-NEXT:  .LBB72_17: # %else17
; AVX512FP16-X64-NEXT:    vmovss %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X64-NEXT:  .LBB72_19: # %else20
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %r14d
; AVX512FP16-X64-NEXT:    orl %ebx, %r14d
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %ebx
; AVX512FP16-X64-NEXT:    shll $16, %ebx
; AVX512FP16-X64-NEXT:    vmovd {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX512FP16-X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X64-NEXT:    callq __truncsfbf2@PLT
; AVX512FP16-X64-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X64-NEXT:    movzwl %ax, %eax
; AVX512FP16-X64-NEXT:    orl %ebx, %eax
; AVX512FP16-X64-NEXT:    shlq $32, %rax
; AVX512FP16-X64-NEXT:    orq %r14, %rax
; AVX512FP16-X64-NEXT:    vmovq %rax, %xmm0
; AVX512FP16-X64-NEXT:    vmovdqa {{[-0-9]+}}(%r{{[sb]}}p), %xmm1 # 16-byte Reload
; AVX512FP16-X64-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512FP16-X64-NEXT:    addq $56, %rsp
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 24
; AVX512FP16-X64-NEXT:    popq %rbx
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 16
; AVX512FP16-X64-NEXT:    popq %r14
; AVX512FP16-X64-NEXT:    .cfi_def_cfa_offset 8
; AVX512FP16-X64-NEXT:    retq
;
; AVX512FP16-X86-LABEL: maskzloadu8bf16:
; AVX512FP16-X86:       # %bb.0:
; AVX512FP16-X86-NEXT:    subl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 120
; AVX512FP16-X86-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vpmovw2m %xmm0, %k0
; AVX512FP16-X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; AVX512FP16-X86-NEXT:    kmovd %k0, %ecx
; AVX512FP16-X86-NEXT:    testb $1, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_1
; AVX512FP16-X86-NEXT:  # %bb.2: # %cond.load
; AVX512FP16-X86-NEXT:    movzwl (%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    movl %edx, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; AVX512FP16-X86-NEXT:    jmp .LBB72_3
; AVX512FP16-X86-NEXT:  .LBB72_1:
; AVX512FP16-X86-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:  .LBB72_3: # %else
; AVX512FP16-X86-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512FP16-X86-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    testb $2, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_4
; AVX512FP16-X86-NEXT:  # %bb.5: # %cond.load1
; AVX512FP16-X86-NEXT:    movzwl 2(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm6
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm5
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm3
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm1
; AVX512FP16-X86-NEXT:    testb $4, %cl
; AVX512FP16-X86-NEXT:    jne .LBB72_7
; AVX512FP16-X86-NEXT:    jmp .LBB72_8
; AVX512FP16-X86-NEXT:  .LBB72_4: # %else
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm4
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm2
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm6
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm5
; AVX512FP16-X86-NEXT:    vmovdqa %xmm1, %xmm3
; AVX512FP16-X86-NEXT:    testb $4, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_8
; AVX512FP16-X86-NEXT:  .LBB72_7: # %cond.load4
; AVX512FP16-X86-NEXT:    movzwl 4(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm4
; AVX512FP16-X86-NEXT:  .LBB72_8: # %else5
; AVX512FP16-X86-NEXT:    testb $8, %cl
; AVX512FP16-X86-NEXT:    jne .LBB72_9
; AVX512FP16-X86-NEXT:  # %bb.10: # %else8
; AVX512FP16-X86-NEXT:    testb $16, %cl
; AVX512FP16-X86-NEXT:    jne .LBB72_11
; AVX512FP16-X86-NEXT:  .LBB72_12: # %else11
; AVX512FP16-X86-NEXT:    testb $32, %cl
; AVX512FP16-X86-NEXT:    jne .LBB72_13
; AVX512FP16-X86-NEXT:  .LBB72_14: # %else14
; AVX512FP16-X86-NEXT:    testb $64, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_16
; AVX512FP16-X86-NEXT:  .LBB72_15: # %cond.load16
; AVX512FP16-X86-NEXT:    movzwl 12(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm3
; AVX512FP16-X86-NEXT:  .LBB72_16: # %else17
; AVX512FP16-X86-NEXT:    testb $-128, %cl
; AVX512FP16-X86-NEXT:    vmovd %xmm1, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm2, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm3, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm4, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm5, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovd %xmm6, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Folded Spill
; AVX512FP16-X86-NEXT:    je .LBB72_18
; AVX512FP16-X86-NEXT:  # %bb.17: # %cond.load19
; AVX512FP16-X86-NEXT:    movzwl 14(%eax), %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd %eax, %xmm0
; AVX512FP16-X86-NEXT:  .LBB72_18: # %else20
; AVX512FP16-X86-NEXT:    vmovd %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    vmovss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; AVX512FP16-X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovss %xmm0, (%esp)
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    calll __truncsfbf2
; AVX512FP16-X86-NEXT:    fstps {{[0-9]+}}(%esp)
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm0, %edx
; AVX512FP16-X86-NEXT:    movzwl %dx, %edx
; AVX512FP16-X86-NEXT:    orl %eax, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm0
; AVX512FP16-X86-NEXT:    vpinsrd $1, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %eax
; AVX512FP16-X86-NEXT:    shll $16, %eax
; AVX512FP16-X86-NEXT:    vmovd {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX512FP16-X86-NEXT:    vmovd %xmm1, %ecx
; AVX512FP16-X86-NEXT:    movzwl %cx, %ecx
; AVX512FP16-X86-NEXT:    orl %eax, %ecx
; AVX512FP16-X86-NEXT:    vpinsrd $3, %ecx, %xmm0, %xmm0
; AVX512FP16-X86-NEXT:    addl $116, %esp
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 4
; AVX512FP16-X86-NEXT:    retl
; AVX512FP16-X86-NEXT:  .LBB72_9: # %cond.load7
; AVX512FP16-X86-NEXT:    .cfi_def_cfa_offset 120
; AVX512FP16-X86-NEXT:    movzwl 6(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm2
; AVX512FP16-X86-NEXT:    testb $16, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_12
; AVX512FP16-X86-NEXT:  .LBB72_11: # %cond.load10
; AVX512FP16-X86-NEXT:    movzwl 8(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm6
; AVX512FP16-X86-NEXT:    testb $32, %cl
; AVX512FP16-X86-NEXT:    je .LBB72_14
; AVX512FP16-X86-NEXT:  .LBB72_13: # %cond.load13
; AVX512FP16-X86-NEXT:    movzwl 10(%eax), %edx
; AVX512FP16-X86-NEXT:    shll $16, %edx
; AVX512FP16-X86-NEXT:    vmovd %edx, %xmm5
; AVX512FP16-X86-NEXT:    testb $64, %cl
; AVX512FP16-X86-NEXT:    jne .LBB72_15
; AVX512FP16-X86-NEXT:    jmp .LBB72_16
  %res = call <8 x bfloat> @llvm.masked.load.v8bf16.p0v8bf16(<8 x bfloat>* %addr, i32 4, <8 x i1> %mask, <8 x bfloat> zeroinitializer)
  ret <8 x bfloat> %res
}

; end INTEL_FEATURE_ISA_BF16_BASE
