; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_fp16
; This test checks that Global FMA optimizes an arbitrary (1 of ~40k) test case.
; For the input expression:
;   +a*b*c*d+a*b*c*e+a*d*f*g+a*e*f*g+b*c*d+b*c*e+d*f*g+e*f*g+b*c+f*g;
; the output code must have only 5 arithmetic instructions:
;   F0=f*g; F1=d+e; F2=b*c+F0; F3=F1*a+F1; F4=F3*F2+F2;

; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mattr=+avx512fp16 -fp-contract=fast -enable-unsafe-fp-math | FileCheck %s

@a16 = common global half 0.000000e+00, align 2
@b16 = common global half 0.000000e+00, align 2
@c16 = common global half 0.000000e+00, align 2
@d16 = common global half 0.000000e+00, align 2
@e16 = common global half 0.000000e+00, align 2
@f16 = common global half 0.000000e+00, align 2
@g16 = common global half 0.000000e+00, align 2
@dst16 = common global half 0.000000e+00, align 2
@h16 = common global half 0.000000e+00, align 2
@i16 = common global half 0.000000e+00, align 2

define void @func16()  {
; CHECK-LABEL: func16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a16@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq b16@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq c16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovsh (%rdx), %xmm0
; CHECK-NEXT:    movq d16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovsh (%rdx), %xmm1
; CHECK-NEXT:    movq e16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    movq f16@GOTPCREL(%rip), %rsi
; CHECK-NEXT:    movq g16@GOTPCREL(%rip), %rdi
; CHECK-NEXT:    vmovsh (%rdi), %xmm2
; CHECK-NEXT:    vmulsh (%rcx), %xmm0, %xmm0
; CHECK-NEXT:    vaddsh (%rdx), %xmm1, %xmm1
; CHECK-NEXT:    vfmadd231sh (%rsi), %xmm2, %xmm0
; CHECK-NEXT:    vfmadd132sh (%rax), %xmm1, %xmm1
; CHECK-NEXT:    vfmadd213sh %xmm0, %xmm0, %xmm1
; CHECK-NEXT:    movq dst16@GOTPCREL(%rip), %rax
; CHECK-NEXT:    vmovsh %xmm1, (%rax)
; CHECK-NEXT:    retq
entry:
  %load_a = load half, half* @a16, align 2
  %load_b = load half, half* @b16, align 2
  %mul = fmul fast half %load_a, %load_b
  %load_c = load half, half* @c16, align 2
  %mul1 = fmul fast half %mul, %load_c
  %load_d = load half, half* @d16, align 2
  %mul2 = fmul fast half %mul1, %load_d
  %load_e = load half, half* @e16, align 2
  %mul5 = fmul fast half %mul1, %load_e
  %add = fadd fast half %mul2, %mul5
  %mul6 = fmul fast half %load_a, %load_d
  %load_f = load half, half* @f16, align 2
  %mul7 = fmul fast half %mul6, %load_f
  %load_g = load half, half* @g16, align 2
  %mul8 = fmul fast half %mul7, %load_g
  %add9 = fadd fast half %add, %mul8
  %mul10 = fmul fast half %load_a, %load_e
  %mul11 = fmul fast half %mul10, %load_f
  %mul12 = fmul fast half %mul11, %load_g
  %add13 = fadd fast half %add9, %mul12
  %mul14 = fmul fast half %load_b, %load_c
  %mul15 = fmul fast half %mul14, %load_d
  %add16 = fadd fast half %add13, %mul15
  %mul18 = fmul fast half %mul14, %load_e
  %add19 = fadd fast half %add16, %mul18
  %mul20 = fmul fast half %load_d, %load_f
  %mul21 = fmul fast half %mul20, %load_g
  %add22 = fadd fast half %add19, %mul21
  %mul23 = fmul fast half %load_e, %load_f
  %mul24 = fmul fast half %mul23, %load_g
  %add25 = fadd fast half %add22, %mul24
  %add27 = fadd fast half %add25, %mul14
  %mul28 = fmul fast half %load_f, %load_g
  %add29 = fadd fast half %add27, %mul28
  store half %add29, half* @dst16, align 2
  ret void
}

@a16x8 = common global <8 x half> zeroinitializer, align 16
@b16x8 = common global <8 x half> zeroinitializer, align 16
@c16x8 = common global <8 x half> zeroinitializer, align 16
@d16x8 = common global <8 x half> zeroinitializer, align 16
@e16x8 = common global <8 x half> zeroinitializer, align 16
@f16x8 = common global <8 x half> zeroinitializer, align 16
@g16x8 = common global <8 x half> zeroinitializer, align 16
@dst16x8 = common global <8 x half> zeroinitializer, align 16
@h16x8 = common global <8 x half> zeroinitializer, align 16
@i16x8 = common global <8 x half> zeroinitializer, align 16

define void @func16x8()  {
; CHECK-LABEL: func16x8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a16x8@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq b16x8@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq c16x8@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %xmm0
; CHECK-NEXT:    movq d16x8@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %xmm1
; CHECK-NEXT:    movq e16x8@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    movq f16x8@GOTPCREL(%rip), %rsi
; CHECK-NEXT:    movq g16x8@GOTPCREL(%rip), %rdi
; CHECK-NEXT:    vmovaps (%rdi), %xmm2
; CHECK-NEXT:    vmulph (%rcx), %xmm0, %xmm0
; CHECK-NEXT:    vaddph (%rdx), %xmm1, %xmm1
; CHECK-NEXT:    vfmadd231ph (%rsi), %xmm2, %xmm0
; CHECK-NEXT:    vfmadd132ph (%rax), %xmm1, %xmm1
; CHECK-NEXT:    vfmadd213ph %xmm0, %xmm0, %xmm1
; CHECK-NEXT:    movq dst16x8@GOTPCREL(%rip), %rax
; CHECK-NEXT:    vmovaps %xmm1, (%rax)
; CHECK-NEXT:    retq
entry:
  %load_a = load <8 x half>, <8 x half>* @a16x8, align 16
  %load_b = load <8 x half>, <8 x half>* @b16x8, align 16
  %mul = fmul fast <8 x half> %load_a, %load_b
  %load_c = load <8 x half>, <8 x half>* @c16x8, align 16
  %mul1 = fmul fast <8 x half> %mul, %load_c
  %load_d = load <8 x half>, <8 x half>* @d16x8, align 16
  %mul2 = fmul fast <8 x half> %mul1, %load_d
  %load_e = load <8 x half>, <8 x half>* @e16x8, align 16
  %mul5 = fmul fast <8 x half> %mul1, %load_e
  %add = fadd fast <8 x half> %mul2, %mul5
  %mul6 = fmul fast <8 x half> %load_a, %load_d
  %load_f = load <8 x half>, <8 x half>* @f16x8, align 16
  %mul7 = fmul fast <8 x half> %mul6, %load_f
  %load_g = load <8 x half>, <8 x half>* @g16x8, align 16
  %mul8 = fmul fast <8 x half> %mul7, %load_g
  %add9 = fadd fast <8 x half> %add, %mul8
  %mul10 = fmul fast <8 x half> %load_a, %load_e
  %mul11 = fmul fast <8 x half> %mul10, %load_f
  %mul12 = fmul fast <8 x half> %mul11, %load_g
  %add13 = fadd fast <8 x half> %add9, %mul12
  %mul14 = fmul fast <8 x half> %load_b, %load_c
  %mul15 = fmul fast <8 x half> %mul14, %load_d
  %add16 = fadd fast <8 x half> %add13, %mul15
  %mul18 = fmul fast <8 x half> %mul14, %load_e
  %add19 = fadd fast <8 x half> %add16, %mul18
  %mul20 = fmul fast <8 x half> %load_d, %load_f
  %mul21 = fmul fast <8 x half> %mul20, %load_g
  %add22 = fadd fast <8 x half> %add19, %mul21
  %mul23 = fmul fast <8 x half> %load_e, %load_f
  %mul24 = fmul fast <8 x half> %mul23, %load_g
  %add25 = fadd fast <8 x half> %add22, %mul24
  %add27 = fadd fast <8 x half> %add25, %mul14
  %mul28 = fmul fast <8 x half> %load_f, %load_g
  %add29 = fadd fast <8 x half> %add27, %mul28
  store <8 x half> %add29, <8 x half>* @dst16x8, align 16
  ret void
}

@a16x16 = common global <16 x half> zeroinitializer, align 32
@b16x16 = common global <16 x half> zeroinitializer, align 32
@c16x16 = common global <16 x half> zeroinitializer, align 32
@d16x16 = common global <16 x half> zeroinitializer, align 32
@e16x16 = common global <16 x half> zeroinitializer, align 32
@f16x16 = common global <16 x half> zeroinitializer, align 32
@g16x16 = common global <16 x half> zeroinitializer, align 32
@dst16x16 = common global <16 x half> zeroinitializer, align 32
@h16x16 = common global <16 x half> zeroinitializer, align 32
@i16x16 = common global <16 x half> zeroinitializer, align 32

define void @func16x16()  {
; CHECK-LABEL: func16x16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a16x16@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq b16x16@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq c16x16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %ymm0
; CHECK-NEXT:    movq d16x16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %ymm1
; CHECK-NEXT:    movq e16x16@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    movq f16x16@GOTPCREL(%rip), %rsi
; CHECK-NEXT:    movq g16x16@GOTPCREL(%rip), %rdi
; CHECK-NEXT:    vmovaps (%rdi), %ymm2
; CHECK-NEXT:    vmulph (%rcx), %ymm0, %ymm0
; CHECK-NEXT:    vaddph (%rdx), %ymm1, %ymm1
; CHECK-NEXT:    vfmadd231ph (%rsi), %ymm2, %ymm0
; CHECK-NEXT:    vfmadd132ph (%rax), %ymm1, %ymm1
; CHECK-NEXT:    vfmadd213ph %ymm0, %ymm0, %ymm1
; CHECK-NEXT:    movq dst16x16@GOTPCREL(%rip), %rax
; CHECK-NEXT:    vmovaps %ymm1, (%rax)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %load_a = load <16 x half>, <16 x half>* @a16x16, align 32
  %load_b = load <16 x half>, <16 x half>* @b16x16, align 32
  %mul = fmul fast <16 x half> %load_a, %load_b
  %load_c = load <16 x half>, <16 x half>* @c16x16, align 32
  %mul1 = fmul fast <16 x half> %mul, %load_c
  %load_d = load <16 x half>, <16 x half>* @d16x16, align 32
  %mul2 = fmul fast <16 x half> %mul1, %load_d
  %load_e = load <16 x half>, <16 x half>* @e16x16, align 32
  %mul5 = fmul fast <16 x half> %mul1, %load_e
  %add = fadd fast <16 x half> %mul2, %mul5
  %mul6 = fmul fast <16 x half> %load_a, %load_d
  %load_f = load <16 x half>, <16 x half>* @f16x16, align 32
  %mul7 = fmul fast <16 x half> %mul6, %load_f
  %load_g = load <16 x half>, <16 x half>* @g16x16, align 32
  %mul8 = fmul fast <16 x half> %mul7, %load_g
  %add9 = fadd fast <16 x half> %add, %mul8
  %mul10 = fmul fast <16 x half> %load_a, %load_e
  %mul11 = fmul fast <16 x half> %mul10, %load_f
  %mul12 = fmul fast <16 x half> %mul11, %load_g
  %add13 = fadd fast <16 x half> %add9, %mul12
  %mul14 = fmul fast <16 x half> %load_b, %load_c
  %mul15 = fmul fast <16 x half> %mul14, %load_d
  %add16 = fadd fast <16 x half> %add13, %mul15
  %mul18 = fmul fast <16 x half> %mul14, %load_e
  %add19 = fadd fast <16 x half> %add16, %mul18
  %mul20 = fmul fast <16 x half> %load_d, %load_f
  %mul21 = fmul fast <16 x half> %mul20, %load_g
  %add22 = fadd fast <16 x half> %add19, %mul21
  %mul23 = fmul fast <16 x half> %load_e, %load_f
  %mul24 = fmul fast <16 x half> %mul23, %load_g
  %add25 = fadd fast <16 x half> %add22, %mul24
  %add27 = fadd fast <16 x half> %add25, %mul14
  %mul28 = fmul fast <16 x half> %load_f, %load_g
  %add29 = fadd fast <16 x half> %add27, %mul28
  store <16 x half> %add29, <16 x half>* @dst16x16, align 32
  ret void
}

@a16x32 = common global <32 x half> zeroinitializer, align 64
@b16x32 = common global <32 x half> zeroinitializer, align 64
@c16x32 = common global <32 x half> zeroinitializer, align 64
@d16x32 = common global <32 x half> zeroinitializer, align 64
@e16x32 = common global <32 x half> zeroinitializer, align 64
@f16x32 = common global <32 x half> zeroinitializer, align 64
@g16x32 = common global <32 x half> zeroinitializer, align 64
@dst16x32 = common global <32 x half> zeroinitializer, align 64
@h16x32 = common global <32 x half> zeroinitializer, align 64
@i16x32 = common global <32 x half> zeroinitializer, align 64

define void @func16x32()  {
; CHECK-LABEL: func16x32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a16x32@GOTPCREL(%rip), %rax
; CHECK-NEXT:    movq b16x32@GOTPCREL(%rip), %rcx
; CHECK-NEXT:    movq c16x32@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %zmm0
; CHECK-NEXT:    movq d16x32@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    vmovaps (%rdx), %zmm1
; CHECK-NEXT:    movq e16x32@GOTPCREL(%rip), %rdx
; CHECK-NEXT:    movq f16x32@GOTPCREL(%rip), %rsi
; CHECK-NEXT:    movq g16x32@GOTPCREL(%rip), %rdi
; CHECK-NEXT:    vmovaps (%rdi), %zmm2
; CHECK-NEXT:    vmulph (%rcx), %zmm0, %zmm0
; CHECK-NEXT:    vaddph (%rdx), %zmm1, %zmm1
; CHECK-NEXT:    vfmadd231ph (%rsi), %zmm2, %zmm0
; CHECK-NEXT:    vfmadd132ph (%rax), %zmm1, %zmm1
; CHECK-NEXT:    vfmadd213ph %zmm0, %zmm0, %zmm1
; CHECK-NEXT:    movq dst16x32@GOTPCREL(%rip), %rax
; CHECK-NEXT:    vmovaps %zmm1, (%rax)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %load_a = load <32 x half>, <32 x half>* @a16x32, align 64
  %load_b = load <32 x half>, <32 x half>* @b16x32, align 64
  %mul = fmul fast <32 x half> %load_a, %load_b
  %load_c = load <32 x half>, <32 x half>* @c16x32, align 64
  %mul1 = fmul fast <32 x half> %mul, %load_c
  %load_d = load <32 x half>, <32 x half>* @d16x32, align 64
  %mul2 = fmul fast <32 x half> %mul1, %load_d
  %load_e = load <32 x half>, <32 x half>* @e16x32, align 64
  %mul5 = fmul fast <32 x half> %mul1, %load_e
  %add = fadd fast <32 x half> %mul2, %mul5
  %mul6 = fmul fast <32 x half> %load_a, %load_d
  %load_f = load <32 x half>, <32 x half>* @f16x32, align 64
  %mul7 = fmul fast <32 x half> %mul6, %load_f
  %load_g = load <32 x half>, <32 x half>* @g16x32, align 64
  %mul8 = fmul fast <32 x half> %mul7, %load_g
  %add9 = fadd fast <32 x half> %add, %mul8
  %mul10 = fmul fast <32 x half> %load_a, %load_e
  %mul11 = fmul fast <32 x half> %mul10, %load_f
  %mul12 = fmul fast <32 x half> %mul11, %load_g
  %add13 = fadd fast <32 x half> %add9, %mul12
  %mul14 = fmul fast <32 x half> %load_b, %load_c
  %mul15 = fmul fast <32 x half> %mul14, %load_d
  %add16 = fadd fast <32 x half> %add13, %mul15
  %mul18 = fmul fast <32 x half> %mul14, %load_e
  %add19 = fadd fast <32 x half> %add16, %mul18
  %mul20 = fmul fast <32 x half> %load_d, %load_f
  %mul21 = fmul fast <32 x half> %mul20, %load_g
  %add22 = fadd fast <32 x half> %add19, %mul21
  %mul23 = fmul fast <32 x half> %load_e, %load_f
  %mul24 = fmul fast <32 x half> %mul23, %load_g
  %add25 = fadd fast <32 x half> %add22, %mul24
  %add27 = fadd fast <32 x half> %add25, %mul14
  %mul28 = fmul fast <32 x half> %load_f, %load_g
  %add29 = fadd fast <32 x half> %add27, %mul28
  store <32 x half> %add29, <32 x half>* @dst16x32, align 64
  ret void
}
