; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_fp16
; RUN: llc < %s -mtriple=x86_64-linux-android -mattr=+avx512fp16 | FileCheck %s --check-prefixes=X64,X64-AVX,X64-AVX512
; RUN: llc < %s -mtriple=x86_64-linux-gnu -mattr=+avx512fp16 | FileCheck %s --check-prefixes=X64,X64-AVX,X64-AVX512

; Check soft floating point conversion function calls.

@vf16 = common global half 0.000000e+00, align 2
@vf128 = common global fp128 0xL00000000000000000000000000000000, align 16

define void @TestFPExtF16_F128() nounwind strictfp {
; X64-SSE-LABEL: TestFPExtF16_F128:
; X64-SSE:       # %bb.0: # %entry
; X64-SSE-NEXT:    pushq %rax
; X64-SSE-NEXT:    movzwl {{.*}}(%rip), %edi
; X64-SSE-NEXT:    callq __gnu_h2f_ieee
; X64-SSE-NEXT:    callq __extendsftf2
; X64-SSE-NEXT:    movaps %xmm0, {{.*}}(%rip)
; X64-SSE-NEXT:    popq %rax
; X64-SSE-NEXT:    retq
;
; X64-AVX512-LABEL: TestFPExtF16_F128:
; X64-AVX512:       # %bb.0: # %entry
; X64-AVX512-NEXT:    pushq %rax
; X64-AVX512-NEXT:    vmovsh {{.*}}(%rip), %xmm0
; X64-AVX512-NEXT:    callq __extendhftf2
; X64-AVX512-NEXT:    vmovaps %xmm0, {{.*}}(%rip)
; X64-AVX512-NEXT:    popq %rax
; X64-AVX512-NEXT:    retq
;
; X86-LABEL: TestFPExtF16_F128:
; X86:       # %bb.0: # %entry
; X86-NEXT:    pushl %esi
; X86-NEXT:    subl $24, %esp
; X86-NEXT:    movzwl vf16, %eax
; X86-NEXT:    movl %eax, (%esp)
; X86-NEXT:    calll __gnu_h2f_ieee
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    wait
; X86-NEXT:    leal {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl %eax, (%esp)
; X86-NEXT:    calll __extendsftf2
; X86-NEXT:    subl $4, %esp
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %esi
; X86-NEXT:    movl %esi, vf128+12
; X86-NEXT:    movl %edx, vf128+8
; X86-NEXT:    movl %ecx, vf128+4
; X86-NEXT:    movl %eax, vf128
; X86-NEXT:    addl $24, %esp
; X86-NEXT:    popl %esi
; X86-NEXT:    retl
entry:
  %0 = load half, half* @vf16, align 2
  %conv = call fp128 @llvm.experimental.constrained.fpext.f128.f16(half %0, metadata !"fpexcept.strict") #0
  store fp128 %conv, fp128* @vf128, align 16
  ret void
}

define void @TestFPTruncF128_F16() nounwind strictfp {
; X64-SSE-LABEL: TestFPTruncF128_F16:
; X64-SSE:       # %bb.0: # %entry
; X64-SSE-NEXT:    pushq %rax
; X64-SSE-NEXT:    movaps {{.*}}(%rip), %xmm0
; X64-SSE-NEXT:    callq __trunctfhf2
; X64-SSE-NEXT:    movw %ax, {{.*}}(%rip)
; X64-SSE-NEXT:    popq %rax
; X64-SSE-NEXT:    retq
;
; X64-AVX512-LABEL: TestFPTruncF128_F16:
; X64-AVX512:       # %bb.0: # %entry
; X64-AVX512-NEXT:    pushq %rax
; X64-AVX512-NEXT:    vmovaps {{.*}}(%rip), %xmm0
; X64-AVX512-NEXT:    callq __trunctfhf2
; X64-AVX512-NEXT:    vmovsh %xmm0, {{.*}}(%rip)
; X64-AVX512-NEXT:    popq %rax
; X64-AVX512-NEXT:    retq
;
; X86-LABEL: TestFPTruncF128_F16:
; X86:       # %bb.0: # %entry
; X86-NEXT:    subl $12, %esp
; X86-NEXT:    pushl vf128+12
; X86-NEXT:    pushl vf128+8
; X86-NEXT:    pushl vf128+4
; X86-NEXT:    pushl vf128
; X86-NEXT:    calll __trunctfhf2
; X86-NEXT:    addl $16, %esp
; X86-NEXT:    movw %ax, vf16
; X86-NEXT:    addl $12, %esp
; X86-NEXT:    retl
entry:
  %0 = load fp128, fp128* @vf128, align 16
  %conv = call half @llvm.experimental.constrained.fptrunc.f16.f128(fp128 %0, metadata !"round.dynamic", metadata !"fpexcept.strict") #0
  store half %conv, half* @vf16, align 2
  ret void
}

declare half @llvm.experimental.constrained.fptrunc.f16.f128(fp128, metadata, metadata)
declare fp128 @llvm.experimental.constrained.fpext.f128.f16(half, metadata)
