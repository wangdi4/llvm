; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mcpu=sapphirerapids -fp-contract=fast -enable-unsafe-fp-math | FileCheck %s

define dso_local <16 x float> @test_fmaconj_r_nofast(<16 x float> %p1, <16 x float> %p2, <16 x float> %p3, <16 x float> %p4, <16 x float> %p5, <16 x float> %p6, <16 x float> %p7, <16 x float> %p8) {
; CHECK-LABEL: test_fmaconj_r_nofast:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfcmaddcph %zmm1, %zmm0, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm3, %zmm2, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm4, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm6, %zmm8
; CHECK-NEXT:    vmovaps %zmm8, %zmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p1, <16 x float> %p2, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r1 = tail call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p3, <16 x float> %p4, <16 x float> %r0, i16 -1, i32 4)
  %r2 = tail call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p5, <16 x float> %p6, <16 x float> %r1, i16 -1, i32 4)
  %r3 = tail call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p7, <16 x float> %p8, <16 x float> %r2, i16 -1, i32 4)
  ret <16 x float> %r3
}

; #define N 4
; __m512 test_fmaconj_sum(__m512 *x0, __m512 *x1) {
;
;   __m512h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm512_fcmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <16 x float> @test_fmaconj_sum(<16 x float>* nocapture readonly %x0, <16 x float>* nocapture readonly %x1) {
; CHECK-LABEL: test_fmaconj_sum:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %zmm1
; CHECK-NEXT:    vmovaps 64(%rdi), %zmm2
; CHECK-NEXT:    vmovaps 128(%rdi), %zmm3
; CHECK-NEXT:    vmovaps 192(%rdi), %zmm4
; CHECK-NEXT:    vfcmaddcph (%rsi), %zmm1, %zmm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfcmaddcph 64(%rsi), %zmm2, %zmm1
; CHECK-NEXT:    vfcmaddcph 128(%rsi), %zmm3, %zmm0
; CHECK-NEXT:    vfcmaddcph 192(%rsi), %zmm4, %zmm1
; CHECK-NEXT:    vaddph %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    retq
entry:
  %0 = load <16 x float>, <16 x float>* %x0, align 64
  %1 = load <16 x float>, <16 x float>* %x1, align 64
  %2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %0, <16 x float> %1, <16 x float> zeroinitializer, i16 -1, i32 4)
  %arrayidx.1 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 1
  %3 = load <16 x float>, <16 x float>* %arrayidx.1, align 64
  %arrayidx2.1 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 1
  %4 = load <16 x float>, <16 x float>* %arrayidx2.1, align 64
  %5 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %3, <16 x float> %4, <16 x float> %2, i16 -1, i32 4)
  %arrayidx.2 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 2
  %6 = load <16 x float>, <16 x float>* %arrayidx.2, align 64
  %arrayidx2.2 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 2
  %7 = load <16 x float>, <16 x float>* %arrayidx2.2, align 64
  %8 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %6, <16 x float> %7, <16 x float> %5, i16 -1, i32 4)
  %arrayidx.3 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 3
  %9 = load <16 x float>, <16 x float>* %arrayidx.3, align 64
  %arrayidx2.3 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 3
  %10 = load <16 x float>, <16 x float>* %arrayidx2.3, align 64
  %11 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %9, <16 x float> %10, <16 x float> %8, i16 -1, i32 4)
  ret <16 x float> %11
}

; __m256 test_fmaconj_sum256(__m256 *x0, __m256 *x1) {
;
;   __m256h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm256_fcmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <8 x float> @test_fmaconj_sum256(<8 x float>* nocapture readonly %x0, <8 x float>* nocapture readonly %x1) local_unnamed_addr {
; CHECK-LABEL: test_fmaconj_sum256:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %ymm1
; CHECK-NEXT:    vmovaps 32(%rdi), %ymm2
; CHECK-NEXT:    vmovaps 64(%rdi), %ymm3
; CHECK-NEXT:    vmovaps 96(%rdi), %ymm4
; CHECK-NEXT:    vfcmaddcph (%rsi), %ymm1, %ymm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfcmaddcph 32(%rsi), %ymm2, %ymm1
; CHECK-NEXT:    vfcmaddcph 64(%rsi), %ymm3, %ymm0
; CHECK-NEXT:    vfcmaddcph 96(%rsi), %ymm4, %ymm1
; CHECK-NEXT:    vaddph %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    retq
entry:
  %0 = load <8 x float>, <8 x float>* %x0, align 32
  %1 = load <8 x float>, <8 x float>* %x1, align 32
  %2 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %0, <8 x float> %1, <8 x float> zeroinitializer, i8 -1)
  %arrayidx.1 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 1
  %3 = load <8 x float>, <8 x float>* %arrayidx.1, align 32
  %arrayidx2.1 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 1
  %4 = load <8 x float>, <8 x float>* %arrayidx2.1, align 32
  %5 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %3, <8 x float> %4, <8 x float> %2, i8 -1)
  %arrayidx.2 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 2
  %6 = load <8 x float>, <8 x float>* %arrayidx.2, align 32
  %arrayidx2.2 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 2
  %7 = load <8 x float>, <8 x float>* %arrayidx2.2, align 32
  %8 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %6, <8 x float> %7, <8 x float> %5, i8 -1)
  %arrayidx.3 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 3
  %9 = load <8 x float>, <8 x float>* %arrayidx.3, align 32
  %arrayidx2.3 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 3
  %10 = load <8 x float>, <8 x float>* %arrayidx2.3, align 32
  %11 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %9, <8 x float> %10, <8 x float> %8, i8 -1)
  ret <8 x float> %11
}

; __m128 test_fmaconj_sum128(__m128 *x0, __m128 *x1) {
;
;   __m128h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm_fcmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <4 x float> @test_fmaconj_sum128(<4 x float>* nocapture readonly %x0, <4 x float>* nocapture readonly %x1) local_unnamed_addr {
; CHECK-LABEL: test_fmaconj_sum128:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %xmm1
; CHECK-NEXT:    vmovaps 16(%rdi), %xmm2
; CHECK-NEXT:    vmovaps 32(%rdi), %xmm3
; CHECK-NEXT:    vmovaps 48(%rdi), %xmm4
; CHECK-NEXT:    vfcmaddcph (%rsi), %xmm1, %xmm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfcmaddcph 16(%rsi), %xmm2, %xmm1
; CHECK-NEXT:    vfcmaddcph 32(%rsi), %xmm3, %xmm0
; CHECK-NEXT:    vfcmaddcph 48(%rsi), %xmm4, %xmm1
; CHECK-NEXT:    vaddph %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = bitcast <4 x float>* %x0 to <8 x float>*
  %CoalescedLoad = load <8 x float>, <8 x float>* %0, align 16
  %LoadCoalescingShuffle_ = shufflevector <8 x float> %CoalescedLoad, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_10 = shufflevector <8 x float> %CoalescedLoad, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1 = bitcast <4 x float>* %x1 to <8 x float>*
  %CoalescedLoad14 = load <8 x float>, <8 x float>* %1, align 16
  %LoadCoalescingShuffle_15 = shufflevector <8 x float> %CoalescedLoad14, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_16 = shufflevector <8 x float> %CoalescedLoad14, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %LoadCoalescingShuffle_, <4 x float> %LoadCoalescingShuffle_15, <4 x float> zeroinitializer, i8 -1)
  %3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %LoadCoalescingShuffle_10, <4 x float> %LoadCoalescingShuffle_16, <4 x float> %2, i8 -1)
  %arrayidx.2 = getelementptr inbounds <4 x float>, <4 x float>* %x0, i64 2
  %arrayidx2.2 = getelementptr inbounds <4 x float>, <4 x float>* %x1, i64 2
  %4 = bitcast <4 x float>* %arrayidx.2 to <8 x float>*
  %CoalescedLoad11 = load <8 x float>, <8 x float>* %4, align 16
  %LoadCoalescingShuffle_12 = shufflevector <8 x float> %CoalescedLoad11, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_13 = shufflevector <8 x float> %CoalescedLoad11, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = bitcast <4 x float>* %arrayidx2.2 to <8 x float>*
  %CoalescedLoad17 = load <8 x float>, <8 x float>* %5, align 16
  %LoadCoalescingShuffle_18 = shufflevector <8 x float> %CoalescedLoad17, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_19 = shufflevector <8 x float> %CoalescedLoad17, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %LoadCoalescingShuffle_12, <4 x float> %LoadCoalescingShuffle_18, <4 x float> %3, i8 -1)
  %7 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %LoadCoalescingShuffle_13, <4 x float> %LoadCoalescingShuffle_19, <4 x float> %6, i8 -1)
  ret <4 x float> %7
}

define dso_local <16 x float> @test_fmaconj_r(<16 x float> %p1, <16 x float> %p2, <16 x float> %p3, <16 x float> %p4, <16 x float> %p5, <16 x float> %p6, <16 x float> %p7, <16 x float> %p8) {
; CHECK-LABEL: test_fmaconj_r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfcmaddcph %zmm1, %zmm0, %zmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfcmaddcph %zmm3, %zmm2, %zmm0
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm4, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm6, %zmm0
; CHECK-NEXT:    vaddph %zmm0, %zmm8, %zmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p1, <16 x float> %p2, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r1 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p3, <16 x float> %p4, <16 x float> %r0, i16 -1, i32 4)
  %r2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p5, <16 x float> %p6, <16 x float> %r1, i16 -1, i32 4)
  %r3 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p7, <16 x float> %p8, <16 x float> %r2, i16 -1, i32 4)
  ret <16 x float> %r3
}

define dso_local <8 x float> @test_fmaconj_256r(<8 x float> %p1, <8 x float> %p2, <8 x float> %p3, <8 x float> %p4, <8 x float> %p5, <8 x float> %p6, <8 x float> %p7, <8 x float> %p8) {
; CHECK-LABEL: test_fmaconj_256r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfcmaddcph %ymm1, %ymm0, %ymm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfcmaddcph %ymm3, %ymm2, %ymm0
; CHECK-NEXT:    vfcmaddcph %ymm5, %ymm4, %ymm8
; CHECK-NEXT:    vfcmaddcph %ymm7, %ymm6, %ymm0
; CHECK-NEXT:    vaddph %ymm0, %ymm8, %ymm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %p1, <8 x float> %p2, <8 x float> zeroinitializer, i8 -1)
  %r1 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %p3, <8 x float> %p4, <8 x float> %r0, i8 -1)
  %r2 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %p5, <8 x float> %p6, <8 x float> %r1, i8 -1)
  %r3 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float> %p7, <8 x float> %p8, <8 x float> %r2, i8 -1)
  ret <8 x float> %r3
}

define dso_local <4 x float> @test_fmaconj_128r(<4 x float> %p1, <4 x float> %p2, <4 x float> %p3, <4 x float> %p4, <4 x float> %p5, <4 x float> %p6, <4 x float> %p7, <4 x float> %p8) {
; CHECK-LABEL: test_fmaconj_128r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfcmaddcph %xmm1, %xmm0, %xmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfcmaddcph %xmm3, %xmm2, %xmm0
; CHECK-NEXT:    vfcmaddcph %xmm5, %xmm4, %xmm8
; CHECK-NEXT:    vfcmaddcph %xmm7, %xmm6, %xmm0
; CHECK-NEXT:    vaddph %xmm0, %xmm8, %xmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %p1, <4 x float> %p2, <4 x float> zeroinitializer, i8 -1)
  %r1 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %p3, <4 x float> %p4, <4 x float> %r0, i8 -1)
  %r2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %p5, <4 x float> %p6, <4 x float> %r1, i8 -1)
  %r3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float> %p7, <4 x float> %p8, <4 x float> %r2, i8 -1)
  ret <4 x float> %r3
}

; __m512 test_fma_sum(__m512 *x0, __m512 *x1) {
;
;   __m512h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm512_fmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <16 x float> @test_fma_sum(<16 x float>* nocapture readonly %x0, <16 x float>* nocapture readonly %x1) local_unnamed_addr {
; CHECK-LABEL: test_fma_sum:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %zmm1
; CHECK-NEXT:    vmovaps 64(%rdi), %zmm2
; CHECK-NEXT:    vmovaps 128(%rdi), %zmm3
; CHECK-NEXT:    vmovaps 192(%rdi), %zmm4
; CHECK-NEXT:    vfmaddcph (%rsi), %zmm1, %zmm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfmaddcph 64(%rsi), %zmm2, %zmm1
; CHECK-NEXT:    vfmaddcph 128(%rsi), %zmm3, %zmm0
; CHECK-NEXT:    vfmaddcph 192(%rsi), %zmm4, %zmm1
; CHECK-NEXT:    vaddph %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    retq
entry:
  %0 = load <16 x float>, <16 x float>* %x0, align 64
  %1 = load <16 x float>, <16 x float>* %x1, align 64
  %2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %0, <16 x float> %1, <16 x float> zeroinitializer, i16 -1, i32 4)
  %arrayidx.1 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 1
  %3 = load <16 x float>, <16 x float>* %arrayidx.1, align 64
  %arrayidx2.1 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 1
  %4 = load <16 x float>, <16 x float>* %arrayidx2.1, align 64
  %5 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %3, <16 x float> %4, <16 x float> %2, i16 -1, i32 4)
  %arrayidx.2 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 2
  %6 = load <16 x float>, <16 x float>* %arrayidx.2, align 64
  %arrayidx2.2 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 2
  %7 = load <16 x float>, <16 x float>* %arrayidx2.2, align 64
  %8 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %6, <16 x float> %7, <16 x float> %5, i16 -1, i32 4)
  %arrayidx.3 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 3
  %9 = load <16 x float>, <16 x float>* %arrayidx.3, align 64
  %arrayidx2.3 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 3
  %10 = load <16 x float>, <16 x float>* %arrayidx2.3, align 64
  %11 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %9, <16 x float> %10, <16 x float> %8, i16 -1, i32 4)
  ret <16 x float> %11
}

; __m256 test_fma_sum256(__m256 *x0, __m256 *x1) {
;
;   __m256h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm256_fmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <8 x float> @test_fma_sum256(<8 x float>* nocapture readonly %x0, <8 x float>* nocapture readonly %x1) local_unnamed_addr {
; CHECK-LABEL: test_fma_sum256:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %ymm1
; CHECK-NEXT:    vmovaps 32(%rdi), %ymm2
; CHECK-NEXT:    vmovaps 64(%rdi), %ymm3
; CHECK-NEXT:    vmovaps 96(%rdi), %ymm4
; CHECK-NEXT:    vfmaddcph (%rsi), %ymm1, %ymm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfmaddcph 32(%rsi), %ymm2, %ymm1
; CHECK-NEXT:    vfmaddcph 64(%rsi), %ymm3, %ymm0
; CHECK-NEXT:    vfmaddcph 96(%rsi), %ymm4, %ymm1
; CHECK-NEXT:    vaddph %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    retq
entry:
  %0 = load <8 x float>, <8 x float>* %x0, align 32
  %1 = load <8 x float>, <8 x float>* %x1, align 32
  %2 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %0, <8 x float> %1, <8 x float> zeroinitializer, i8 -1)
  %arrayidx.1 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 1
  %3 = load <8 x float>, <8 x float>* %arrayidx.1, align 32
  %arrayidx2.1 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 1
  %4 = load <8 x float>, <8 x float>* %arrayidx2.1, align 32
  %5 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %3, <8 x float> %4, <8 x float> %2, i8 -1)
  %arrayidx.2 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 2
  %6 = load <8 x float>, <8 x float>* %arrayidx.2, align 32
  %arrayidx2.2 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 2
  %7 = load <8 x float>, <8 x float>* %arrayidx2.2, align 32
  %8 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %6, <8 x float> %7, <8 x float> %5, i8 -1)
  %arrayidx.3 = getelementptr inbounds <8 x float>, <8 x float>* %x0, i64 3
  %9 = load <8 x float>, <8 x float>* %arrayidx.3, align 32
  %arrayidx2.3 = getelementptr inbounds <8 x float>, <8 x float>* %x1, i64 3
  %10 = load <8 x float>, <8 x float>* %arrayidx2.3, align 32
  %11 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %9, <8 x float> %10, <8 x float> %8, i8 -1)
  ret <8 x float> %11
}

; __m128 test_fma_sum128(__m128 *x0, __m128 *x1) {
;
;   __m128h sum = { 0 };
;   for (int i = 0; i < N; i++) {
;     sum = _mm_fmadd_pch(x0[i], x1[i], sum);
;   }
;
;   return sum;
; }
define dso_local <4 x float> @test_fma_sum128(<4 x float>* nocapture readonly %x0, <4 x float>* nocapture readonly %x1) local_unnamed_addr {
; CHECK-LABEL: test_fma_sum128:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps (%rdi), %xmm1
; CHECK-NEXT:    vmovaps 16(%rdi), %xmm2
; CHECK-NEXT:    vmovaps 32(%rdi), %xmm3
; CHECK-NEXT:    vmovaps 48(%rdi), %xmm4
; CHECK-NEXT:    vfmaddcph (%rsi), %xmm1, %xmm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfmaddcph 16(%rsi), %xmm2, %xmm1
; CHECK-NEXT:    vfmaddcph 32(%rsi), %xmm3, %xmm0
; CHECK-NEXT:    vfmaddcph 48(%rsi), %xmm4, %xmm1
; CHECK-NEXT:    vaddph %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = bitcast <4 x float>* %x0 to <8 x float>*
  %CoalescedLoad = load <8 x float>, <8 x float>* %0, align 16
  %LoadCoalescingShuffle_ = shufflevector <8 x float> %CoalescedLoad, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_10 = shufflevector <8 x float> %CoalescedLoad, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %1 = bitcast <4 x float>* %x1 to <8 x float>*
  %CoalescedLoad14 = load <8 x float>, <8 x float>* %1, align 16
  %LoadCoalescingShuffle_15 = shufflevector <8 x float> %CoalescedLoad14, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_16 = shufflevector <8 x float> %CoalescedLoad14, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %LoadCoalescingShuffle_, <4 x float> %LoadCoalescingShuffle_15, <4 x float> zeroinitializer, i8 -1)
  %3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %LoadCoalescingShuffle_10, <4 x float> %LoadCoalescingShuffle_16, <4 x float> %2, i8 -1)
  %arrayidx.2 = getelementptr inbounds <4 x float>, <4 x float>* %x0, i64 2
  %arrayidx2.2 = getelementptr inbounds <4 x float>, <4 x float>* %x1, i64 2
  %4 = bitcast <4 x float>* %arrayidx.2 to <8 x float>*
  %CoalescedLoad11 = load <8 x float>, <8 x float>* %4, align 16
  %LoadCoalescingShuffle_12 = shufflevector <8 x float> %CoalescedLoad11, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_13 = shufflevector <8 x float> %CoalescedLoad11, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %5 = bitcast <4 x float>* %arrayidx2.2 to <8 x float>*
  %CoalescedLoad17 = load <8 x float>, <8 x float>* %5, align 16
  %LoadCoalescingShuffle_18 = shufflevector <8 x float> %CoalescedLoad17, <8 x float> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
  %LoadCoalescingShuffle_19 = shufflevector <8 x float> %CoalescedLoad17, <8 x float> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %6 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %LoadCoalescingShuffle_12, <4 x float> %LoadCoalescingShuffle_18, <4 x float> %3, i8 -1)
  %7 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %LoadCoalescingShuffle_13, <4 x float> %LoadCoalescingShuffle_19, <4 x float> %6, i8 -1)
  ret <4 x float> %7
}

define dso_local <16 x float> @test_fma_r(<16 x float> %p1, <16 x float> %p2, <16 x float> %p3, <16 x float> %p4, <16 x float> %p5, <16 x float> %p6, <16 x float> %p7, <16 x float> %p8) {
; CHECK-LABEL: test_fma_r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfmaddcph %zmm1, %zmm0, %zmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %zmm3, %zmm2, %zmm0
; CHECK-NEXT:    vfmaddcph %zmm5, %zmm4, %zmm8
; CHECK-NEXT:    vfmaddcph %zmm7, %zmm6, %zmm0
; CHECK-NEXT:    vaddph %zmm0, %zmm8, %zmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %p1, <16 x float> %p2, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r1 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %p3, <16 x float> %p4, <16 x float> %r0, i16 -1, i32 4)
  %r2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %p5, <16 x float> %p6, <16 x float> %r1, i16 -1, i32 4)
  %r3 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> %p7, <16 x float> %p8, <16 x float> %r2, i16 -1, i32 4)
  ret <16 x float> %r3
}

define dso_local <8 x float> @test_fma_256r(<8 x float> %p1, <8 x float> %p2, <8 x float> %p3, <8 x float> %p4, <8 x float> %p5, <8 x float> %p6, <8 x float> %p7, <8 x float> %p8) {
; CHECK-LABEL: test_fma_256r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfmaddcph %ymm1, %ymm0, %ymm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %ymm3, %ymm2, %ymm0
; CHECK-NEXT:    vfmaddcph %ymm5, %ymm4, %ymm8
; CHECK-NEXT:    vfmaddcph %ymm7, %ymm6, %ymm0
; CHECK-NEXT:    vaddph %ymm0, %ymm8, %ymm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %p1, <8 x float> %p2, <8 x float> zeroinitializer, i8 -1)
  %r1 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %p3, <8 x float> %p4, <8 x float> %r0, i8 -1)
  %r2 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %p5, <8 x float> %p6, <8 x float> %r1, i8 -1)
  %r3 = tail call fast <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float> %p7, <8 x float> %p8, <8 x float> %r2, i8 -1)
  ret <8 x float> %r3
}

define dso_local <4 x float> @test_fma_128r(<4 x float> %p1, <4 x float> %p2, <4 x float> %p3, <4 x float> %p4, <4 x float> %p5, <4 x float> %p6, <4 x float> %p7, <4 x float> %p8) {
; CHECK-LABEL: test_fma_128r:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfmaddcph %xmm1, %xmm0, %xmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm3, %xmm2, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm5, %xmm4, %xmm8
; CHECK-NEXT:    vfmaddcph %xmm7, %xmm6, %xmm0
; CHECK-NEXT:    vaddph %xmm0, %xmm8, %xmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p1, <4 x float> %p2, <4 x float> zeroinitializer, i8 -1)
  %r1 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p3, <4 x float> %p4, <4 x float> %r0, i8 -1)
  %r2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p5, <4 x float> %p6, <4 x float> %r1, i8 -1)
  %r3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p7, <4 x float> %p8, <4 x float> %r2, i8 -1)
  ret <4 x float> %r3
}

define dso_local <4 x float> @test_fma_128r_2user(<4 x float> %p1, <4 x float> %p2, <4 x float> %p3, <4 x float> %p4, <4 x float> %p5, <4 x float> %p6, <4 x float> %p7, <4 x float> %p8) {
; CHECK-LABEL: test_fma_128r_2user:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfmaddcph %xmm1, %xmm0, %xmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm3, %xmm2, %xmm0
; CHECK-NEXT:    vaddph %xmm0, %xmm8, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, %xmm1
; CHECK-NEXT:    vfmaddcph %xmm3, %xmm2, %xmm1
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm5, %xmm4, %xmm0
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfmaddcph %xmm7, %xmm6, %xmm1
; CHECK-NEXT:    vaddph %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p1, <4 x float> %p2, <4 x float> zeroinitializer, i8 -1)
  %r10 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p3, <4 x float> %p4, <4 x float> %r0, i8 -1)
  %r11 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p3, <4 x float> %p4, <4 x float> %r10,i8 -1)
  %r1 = fadd fast <4 x float> %r10, %r11
  %r2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p5, <4 x float> %p6, <4 x float> %r1, i8 -1)
  %r3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p7, <4 x float> %p8, <4 x float> %r2, i8 -1)
  ret <4 x float> %r3
}

define dso_local <4 x float> @test_fma_128r_result2user(<4 x float> %p1, <4 x float> %p2, <4 x float> %p3, <4 x float> %p4, <4 x float> %p5, <4 x float> %p6, <4 x float> %p7, <4 x float> %p8) {
; CHECK-LABEL: test_fma_128r_result2user:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfmaddcph %xmm1, %xmm0, %xmm8
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm3, %xmm2, %xmm0
; CHECK-NEXT:    vfmaddcph %xmm5, %xmm4, %xmm8
; CHECK-NEXT:    vaddph %xmm8, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, %xmm1
; CHECK-NEXT:    vfmaddcph %xmm7, %xmm6, %xmm1
; CHECK-NEXT:    vaddps %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p1, <4 x float> %p2, <4 x float> zeroinitializer, i8 -1)
  %r1 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p3, <4 x float> %p4, <4 x float> %r0, i8 -1)
  %r2 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p5, <4 x float> %p6, <4 x float> %r1, i8 -1)
  %r3 = tail call fast <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float> %p7, <4 x float> %p8, <4 x float> %r2, i8 -1)
  %r4 = fadd fast <4 x float> %r3, %r2
  ret <4 x float> %r4
}

define dso_local <16 x float> @test_fmaconj_sum_loop5(<16 x float>* nocapture readonly %x0, <16 x float>* nocapture readonly %x1) {
; CHECK-LABEL: test_fmaconj_sum_loop5:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vmovaps (%rdi), %zmm0
; CHECK-NEXT:    vmovaps 64(%rdi), %zmm1
; CHECK-NEXT:    vmovaps 128(%rdi), %zmm2
; CHECK-NEXT:    vxorps %xmm3, %xmm3, %xmm3
; CHECK-NEXT:    vmovaps 192(%rdi), %zmm4
; CHECK-NEXT:    vfcmaddcph (%rsi), %zmm0, %zmm3
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    vfcmaddcph 64(%rsi), %zmm1, %zmm0
; CHECK-NEXT:    vfcmaddcph 128(%rsi), %zmm2, %zmm3
; CHECK-NEXT:    vfcmaddcph 192(%rsi), %zmm4, %zmm0
; CHECK-NEXT:    vmovaps 256(%rdi), %zmm1
; CHECK-NEXT:    vfcmaddcph 256(%rsi), %zmm1, %zmm3
; CHECK-NEXT:    vaddph %zmm3, %zmm0, %zmm0
; CHECK-NEXT:    retq
entry:
  %0 = load <16 x float>, <16 x float>* %x0, align 64
  %1 = load <16 x float>, <16 x float>* %x1, align 64
  %2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %0, <16 x float> %1, <16 x float> zeroinitializer, i16 -1, i32 4)
  %arrayidx.1 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 1
  %3 = load <16 x float>, <16 x float>* %arrayidx.1, align 64
  %arrayidx2.1 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 1
  %4 = load <16 x float>, <16 x float>* %arrayidx2.1, align 64
  %5 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %3, <16 x float> %4, <16 x float> %2, i16 -1, i32 4)
  %arrayidx.2 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 2
  %6 = load <16 x float>, <16 x float>* %arrayidx.2, align 64
  %arrayidx2.2 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 2
  %7 = load <16 x float>, <16 x float>* %arrayidx2.2, align 64
  %8 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %6, <16 x float> %7, <16 x float> %5, i16 -1, i32 4)
  %arrayidx.3 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 3
  %9 = load <16 x float>, <16 x float>* %arrayidx.3, align 64
  %arrayidx2.3 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 3
  %10 = load <16 x float>, <16 x float>* %arrayidx2.3, align 64
  %11 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %9, <16 x float> %10, <16 x float> %8, i16 -1, i32 4)
  %arrayidx.4 = getelementptr inbounds <16 x float>, <16 x float>* %x0, i64 4
  %12 = load <16 x float>, <16 x float>* %arrayidx.4, align 64
  %arrayidx2.4 = getelementptr inbounds <16 x float>, <16 x float>* %x1, i64 4
  %13 = load <16 x float>, <16 x float>* %arrayidx2.4, align 64
  %14 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %12, <16 x float> %13, <16 x float> %11, i16 -1, i32 4)
  ret <16 x float> %14
}

define dso_local <16 x float> @test_fmaconj_r_op2(<16 x float> %p1, <16 x float> %p2, <16 x float> %p3, <16 x float> %p4, <16 x float> %p5, <16 x float> %p6, <16 x float> %p7, <16 x float> %p8) {
; CHECK-LABEL: test_fmaconj_r_op2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vfcmaddcph %zmm1, %zmm0, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm3, %zmm8, %zmm2
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm2, %zmm4
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm4, %zmm6
; CHECK-NEXT:    vmovaps %zmm6, %zmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p1, <16 x float> %p2, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r1 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %r0, <16 x float> %p4, <16 x float> %p3, i16 -1, i32 4)
  %r2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %r1, <16 x float> %p6, <16 x float> %p5, i16 -1, i32 4)
  %r3 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %r2, <16 x float> %p8, <16 x float> %p7, i16 -1, i32 4)
  ret <16 x float> %r3
}

define dso_local <16 x float> @test_fmaconj_r_3list(<16 x float> %p1, <16 x float> %p2, <16 x float> %p3, <16 x float> %p4, <16 x float> %p5, <16 x float> %p6, <16 x float> %p7, <16 x float> %p8) {
; CHECK-LABEL: test_fmaconj_r_3list:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm8, %xmm8, %xmm8
; CHECK-NEXT:    vxorps %xmm9, %xmm9, %xmm9
; CHECK-NEXT:    vfcmaddcph %zmm1, %zmm0, %zmm9
; CHECK-NEXT:    vfcmaddcph %zmm3, %zmm2, %zmm9
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm4, %zmm9
; CHECK-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm6, %zmm1
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm4, %zmm1
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm6, %zmm1
; CHECK-NEXT:    vaddps %zmm1, %zmm9, %zmm1
; CHECK-NEXT:    vfcmaddcph %zmm6, %zmm0, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm5, %zmm2, %zmm8
; CHECK-NEXT:    vfcmaddcph %zmm7, %zmm4, %zmm8
; CHECK-NEXT:    vaddps %zmm8, %zmm1, %zmm0
; CHECK-NEXT:    retq
entry:
  %r0 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p1, <16 x float> %p2, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r1 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p3, <16 x float> %p4, <16 x float> %r0, i16 -1, i32 4)
  %r2 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p5, <16 x float> %p6, <16 x float> %r1, i16 -1, i32 4)

  %r3 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p7, <16 x float> %p8, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r4 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p5, <16 x float> %p6, <16 x float> %r3, i16 -1, i32 4)
  %r5 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p7, <16 x float> %p8, <16 x float> %r4, i16 -1, i32 4)

  %r6 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p1, <16 x float> %p7, <16 x float> zeroinitializer, i16 -1, i32 4)
  %r7 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p3, <16 x float> %p6, <16 x float> %r6, i16 -1, i32 4)
  %r8 = tail call fast <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> %p5, <16 x float> %p8, <16 x float> %r7, i16 -1, i32 4)

  %r9 = fadd fast <16 x float> %r2, %r5
  %r10 = fadd fast <16 x float> %r9, %r8
  ret <16 x float> %r10
}

declare <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float>, <16 x float>, <16 x float>, i16, i32 immarg)
declare <8 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.256(<8 x float>, <8 x float>, <8 x float>, i8)
declare <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.128(<4 x float>, <4 x float>, <4 x float>, i8)
declare <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float>, <16 x float>, <16 x float>, i16, i32 immarg)
declare <8 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.256(<8 x float>, <8 x float>, <8 x float>, i8)
declare <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.128(<4 x float>, <4 x float>, <4 x float>, i8)
