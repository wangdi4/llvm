; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; REQUIRES: intel_feature_isa_fp16
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+sse2 | FileCheck %s --check-prefixes=SSE
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+sse4.1 | FileCheck %s --check-prefixes=SSE
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx | FileCheck %s --check-prefixes=AVX
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx2 | FileCheck %s --check-prefixes=AVX
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512bw | FileCheck %s --check-prefixes=AVX512
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512bw,+avx512vl | FileCheck %s --check-prefixes=AVX512

define half @test_v2f16(<2 x half> %a0) nounwind {
; SSE-LABEL: test_v2f16:
; SSE:       # %bb.0:
; SSE-NEXT:    pushq %rbp
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    pushq %rax
; SSE-NEXT:    movl %esi, %ebx
; SSE-NEXT:    movl %edi, %ebp
; SSE-NEXT:    movzwl %bx, %edi
; SSE-NEXT:    callq __gnu_h2f_ieee@PLT
; SSE-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; SSE-NEXT:    movzwl %bp, %edi
; SSE-NEXT:    callq __gnu_h2f_ieee@PLT
; SSE-NEXT:    xorl %eax, %eax
; SSE-NEXT:    ucomiss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; SSE-NEXT:    seta %al
; SSE-NEXT:    negl %eax
; SSE-NEXT:    testb $1, %al
; SSE-NEXT:    cmovnel %ebp, %ebx
; SSE-NEXT:    movl %ebx, %eax
; SSE-NEXT:    addq $8, %rsp
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %rbp
; SSE-NEXT:    retq
;
; AVX-LABEL: test_v2f16:
; AVX:       # %bb.0:
; AVX-NEXT:    pushq %rbp
; AVX-NEXT:    pushq %rbx
; AVX-NEXT:    pushq %rax
; AVX-NEXT:    movl %esi, %ebx
; AVX-NEXT:    movl %edi, %ebp
; AVX-NEXT:    movzwl %bx, %edi
; AVX-NEXT:    callq __gnu_h2f_ieee@PLT
; AVX-NEXT:    vmovss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; AVX-NEXT:    movzwl %bp, %edi
; AVX-NEXT:    callq __gnu_h2f_ieee@PLT
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    vucomiss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; AVX-NEXT:    seta %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    testb $1, %al
; AVX-NEXT:    cmovnel %ebp, %ebx
; AVX-NEXT:    movl %ebx, %eax
; AVX-NEXT:    addq $8, %rsp
; AVX-NEXT:    popq %rbx
; AVX-NEXT:    popq %rbp
; AVX-NEXT:    retq
;
; AVX512-LABEL: test_v2f16:
; AVX512:       # %bb.0:
; AVX512-NEXT:    movl %esi, %eax
; AVX512-NEXT:    movzwl %ax, %ecx
; AVX512-NEXT:    vmovd %ecx, %xmm0
; AVX512-NEXT:    vcvtph2ps %xmm0, %xmm0
; AVX512-NEXT:    movzwl %di, %ecx
; AVX512-NEXT:    vmovd %ecx, %xmm1
; AVX512-NEXT:    vcvtph2ps %xmm1, %xmm1
; AVX512-NEXT:    vucomiss %xmm0, %xmm1
; AVX512-NEXT:    cmoval %edi, %eax
; AVX512-NEXT:    # kill: def $ax killed $ax killed $eax
; AVX512-NEXT:    retq
  %1 = call nnan half @llvm.vector.reduce.fmax.v2f16(<2 x half> %a0)
  ret half %1
}
declare float @llvm.vector.reduce.fmax.v1f32(<1 x float>)
declare float @llvm.vector.reduce.fmax.v2f32(<2 x float>)
declare float @llvm.vector.reduce.fmax.v4f32(<4 x float>)
declare float @llvm.vector.reduce.fmax.v8f32(<8 x float>)
declare float @llvm.vector.reduce.fmax.v16f32(<16 x float>)

declare double @llvm.vector.reduce.fmax.v2f64(<2 x double>)
declare double @llvm.vector.reduce.fmax.v3f64(<3 x double>)
declare double @llvm.vector.reduce.fmax.v4f64(<4 x double>)
declare double @llvm.vector.reduce.fmax.v8f64(<8 x double>)
declare double @llvm.vector.reduce.fmax.v16f64(<16 x double>)

declare half @llvm.vector.reduce.fmax.v2f16(<2 x half>)
