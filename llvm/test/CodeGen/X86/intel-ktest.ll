; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -disable-peephole -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512dq,+avx512bw -enable-intel-advanced-opts | FileCheck %s --check-prefixes=CHECK,X64
; RUN: llc < %s -disable-peephole -mtriple=i686-unknown-unknown -mattr=+avx512f,+avx512dq,+avx512bw -enable-intel-advanced-opts | FileCheck %s --check-prefixes=CHECK,X86

declare i8 @llvm.x86.avx512.mask.cmp.sd(<2 x double>, <2 x double>, i32, i8, i32)

define i8@test_int_x86_avx512_mask_cmp_sd_all(<2 x double> %x0, <2 x double> %x1, i8 %x3, i32 %x4) {
; X64-LABEL: test_int_x86_avx512_mask_cmp_sd_all:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1
; X64-NEXT:    vcmplesd %xmm1, %xmm0, %k0
; X64-NEXT:    vcmpunordsd {sae}, %xmm1, %xmm0, %k2
; X64-NEXT:    vcmpneqsd %xmm1, %xmm0, %k3 {%k1}
; X64-NEXT:    vcmpnltsd {sae}, %xmm1, %xmm0, %k1 {%k1}
; X64-NEXT:    korb %k2, %k0, %k0
; X64-NEXT:    korb %k1, %k3, %k1
; X64-NEXT:    korb %k1, %k0, %k0
; X64-NEXT:    kmovd %k0, %eax
; X64-NEXT:    # kill: def $al killed $al killed $eax
; X64-NEXT:    retq
;
; X86-LABEL: test_int_x86_avx512_mask_cmp_sd_all:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-NEXT:    vcmplesd %xmm1, %xmm0, %k0
; X86-NEXT:    vcmpunordsd {sae}, %xmm1, %xmm0, %k2
; X86-NEXT:    vcmpneqsd %xmm1, %xmm0, %k3 {%k1}
; X86-NEXT:    vcmpnltsd {sae}, %xmm1, %xmm0, %k1 {%k1}
; X86-NEXT:    korb %k2, %k0, %k0
; X86-NEXT:    korb %k1, %k3, %k1
; X86-NEXT:    korb %k1, %k0, %k0
; X86-NEXT:    kmovd %k0, %eax
; X86-NEXT:    # kill: def $al killed $al killed $eax
; X86-NEXT:    retl

  %res1 = call i8 @llvm.x86.avx512.mask.cmp.sd(<2 x double> %x0, <2 x double> %x1, i32 2, i8 -1, i32 4)
  %res2 = call i8 @llvm.x86.avx512.mask.cmp.sd(<2 x double> %x0, <2 x double> %x1, i32 3, i8 -1, i32 8)
  %res3 = call i8 @llvm.x86.avx512.mask.cmp.sd(<2 x double> %x0, <2 x double> %x1, i32 4, i8 %x3, i32 4)
  %res4 = call i8 @llvm.x86.avx512.mask.cmp.sd(<2 x double> %x0, <2 x double> %x1, i32 5, i8 %x3, i32 8)

  %res11 = or i8 %res1, %res2
  %res12 = or i8 %res3, %res4
  %res13 = or i8 %res11, %res12
  ret i8 %res13
}

declare i8 @llvm.x86.avx512.mask.cmp.ss(<4 x float>, <4 x float>, i32, i8, i32)

define i8@test_int_x86_avx512_mask_cmp_ss_all(<4 x float> %x0, <4 x float> %x1, i8 %x3, i32 %x4) {
; X64-LABEL: test_int_x86_avx512_mask_cmp_ss_all:
; X64:       # %bb.0:
; X64-NEXT:    kmovd %edi, %k1
; X64-NEXT:    vcmpless %xmm1, %xmm0, %k2
; X64-NEXT:    vcmpneqss %xmm1, %xmm0, %k0 {%k1}
; X64-NEXT:    vcmpnltss {sae}, %xmm1, %xmm0, %k1 {%k1}
; X64-NEXT:    vcmpunordss {sae}, %xmm1, %xmm0, %k2 {%k2}
; X64-NEXT:    kandb %k1, %k0, %k0
; X64-NEXT:    kandb %k0, %k2, %k0
; X64-NEXT:    kmovd %k0, %eax
; X64-NEXT:    # kill: def $al killed $al killed $eax
; X64-NEXT:    retq
;
; X86-LABEL: test_int_x86_avx512_mask_cmp_ss_all:
; X86:       # %bb.0:
; X86-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-NEXT:    vcmpless %xmm1, %xmm0, %k2
; X86-NEXT:    vcmpneqss %xmm1, %xmm0, %k0 {%k1}
; X86-NEXT:    vcmpnltss {sae}, %xmm1, %xmm0, %k1 {%k1}
; X86-NEXT:    vcmpunordss {sae}, %xmm1, %xmm0, %k2 {%k2}
; X86-NEXT:    kandb %k1, %k0, %k0
; X86-NEXT:    kandb %k0, %k2, %k0
; X86-NEXT:    kmovd %k0, %eax
; X86-NEXT:    # kill: def $al killed $al killed $eax
; X86-NEXT:    retl
  %res1 = call i8 @llvm.x86.avx512.mask.cmp.ss(<4 x float> %x0, <4 x float> %x1, i32 2, i8 -1, i32 4)
  %res2 = call i8 @llvm.x86.avx512.mask.cmp.ss(<4 x float> %x0, <4 x float> %x1, i32 3, i8 -1, i32 8)
  %res3 = call i8 @llvm.x86.avx512.mask.cmp.ss(<4 x float> %x0, <4 x float> %x1, i32 4, i8 %x3, i32 4)
  %res4 = call i8 @llvm.x86.avx512.mask.cmp.ss(<4 x float> %x0, <4 x float> %x1, i32 5, i8 %x3, i32 8)

  %res11 = and i8 %res1, %res2
  %res12 = and i8 %res3, %res4
  %res13 = and i8 %res11, %res12
  ret i8 %res13
}

define dso_local i1 @test_mask(<6 x float>* %in_ptr) {
; X64-LABEL: test_mask:
; X64:       # %bb.0: # %entry
; X64-NEXT:    vmovaps (%rdi), %ymm0
; X64-NEXT:    vmovsldup {{.*#+}} ymm1 = ymm0[0,0,2,2,4,4,6,6]
; X64-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X64-NEXT:    movb $63, %al
; X64-NEXT:    kmovb %eax, %k1
; X64-NEXT:    vcmpltps %zmm1, %zmm2, %k0 {%k1}
; X64-NEXT:    movb $42, %al
; X64-NEXT:    kmovb %eax, %k1
; X64-NEXT:    vcmpltps %zmm0, %zmm2, %k1 {%k1}
; X64-NEXT:    kortestb %k1, %k0
; X64-NEXT:    sete %al
; X64-NEXT:    vzeroupper
; X64-NEXT:    retq
;
; X86-LABEL: test_mask:
; X86:       # %bb.0: # %entry
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    vmovaps (%eax), %ymm0
; X86-NEXT:    vmovsldup {{.*#+}} ymm1 = ymm0[0,0,2,2,4,4,6,6]
; X86-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; X86-NEXT:    movb $63, %al
; X86-NEXT:    kmovb %eax, %k1
; X86-NEXT:    vcmpltps %zmm1, %zmm2, %k0 {%k1}
; X86-NEXT:    movb $42, %al
; X86-NEXT:    kmovb %eax, %k1
; X86-NEXT:    vcmpltps %zmm0, %zmm2, %k1 {%k1}
; X86-NEXT:    kortestb %k1, %k0
; X86-NEXT:    sete %al
; X86-NEXT:    vzeroupper
; X86-NEXT:    retl
entry:
  %in = load <6 x float>, <6 x float>* %in_ptr
  %shf0 = shufflevector <6 x float> %in, <6 x float> undef, <6 x i32> <i32 undef, i32 0, i32 undef, i32 2, i32 undef, i32 4>
  %fcmp0 = fcmp ogt <6 x float> %shf0, zeroinitializer
  %bitcast0 = bitcast <6 x i1> %fcmp0 to i6
  %icmp0 = icmp eq i6 %bitcast0, 0
  %shfl = shufflevector <6 x float> %in, <6 x float> undef, <8 x i32> <i32 undef, i32 1, i32 undef, i32 3, i32 undef, i32 5, i32 undef, i32 undef>
  %fcmp1 = fcmp ogt <8 x float> %shfl, zeroinitializer
  %and1 = and <8 x i1> %fcmp1, <i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 false>
  %bitcast1 = bitcast <8 x i1> %and1 to i8
  %icmp1 = icmp eq i8 %bitcast1, 0
  %ret = and i1 %icmp0, %icmp1
  ret i1 %ret
}

define <8 x i1> @test_pseudo_kmovb() {
; CHECK-LABEL: test_pseudo_kmovb:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movb $3, %al
; CHECK-NEXT:    kmovb %eax, %k0
; CHECK-NEXT:    vpmovm2w %k0, %zmm0
; CHECK-NEXT:    # kill: def $xmm0 killed $xmm0 killed $zmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    ret{{[l|q]}}
  %1 = bitcast i8 3 to <8 x i1>
  ret <8 x i1> %1
}

define <16 x i1> @test_pseudo_kmovw() {
; CHECK-LABEL: test_pseudo_kmovw:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movw $7, %ax
; CHECK-NEXT:    kmovw %eax, %k0
; CHECK-NEXT:    vpmovm2b %k0, %zmm0
; CHECK-NEXT:    # kill: def $xmm0 killed $xmm0 killed $zmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    ret{{[l|q]}}
  %1 = bitcast i16 7 to <16 x i1>
  ret <16 x i1> %1
}
