; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt --codegen-opt-level=2 -mtriple=x86_64 -lower-amx-type %s -S | FileCheck %s

define void @test() {
; CHECK-LABEL: @test(
; CHECK-NEXT:  wrapper_entry:
; CHECK-NEXT:    [[TMP0:%.*]] = alloca <256 x i8>, align 64
; CHECK-NEXT:    [[TMP1:%.*]] = alloca <256 x i8>, align 64
; CHECK-NEXT:    br label [[SCALAR_IF:%.*]]
; CHECK:       scalar_if:
; CHECK-NEXT:    br i1 poison, label [[EXIT:%.*]], label [[DIM_1_PRE_HEAD_PREHEADER:%.*]]
; CHECK:       dim_1_pre_head.preheader:
; CHECK-NEXT:    [[TMP2:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 32, i8* poison, i64 64)
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <256 x i8>* [[TMP0]] to i8*
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 32, i8* [[TMP3]], i64 32, x86_amx [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = load <256 x i8>, <256 x i8>* [[TMP0]], align 256
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 8, i16 32, i8* poison, i64 64, x86_amx [[TMP2]])
; CHECK-NEXT:    store <256 x i8> [[TMP4]], <256 x i8>* [[TMP1]], align 256
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast <256 x i8>* [[TMP1]] to i8*
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr i8, i8* [[TMP5]], i32 poison
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP6]] to <1 x i8>*
; CHECK-NEXT:    [[TMP8:%.*]] = load <1 x i8>, <1 x i8>* [[TMP7]], align 1
; CHECK-NEXT:    unreachable
; CHECK:       exit:
; CHECK-NEXT:    ret void
;
wrapper_entry:
  %0 = alloca <256 x i8>, align 64
  br label %scalar_if

scalar_if:                                        ; preds = %wrapper_entry
  br i1 poison, label %exit, label %dim_1_pre_head.preheader

dim_1_pre_head.preheader:                         ; preds = %scalar_if
  %1 = call x86_amx @llvm.x86.tileloadd64.internal(i16 8, i16 32, i8* poison, i64 64)
  %2 = call <256 x i8> @llvm.x86.cast.tile.to.vector.v256i8(x86_amx %1)
  call void @llvm.x86.tilestored64.internal(i16 8, i16 32, i8* poison, i64 64, x86_amx %1)
  store <256 x i8> %2, <256 x i8>* %0, align 256
  %3 = bitcast <256 x i8>* %0 to i8*
  %4 = getelementptr i8, i8* %3, i32 poison
  %5 = bitcast i8* %4 to <1 x i8>*
  %6 = load <1 x i8>, <1 x i8>* %5, align 1
  unreachable

exit:                                             ; preds = %scalar_if
  ret void
}


declare x86_amx @llvm.x86.tileloadd64.internal(i16, i16, i8*, i64)
declare <256 x i8> @llvm.x86.cast.tile.to.vector.v256i8(x86_amx)
declare x86_amx @llvm.x86.cast.vector.to.tile.v256i8(<256 x i8>)
declare void @llvm.x86.tilestored64.internal(i16, i16, i8*, i64, x86_amx)
