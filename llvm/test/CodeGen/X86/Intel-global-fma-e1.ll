; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; INTEL_CUSTOMIZATION:
; This test checks that Global FMA optimizes an arbitrary (1 of ~40k) test case.
; For the input expression:
;   +a*b*c*d+a*b*c*e+a*d*f*g+a*e*f*g+b*c*d+b*c*e+d*f*g+e*f*g+b*c+f*g;
; the output code must have only 5 arithmetic instructions:
;   F0=f*g; F1=d+e; F2=b*c+F0; F3=F1*a+F1; F4=F3*F2+F2;

; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mcpu=core-avx2 -fp-contract=fast -enable-unsafe-fp-math | FileCheck %s --check-prefixes=CHECK,AVX2
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mcpu=skx       -fp-contract=fast -enable-unsafe-fp-math | FileCheck %s --check-prefixes=CHECK,SKX
; RUN: llc < %s -verify-machineinstrs -mtriple=x86_64-unknown-unknown -mcpu=knl       -fp-contract=fast -enable-unsafe-fp-math | FileCheck %s --check-prefixes=CHECK,KNL

; These attributes are used for functions testing scalar and 128/256 bit types.
attributes #0 = { nounwind "target-features"="+avx2,+fma" }

; These attributes are used for functions testing 512 bit types.
attributes #1 = { nounwind "target-cpu"="skx" "target-features"="+avx512f,+fma" }

@a32 = common global float 0.000000e+00, align 4
@b32 = common global float 0.000000e+00, align 4
@c32 = common global float 0.000000e+00, align 4
@d32 = common global float 0.000000e+00, align 4
@e32 = common global float 0.000000e+00, align 4
@f32 = common global float 0.000000e+00, align 4
@g32 = common global float 0.000000e+00, align 4
@dst32 = common global float 0.000000e+00, align 4
@h32 = common global float 0.000000e+00, align 4
@i32 = common global float 0.000000e+00, align 4

define void @func32() #0 {
; AVX2-LABEL: func32:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a32@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b32@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c32@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX2-NEXT:    movq d32@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; AVX2-NEXT:    movq e32@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f32@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g32@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovss {{.*#+}} xmm2 = mem[0],zero,zero,zero
; AVX2-NEXT:    vmulss (%rcx), %xmm0, %xmm0
; AVX2-NEXT:    vaddss (%rdx), %xmm1, %xmm1
; AVX2-NEXT:    vfmadd231ss {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; AVX2-NEXT:    vfmadd132ss {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; AVX2-NEXT:    vfmadd213ss {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; AVX2-NEXT:    movq dst32@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovss %xmm1, (%rax)
; AVX2-NEXT:    retq
;
; SKX-LABEL: func32:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a32@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c32@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; SKX-NEXT:    movq d32@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; SKX-NEXT:    movq b32@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e32@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f32@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g32@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovss {{.*#+}} xmm2 = mem[0],zero,zero,zero
; SKX-NEXT:    vmulss (%rcx), %xmm0, %xmm0
; SKX-NEXT:    vaddss (%rdx), %xmm1, %xmm1
; SKX-NEXT:    vfmadd231ss {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; SKX-NEXT:    vfmadd132ss {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; SKX-NEXT:    vfmadd213ss {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; SKX-NEXT:    movq dst32@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovss %xmm1, (%rax)
; SKX-NEXT:    retq
;
; KNL-LABEL: func32:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a32@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b32@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c32@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; KNL-NEXT:    movq d32@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; KNL-NEXT:    movq e32@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f32@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g32@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovss {{.*#+}} xmm2 = mem[0],zero,zero,zero
; KNL-NEXT:    vmulss (%rcx), %xmm0, %xmm0
; KNL-NEXT:    vaddss (%rdx), %xmm1, %xmm1
; KNL-NEXT:    vfmadd231ss {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; KNL-NEXT:    vfmadd132ss {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; KNL-NEXT:    vfmadd213ss {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; KNL-NEXT:    movq dst32@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovss %xmm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load float, float* @a32, align 4
  %load_b = load float, float* @b32, align 4
  %mul = fmul fast float %load_a, %load_b
  %load_c = load float, float* @c32, align 4
  %mul1 = fmul fast float %mul, %load_c
  %load_d = load float, float* @d32, align 4
  %mul2 = fmul fast float %mul1, %load_d
  %load_e = load float, float* @e32, align 4
  %mul5 = fmul fast float %mul1, %load_e
  %add = fadd fast float %mul2, %mul5
  %mul6 = fmul fast float %load_a, %load_d
  %load_f = load float, float* @f32, align 4
  %mul7 = fmul fast float %mul6, %load_f
  %load_g = load float, float* @g32, align 4
  %mul8 = fmul fast float %mul7, %load_g
  %add9 = fadd fast float %add, %mul8
  %mul10 = fmul fast float %load_a, %load_e
  %mul11 = fmul fast float %mul10, %load_f
  %mul12 = fmul fast float %mul11, %load_g
  %add13 = fadd fast float %add9, %mul12
  %mul14 = fmul fast float %load_b, %load_c
  %mul15 = fmul fast float %mul14, %load_d
  %add16 = fadd fast float %add13, %mul15
  %mul18 = fmul fast float %mul14, %load_e
  %add19 = fadd fast float %add16, %mul18
  %mul20 = fmul fast float %load_d, %load_f
  %mul21 = fmul fast float %mul20, %load_g
  %add22 = fadd fast float %add19, %mul21
  %mul23 = fmul fast float %load_e, %load_f
  %mul24 = fmul fast float %mul23, %load_g
  %add25 = fadd fast float %add22, %mul24
  %add27 = fadd fast float %add25, %mul14
  %mul28 = fmul fast float %load_f, %load_g
  %add29 = fadd fast float %add27, %mul28
  store float %add29, float* @dst32, align 4
  ret void
}

@a64 = common global double 0.000000e+00, align 8
@b64 = common global double 0.000000e+00, align 8
@c64 = common global double 0.000000e+00, align 8
@d64 = common global double 0.000000e+00, align 8
@e64 = common global double 0.000000e+00, align 8
@f64 = common global double 0.000000e+00, align 8
@g64 = common global double 0.000000e+00, align 8
@dst64 = common global double 0.000000e+00, align 8
@h64 = common global double 0.000000e+00, align 8
@i64 = common global double 0.000000e+00, align 8

define void @func64() #0 {
; AVX2-LABEL: func64:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a64@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b64@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c64@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; AVX2-NEXT:    movq d64@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; AVX2-NEXT:    movq e64@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f64@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g64@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovsd {{.*#+}} xmm2 = mem[0],zero
; AVX2-NEXT:    vmulsd (%rcx), %xmm0, %xmm0
; AVX2-NEXT:    vaddsd (%rdx), %xmm1, %xmm1
; AVX2-NEXT:    vfmadd231sd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; AVX2-NEXT:    vfmadd132sd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; AVX2-NEXT:    vfmadd213sd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; AVX2-NEXT:    movq dst64@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovsd %xmm1, (%rax)
; AVX2-NEXT:    retq
;
; SKX-LABEL: func64:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a64@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c64@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; SKX-NEXT:    movq d64@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; SKX-NEXT:    movq b64@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e64@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f64@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g64@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovsd {{.*#+}} xmm2 = mem[0],zero
; SKX-NEXT:    vmulsd (%rcx), %xmm0, %xmm0
; SKX-NEXT:    vaddsd (%rdx), %xmm1, %xmm1
; SKX-NEXT:    vfmadd231sd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; SKX-NEXT:    vfmadd132sd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; SKX-NEXT:    vfmadd213sd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; SKX-NEXT:    movq dst64@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovsd %xmm1, (%rax)
; SKX-NEXT:    retq
;
; KNL-LABEL: func64:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a64@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b64@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c64@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; KNL-NEXT:    movq d64@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovsd {{.*#+}} xmm1 = mem[0],zero
; KNL-NEXT:    movq e64@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f64@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g64@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovsd {{.*#+}} xmm2 = mem[0],zero
; KNL-NEXT:    vmulsd (%rcx), %xmm0, %xmm0
; KNL-NEXT:    vaddsd (%rdx), %xmm1, %xmm1
; KNL-NEXT:    vfmadd231sd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; KNL-NEXT:    vfmadd132sd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; KNL-NEXT:    vfmadd213sd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; KNL-NEXT:    movq dst64@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovsd %xmm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load double, double* @a64, align 8
  %load_b = load double, double* @b64, align 8
  %mul = fmul fast double %load_a, %load_b
  %load_c = load double, double* @c64, align 8
  %mul1 = fmul fast double %mul, %load_c
  %load_d = load double, double* @d64, align 8
  %mul2 = fmul fast double %mul1, %load_d
  %load_e = load double, double* @e64, align 8
  %mul5 = fmul fast double %mul1, %load_e
  %add = fadd fast double %mul2, %mul5
  %mul6 = fmul fast double %load_a, %load_d
  %load_f = load double, double* @f64, align 8
  %mul7 = fmul fast double %mul6, %load_f
  %load_g = load double, double* @g64, align 8
  %mul8 = fmul fast double %mul7, %load_g
  %add9 = fadd fast double %add, %mul8
  %mul10 = fmul fast double %load_a, %load_e
  %mul11 = fmul fast double %mul10, %load_f
  %mul12 = fmul fast double %mul11, %load_g
  %add13 = fadd fast double %add9, %mul12
  %mul14 = fmul fast double %load_b, %load_c
  %mul15 = fmul fast double %mul14, %load_d
  %add16 = fadd fast double %add13, %mul15
  %mul18 = fmul fast double %mul14, %load_e
  %add19 = fadd fast double %add16, %mul18
  %mul20 = fmul fast double %load_d, %load_f
  %mul21 = fmul fast double %mul20, %load_g
  %add22 = fadd fast double %add19, %mul21
  %mul23 = fmul fast double %load_e, %load_f
  %mul24 = fmul fast double %mul23, %load_g
  %add25 = fadd fast double %add22, %mul24
  %add27 = fadd fast double %add25, %mul14
  %mul28 = fmul fast double %load_f, %load_g
  %add29 = fadd fast double %add27, %mul28
  store double %add29, double* @dst64, align 8
  ret void
}

@a32x4 = common global <4 x float> zeroinitializer, align 16
@b32x4 = common global <4 x float> zeroinitializer, align 16
@c32x4 = common global <4 x float> zeroinitializer, align 16
@d32x4 = common global <4 x float> zeroinitializer, align 16
@e32x4 = common global <4 x float> zeroinitializer, align 16
@f32x4 = common global <4 x float> zeroinitializer, align 16
@g32x4 = common global <4 x float> zeroinitializer, align 16
@dst32x4 = common global <4 x float> zeroinitializer, align 16
@h32x4 = common global <4 x float> zeroinitializer, align 16
@i32x4 = common global <4 x float> zeroinitializer, align 16

define void @func32x4() #0 {
; AVX2-LABEL: func32x4:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a32x4@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b32x4@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c32x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovaps (%rdx), %xmm0
; AVX2-NEXT:    movq d32x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovaps (%rdx), %xmm1
; AVX2-NEXT:    movq e32x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f32x4@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g32x4@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovaps (%rdi), %xmm2
; AVX2-NEXT:    vmulps (%rcx), %xmm0, %xmm0
; AVX2-NEXT:    vaddps (%rdx), %xmm1, %xmm1
; AVX2-NEXT:    vfmadd231ps {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; AVX2-NEXT:    vfmadd132ps {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; AVX2-NEXT:    vfmadd213ps {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; AVX2-NEXT:    movq dst32x4@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovaps %xmm1, (%rax)
; AVX2-NEXT:    retq
;
; SKX-LABEL: func32x4:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a32x4@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c32x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovaps (%rcx), %xmm0
; SKX-NEXT:    movq d32x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovaps (%rcx), %xmm1
; SKX-NEXT:    movq b32x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e32x4@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f32x4@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g32x4@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovaps (%rdi), %xmm2
; SKX-NEXT:    vmulps (%rcx), %xmm0, %xmm0
; SKX-NEXT:    vaddps (%rdx), %xmm1, %xmm1
; SKX-NEXT:    vfmadd231ps {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; SKX-NEXT:    vfmadd132ps {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; SKX-NEXT:    vfmadd213ps {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; SKX-NEXT:    movq dst32x4@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovaps %xmm1, (%rax)
; SKX-NEXT:    retq
;
; KNL-LABEL: func32x4:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a32x4@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b32x4@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c32x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovaps (%rdx), %xmm0
; KNL-NEXT:    movq d32x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovaps (%rdx), %xmm1
; KNL-NEXT:    movq e32x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f32x4@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g32x4@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovaps (%rdi), %xmm2
; KNL-NEXT:    vmulps (%rcx), %xmm0, %xmm0
; KNL-NEXT:    vaddps (%rdx), %xmm1, %xmm1
; KNL-NEXT:    vfmadd231ps {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; KNL-NEXT:    vfmadd132ps {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; KNL-NEXT:    vfmadd213ps {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; KNL-NEXT:    movq dst32x4@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovaps %xmm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load <4 x float>, <4 x float>* @a32x4, align 16
  %load_b = load <4 x float>, <4 x float>* @b32x4, align 16
  %mul = fmul fast <4 x float> %load_a, %load_b
  %load_c = load <4 x float>, <4 x float>* @c32x4, align 16
  %mul1 = fmul fast <4 x float> %mul, %load_c
  %load_d = load <4 x float>, <4 x float>* @d32x4, align 16
  %mul2 = fmul fast <4 x float> %mul1, %load_d
  %load_e = load <4 x float>, <4 x float>* @e32x4, align 16
  %mul5 = fmul fast <4 x float> %mul1, %load_e
  %add = fadd fast <4 x float> %mul2, %mul5
  %mul6 = fmul fast <4 x float> %load_a, %load_d
  %load_f = load <4 x float>, <4 x float>* @f32x4, align 16
  %mul7 = fmul fast <4 x float> %mul6, %load_f
  %load_g = load <4 x float>, <4 x float>* @g32x4, align 16
  %mul8 = fmul fast <4 x float> %mul7, %load_g
  %add9 = fadd fast <4 x float> %add, %mul8
  %mul10 = fmul fast <4 x float> %load_a, %load_e
  %mul11 = fmul fast <4 x float> %mul10, %load_f
  %mul12 = fmul fast <4 x float> %mul11, %load_g
  %add13 = fadd fast <4 x float> %add9, %mul12
  %mul14 = fmul fast <4 x float> %load_b, %load_c
  %mul15 = fmul fast <4 x float> %mul14, %load_d
  %add16 = fadd fast <4 x float> %add13, %mul15
  %mul18 = fmul fast <4 x float> %mul14, %load_e
  %add19 = fadd fast <4 x float> %add16, %mul18
  %mul20 = fmul fast <4 x float> %load_d, %load_f
  %mul21 = fmul fast <4 x float> %mul20, %load_g
  %add22 = fadd fast <4 x float> %add19, %mul21
  %mul23 = fmul fast <4 x float> %load_e, %load_f
  %mul24 = fmul fast <4 x float> %mul23, %load_g
  %add25 = fadd fast <4 x float> %add22, %mul24
  %add27 = fadd fast <4 x float> %add25, %mul14
  %mul28 = fmul fast <4 x float> %load_f, %load_g
  %add29 = fadd fast <4 x float> %add27, %mul28
  store <4 x float> %add29, <4 x float>* @dst32x4, align 16
  ret void
}

@a64x2 = common global <2 x double> zeroinitializer, align 16
@b64x2 = common global <2 x double> zeroinitializer, align 16
@c64x2 = common global <2 x double> zeroinitializer, align 16
@d64x2 = common global <2 x double> zeroinitializer, align 16
@e64x2 = common global <2 x double> zeroinitializer, align 16
@f64x2 = common global <2 x double> zeroinitializer, align 16
@g64x2 = common global <2 x double> zeroinitializer, align 16
@dst64x2 = common global <2 x double> zeroinitializer, align 16
@h64x2 = common global <2 x double> zeroinitializer, align 16
@i64x2 = common global <2 x double> zeroinitializer, align 16

define void @func64x2() #0 {
; AVX2-LABEL: func64x2:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a64x2@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b64x2@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c64x2@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovapd (%rdx), %xmm0
; AVX2-NEXT:    movq d64x2@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovapd (%rdx), %xmm1
; AVX2-NEXT:    movq e64x2@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f64x2@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g64x2@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovapd (%rdi), %xmm2
; AVX2-NEXT:    vmulpd (%rcx), %xmm0, %xmm0
; AVX2-NEXT:    vaddpd (%rdx), %xmm1, %xmm1
; AVX2-NEXT:    vfmadd231pd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; AVX2-NEXT:    vfmadd132pd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; AVX2-NEXT:    vfmadd213pd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; AVX2-NEXT:    movq dst64x2@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovapd %xmm1, (%rax)
; AVX2-NEXT:    retq
;
; SKX-LABEL: func64x2:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a64x2@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c64x2@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovapd (%rcx), %xmm0
; SKX-NEXT:    movq d64x2@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovapd (%rcx), %xmm1
; SKX-NEXT:    movq b64x2@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e64x2@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f64x2@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g64x2@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovapd (%rdi), %xmm2
; SKX-NEXT:    vmulpd (%rcx), %xmm0, %xmm0
; SKX-NEXT:    vaddpd (%rdx), %xmm1, %xmm1
; SKX-NEXT:    vfmadd231pd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; SKX-NEXT:    vfmadd132pd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; SKX-NEXT:    vfmadd213pd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; SKX-NEXT:    movq dst64x2@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovapd %xmm1, (%rax)
; SKX-NEXT:    retq
;
; KNL-LABEL: func64x2:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a64x2@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b64x2@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c64x2@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovapd (%rdx), %xmm0
; KNL-NEXT:    movq d64x2@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovapd (%rdx), %xmm1
; KNL-NEXT:    movq e64x2@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f64x2@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g64x2@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovapd (%rdi), %xmm2
; KNL-NEXT:    vmulpd (%rcx), %xmm0, %xmm0
; KNL-NEXT:    vaddpd (%rdx), %xmm1, %xmm1
; KNL-NEXT:    vfmadd231pd {{.*#+}} xmm0 = (xmm2 * mem) + xmm0
; KNL-NEXT:    vfmadd132pd {{.*#+}} xmm1 = (xmm1 * mem) + xmm1
; KNL-NEXT:    vfmadd213pd {{.*#+}} xmm1 = (xmm0 * xmm1) + xmm0
; KNL-NEXT:    movq dst64x2@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovapd %xmm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load <2 x double>, <2 x double>* @a64x2, align 16
  %load_b = load <2 x double>, <2 x double>* @b64x2, align 16
  %mul = fmul fast <2 x double> %load_a, %load_b
  %load_c = load <2 x double>, <2 x double>* @c64x2, align 16
  %mul1 = fmul fast <2 x double> %mul, %load_c
  %load_d = load <2 x double>, <2 x double>* @d64x2, align 16
  %mul2 = fmul fast <2 x double> %mul1, %load_d
  %load_e = load <2 x double>, <2 x double>* @e64x2, align 16
  %mul5 = fmul fast <2 x double> %mul1, %load_e
  %add = fadd fast <2 x double> %mul2, %mul5
  %mul6 = fmul fast <2 x double> %load_a, %load_d
  %load_f = load <2 x double>, <2 x double>* @f64x2, align 16
  %mul7 = fmul fast <2 x double> %mul6, %load_f
  %load_g = load <2 x double>, <2 x double>* @g64x2, align 16
  %mul8 = fmul fast <2 x double> %mul7, %load_g
  %add9 = fadd fast <2 x double> %add, %mul8
  %mul10 = fmul fast <2 x double> %load_a, %load_e
  %mul11 = fmul fast <2 x double> %mul10, %load_f
  %mul12 = fmul fast <2 x double> %mul11, %load_g
  %add13 = fadd fast <2 x double> %add9, %mul12
  %mul14 = fmul fast <2 x double> %load_b, %load_c
  %mul15 = fmul fast <2 x double> %mul14, %load_d
  %add16 = fadd fast <2 x double> %add13, %mul15
  %mul18 = fmul fast <2 x double> %mul14, %load_e
  %add19 = fadd fast <2 x double> %add16, %mul18
  %mul20 = fmul fast <2 x double> %load_d, %load_f
  %mul21 = fmul fast <2 x double> %mul20, %load_g
  %add22 = fadd fast <2 x double> %add19, %mul21
  %mul23 = fmul fast <2 x double> %load_e, %load_f
  %mul24 = fmul fast <2 x double> %mul23, %load_g
  %add25 = fadd fast <2 x double> %add22, %mul24
  %add27 = fadd fast <2 x double> %add25, %mul14
  %mul28 = fmul fast <2 x double> %load_f, %load_g
  %add29 = fadd fast <2 x double> %add27, %mul28
  store <2 x double> %add29, <2 x double>* @dst64x2, align 16
  ret void
}

@a32x8 = common global <8 x float> zeroinitializer, align 32
@b32x8 = common global <8 x float> zeroinitializer, align 32
@c32x8 = common global <8 x float> zeroinitializer, align 32
@d32x8 = common global <8 x float> zeroinitializer, align 32
@e32x8 = common global <8 x float> zeroinitializer, align 32
@f32x8 = common global <8 x float> zeroinitializer, align 32
@g32x8 = common global <8 x float> zeroinitializer, align 32
@dst32x8 = common global <8 x float> zeroinitializer, align 32
@h32x8 = common global <8 x float> zeroinitializer, align 32
@i32x8 = common global <8 x float> zeroinitializer, align 32

define void @func32x8() #0 {
; AVX2-LABEL: func32x8:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a32x8@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b32x8@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c32x8@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovaps (%rdx), %ymm0
; AVX2-NEXT:    movq d32x8@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovaps (%rdx), %ymm1
; AVX2-NEXT:    movq e32x8@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f32x8@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g32x8@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovaps (%rdi), %ymm2
; AVX2-NEXT:    vmulps (%rcx), %ymm0, %ymm0
; AVX2-NEXT:    vaddps (%rdx), %ymm1, %ymm1
; AVX2-NEXT:    vfmadd231ps {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; AVX2-NEXT:    vfmadd132ps {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; AVX2-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; AVX2-NEXT:    movq dst32x8@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovaps %ymm1, (%rax)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; SKX-LABEL: func32x8:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a32x8@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c32x8@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovaps (%rcx), %ymm0
; SKX-NEXT:    movq d32x8@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovaps (%rcx), %ymm1
; SKX-NEXT:    movq b32x8@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e32x8@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f32x8@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g32x8@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovaps (%rdi), %ymm2
; SKX-NEXT:    vmulps (%rcx), %ymm0, %ymm0
; SKX-NEXT:    vaddps (%rdx), %ymm1, %ymm1
; SKX-NEXT:    vfmadd231ps {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; SKX-NEXT:    vfmadd132ps {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; SKX-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; SKX-NEXT:    movq dst32x8@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovaps %ymm1, (%rax)
; SKX-NEXT:    vzeroupper
; SKX-NEXT:    retq
;
; KNL-LABEL: func32x8:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a32x8@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b32x8@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c32x8@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovaps (%rdx), %ymm0
; KNL-NEXT:    movq d32x8@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovaps (%rdx), %ymm1
; KNL-NEXT:    movq e32x8@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f32x8@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g32x8@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovaps (%rdi), %ymm2
; KNL-NEXT:    vmulps (%rcx), %ymm0, %ymm0
; KNL-NEXT:    vaddps (%rdx), %ymm1, %ymm1
; KNL-NEXT:    vfmadd231ps {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; KNL-NEXT:    vfmadd132ps {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; KNL-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; KNL-NEXT:    movq dst32x8@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovaps %ymm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load <8 x float>, <8 x float>* @a32x8, align 32
  %load_b = load <8 x float>, <8 x float>* @b32x8, align 32
  %mul = fmul fast <8 x float> %load_a, %load_b
  %load_c = load <8 x float>, <8 x float>* @c32x8, align 32
  %mul1 = fmul fast <8 x float> %mul, %load_c
  %load_d = load <8 x float>, <8 x float>* @d32x8, align 32
  %mul2 = fmul fast <8 x float> %mul1, %load_d
  %load_e = load <8 x float>, <8 x float>* @e32x8, align 32
  %mul5 = fmul fast <8 x float> %mul1, %load_e
  %add = fadd fast <8 x float> %mul2, %mul5
  %mul6 = fmul fast <8 x float> %load_a, %load_d
  %load_f = load <8 x float>, <8 x float>* @f32x8, align 32
  %mul7 = fmul fast <8 x float> %mul6, %load_f
  %load_g = load <8 x float>, <8 x float>* @g32x8, align 32
  %mul8 = fmul fast <8 x float> %mul7, %load_g
  %add9 = fadd fast <8 x float> %add, %mul8
  %mul10 = fmul fast <8 x float> %load_a, %load_e
  %mul11 = fmul fast <8 x float> %mul10, %load_f
  %mul12 = fmul fast <8 x float> %mul11, %load_g
  %add13 = fadd fast <8 x float> %add9, %mul12
  %mul14 = fmul fast <8 x float> %load_b, %load_c
  %mul15 = fmul fast <8 x float> %mul14, %load_d
  %add16 = fadd fast <8 x float> %add13, %mul15
  %mul18 = fmul fast <8 x float> %mul14, %load_e
  %add19 = fadd fast <8 x float> %add16, %mul18
  %mul20 = fmul fast <8 x float> %load_d, %load_f
  %mul21 = fmul fast <8 x float> %mul20, %load_g
  %add22 = fadd fast <8 x float> %add19, %mul21
  %mul23 = fmul fast <8 x float> %load_e, %load_f
  %mul24 = fmul fast <8 x float> %mul23, %load_g
  %add25 = fadd fast <8 x float> %add22, %mul24
  %add27 = fadd fast <8 x float> %add25, %mul14
  %mul28 = fmul fast <8 x float> %load_f, %load_g
  %add29 = fadd fast <8 x float> %add27, %mul28
  store <8 x float> %add29, <8 x float>* @dst32x8, align 32
  ret void
}

@a64x4 = common global <4 x double> zeroinitializer, align 32
@b64x4 = common global <4 x double> zeroinitializer, align 32
@c64x4 = common global <4 x double> zeroinitializer, align 32
@d64x4 = common global <4 x double> zeroinitializer, align 32
@e64x4 = common global <4 x double> zeroinitializer, align 32
@f64x4 = common global <4 x double> zeroinitializer, align 32
@g64x4 = common global <4 x double> zeroinitializer, align 32
@dst64x4 = common global <4 x double> zeroinitializer, align 32
@h64x4 = common global <4 x double> zeroinitializer, align 32
@i64x4 = common global <4 x double> zeroinitializer, align 32

define void @func64x4() #0 {
; AVX2-LABEL: func64x4:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    movq a64x4@{{.*}}(%rip), %rax
; AVX2-NEXT:    movq b64x4@{{.*}}(%rip), %rcx
; AVX2-NEXT:    movq c64x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovapd (%rdx), %ymm0
; AVX2-NEXT:    movq d64x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    vmovapd (%rdx), %ymm1
; AVX2-NEXT:    movq e64x4@{{.*}}(%rip), %rdx
; AVX2-NEXT:    movq f64x4@{{.*}}(%rip), %rsi
; AVX2-NEXT:    movq g64x4@{{.*}}(%rip), %rdi
; AVX2-NEXT:    vmovapd (%rdi), %ymm2
; AVX2-NEXT:    vmulpd (%rcx), %ymm0, %ymm0
; AVX2-NEXT:    vaddpd (%rdx), %ymm1, %ymm1
; AVX2-NEXT:    vfmadd231pd {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; AVX2-NEXT:    vfmadd132pd {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; AVX2-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; AVX2-NEXT:    movq dst64x4@{{.*}}(%rip), %rax
; AVX2-NEXT:    vmovapd %ymm1, (%rax)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; SKX-LABEL: func64x4:
; SKX:       # %bb.0: # %entry
; SKX-NEXT:    movq a64x4@{{.*}}(%rip), %rax
; SKX-NEXT:    movq c64x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovapd (%rcx), %ymm0
; SKX-NEXT:    movq d64x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    vmovapd (%rcx), %ymm1
; SKX-NEXT:    movq b64x4@{{.*}}(%rip), %rcx
; SKX-NEXT:    movq e64x4@{{.*}}(%rip), %rdx
; SKX-NEXT:    movq f64x4@{{.*}}(%rip), %rsi
; SKX-NEXT:    movq g64x4@{{.*}}(%rip), %rdi
; SKX-NEXT:    vmovapd (%rdi), %ymm2
; SKX-NEXT:    vmulpd (%rcx), %ymm0, %ymm0
; SKX-NEXT:    vaddpd (%rdx), %ymm1, %ymm1
; SKX-NEXT:    vfmadd231pd {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; SKX-NEXT:    vfmadd132pd {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; SKX-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; SKX-NEXT:    movq dst64x4@{{.*}}(%rip), %rax
; SKX-NEXT:    vmovapd %ymm1, (%rax)
; SKX-NEXT:    vzeroupper
; SKX-NEXT:    retq
;
; KNL-LABEL: func64x4:
; KNL:       # %bb.0: # %entry
; KNL-NEXT:    movq a64x4@{{.*}}(%rip), %rax
; KNL-NEXT:    movq b64x4@{{.*}}(%rip), %rcx
; KNL-NEXT:    movq c64x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovapd (%rdx), %ymm0
; KNL-NEXT:    movq d64x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    vmovapd (%rdx), %ymm1
; KNL-NEXT:    movq e64x4@{{.*}}(%rip), %rdx
; KNL-NEXT:    movq f64x4@{{.*}}(%rip), %rsi
; KNL-NEXT:    movq g64x4@{{.*}}(%rip), %rdi
; KNL-NEXT:    vmovapd (%rdi), %ymm2
; KNL-NEXT:    vmulpd (%rcx), %ymm0, %ymm0
; KNL-NEXT:    vaddpd (%rdx), %ymm1, %ymm1
; KNL-NEXT:    vfmadd231pd {{.*#+}} ymm0 = (ymm2 * mem) + ymm0
; KNL-NEXT:    vfmadd132pd {{.*#+}} ymm1 = (ymm1 * mem) + ymm1
; KNL-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm0 * ymm1) + ymm0
; KNL-NEXT:    movq dst64x4@{{.*}}(%rip), %rax
; KNL-NEXT:    vmovapd %ymm1, (%rax)
; KNL-NEXT:    retq
entry:
  %load_a = load <4 x double>, <4 x double>* @a64x4, align 32
  %load_b = load <4 x double>, <4 x double>* @b64x4, align 32
  %mul = fmul fast <4 x double> %load_a, %load_b
  %load_c = load <4 x double>, <4 x double>* @c64x4, align 32
  %mul1 = fmul fast <4 x double> %mul, %load_c
  %load_d = load <4 x double>, <4 x double>* @d64x4, align 32
  %mul2 = fmul fast <4 x double> %mul1, %load_d
  %load_e = load <4 x double>, <4 x double>* @e64x4, align 32
  %mul5 = fmul fast <4 x double> %mul1, %load_e
  %add = fadd fast <4 x double> %mul2, %mul5
  %mul6 = fmul fast <4 x double> %load_a, %load_d
  %load_f = load <4 x double>, <4 x double>* @f64x4, align 32
  %mul7 = fmul fast <4 x double> %mul6, %load_f
  %load_g = load <4 x double>, <4 x double>* @g64x4, align 32
  %mul8 = fmul fast <4 x double> %mul7, %load_g
  %add9 = fadd fast <4 x double> %add, %mul8
  %mul10 = fmul fast <4 x double> %load_a, %load_e
  %mul11 = fmul fast <4 x double> %mul10, %load_f
  %mul12 = fmul fast <4 x double> %mul11, %load_g
  %add13 = fadd fast <4 x double> %add9, %mul12
  %mul14 = fmul fast <4 x double> %load_b, %load_c
  %mul15 = fmul fast <4 x double> %mul14, %load_d
  %add16 = fadd fast <4 x double> %add13, %mul15
  %mul18 = fmul fast <4 x double> %mul14, %load_e
  %add19 = fadd fast <4 x double> %add16, %mul18
  %mul20 = fmul fast <4 x double> %load_d, %load_f
  %mul21 = fmul fast <4 x double> %mul20, %load_g
  %add22 = fadd fast <4 x double> %add19, %mul21
  %mul23 = fmul fast <4 x double> %load_e, %load_f
  %mul24 = fmul fast <4 x double> %mul23, %load_g
  %add25 = fadd fast <4 x double> %add22, %mul24
  %add27 = fadd fast <4 x double> %add25, %mul14
  %mul28 = fmul fast <4 x double> %load_f, %load_g
  %add29 = fadd fast <4 x double> %add27, %mul28
  store <4 x double> %add29, <4 x double>* @dst64x4, align 32
  ret void
}

@a32x16 = common global <16 x float> zeroinitializer, align 64
@b32x16 = common global <16 x float> zeroinitializer, align 64
@c32x16 = common global <16 x float> zeroinitializer, align 64
@d32x16 = common global <16 x float> zeroinitializer, align 64
@e32x16 = common global <16 x float> zeroinitializer, align 64
@f32x16 = common global <16 x float> zeroinitializer, align 64
@g32x16 = common global <16 x float> zeroinitializer, align 64
@dst32x16 = common global <16 x float> zeroinitializer, align 64
@h32x16 = common global <16 x float> zeroinitializer, align 64
@i32x16 = common global <16 x float> zeroinitializer, align 64

define void @func32x16() #1 {
; CHECK-LABEL: func32x16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a32x16@{{.*}}(%rip), %rax
; CHECK-NEXT:    movq c32x16@{{.*}}(%rip), %rcx
; CHECK-NEXT:    vmovaps (%rcx), %zmm0
; CHECK-NEXT:    movq d32x16@{{.*}}(%rip), %rcx
; CHECK-NEXT:    vmovaps (%rcx), %zmm1
; CHECK-NEXT:    movq b32x16@{{.*}}(%rip), %rcx
; CHECK-NEXT:    movq e32x16@{{.*}}(%rip), %rdx
; CHECK-NEXT:    movq f32x16@{{.*}}(%rip), %rsi
; CHECK-NEXT:    movq g32x16@{{.*}}(%rip), %rdi
; CHECK-NEXT:    vmovaps (%rdi), %zmm2
; CHECK-NEXT:    vmulps (%rcx), %zmm0, %zmm0
; CHECK-NEXT:    vaddps (%rdx), %zmm1, %zmm1
; CHECK-NEXT:    vfmadd231ps {{.*#+}} zmm0 = (zmm2 * mem) + zmm0
; CHECK-NEXT:    vfmadd132ps {{.*#+}} zmm1 = (zmm1 * mem) + zmm1
; CHECK-NEXT:    vfmadd213ps {{.*#+}} zmm1 = (zmm0 * zmm1) + zmm0
; CHECK-NEXT:    movq dst32x16@{{.*}}(%rip), %rax
; CHECK-NEXT:    vmovaps %zmm1, (%rax)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %load_a = load <16 x float>, <16 x float>* @a32x16, align 64
  %load_b = load <16 x float>, <16 x float>* @b32x16, align 64
  %mul = fmul fast <16 x float> %load_a, %load_b
  %load_c = load <16 x float>, <16 x float>* @c32x16, align 64
  %mul1 = fmul fast <16 x float> %mul, %load_c
  %load_d = load <16 x float>, <16 x float>* @d32x16, align 64
  %mul2 = fmul fast <16 x float> %mul1, %load_d
  %load_e = load <16 x float>, <16 x float>* @e32x16, align 64
  %mul5 = fmul fast <16 x float> %mul1, %load_e
  %add = fadd fast <16 x float> %mul2, %mul5
  %mul6 = fmul fast <16 x float> %load_a, %load_d
  %load_f = load <16 x float>, <16 x float>* @f32x16, align 64
  %mul7 = fmul fast <16 x float> %mul6, %load_f
  %load_g = load <16 x float>, <16 x float>* @g32x16, align 64
  %mul8 = fmul fast <16 x float> %mul7, %load_g
  %add9 = fadd fast <16 x float> %add, %mul8
  %mul10 = fmul fast <16 x float> %load_a, %load_e
  %mul11 = fmul fast <16 x float> %mul10, %load_f
  %mul12 = fmul fast <16 x float> %mul11, %load_g
  %add13 = fadd fast <16 x float> %add9, %mul12
  %mul14 = fmul fast <16 x float> %load_b, %load_c
  %mul15 = fmul fast <16 x float> %mul14, %load_d
  %add16 = fadd fast <16 x float> %add13, %mul15
  %mul18 = fmul fast <16 x float> %mul14, %load_e
  %add19 = fadd fast <16 x float> %add16, %mul18
  %mul20 = fmul fast <16 x float> %load_d, %load_f
  %mul21 = fmul fast <16 x float> %mul20, %load_g
  %add22 = fadd fast <16 x float> %add19, %mul21
  %mul23 = fmul fast <16 x float> %load_e, %load_f
  %mul24 = fmul fast <16 x float> %mul23, %load_g
  %add25 = fadd fast <16 x float> %add22, %mul24
  %add27 = fadd fast <16 x float> %add25, %mul14
  %mul28 = fmul fast <16 x float> %load_f, %load_g
  %add29 = fadd fast <16 x float> %add27, %mul28
  store <16 x float> %add29, <16 x float>* @dst32x16, align 64
  ret void
}

@a64x8 = common global <8 x double> zeroinitializer, align 64
@b64x8 = common global <8 x double> zeroinitializer, align 64
@c64x8 = common global <8 x double> zeroinitializer, align 64
@d64x8 = common global <8 x double> zeroinitializer, align 64
@e64x8 = common global <8 x double> zeroinitializer, align 64
@f64x8 = common global <8 x double> zeroinitializer, align 64
@g64x8 = common global <8 x double> zeroinitializer, align 64
@dst64x8 = common global <8 x double> zeroinitializer, align 64
@h64x8 = common global <8 x double> zeroinitializer, align 64
@i64x8 = common global <8 x double> zeroinitializer, align 64

define void @func64x8() #1 {
; CHECK-LABEL: func64x8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movq a64x8@{{.*}}(%rip), %rax
; CHECK-NEXT:    movq c64x8@{{.*}}(%rip), %rcx
; CHECK-NEXT:    vmovapd (%rcx), %zmm0
; CHECK-NEXT:    movq d64x8@{{.*}}(%rip), %rcx
; CHECK-NEXT:    vmovapd (%rcx), %zmm1
; CHECK-NEXT:    movq b64x8@{{.*}}(%rip), %rcx
; CHECK-NEXT:    movq e64x8@{{.*}}(%rip), %rdx
; CHECK-NEXT:    movq f64x8@{{.*}}(%rip), %rsi
; CHECK-NEXT:    movq g64x8@{{.*}}(%rip), %rdi
; CHECK-NEXT:    vmovapd (%rdi), %zmm2
; CHECK-NEXT:    vmulpd (%rcx), %zmm0, %zmm0
; CHECK-NEXT:    vaddpd (%rdx), %zmm1, %zmm1
; CHECK-NEXT:    vfmadd231pd {{.*#+}} zmm0 = (zmm2 * mem) + zmm0
; CHECK-NEXT:    vfmadd132pd {{.*#+}} zmm1 = (zmm1 * mem) + zmm1
; CHECK-NEXT:    vfmadd213pd {{.*#+}} zmm1 = (zmm0 * zmm1) + zmm0
; CHECK-NEXT:    movq dst64x8@{{.*}}(%rip), %rax
; CHECK-NEXT:    vmovapd %zmm1, (%rax)
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %load_a = load <8 x double>, <8 x double>* @a64x8, align 64
  %load_b = load <8 x double>, <8 x double>* @b64x8, align 64
  %mul = fmul fast <8 x double> %load_a, %load_b
  %load_c = load <8 x double>, <8 x double>* @c64x8, align 64
  %mul1 = fmul fast <8 x double> %mul, %load_c
  %load_d = load <8 x double>, <8 x double>* @d64x8, align 64
  %mul2 = fmul fast <8 x double> %mul1, %load_d
  %load_e = load <8 x double>, <8 x double>* @e64x8, align 64
  %mul5 = fmul fast <8 x double> %mul1, %load_e
  %add = fadd fast <8 x double> %mul2, %mul5
  %mul6 = fmul fast <8 x double> %load_a, %load_d
  %load_f = load <8 x double>, <8 x double>* @f64x8, align 64
  %mul7 = fmul fast <8 x double> %mul6, %load_f
  %load_g = load <8 x double>, <8 x double>* @g64x8, align 64
  %mul8 = fmul fast <8 x double> %mul7, %load_g
  %add9 = fadd fast <8 x double> %add, %mul8
  %mul10 = fmul fast <8 x double> %load_a, %load_e
  %mul11 = fmul fast <8 x double> %mul10, %load_f
  %mul12 = fmul fast <8 x double> %mul11, %load_g
  %add13 = fadd fast <8 x double> %add9, %mul12
  %mul14 = fmul fast <8 x double> %load_b, %load_c
  %mul15 = fmul fast <8 x double> %mul14, %load_d
  %add16 = fadd fast <8 x double> %add13, %mul15
  %mul18 = fmul fast <8 x double> %mul14, %load_e
  %add19 = fadd fast <8 x double> %add16, %mul18
  %mul20 = fmul fast <8 x double> %load_d, %load_f
  %mul21 = fmul fast <8 x double> %mul20, %load_g
  %add22 = fadd fast <8 x double> %add19, %mul21
  %mul23 = fmul fast <8 x double> %load_e, %load_f
  %mul24 = fmul fast <8 x double> %mul23, %load_g
  %add25 = fadd fast <8 x double> %add22, %mul24
  %add27 = fadd fast <8 x double> %add25, %mul14
  %mul28 = fmul fast <8 x double> %load_f, %load_g
  %add29 = fadd fast <8 x double> %add27, %mul28
  store <8 x double> %add29, <8 x double>* @dst64x8, align 64
  ret void
}

