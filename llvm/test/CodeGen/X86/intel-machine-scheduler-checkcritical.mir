# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -o - %s -run-pass=machine-scheduler | FileCheck %s
--- |
  ; ModuleID = 'intel-machine-scheduler-checkcritical.ll'
  source_filename = "foo.c"
  target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
  target triple = "x86_64-unknown-linux-gnu"

  define void @foo(i32* %gload_ptr, i32* %gstore_ptr, float** %f_ptrptr) #0 {
  entry:
    %gloadptridx0 = getelementptr i32, i32* %gload_ptr, i64 0
    %gloadptridx1 = getelementptr i32, i32* %gload_ptr, i64 1
    %gloadptridx2 = getelementptr i32, i32* %gload_ptr, i64 2
    %gloadptridx3 = getelementptr i32, i32* %gload_ptr, i64 3
    %gloadptridx4 = getelementptr i32, i32* %gload_ptr, i64 4
    %gloadptridx5 = getelementptr i32, i32* %gload_ptr, i64 5
    %gloadptridx6 = getelementptr i32, i32* %gload_ptr, i64 6
    %gloadptridx7 = getelementptr i32, i32* %gload_ptr, i64 7
    %gloadptridx8 = getelementptr i32, i32* %gload_ptr, i64 8
    %gloadptridx9 = getelementptr i32, i32* %gload_ptr, i64 9
    %gloadptridx10 = getelementptr i32, i32* %gload_ptr, i64 10
    %gloadptridx11 = getelementptr i32, i32* %gload_ptr, i64 11
    %gloadptridx12 = getelementptr i32, i32* %gload_ptr, i64 12
    %gloadptridx13 = getelementptr i32, i32* %gload_ptr, i64 13
    %gloadptridx14 = getelementptr i32, i32* %gload_ptr, i64 14
    %gloadptridx15 = getelementptr i32, i32* %gload_ptr, i64 15
    %livethr0 = load i32, i32* %gloadptridx0, align 4
    %livethr1 = load i32, i32* %gloadptridx1, align 4
    %livethr2 = load i32, i32* %gloadptridx2, align 4
    %livethr3 = load i32, i32* %gloadptridx3, align 4
    %livethr4 = load i32, i32* %gloadptridx4, align 4
    %livethr5 = load i32, i32* %gloadptridx5, align 4
    %livethr6 = load i32, i32* %gloadptridx6, align 4
    %livethr7 = load i32, i32* %gloadptridx7, align 4
    %livethr8 = load i32, i32* %gloadptridx8, align 4
    %livethr9 = load i32, i32* %gloadptridx9, align 4
    %livethr10 = load i32, i32* %gloadptridx10, align 4
    %livethr11 = load i32, i32* %gloadptridx11, align 4
    %livethr12 = load i32, i32* %gloadptridx12, align 4
    %livethr13 = load i32, i32* %gloadptridx13, align 4
    %livethr14 = load i32, i32* %gloadptridx14, align 4
    %livethr15 = load i32, i32* %gloadptridx15, align 4
    %fptridx0 = getelementptr float*, float** %f_ptrptr, i64 0
    %fload_ptr0 = load float*, float** %fptridx0, align 8
    %fptridx1 = getelementptr float*, float** %f_ptrptr, i64 1
    %fload_ptr1 = load float*, float** %fptridx1, align 8
    %fptridx2 = getelementptr float*, float** %f_ptrptr, i64 2
    %fstore_front = load float*, float** %fptridx2, align 8
    %fstore_ptr = bitcast float* %fstore_front to [784 x [1024 x float]]*
    %fptridx3 = getelementptr float*, float** %f_ptrptr, i64 3
    %fstore_ptr2 = load float*, float** %fptridx2, align 8
    br label %body

  body:                                             ; preds = %entry
    %sum0 = add i32 %livethr0, %livethr1
    %sum1 = add i32 %sum0, %livethr2
    %sum2 = add i32 %sum1, %livethr3
    %sum3 = add i32 %sum2, %livethr4
    %sum4 = add i32 %sum3, %livethr5
    %sum5 = add i32 %sum4, %livethr6
    %sum6 = add i32 %sum5, %livethr7
    %sum7 = add i32 %sum6, %livethr8
    %sum8 = add i32 %sum7, %livethr9
    %sum9 = add i32 %sum8, %livethr10
    %sum10 = add i32 %sum9, %livethr11
    %sum11 = add i32 %sum10, %livethr12
    %sum12 = add i32 %sum11, %livethr13
    %sum13 = add i32 %sum12, %livethr14
    %sum14 = add i32 %sum13, %livethr15
    %idx0 = sext i32 %livethr0 to i64
    %idx1 = sext i32 %livethr1 to i64
    %aptridx0 = getelementptr float, float* %fload_ptr0, i64 0
    %bptridx0 = getelementptr float, float* %fload_ptr1, i64 0
    %a0 = load float, float* %aptridx0, align 4
    %b0 = load float, float* %bptridx0, align 4
    %aa0 = fadd float %a0, %a0
    %am0 = fmul float %a0, %a0
    %bb0 = fadd float %b0, %b0
    %bm0 = fmul float %b0, %b0
    %ab0 = fsub float %a0, %b0
    %fstoreptridx0 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 0
    store float %ab0, float* %fstoreptridx0, align 4
    %aptridx1 = getelementptr float, float* %fload_ptr0, i64 1
    %bptridx1 = getelementptr float, float* %fload_ptr1, i64 1
    %a1 = load float, float* %aptridx1, align 4
    %b1 = load float, float* %bptridx1, align 4
    %aa1 = fadd float %aa0, %a1
    %am1 = fmul float %a1, %am0
    %bb1 = fadd float %bb0, %b1
    %bm1 = fmul float %b1, %bm0
    %ab1 = fsub float %a1, %b1
    %fstoreptridx1 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 1
    store float %ab1, float* %fstoreptridx1, align 4
    %aptridx2 = getelementptr float, float* %fload_ptr0, i64 2
    %bptridx2 = getelementptr float, float* %fload_ptr1, i64 2
    %a2 = load float, float* %aptridx2, align 4
    %b2 = load float, float* %bptridx2, align 4
    %aa2 = fadd float %aa1, %a2
    %am2 = fmul float %a2, %am1
    %bb2 = fadd float %bb1, %b2
    %bm2 = fmul float %b2, %bm1
    %ab2 = fsub float %a2, %b2
    %fstoreptridx2 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 2
    store float %ab2, float* %fstoreptridx2, align 4
    %aptridx3 = getelementptr float, float* %fload_ptr0, i64 3
    %bptridx3 = getelementptr float, float* %fload_ptr1, i64 3
    %a3 = load float, float* %aptridx3, align 4
    %b3 = load float, float* %bptridx3, align 4
    %aa3 = fadd float %aa2, %a3
    %am3 = fmul float %a3, %am2
    %bb3 = fadd float %bb2, %b3
    %bm3 = fmul float %b3, %bm2
    %ab3 = fsub float %a3, %b3
    %fstoreptridx3 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 3
    store float %ab3, float* %fstoreptridx3, align 4
    %aptridx4 = getelementptr float, float* %fload_ptr0, i64 4
    %bptridx4 = getelementptr float, float* %fload_ptr1, i64 4
    %a4 = load float, float* %aptridx4, align 4
    %b4 = load float, float* %bptridx4, align 4
    %aa4 = fadd float %aa3, %a4
    %am4 = fmul float %a4, %am3
    %bb4 = fadd float %bb3, %b4
    %bm4 = fmul float %b4, %bm3
    %ab4 = fsub float %a4, %b4
    %fstoreptridx4 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 4
    store float %ab4, float* %fstoreptridx4, align 4
    %aptridx5 = getelementptr float, float* %fload_ptr0, i64 5
    %bptridx5 = getelementptr float, float* %fload_ptr1, i64 5
    %a5 = load float, float* %aptridx5, align 4
    %b5 = load float, float* %bptridx5, align 4
    %aa5 = fadd float %aa4, %a5
    %am5 = fmul float %a5, %am4
    %bb5 = fadd float %bb4, %b5
    %bm5 = fmul float %b5, %bm4
    %ab5 = fsub float %a5, %b5
    %fstoreptridx5 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 5
    store float %ab5, float* %fstoreptridx5, align 4
    %aptridx6 = getelementptr float, float* %fload_ptr0, i64 6
    %bptridx6 = getelementptr float, float* %fload_ptr1, i64 6
    %a6 = load float, float* %aptridx6, align 4
    %b6 = load float, float* %bptridx6, align 4
    %aa6 = fadd float %aa5, %a6
    %am6 = fmul float %a6, %am5
    %bb6 = fadd float %bb5, %b6
    %bm6 = fmul float %b6, %bm5
    %ab6 = fsub float %a6, %b6
    %fstoreptridx6 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 6
    store float %ab6, float* %fstoreptridx6, align 4
    %aptridx7 = getelementptr float, float* %fload_ptr0, i64 7
    %bptridx7 = getelementptr float, float* %fload_ptr1, i64 7
    %a7 = load float, float* %aptridx7, align 4
    %b7 = load float, float* %bptridx7, align 4
    %aa7 = fadd float %aa6, %a7
    %am7 = fmul float %a7, %am6
    %bb7 = fadd float %bb6, %b7
    %bm7 = fmul float %b7, %bm6
    %ab7 = fsub float %a7, %b7
    %fstoreptridx7 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 7
    store float %ab7, float* %fstoreptridx7, align 4
    %aptridx8 = getelementptr float, float* %fload_ptr0, i64 8
    %bptridx8 = getelementptr float, float* %fload_ptr1, i64 8
    %a8 = load float, float* %aptridx8, align 4
    %b8 = load float, float* %bptridx8, align 4
    %aa8 = fadd float %aa7, %a8
    %am8 = fmul float %a8, %am7
    %bb8 = fadd float %bb7, %b8
    %bm8 = fmul float %b8, %bm7
    %ab8 = fsub float %a8, %b8
    %fstoreptridx8 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 8
    store float %ab8, float* %fstoreptridx8, align 4
    %aptridx9 = getelementptr float, float* %fload_ptr0, i64 9
    %bptridx9 = getelementptr float, float* %fload_ptr1, i64 9
    %a9 = load float, float* %aptridx9, align 4
    %b9 = load float, float* %bptridx9, align 4
    %aa9 = fadd float %aa8, %a9
    %am9 = fmul float %a9, %am8
    %bb9 = fadd float %bb8, %b9
    %bm9 = fmul float %b9, %bm8
    %ab9 = fsub float %a9, %b9
    %fstoreptridx9 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 9
    store float %ab9, float* %fstoreptridx9, align 4
    %aptridx10 = getelementptr float, float* %fload_ptr0, i64 10
    %bptridx10 = getelementptr float, float* %fload_ptr1, i64 10
    %a10 = load float, float* %aptridx10, align 4
    %b10 = load float, float* %bptridx10, align 4
    %aa10 = fadd float %aa9, %a10
    %am10 = fmul float %a10, %am9
    %bb10 = fadd float %bb9, %b10
    %bm10 = fmul float %b10, %bm9
    %ab10 = fsub float %a10, %b10
    %fstoreptridx10 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 10
    store float %ab10, float* %fstoreptridx10, align 4
    %aptridx11 = getelementptr float, float* %fload_ptr0, i64 11
    %bptridx11 = getelementptr float, float* %fload_ptr1, i64 11
    %a11 = load float, float* %aptridx11, align 4
    %b11 = load float, float* %bptridx11, align 4
    %aa11 = fadd float %aa10, %a11
    %am11 = fmul float %a11, %am10
    %bb11 = fadd float %bb10, %b11
    %bm11 = fmul float %b11, %bm10
    %ab11 = fsub float %a11, %b11
    %fstoreptridx11 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 11
    store float %ab11, float* %fstoreptridx11, align 4
    %aptridx12 = getelementptr float, float* %fload_ptr0, i64 12
    %bptridx12 = getelementptr float, float* %fload_ptr1, i64 12
    %a12 = load float, float* %aptridx12, align 4
    %b12 = load float, float* %bptridx12, align 4
    %aa12 = fadd float %aa11, %a12
    %am12 = fmul float %a12, %am11
    %bb12 = fadd float %bb11, %b12
    %bm12 = fmul float %b12, %bm11
    %ab12 = fsub float %a12, %b12
    %fstoreptridx12 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 12
    store float %ab12, float* %fstoreptridx12, align 4
    %aptridx13 = getelementptr float, float* %fload_ptr0, i64 13
    %bptridx13 = getelementptr float, float* %fload_ptr1, i64 13
    %a13 = load float, float* %aptridx13, align 4
    %b13 = load float, float* %bptridx13, align 4
    %aa13 = fadd float %aa12, %a13
    %am13 = fmul float %a13, %am12
    %bb13 = fadd float %bb12, %b13
    %bm13 = fmul float %b13, %bm12
    %ab13 = fsub float %a13, %b13
    %fstoreptridx13 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 13
    store float %ab13, float* %fstoreptridx13, align 4
    %aptridx14 = getelementptr float, float* %fload_ptr0, i64 14
    %bptridx14 = getelementptr float, float* %fload_ptr1, i64 14
    %a14 = load float, float* %aptridx14, align 4
    %b14 = load float, float* %bptridx14, align 4
    %aa14 = fadd float %aa13, %a14
    %am14 = fmul float %a14, %am13
    %bb14 = fadd float %bb13, %b14
    %bm14 = fmul float %b14, %bm13
    %ab14 = fsub float %a14, %b14
    %fstoreptridx14 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 14
    store float %ab14, float* %fstoreptridx14, align 4
    %aptridx15 = getelementptr float, float* %fload_ptr0, i64 15
    %bptridx15 = getelementptr float, float* %fload_ptr1, i64 15
    %a15 = load float, float* %aptridx15, align 4
    %b15 = load float, float* %bptridx15, align 4
    %aa15 = fadd float %aa14, %a15
    %am15 = fmul float %a15, %am14
    %bb15 = fadd float %bb14, %b15
    %bm15 = fmul float %b15, %bm14
    %ab15 = fsub float %a15, %b15
    %fstoreptridx15 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 15
    store float %ab15, float* %fstoreptridx15, align 4
    %aptridx16 = getelementptr float, float* %fload_ptr0, i64 16
    %bptridx16 = getelementptr float, float* %fload_ptr1, i64 16
    %a16 = load float, float* %aptridx16, align 4
    %b16 = load float, float* %bptridx16, align 4
    %aa16 = fadd float %aa15, %a16
    %am16 = fmul float %a16, %am15
    %bb16 = fadd float %bb15, %b16
    %bm16 = fmul float %b16, %bm15
    %ab16 = fsub float %a16, %b16
    %fstoreptridx16 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 16
    store float %ab16, float* %fstoreptridx16, align 4
    %aptridx17 = getelementptr float, float* %fload_ptr0, i64 17
    %bptridx17 = getelementptr float, float* %fload_ptr1, i64 17
    %a17 = load float, float* %aptridx17, align 4
    %b17 = load float, float* %bptridx17, align 4
    %aa17 = fadd float %aa16, %a17
    %am17 = fmul float %a17, %am16
    %bb17 = fadd float %bb16, %b17
    %bm17 = fmul float %b17, %bm16
    %ab17 = fsub float %a17, %b17
    %fstoreptridx17 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 17
    store float %ab17, float* %fstoreptridx17, align 4
    %aptridx18 = getelementptr float, float* %fload_ptr0, i64 18
    %bptridx18 = getelementptr float, float* %fload_ptr1, i64 18
    %a18 = load float, float* %aptridx18, align 4
    %b18 = load float, float* %bptridx18, align 4
    %aa18 = fadd float %aa17, %a18
    %am18 = fmul float %a18, %am17
    %bb18 = fadd float %bb17, %b18
    %bm18 = fmul float %b18, %bm17
    %ab18 = fsub float %a18, %b18
    %fstoreptridx18 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 18
    store float %ab18, float* %fstoreptridx18, align 4
    %aptridx19 = getelementptr float, float* %fload_ptr0, i64 19
    %bptridx19 = getelementptr float, float* %fload_ptr1, i64 19
    %a19 = load float, float* %aptridx19, align 4
    %b19 = load float, float* %bptridx19, align 4
    %aa19 = fadd float %aa18, %a19
    %am19 = fmul float %a19, %am18
    %bb19 = fadd float %bb18, %b19
    %bm19 = fmul float %b19, %bm18
    %ab19 = fsub float %a19, %b19
    %fstoreptridx19 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 19
    store float %ab19, float* %fstoreptridx19, align 4
    %aptridx20 = getelementptr float, float* %fload_ptr0, i64 20
    %bptridx20 = getelementptr float, float* %fload_ptr1, i64 20
    %a20 = load float, float* %aptridx20, align 4
    %b20 = load float, float* %bptridx20, align 4
    %aa20 = fadd float %aa19, %a20
    %am20 = fmul float %a20, %am19
    %bb20 = fadd float %bb19, %b20
    %bm20 = fmul float %b20, %bm19
    %ab20 = fsub float %a20, %b20
    %fstoreptridx20 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 20
    store float %ab20, float* %fstoreptridx20, align 4
    %aptridx21 = getelementptr float, float* %fload_ptr0, i64 21
    %bptridx21 = getelementptr float, float* %fload_ptr1, i64 21
    %a21 = load float, float* %aptridx21, align 4
    %b21 = load float, float* %bptridx21, align 4
    %aa21 = fadd float %aa20, %a21
    %am21 = fmul float %a21, %am20
    %bb21 = fadd float %bb20, %b21
    %bm21 = fmul float %b21, %bm20
    %ab21 = fsub float %a21, %b21
    %fstoreptridx21 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 21
    store float %ab21, float* %fstoreptridx21, align 4
    %aptridx22 = getelementptr float, float* %fload_ptr0, i64 22
    %bptridx22 = getelementptr float, float* %fload_ptr1, i64 22
    %a22 = load float, float* %aptridx22, align 4
    %b22 = load float, float* %bptridx22, align 4
    %aa22 = fadd float %aa21, %a22
    %am22 = fmul float %a22, %am21
    %bb22 = fadd float %bb21, %b22
    %bm22 = fmul float %b22, %bm21
    %ab22 = fsub float %a22, %b22
    %fstoreptridx22 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 22
    store float %ab22, float* %fstoreptridx22, align 4
    %aptridx23 = getelementptr float, float* %fload_ptr0, i64 23
    %bptridx23 = getelementptr float, float* %fload_ptr1, i64 23
    %a23 = load float, float* %aptridx23, align 4
    %b23 = load float, float* %bptridx23, align 4
    %aa23 = fadd float %aa22, %a23
    %am23 = fmul float %a23, %am22
    %bb23 = fadd float %bb22, %b23
    %bm23 = fmul float %b23, %bm22
    %ab23 = fsub float %a23, %b23
    %fstoreptridx23 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 23
    store float %ab23, float* %fstoreptridx23, align 4
    %aptridx24 = getelementptr float, float* %fload_ptr0, i64 24
    %bptridx24 = getelementptr float, float* %fload_ptr1, i64 24
    %a24 = load float, float* %aptridx24, align 4
    %b24 = load float, float* %bptridx24, align 4
    %aa24 = fadd float %aa23, %a24
    %am24 = fmul float %a24, %am23
    %bb24 = fadd float %bb23, %b24
    %bm24 = fmul float %b24, %bm23
    %ab24 = fsub float %a24, %b24
    %fstoreptridx24 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 24
    store float %ab24, float* %fstoreptridx24, align 4
    %aptridx25 = getelementptr float, float* %fload_ptr0, i64 25
    %bptridx25 = getelementptr float, float* %fload_ptr1, i64 25
    %a25 = load float, float* %aptridx25, align 4
    %b25 = load float, float* %bptridx25, align 4
    %aa25 = fadd float %aa24, %a25
    %am25 = fmul float %a25, %am24
    %bb25 = fadd float %bb24, %b25
    %bm25 = fmul float %b25, %bm24
    %ab25 = fsub float %a25, %b25
    %fstoreptridx25 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 25
    store float %ab25, float* %fstoreptridx25, align 4
    %aptridx26 = getelementptr float, float* %fload_ptr0, i64 26
    %bptridx26 = getelementptr float, float* %fload_ptr1, i64 26
    %a26 = load float, float* %aptridx26, align 4
    %b26 = load float, float* %bptridx26, align 4
    %aa26 = fadd float %aa25, %a26
    %am26 = fmul float %a26, %am25
    %bb26 = fadd float %bb25, %b26
    %bm26 = fmul float %b26, %bm25
    %ab26 = fsub float %a26, %b26
    %fstoreptridx26 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 26
    store float %ab26, float* %fstoreptridx26, align 4
    %aptridx27 = getelementptr float, float* %fload_ptr0, i64 27
    %bptridx27 = getelementptr float, float* %fload_ptr1, i64 27
    %a27 = load float, float* %aptridx27, align 4
    %b27 = load float, float* %bptridx27, align 4
    %aa27 = fadd float %aa26, %a27
    %am27 = fmul float %a27, %am26
    %bb27 = fadd float %bb26, %b27
    %bm27 = fmul float %b27, %bm26
    %ab27 = fsub float %a27, %b27
    %fstoreptridx27 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 27
    store float %ab27, float* %fstoreptridx27, align 4
    %aptridx28 = getelementptr float, float* %fload_ptr0, i64 28
    %bptridx28 = getelementptr float, float* %fload_ptr1, i64 28
    %a28 = load float, float* %aptridx28, align 4
    %b28 = load float, float* %bptridx28, align 4
    %aa28 = fadd float %aa27, %a28
    %am28 = fmul float %a28, %am27
    %bb28 = fadd float %bb27, %b28
    %bm28 = fmul float %b28, %bm27
    %ab28 = fsub float %a28, %b28
    %fstoreptridx28 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 28
    store float %ab28, float* %fstoreptridx28, align 4
    %aptridx29 = getelementptr float, float* %fload_ptr0, i64 29
    %bptridx29 = getelementptr float, float* %fload_ptr1, i64 29
    %a29 = load float, float* %aptridx29, align 4
    %b29 = load float, float* %bptridx29, align 4
    %aa29 = fadd float %aa28, %a29
    %am29 = fmul float %a29, %am28
    %bb29 = fadd float %bb28, %b29
    %bm29 = fmul float %b29, %bm28
    %ab29 = fsub float %a29, %b29
    %fstoreptridx29 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 29
    store float %ab29, float* %fstoreptridx29, align 4
    %aptridx30 = getelementptr float, float* %fload_ptr0, i64 30
    %bptridx30 = getelementptr float, float* %fload_ptr1, i64 30
    %a30 = load float, float* %aptridx30, align 4
    %b30 = load float, float* %bptridx30, align 4
    %aa30 = fadd float %aa29, %a30
    %am30 = fmul float %a30, %am29
    %bb30 = fadd float %bb29, %b30
    %bm30 = fmul float %b30, %bm29
    %ab30 = fsub float %a30, %b30
    %fstoreptridx30 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 30
    store float %ab30, float* %fstoreptridx30, align 4
    %aptridx31 = getelementptr float, float* %fload_ptr0, i64 31
    %bptridx31 = getelementptr float, float* %fload_ptr1, i64 31
    %a31 = load float, float* %aptridx31, align 4
    %b31 = load float, float* %bptridx31, align 4
    %aa31 = fadd float %aa30, %a31
    %am31 = fmul float %a31, %am30
    %bb31 = fadd float %bb30, %b31
    %bm31 = fmul float %b31, %bm30
    %ab31 = fsub float %a31, %b31
    %fstoreptridx31 = getelementptr [784 x [1024 x float]], [784 x [1024 x float]]* %fstore_ptr, i64 %idx0, i64 %idx1, i64 31
    store float %ab31, float* %fstoreptridx31, align 4
    %fstoreptr2idx0 = getelementptr float, float* %fstore_ptr2, i64 0
    store float %aa31, float* %fstoreptr2idx0, align 4
    %fstoreptr2idx1 = getelementptr float, float* %fstore_ptr2, i64 1
    store float %am31, float* %fstoreptr2idx1, align 4
    %fstoreptr2idx2 = getelementptr float, float* %fstore_ptr2, i64 2
    store float %bb31, float* %fstoreptr2idx2, align 4
    %fstoreptr2idx3 = getelementptr float, float* %fstore_ptr2, i64 3
    store float %bm31, float* %fstoreptr2idx3, align 4
    br label %exit

  exit:                                             ; preds = %body
    %gstoreptridx0 = getelementptr i32, i32* %gstore_ptr, i32 0
    %gstoreptridx1 = getelementptr i32, i32* %gstore_ptr, i32 1
    %gstoreptridx2 = getelementptr i32, i32* %gstore_ptr, i32 2
    %gstoreptridx3 = getelementptr i32, i32* %gstore_ptr, i32 3
    %gstoreptridx4 = getelementptr i32, i32* %gstore_ptr, i32 4
    %gstoreptridx5 = getelementptr i32, i32* %gstore_ptr, i32 5
    %gstoreptridx6 = getelementptr i32, i32* %gstore_ptr, i32 6
    %gstoreptridx7 = getelementptr i32, i32* %gstore_ptr, i32 7
    %gstoreptridx8 = getelementptr i32, i32* %gstore_ptr, i32 8
    %gstoreptridx9 = getelementptr i32, i32* %gstore_ptr, i32 9
    %gstoreptridx10 = getelementptr i32, i32* %gstore_ptr, i32 10
    %gstoreptridx11 = getelementptr i32, i32* %gstore_ptr, i32 11
    %gstoreptridx12 = getelementptr i32, i32* %gstore_ptr, i32 12
    %gstoreptridx13 = getelementptr i32, i32* %gstore_ptr, i32 13
    %gstoreptridx14 = getelementptr i32, i32* %gstore_ptr, i32 14
    %gstoreptridx15 = getelementptr i32, i32* %gstore_ptr, i32 15
    store i32 %livethr0, i32* %gstoreptridx0, align 4
    store i32 %livethr1, i32* %gstoreptridx1, align 4
    store i32 %livethr2, i32* %gstoreptridx2, align 4
    store i32 %livethr3, i32* %gstoreptridx3, align 4
    store i32 %livethr4, i32* %gstoreptridx4, align 4
    store i32 %livethr5, i32* %gstoreptridx5, align 4
    store i32 %livethr6, i32* %gstoreptridx6, align 4
    store i32 %livethr7, i32* %gstoreptridx7, align 4
    store i32 %livethr8, i32* %gstoreptridx8, align 4
    store i32 %livethr9, i32* %gstoreptridx9, align 4
    store i32 %livethr10, i32* %gstoreptridx10, align 4
    store i32 %livethr11, i32* %gstoreptridx11, align 4
    store i32 %livethr12, i32* %gstoreptridx12, align 4
    store i32 %livethr13, i32* %gstoreptridx13, align 4
    store i32 %livethr14, i32* %gstoreptridx14, align 4
    store i32 %sum14, i32* %gstoreptridx15, align 4
    ret void
  }

  attributes #0 = { "target-cpu"="skylake-avx512" }

...
---
name:            foo
alignment:       16
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
registers:
  - { id: 0, class: gr32, preferred-register: '' }
  - { id: 1, class: gr32, preferred-register: '' }
  - { id: 2, class: gr32, preferred-register: '' }
  - { id: 3, class: gr32, preferred-register: '' }
  - { id: 4, class: gr32, preferred-register: '' }
  - { id: 5, class: gr32, preferred-register: '' }
  - { id: 6, class: gr32, preferred-register: '' }
  - { id: 7, class: gr32, preferred-register: '' }
  - { id: 8, class: gr32, preferred-register: '' }
  - { id: 9, class: gr32, preferred-register: '' }
  - { id: 10, class: gr32, preferred-register: '' }
  - { id: 11, class: gr32, preferred-register: '' }
  - { id: 12, class: gr32, preferred-register: '' }
  - { id: 13, class: gr32, preferred-register: '' }
  - { id: 14, class: gr32, preferred-register: '' }
  - { id: 15, class: gr32, preferred-register: '' }
  - { id: 16, class: gr64, preferred-register: '' }
  - { id: 17, class: gr64, preferred-register: '' }
  - { id: 18, class: gr64, preferred-register: '' }
  - { id: 19, class: gr64, preferred-register: '' }
  - { id: 20, class: gr32, preferred-register: '' }
  - { id: 21, class: gr64, preferred-register: '' }
  - { id: 22, class: gr64, preferred-register: '' }
  - { id: 23, class: gr64, preferred-register: '' }
  - { id: 24, class: gr32, preferred-register: '' }
  - { id: 25, class: gr32, preferred-register: '' }
  - { id: 26, class: gr32, preferred-register: '' }
  - { id: 27, class: gr32, preferred-register: '' }
  - { id: 28, class: gr32, preferred-register: '' }
  - { id: 29, class: gr32, preferred-register: '' }
  - { id: 30, class: gr32, preferred-register: '' }
  - { id: 31, class: gr32, preferred-register: '' }
  - { id: 32, class: gr32, preferred-register: '' }
  - { id: 33, class: gr32, preferred-register: '' }
  - { id: 34, class: gr32, preferred-register: '' }
  - { id: 35, class: gr32, preferred-register: '' }
  - { id: 36, class: gr32, preferred-register: '' }
  - { id: 37, class: gr32, preferred-register: '' }
  - { id: 38, class: gr64, preferred-register: '' }
  - { id: 39, class: gr64, preferred-register: '' }
  - { id: 40, class: fr32x, preferred-register: '' }
  - { id: 41, class: fr32x, preferred-register: '' }
  - { id: 42, class: fr32x, preferred-register: '' }
  - { id: 43, class: fr32x, preferred-register: '' }
  - { id: 44, class: fr32x, preferred-register: '' }
  - { id: 45, class: fr32x, preferred-register: '' }
  - { id: 46, class: fr32x, preferred-register: '' }
  - { id: 47, class: gr64, preferred-register: '' }
  - { id: 48, class: gr64_nosp, preferred-register: '' }
  - { id: 49, class: gr64, preferred-register: '' }
  - { id: 50, class: fr32x, preferred-register: '' }
  - { id: 51, class: fr32x, preferred-register: '' }
  - { id: 52, class: fr32x, preferred-register: '' }
  - { id: 53, class: fr32x, preferred-register: '' }
  - { id: 54, class: fr32x, preferred-register: '' }
  - { id: 55, class: fr32x, preferred-register: '' }
  - { id: 56, class: fr32x, preferred-register: '' }
  - { id: 57, class: fr32x, preferred-register: '' }
  - { id: 58, class: fr32x, preferred-register: '' }
  - { id: 59, class: fr32x, preferred-register: '' }
  - { id: 60, class: fr32x, preferred-register: '' }
  - { id: 61, class: fr32x, preferred-register: '' }
  - { id: 62, class: fr32x, preferred-register: '' }
  - { id: 63, class: fr32x, preferred-register: '' }
  - { id: 64, class: fr32x, preferred-register: '' }
  - { id: 65, class: fr32x, preferred-register: '' }
  - { id: 66, class: fr32x, preferred-register: '' }
  - { id: 67, class: fr32x, preferred-register: '' }
  - { id: 68, class: fr32x, preferred-register: '' }
  - { id: 69, class: fr32x, preferred-register: '' }
  - { id: 70, class: fr32x, preferred-register: '' }
  - { id: 71, class: fr32x, preferred-register: '' }
  - { id: 72, class: fr32x, preferred-register: '' }
  - { id: 73, class: fr32x, preferred-register: '' }
  - { id: 74, class: fr32x, preferred-register: '' }
  - { id: 75, class: fr32x, preferred-register: '' }
  - { id: 76, class: fr32x, preferred-register: '' }
  - { id: 77, class: fr32x, preferred-register: '' }
  - { id: 78, class: fr32x, preferred-register: '' }
  - { id: 79, class: fr32x, preferred-register: '' }
  - { id: 80, class: fr32x, preferred-register: '' }
  - { id: 81, class: fr32x, preferred-register: '' }
  - { id: 82, class: fr32x, preferred-register: '' }
  - { id: 83, class: fr32x, preferred-register: '' }
  - { id: 84, class: fr32x, preferred-register: '' }
  - { id: 85, class: fr32x, preferred-register: '' }
  - { id: 86, class: fr32x, preferred-register: '' }
  - { id: 87, class: fr32x, preferred-register: '' }
  - { id: 88, class: fr32x, preferred-register: '' }
  - { id: 89, class: fr32x, preferred-register: '' }
  - { id: 90, class: fr32x, preferred-register: '' }
  - { id: 91, class: fr32x, preferred-register: '' }
  - { id: 92, class: fr32x, preferred-register: '' }
  - { id: 93, class: fr32x, preferred-register: '' }
  - { id: 94, class: fr32x, preferred-register: '' }
  - { id: 95, class: fr32x, preferred-register: '' }
  - { id: 96, class: fr32x, preferred-register: '' }
  - { id: 97, class: fr32x, preferred-register: '' }
  - { id: 98, class: fr32x, preferred-register: '' }
  - { id: 99, class: fr32x, preferred-register: '' }
  - { id: 100, class: fr32x, preferred-register: '' }
  - { id: 101, class: fr32x, preferred-register: '' }
  - { id: 102, class: fr32x, preferred-register: '' }
  - { id: 103, class: fr32x, preferred-register: '' }
  - { id: 104, class: fr32x, preferred-register: '' }
  - { id: 105, class: fr32x, preferred-register: '' }
  - { id: 106, class: fr32x, preferred-register: '' }
  - { id: 107, class: fr32x, preferred-register: '' }
  - { id: 108, class: fr32x, preferred-register: '' }
  - { id: 109, class: fr32x, preferred-register: '' }
  - { id: 110, class: fr32x, preferred-register: '' }
  - { id: 111, class: fr32x, preferred-register: '' }
  - { id: 112, class: fr32x, preferred-register: '' }
  - { id: 113, class: fr32x, preferred-register: '' }
  - { id: 114, class: fr32x, preferred-register: '' }
  - { id: 115, class: fr32x, preferred-register: '' }
  - { id: 116, class: fr32x, preferred-register: '' }
  - { id: 117, class: fr32x, preferred-register: '' }
  - { id: 118, class: fr32x, preferred-register: '' }
  - { id: 119, class: fr32x, preferred-register: '' }
  - { id: 120, class: fr32x, preferred-register: '' }
  - { id: 121, class: fr32x, preferred-register: '' }
  - { id: 122, class: fr32x, preferred-register: '' }
  - { id: 123, class: fr32x, preferred-register: '' }
  - { id: 124, class: fr32x, preferred-register: '' }
  - { id: 125, class: fr32x, preferred-register: '' }
  - { id: 126, class: fr32x, preferred-register: '' }
  - { id: 127, class: fr32x, preferred-register: '' }
  - { id: 128, class: fr32x, preferred-register: '' }
  - { id: 129, class: fr32x, preferred-register: '' }
  - { id: 130, class: fr32x, preferred-register: '' }
  - { id: 131, class: fr32x, preferred-register: '' }
  - { id: 132, class: fr32x, preferred-register: '' }
  - { id: 133, class: fr32x, preferred-register: '' }
  - { id: 134, class: fr32x, preferred-register: '' }
  - { id: 135, class: fr32x, preferred-register: '' }
  - { id: 136, class: fr32x, preferred-register: '' }
  - { id: 137, class: fr32x, preferred-register: '' }
  - { id: 138, class: fr32x, preferred-register: '' }
  - { id: 139, class: fr32x, preferred-register: '' }
  - { id: 140, class: fr32x, preferred-register: '' }
  - { id: 141, class: fr32x, preferred-register: '' }
  - { id: 142, class: fr32x, preferred-register: '' }
  - { id: 143, class: fr32x, preferred-register: '' }
  - { id: 144, class: fr32x, preferred-register: '' }
  - { id: 145, class: fr32x, preferred-register: '' }
  - { id: 146, class: fr32x, preferred-register: '' }
  - { id: 147, class: fr32x, preferred-register: '' }
  - { id: 148, class: fr32x, preferred-register: '' }
  - { id: 149, class: fr32x, preferred-register: '' }
  - { id: 150, class: fr32x, preferred-register: '' }
  - { id: 151, class: fr32x, preferred-register: '' }
  - { id: 152, class: fr32x, preferred-register: '' }
  - { id: 153, class: fr32x, preferred-register: '' }
  - { id: 154, class: fr32x, preferred-register: '' }
  - { id: 155, class: fr32x, preferred-register: '' }
  - { id: 156, class: fr32x, preferred-register: '' }
  - { id: 157, class: fr32x, preferred-register: '' }
  - { id: 158, class: fr32x, preferred-register: '' }
  - { id: 159, class: fr32x, preferred-register: '' }
  - { id: 160, class: fr32x, preferred-register: '' }
  - { id: 161, class: fr32x, preferred-register: '' }
  - { id: 162, class: fr32x, preferred-register: '' }
  - { id: 163, class: fr32x, preferred-register: '' }
  - { id: 164, class: fr32x, preferred-register: '' }
  - { id: 165, class: fr32x, preferred-register: '' }
  - { id: 166, class: fr32x, preferred-register: '' }
  - { id: 167, class: fr32x, preferred-register: '' }
  - { id: 168, class: fr32x, preferred-register: '' }
  - { id: 169, class: fr32x, preferred-register: '' }
  - { id: 170, class: fr32x, preferred-register: '' }
  - { id: 171, class: fr32x, preferred-register: '' }
  - { id: 172, class: fr32x, preferred-register: '' }
  - { id: 173, class: fr32x, preferred-register: '' }
  - { id: 174, class: fr32x, preferred-register: '' }
  - { id: 175, class: fr32x, preferred-register: '' }
  - { id: 176, class: fr32x, preferred-register: '' }
  - { id: 177, class: fr32x, preferred-register: '' }
  - { id: 178, class: fr32x, preferred-register: '' }
  - { id: 179, class: fr32x, preferred-register: '' }
  - { id: 180, class: fr32x, preferred-register: '' }
  - { id: 181, class: fr32x, preferred-register: '' }
  - { id: 182, class: fr32x, preferred-register: '' }
  - { id: 183, class: fr32x, preferred-register: '' }
  - { id: 184, class: fr32x, preferred-register: '' }
  - { id: 185, class: fr32x, preferred-register: '' }
  - { id: 186, class: fr32x, preferred-register: '' }
  - { id: 187, class: fr32x, preferred-register: '' }
  - { id: 188, class: fr32x, preferred-register: '' }
  - { id: 189, class: fr32x, preferred-register: '' }
  - { id: 190, class: fr32x, preferred-register: '' }
  - { id: 191, class: fr32x, preferred-register: '' }
  - { id: 192, class: fr32x, preferred-register: '' }
  - { id: 193, class: fr32x, preferred-register: '' }
  - { id: 194, class: fr32x, preferred-register: '' }
  - { id: 195, class: fr32x, preferred-register: '' }
  - { id: 196, class: fr32x, preferred-register: '' }
  - { id: 197, class: fr32x, preferred-register: '' }
  - { id: 198, class: fr32x, preferred-register: '' }
  - { id: 199, class: fr32x, preferred-register: '' }
  - { id: 200, class: fr32x, preferred-register: '' }
  - { id: 201, class: fr32x, preferred-register: '' }
  - { id: 202, class: fr32x, preferred-register: '' }
  - { id: 203, class: fr32x, preferred-register: '' }
  - { id: 204, class: fr32x, preferred-register: '' }
  - { id: 205, class: fr32x, preferred-register: '' }
  - { id: 206, class: fr32x, preferred-register: '' }
  - { id: 207, class: fr32x, preferred-register: '' }
  - { id: 208, class: fr32x, preferred-register: '' }
  - { id: 209, class: fr32x, preferred-register: '' }
  - { id: 210, class: fr32x, preferred-register: '' }
  - { id: 211, class: fr32x, preferred-register: '' }
  - { id: 212, class: fr32x, preferred-register: '' }
  - { id: 213, class: fr32x, preferred-register: '' }
  - { id: 214, class: fr32x, preferred-register: '' }
  - { id: 215, class: fr32x, preferred-register: '' }
  - { id: 216, class: fr32x, preferred-register: '' }
  - { id: 217, class: fr32x, preferred-register: '' }
  - { id: 218, class: fr32x, preferred-register: '' }
  - { id: 219, class: fr32x, preferred-register: '' }
  - { id: 220, class: fr32x, preferred-register: '' }
  - { id: 221, class: fr32x, preferred-register: '' }
  - { id: 222, class: fr32x, preferred-register: '' }
  - { id: 223, class: fr32x, preferred-register: '' }
  - { id: 224, class: fr32x, preferred-register: '' }
  - { id: 225, class: fr32x, preferred-register: '' }
  - { id: 226, class: fr32x, preferred-register: '' }
  - { id: 227, class: fr32x, preferred-register: '' }
  - { id: 228, class: fr32x, preferred-register: '' }
  - { id: 229, class: fr32x, preferred-register: '' }
  - { id: 230, class: fr32x, preferred-register: '' }
  - { id: 231, class: fr32x, preferred-register: '' }
  - { id: 232, class: fr32x, preferred-register: '' }
  - { id: 233, class: fr32x, preferred-register: '' }
  - { id: 234, class: fr32x, preferred-register: '' }
  - { id: 235, class: fr32x, preferred-register: '' }
  - { id: 236, class: fr32x, preferred-register: '' }
  - { id: 237, class: fr32x, preferred-register: '' }
  - { id: 238, class: fr32x, preferred-register: '' }
  - { id: 239, class: fr32x, preferred-register: '' }
  - { id: 240, class: fr32x, preferred-register: '' }
  - { id: 241, class: fr32x, preferred-register: '' }
  - { id: 242, class: fr32x, preferred-register: '' }
  - { id: 243, class: fr32x, preferred-register: '' }
  - { id: 244, class: fr32x, preferred-register: '' }
  - { id: 245, class: fr32x, preferred-register: '' }
  - { id: 246, class: fr32x, preferred-register: '' }
  - { id: 247, class: fr32x, preferred-register: '' }
  - { id: 248, class: fr32x, preferred-register: '' }
  - { id: 249, class: fr32x, preferred-register: '' }
  - { id: 250, class: fr32x, preferred-register: '' }
  - { id: 251, class: fr32x, preferred-register: '' }
  - { id: 252, class: fr32x, preferred-register: '' }
  - { id: 253, class: fr32x, preferred-register: '' }
  - { id: 254, class: fr32x, preferred-register: '' }
  - { id: 255, class: fr32x, preferred-register: '' }
  - { id: 256, class: fr32x, preferred-register: '' }
  - { id: 257, class: fr32x, preferred-register: '' }
  - { id: 258, class: fr32x, preferred-register: '' }
  - { id: 259, class: fr32x, preferred-register: '' }
  - { id: 260, class: fr32x, preferred-register: '' }
  - { id: 261, class: fr32x, preferred-register: '' }
  - { id: 262, class: fr32x, preferred-register: '' }
  - { id: 263, class: fr32x, preferred-register: '' }
  - { id: 264, class: fr32x, preferred-register: '' }
  - { id: 265, class: fr32x, preferred-register: '' }
  - { id: 266, class: fr32x, preferred-register: '' }
  - { id: 267, class: gr64_with_sub_8bit, preferred-register: '' }
  - { id: 268, class: gr64_nosp, preferred-register: '' }
liveins:
  - { reg: '$rdi', virtual-reg: '%21' }
  - { reg: '$rsi', virtual-reg: '%22' }
  - { reg: '$rdx', virtual-reg: '%23' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    false
  hasCalls:        false
  stackProtector:  ''
  maxCallFrameSize: 4294967295
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
constants:       []
machineFunctionInfo: {}
body:             |
  ; CHECK-LABEL: name: foo
  ; CHECK: bb.0.entry:
  ; CHECK:   successors: %bb.1(0x80000000)
  ; CHECK:   liveins: $rdi, $rsi, $rdx
  ; CHECK:   [[COPY:%[0-9]+]]:gr64 = COPY $rdx
  ; CHECK:   [[COPY1:%[0-9]+]]:gr64 = COPY $rsi
  ; CHECK:   [[COPY2:%[0-9]+]]:gr64 = COPY $rdi
  ; CHECK:   undef %267.sub_32bit:gr64_with_sub_8bit = MOV32rm [[COPY2]], 1, $noreg, 0, $noreg :: (load 4 from %ir.gloadptridx0)
  ; CHECK:   undef %268.sub_32bit:gr64_nosp = MOV32rm [[COPY2]], 1, $noreg, 4, $noreg :: (load 4 from %ir.gloadptridx1)
  ; CHECK:   [[MOV32rm:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 8, $noreg :: (load 4 from %ir.gloadptridx2)
  ; CHECK:   [[MOV32rm1:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 12, $noreg :: (load 4 from %ir.gloadptridx3)
  ; CHECK:   [[MOV32rm2:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 16, $noreg :: (load 4 from %ir.gloadptridx4)
  ; CHECK:   [[MOV32rm3:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 20, $noreg :: (load 4 from %ir.gloadptridx5)
  ; CHECK:   [[MOV32rm4:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 24, $noreg :: (load 4 from %ir.gloadptridx6)
  ; CHECK:   [[MOV32rm5:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 28, $noreg :: (load 4 from %ir.gloadptridx7)
  ; CHECK:   [[MOV32rm6:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 32, $noreg :: (load 4 from %ir.gloadptridx8)
  ; CHECK:   [[MOV32rm7:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 36, $noreg :: (load 4 from %ir.gloadptridx9)
  ; CHECK:   [[MOV32rm8:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 40, $noreg :: (load 4 from %ir.gloadptridx10)
  ; CHECK:   [[MOV32rm9:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 44, $noreg :: (load 4 from %ir.gloadptridx11)
  ; CHECK:   [[MOV32rm10:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 48, $noreg :: (load 4 from %ir.gloadptridx12)
  ; CHECK:   [[MOV32rm11:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 52, $noreg :: (load 4 from %ir.gloadptridx13)
  ; CHECK:   [[MOV32rm12:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 56, $noreg :: (load 4 from %ir.gloadptridx14)
  ; CHECK:   [[MOV32rm13:%[0-9]+]]:gr32 = MOV32rm [[COPY2]], 1, $noreg, 60, $noreg :: (load 4 from %ir.gloadptridx15)
  ; CHECK:   [[MOV64rm:%[0-9]+]]:gr64 = MOV64rm [[COPY]], 1, $noreg, 0, $noreg :: (load 8 from %ir.fptridx0)
  ; CHECK:   [[MOV64rm1:%[0-9]+]]:gr64 = MOV64rm [[COPY]], 1, $noreg, 8, $noreg :: (load 8 from %ir.fptridx1)
  ; CHECK:   [[MOV64rm2:%[0-9]+]]:gr64 = MOV64rm [[COPY]], 1, $noreg, 16, $noreg :: (load 8 from %ir.fptridx2)
  ; CHECK: bb.1.body:
  ; CHECK:   successors: %bb.2(0x80000000)
  ; CHECK:   [[MOVSX64rr32_:%[0-9]+]]:gr64 = MOVSX64rr32 %267.sub_32bit
  ; CHECK:   [[MOVSX64rr32_1:%[0-9]+]]:gr64 = MOVSX64rr32 %268.sub_32bit
  ; CHECK:   [[VMOVSSZrm_alt:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 0, $noreg :: (load 4 from %ir.aptridx0)
  ; CHECK:   [[VMOVSSZrm_alt1:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 0, $noreg :: (load 4 from %ir.bptridx0)
  ; CHECK:   %42:fr32x = nofpexcept VADDSSZrr [[VMOVSSZrm_alt]], [[VMOVSSZrm_alt]], implicit $mxcsr
  ; CHECK:   %43:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt]], [[VMOVSSZrm_alt]], implicit $mxcsr
  ; CHECK:   %44:fr32x = nofpexcept VADDSSZrr [[VMOVSSZrm_alt1]], [[VMOVSSZrm_alt1]], implicit $mxcsr
  ; CHECK:   %45:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt1]], [[VMOVSSZrm_alt1]], implicit $mxcsr
  ; CHECK:   %46:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt]], [[VMOVSSZrm_alt1]], implicit $mxcsr
  ; CHECK:   [[IMUL64rri32_:%[0-9]+]]:gr64_nosp = IMUL64rri32 [[MOVSX64rr32_]], 3211264, implicit-def dead $eflags
  ; CHECK:   [[ADD64rr:%[0-9]+]]:gr64_nosp = ADD64rr [[ADD64rr]], [[MOV64rm2]], implicit-def dead $eflags
  ; CHECK:   [[SHL64ri:%[0-9]+]]:gr64 = SHL64ri [[SHL64ri]], 12, implicit-def dead $eflags
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 0, $noreg, %46 :: (store 4 into %ir.fstoreptridx0)
  ; CHECK:   [[VMOVSSZrm_alt2:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 4, $noreg :: (load 4 from %ir.aptridx1)
  ; CHECK:   [[VMOVSSZrm_alt3:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 4, $noreg :: (load 4 from %ir.bptridx1)
  ; CHECK:   %52:fr32x = nofpexcept VADDSSZrr %42, [[VMOVSSZrm_alt2]], implicit $mxcsr
  ; CHECK:   %53:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt2]], %43, implicit $mxcsr
  ; CHECK:   %54:fr32x = nofpexcept VADDSSZrr %44, [[VMOVSSZrm_alt3]], implicit $mxcsr
  ; CHECK:   %56:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt2]], [[VMOVSSZrm_alt3]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 4, $noreg, %56 :: (store 4 into %ir.fstoreptridx1)
  ; CHECK:   %55:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt3]], %45, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt4:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 8, $noreg :: (load 4 from %ir.aptridx2)
  ; CHECK:   [[VMOVSSZrm_alt5:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 8, $noreg :: (load 4 from %ir.bptridx2)
  ; CHECK:   %59:fr32x = nofpexcept VADDSSZrr %52, [[VMOVSSZrm_alt4]], implicit $mxcsr
  ; CHECK:   %60:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt4]], %53, implicit $mxcsr
  ; CHECK:   %61:fr32x = nofpexcept VADDSSZrr %54, [[VMOVSSZrm_alt5]], implicit $mxcsr
  ; CHECK:   %62:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt5]], %55, implicit $mxcsr
  ; CHECK:   %63:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt4]], [[VMOVSSZrm_alt5]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 8, $noreg, %63 :: (store 4 into %ir.fstoreptridx2)
  ; CHECK:   [[VMOVSSZrm_alt6:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 12, $noreg :: (load 4 from %ir.aptridx3)
  ; CHECK:   [[VMOVSSZrm_alt7:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 12, $noreg :: (load 4 from %ir.bptridx3)
  ; CHECK:   %66:fr32x = nofpexcept VADDSSZrr %59, [[VMOVSSZrm_alt6]], implicit $mxcsr
  ; CHECK:   %67:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt6]], %60, implicit $mxcsr
  ; CHECK:   %68:fr32x = nofpexcept VADDSSZrr %61, [[VMOVSSZrm_alt7]], implicit $mxcsr
  ; CHECK:   %70:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt6]], [[VMOVSSZrm_alt7]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 12, $noreg, %70 :: (store 4 into %ir.fstoreptridx3)
  ; CHECK:   [[VMOVSSZrm_alt8:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 16, $noreg :: (load 4 from %ir.aptridx4)
  ; CHECK:   %69:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt7]], %62, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt9:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 16, $noreg :: (load 4 from %ir.bptridx4)
  ; CHECK:   %73:fr32x = nofpexcept VADDSSZrr %66, [[VMOVSSZrm_alt8]], implicit $mxcsr
  ; CHECK:   %74:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt8]], %67, implicit $mxcsr
  ; CHECK:   %75:fr32x = nofpexcept VADDSSZrr %68, [[VMOVSSZrm_alt9]], implicit $mxcsr
  ; CHECK:   %76:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt9]], %69, implicit $mxcsr
  ; CHECK:   %77:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt8]], [[VMOVSSZrm_alt9]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 16, $noreg, %77 :: (store 4 into %ir.fstoreptridx4)
  ; CHECK:   [[VMOVSSZrm_alt10:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 20, $noreg :: (load 4 from %ir.aptridx5)
  ; CHECK:   [[VMOVSSZrm_alt11:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 20, $noreg :: (load 4 from %ir.bptridx5)
  ; CHECK:   %80:fr32x = nofpexcept VADDSSZrr %73, [[VMOVSSZrm_alt10]], implicit $mxcsr
  ; CHECK:   %81:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt10]], %74, implicit $mxcsr
  ; CHECK:   %82:fr32x = nofpexcept VADDSSZrr %75, [[VMOVSSZrm_alt11]], implicit $mxcsr
  ; CHECK:   %84:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt10]], [[VMOVSSZrm_alt11]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 20, $noreg, %84 :: (store 4 into %ir.fstoreptridx5)
  ; CHECK:   [[VMOVSSZrm_alt12:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 24, $noreg :: (load 4 from %ir.aptridx6)
  ; CHECK:   [[VMOVSSZrm_alt13:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 24, $noreg :: (load 4 from %ir.bptridx6)
  ; CHECK:   %83:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt11]], %76, implicit $mxcsr
  ; CHECK:   %87:fr32x = nofpexcept VADDSSZrr %80, [[VMOVSSZrm_alt12]], implicit $mxcsr
  ; CHECK:   %88:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt12]], %81, implicit $mxcsr
  ; CHECK:   %89:fr32x = nofpexcept VADDSSZrr %82, [[VMOVSSZrm_alt13]], implicit $mxcsr
  ; CHECK:   %90:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt13]], %83, implicit $mxcsr
  ; CHECK:   %91:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt12]], [[VMOVSSZrm_alt13]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 24, $noreg, %91 :: (store 4 into %ir.fstoreptridx6)
  ; CHECK:   [[VMOVSSZrm_alt14:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 28, $noreg :: (load 4 from %ir.aptridx7)
  ; CHECK:   [[VMOVSSZrm_alt15:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 28, $noreg :: (load 4 from %ir.bptridx7)
  ; CHECK:   %94:fr32x = nofpexcept VADDSSZrr %87, [[VMOVSSZrm_alt14]], implicit $mxcsr
  ; CHECK:   %95:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt14]], %88, implicit $mxcsr
  ; CHECK:   %96:fr32x = nofpexcept VADDSSZrr %89, [[VMOVSSZrm_alt15]], implicit $mxcsr
  ; CHECK:   %98:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt14]], [[VMOVSSZrm_alt15]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 28, $noreg, %98 :: (store 4 into %ir.fstoreptridx7)
  ; CHECK:   %97:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt15]], %90, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt16:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 32, $noreg :: (load 4 from %ir.aptridx8)
  ; CHECK:   [[VMOVSSZrm_alt17:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 32, $noreg :: (load 4 from %ir.bptridx8)
  ; CHECK:   %101:fr32x = nofpexcept VADDSSZrr %94, [[VMOVSSZrm_alt16]], implicit $mxcsr
  ; CHECK:   %102:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt16]], %95, implicit $mxcsr
  ; CHECK:   %103:fr32x = nofpexcept VADDSSZrr %96, [[VMOVSSZrm_alt17]], implicit $mxcsr
  ; CHECK:   %104:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt17]], %97, implicit $mxcsr
  ; CHECK:   %105:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt16]], [[VMOVSSZrm_alt17]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 32, $noreg, %105 :: (store 4 into %ir.fstoreptridx8)
  ; CHECK:   [[VMOVSSZrm_alt18:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 36, $noreg :: (load 4 from %ir.aptridx9)
  ; CHECK:   [[VMOVSSZrm_alt19:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 36, $noreg :: (load 4 from %ir.bptridx9)
  ; CHECK:   %108:fr32x = nofpexcept VADDSSZrr %101, [[VMOVSSZrm_alt18]], implicit $mxcsr
  ; CHECK:   %109:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt18]], %102, implicit $mxcsr
  ; CHECK:   %110:fr32x = nofpexcept VADDSSZrr %103, [[VMOVSSZrm_alt19]], implicit $mxcsr
  ; CHECK:   %112:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt18]], [[VMOVSSZrm_alt19]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 36, $noreg, %112 :: (store 4 into %ir.fstoreptridx9)
  ; CHECK:   [[VMOVSSZrm_alt20:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 40, $noreg :: (load 4 from %ir.aptridx10)
  ; CHECK:   %111:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt19]], %104, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt21:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 40, $noreg :: (load 4 from %ir.bptridx10)
  ; CHECK:   %115:fr32x = nofpexcept VADDSSZrr %108, [[VMOVSSZrm_alt20]], implicit $mxcsr
  ; CHECK:   %116:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt20]], %109, implicit $mxcsr
  ; CHECK:   %117:fr32x = nofpexcept VADDSSZrr %110, [[VMOVSSZrm_alt21]], implicit $mxcsr
  ; CHECK:   %118:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt21]], %111, implicit $mxcsr
  ; CHECK:   %119:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt20]], [[VMOVSSZrm_alt21]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 40, $noreg, %119 :: (store 4 into %ir.fstoreptridx10)
  ; CHECK:   [[VMOVSSZrm_alt22:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 44, $noreg :: (load 4 from %ir.aptridx11)
  ; CHECK:   [[VMOVSSZrm_alt23:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 44, $noreg :: (load 4 from %ir.bptridx11)
  ; CHECK:   %122:fr32x = nofpexcept VADDSSZrr %115, [[VMOVSSZrm_alt22]], implicit $mxcsr
  ; CHECK:   %123:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt22]], %116, implicit $mxcsr
  ; CHECK:   %124:fr32x = nofpexcept VADDSSZrr %117, [[VMOVSSZrm_alt23]], implicit $mxcsr
  ; CHECK:   %126:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt22]], [[VMOVSSZrm_alt23]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 44, $noreg, %126 :: (store 4 into %ir.fstoreptridx11)
  ; CHECK:   [[VMOVSSZrm_alt24:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 48, $noreg :: (load 4 from %ir.aptridx12)
  ; CHECK:   [[VMOVSSZrm_alt25:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 48, $noreg :: (load 4 from %ir.bptridx12)
  ; CHECK:   %125:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt23]], %118, implicit $mxcsr
  ; CHECK:   %129:fr32x = nofpexcept VADDSSZrr %122, [[VMOVSSZrm_alt24]], implicit $mxcsr
  ; CHECK:   %130:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt24]], %123, implicit $mxcsr
  ; CHECK:   %131:fr32x = nofpexcept VADDSSZrr %124, [[VMOVSSZrm_alt25]], implicit $mxcsr
  ; CHECK:   %132:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt25]], %125, implicit $mxcsr
  ; CHECK:   %133:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt24]], [[VMOVSSZrm_alt25]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 48, $noreg, %133 :: (store 4 into %ir.fstoreptridx12)
  ; CHECK:   [[VMOVSSZrm_alt26:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 52, $noreg :: (load 4 from %ir.aptridx13)
  ; CHECK:   [[VMOVSSZrm_alt27:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 52, $noreg :: (load 4 from %ir.bptridx13)
  ; CHECK:   %136:fr32x = nofpexcept VADDSSZrr %129, [[VMOVSSZrm_alt26]], implicit $mxcsr
  ; CHECK:   %140:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt26]], [[VMOVSSZrm_alt27]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 52, $noreg, %140 :: (store 4 into %ir.fstoreptridx13)
  ; CHECK:   [[VMOVSSZrm_alt28:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 56, $noreg :: (load 4 from %ir.aptridx14)
  ; CHECK:   %137:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt26]], %130, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt29:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 56, $noreg :: (load 4 from %ir.bptridx14)
  ; CHECK:   %147:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt28]], [[VMOVSSZrm_alt29]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 56, $noreg, %147 :: (store 4 into %ir.fstoreptridx14)
  ; CHECK:   %138:fr32x = nofpexcept VADDSSZrr %131, [[VMOVSSZrm_alt27]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt30:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 60, $noreg :: (load 4 from %ir.aptridx15)
  ; CHECK:   [[VMOVSSZrm_alt31:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 60, $noreg :: (load 4 from %ir.bptridx15)
  ; CHECK:   %154:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt30]], [[VMOVSSZrm_alt31]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 60, $noreg, %154 :: (store 4 into %ir.fstoreptridx15)
  ; CHECK:   [[VMOVSSZrm_alt32:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 64, $noreg :: (load 4 from %ir.aptridx16)
  ; CHECK:   [[VMOVSSZrm_alt33:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 64, $noreg :: (load 4 from %ir.bptridx16)
  ; CHECK:   %139:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt27]], %132, implicit $mxcsr
  ; CHECK:   %161:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt32]], [[VMOVSSZrm_alt33]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 64, $noreg, %161 :: (store 4 into %ir.fstoreptridx16)
  ; CHECK:   [[VMOVSSZrm_alt34:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 68, $noreg :: (load 4 from %ir.aptridx17)
  ; CHECK:   %143:fr32x = nofpexcept VADDSSZrr %136, [[VMOVSSZrm_alt28]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt35:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 68, $noreg :: (load 4 from %ir.bptridx17)
  ; CHECK:   %168:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt34]], [[VMOVSSZrm_alt35]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 68, $noreg, %168 :: (store 4 into %ir.fstoreptridx17)
  ; CHECK:   %144:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt28]], %137, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt36:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 72, $noreg :: (load 4 from %ir.aptridx18)
  ; CHECK:   [[VMOVSSZrm_alt37:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 72, $noreg :: (load 4 from %ir.bptridx18)
  ; CHECK:   %175:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt36]], [[VMOVSSZrm_alt37]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 72, $noreg, %175 :: (store 4 into %ir.fstoreptridx18)
  ; CHECK:   [[VMOVSSZrm_alt38:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 76, $noreg :: (load 4 from %ir.aptridx19)
  ; CHECK:   [[VMOVSSZrm_alt39:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 76, $noreg :: (load 4 from %ir.bptridx19)
  ; CHECK:   %145:fr32x = nofpexcept VADDSSZrr %138, [[VMOVSSZrm_alt29]], implicit $mxcsr
  ; CHECK:   %182:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt38]], [[VMOVSSZrm_alt39]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 76, $noreg, %182 :: (store 4 into %ir.fstoreptridx19)
  ; CHECK:   [[VMOVSSZrm_alt40:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 80, $noreg :: (load 4 from %ir.aptridx20)
  ; CHECK:   %146:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt29]], %139, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt41:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 80, $noreg :: (load 4 from %ir.bptridx20)
  ; CHECK:   %189:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt40]], [[VMOVSSZrm_alt41]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 80, $noreg, %189 :: (store 4 into %ir.fstoreptridx20)
  ; CHECK:   %150:fr32x = nofpexcept VADDSSZrr %143, [[VMOVSSZrm_alt30]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt42:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 84, $noreg :: (load 4 from %ir.aptridx21)
  ; CHECK:   [[VMOVSSZrm_alt43:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 84, $noreg :: (load 4 from %ir.bptridx21)
  ; CHECK:   %196:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt42]], [[VMOVSSZrm_alt43]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 84, $noreg, %196 :: (store 4 into %ir.fstoreptridx21)
  ; CHECK:   [[VMOVSSZrm_alt44:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 88, $noreg :: (load 4 from %ir.aptridx22)
  ; CHECK:   [[VMOVSSZrm_alt45:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 88, $noreg :: (load 4 from %ir.bptridx22)
  ; CHECK:   %151:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt30]], %144, implicit $mxcsr
  ; CHECK:   %203:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt44]], [[VMOVSSZrm_alt45]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 88, $noreg, %203 :: (store 4 into %ir.fstoreptridx22)
  ; CHECK:   [[VMOVSSZrm_alt46:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 92, $noreg :: (load 4 from %ir.aptridx23)
  ; CHECK:   %152:fr32x = nofpexcept VADDSSZrr %145, [[VMOVSSZrm_alt31]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt47:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 92, $noreg :: (load 4 from %ir.bptridx23)
  ; CHECK:   %210:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt46]], [[VMOVSSZrm_alt47]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 92, $noreg, %210 :: (store 4 into %ir.fstoreptridx23)
  ; CHECK:   %153:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt31]], %146, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt48:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 96, $noreg :: (load 4 from %ir.aptridx24)
  ; CHECK:   [[VMOVSSZrm_alt49:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 96, $noreg :: (load 4 from %ir.bptridx24)
  ; CHECK:   %217:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt48]], [[VMOVSSZrm_alt49]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 96, $noreg, %217 :: (store 4 into %ir.fstoreptridx24)
  ; CHECK:   [[VMOVSSZrm_alt50:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 100, $noreg :: (load 4 from %ir.aptridx25)
  ; CHECK:   [[VMOVSSZrm_alt51:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 100, $noreg :: (load 4 from %ir.bptridx25)
  ; CHECK:   %157:fr32x = nofpexcept VADDSSZrr %150, [[VMOVSSZrm_alt32]], implicit $mxcsr
  ; CHECK:   %224:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt50]], [[VMOVSSZrm_alt51]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 100, $noreg, %224 :: (store 4 into %ir.fstoreptridx25)
  ; CHECK:   [[VMOVSSZrm_alt52:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 104, $noreg :: (load 4 from %ir.aptridx26)
  ; CHECK:   %158:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt32]], %151, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt53:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 104, $noreg :: (load 4 from %ir.bptridx26)
  ; CHECK:   %231:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt52]], [[VMOVSSZrm_alt53]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 104, $noreg, %231 :: (store 4 into %ir.fstoreptridx26)
  ; CHECK:   %159:fr32x = nofpexcept VADDSSZrr %152, [[VMOVSSZrm_alt33]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt54:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 108, $noreg :: (load 4 from %ir.aptridx27)
  ; CHECK:   [[VMOVSSZrm_alt55:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 108, $noreg :: (load 4 from %ir.bptridx27)
  ; CHECK:   %238:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt54]], [[VMOVSSZrm_alt55]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 108, $noreg, %238 :: (store 4 into %ir.fstoreptridx27)
  ; CHECK:   [[VMOVSSZrm_alt56:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 112, $noreg :: (load 4 from %ir.aptridx28)
  ; CHECK:   [[VMOVSSZrm_alt57:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 112, $noreg :: (load 4 from %ir.bptridx28)
  ; CHECK:   %160:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt33]], %153, implicit $mxcsr
  ; CHECK:   %245:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt56]], [[VMOVSSZrm_alt57]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 112, $noreg, %245 :: (store 4 into %ir.fstoreptridx28)
  ; CHECK:   [[VMOVSSZrm_alt58:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 116, $noreg :: (load 4 from %ir.aptridx29)
  ; CHECK:   %164:fr32x = nofpexcept VADDSSZrr %157, [[VMOVSSZrm_alt34]], implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt59:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 116, $noreg :: (load 4 from %ir.bptridx29)
  ; CHECK:   %252:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt58]], [[VMOVSSZrm_alt59]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 116, $noreg, %252 :: (store 4 into %ir.fstoreptridx29)
  ; CHECK:   %165:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt34]], %158, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt60:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 120, $noreg :: (load 4 from %ir.aptridx30)
  ; CHECK:   [[VMOVSSZrm_alt61:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 120, $noreg :: (load 4 from %ir.bptridx30)
  ; CHECK:   %259:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt60]], [[VMOVSSZrm_alt61]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 120, $noreg, %259 :: (store 4 into %ir.fstoreptridx30)
  ; CHECK:   [[VMOVSSZrm_alt62:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm]], 1, $noreg, 124, $noreg :: (load 4 from %ir.aptridx31)
  ; CHECK:   %166:fr32x = nofpexcept VADDSSZrr %159, [[VMOVSSZrm_alt35]], implicit $mxcsr
  ; CHECK:   %167:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt35]], %160, implicit $mxcsr
  ; CHECK:   [[VMOVSSZrm_alt63:%[0-9]+]]:fr32x = VMOVSSZrm_alt [[MOV64rm1]], 1, $noreg, 124, $noreg :: (load 4 from %ir.bptridx31)
  ; CHECK:   %266:fr32x = nofpexcept VSUBSSZrr [[VMOVSSZrm_alt62]], [[VMOVSSZrm_alt63]], implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[SHL64ri]], 1, [[ADD64rr]], 124, $noreg, %266 :: (store 4 into %ir.fstoreptridx31)
  ; CHECK:   [[LEA64_32r:%[0-9]+]]:gr32 = LEA64_32r %267, 1, %268, 0, $noreg
  ; CHECK:   [[ADD32rr:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr]], [[MOV32rm]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm1]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm2]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm3]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm4]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm5]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm6]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm7]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm8]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm9]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm10]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm11]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm12]], implicit-def dead $eflags
  ; CHECK:   [[ADD32rr1:%[0-9]+]]:gr32 = ADD32rr [[ADD32rr1]], [[MOV32rm13]], implicit-def dead $eflags
  ; CHECK:   %171:fr32x = nofpexcept VADDSSZrr %164, [[VMOVSSZrm_alt36]], implicit $mxcsr
  ; CHECK:   %172:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt36]], %165, implicit $mxcsr
  ; CHECK:   %173:fr32x = nofpexcept VADDSSZrr %166, [[VMOVSSZrm_alt37]], implicit $mxcsr
  ; CHECK:   %174:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt37]], %167, implicit $mxcsr
  ; CHECK:   %178:fr32x = nofpexcept VADDSSZrr %171, [[VMOVSSZrm_alt38]], implicit $mxcsr
  ; CHECK:   %179:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt38]], %172, implicit $mxcsr
  ; CHECK:   %180:fr32x = nofpexcept VADDSSZrr %173, [[VMOVSSZrm_alt39]], implicit $mxcsr
  ; CHECK:   %181:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt39]], %174, implicit $mxcsr
  ; CHECK:   %185:fr32x = nofpexcept VADDSSZrr %178, [[VMOVSSZrm_alt40]], implicit $mxcsr
  ; CHECK:   %186:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt40]], %179, implicit $mxcsr
  ; CHECK:   %187:fr32x = nofpexcept VADDSSZrr %180, [[VMOVSSZrm_alt41]], implicit $mxcsr
  ; CHECK:   %188:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt41]], %181, implicit $mxcsr
  ; CHECK:   %192:fr32x = nofpexcept VADDSSZrr %185, [[VMOVSSZrm_alt42]], implicit $mxcsr
  ; CHECK:   %193:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt42]], %186, implicit $mxcsr
  ; CHECK:   %194:fr32x = nofpexcept VADDSSZrr %187, [[VMOVSSZrm_alt43]], implicit $mxcsr
  ; CHECK:   %195:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt43]], %188, implicit $mxcsr
  ; CHECK:   %199:fr32x = nofpexcept VADDSSZrr %192, [[VMOVSSZrm_alt44]], implicit $mxcsr
  ; CHECK:   %200:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt44]], %193, implicit $mxcsr
  ; CHECK:   %201:fr32x = nofpexcept VADDSSZrr %194, [[VMOVSSZrm_alt45]], implicit $mxcsr
  ; CHECK:   %202:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt45]], %195, implicit $mxcsr
  ; CHECK:   %206:fr32x = nofpexcept VADDSSZrr %199, [[VMOVSSZrm_alt46]], implicit $mxcsr
  ; CHECK:   %207:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt46]], %200, implicit $mxcsr
  ; CHECK:   %208:fr32x = nofpexcept VADDSSZrr %201, [[VMOVSSZrm_alt47]], implicit $mxcsr
  ; CHECK:   %209:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt47]], %202, implicit $mxcsr
  ; CHECK:   %213:fr32x = nofpexcept VADDSSZrr %206, [[VMOVSSZrm_alt48]], implicit $mxcsr
  ; CHECK:   %214:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt48]], %207, implicit $mxcsr
  ; CHECK:   %215:fr32x = nofpexcept VADDSSZrr %208, [[VMOVSSZrm_alt49]], implicit $mxcsr
  ; CHECK:   %216:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt49]], %209, implicit $mxcsr
  ; CHECK:   %220:fr32x = nofpexcept VADDSSZrr %213, [[VMOVSSZrm_alt50]], implicit $mxcsr
  ; CHECK:   %221:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt50]], %214, implicit $mxcsr
  ; CHECK:   %222:fr32x = nofpexcept VADDSSZrr %215, [[VMOVSSZrm_alt51]], implicit $mxcsr
  ; CHECK:   %223:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt51]], %216, implicit $mxcsr
  ; CHECK:   %227:fr32x = nofpexcept VADDSSZrr %220, [[VMOVSSZrm_alt52]], implicit $mxcsr
  ; CHECK:   %228:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt52]], %221, implicit $mxcsr
  ; CHECK:   %229:fr32x = nofpexcept VADDSSZrr %222, [[VMOVSSZrm_alt53]], implicit $mxcsr
  ; CHECK:   %230:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt53]], %223, implicit $mxcsr
  ; CHECK:   %234:fr32x = nofpexcept VADDSSZrr %227, [[VMOVSSZrm_alt54]], implicit $mxcsr
  ; CHECK:   %235:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt54]], %228, implicit $mxcsr
  ; CHECK:   %236:fr32x = nofpexcept VADDSSZrr %229, [[VMOVSSZrm_alt55]], implicit $mxcsr
  ; CHECK:   %237:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt55]], %230, implicit $mxcsr
  ; CHECK:   %241:fr32x = nofpexcept VADDSSZrr %234, [[VMOVSSZrm_alt56]], implicit $mxcsr
  ; CHECK:   %242:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt56]], %235, implicit $mxcsr
  ; CHECK:   %243:fr32x = nofpexcept VADDSSZrr %236, [[VMOVSSZrm_alt57]], implicit $mxcsr
  ; CHECK:   %244:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt57]], %237, implicit $mxcsr
  ; CHECK:   %248:fr32x = nofpexcept VADDSSZrr %241, [[VMOVSSZrm_alt58]], implicit $mxcsr
  ; CHECK:   %249:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt58]], %242, implicit $mxcsr
  ; CHECK:   %250:fr32x = nofpexcept VADDSSZrr %243, [[VMOVSSZrm_alt59]], implicit $mxcsr
  ; CHECK:   %251:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt59]], %244, implicit $mxcsr
  ; CHECK:   %255:fr32x = nofpexcept VADDSSZrr %248, [[VMOVSSZrm_alt60]], implicit $mxcsr
  ; CHECK:   %256:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt60]], %249, implicit $mxcsr
  ; CHECK:   %257:fr32x = nofpexcept VADDSSZrr %250, [[VMOVSSZrm_alt61]], implicit $mxcsr
  ; CHECK:   %258:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt61]], %251, implicit $mxcsr
  ; CHECK:   %262:fr32x = nofpexcept VADDSSZrr %255, [[VMOVSSZrm_alt62]], implicit $mxcsr
  ; CHECK:   %263:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt62]], %256, implicit $mxcsr
  ; CHECK:   %264:fr32x = nofpexcept VADDSSZrr %257, [[VMOVSSZrm_alt63]], implicit $mxcsr
  ; CHECK:   %265:fr32x = nofpexcept VMULSSZrr [[VMOVSSZrm_alt63]], %258, implicit $mxcsr
  ; CHECK:   VMOVSSZmr [[MOV64rm2]], 1, $noreg, 0, $noreg, %262 :: (store 4 into %ir.fstoreptr2idx0)
  ; CHECK:   VMOVSSZmr [[MOV64rm2]], 1, $noreg, 4, $noreg, %263 :: (store 4 into %ir.fstoreptr2idx1)
  ; CHECK:   VMOVSSZmr [[MOV64rm2]], 1, $noreg, 8, $noreg, %264 :: (store 4 into %ir.fstoreptr2idx2)
  ; CHECK:   VMOVSSZmr [[MOV64rm2]], 1, $noreg, 12, $noreg, %265 :: (store 4 into %ir.fstoreptr2idx3)
  ; CHECK: bb.2.exit:
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 0, $noreg, %267.sub_32bit :: (store 4 into %ir.gstoreptridx0)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 4, $noreg, %268.sub_32bit :: (store 4 into %ir.gstoreptridx1)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 8, $noreg, [[MOV32rm]] :: (store 4 into %ir.gstoreptridx2)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 12, $noreg, [[MOV32rm1]] :: (store 4 into %ir.gstoreptridx3)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 16, $noreg, [[MOV32rm2]] :: (store 4 into %ir.gstoreptridx4)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 20, $noreg, [[MOV32rm3]] :: (store 4 into %ir.gstoreptridx5)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 24, $noreg, [[MOV32rm4]] :: (store 4 into %ir.gstoreptridx6)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 28, $noreg, [[MOV32rm5]] :: (store 4 into %ir.gstoreptridx7)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 32, $noreg, [[MOV32rm6]] :: (store 4 into %ir.gstoreptridx8)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 36, $noreg, [[MOV32rm7]] :: (store 4 into %ir.gstoreptridx9)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 40, $noreg, [[MOV32rm8]] :: (store 4 into %ir.gstoreptridx10)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 44, $noreg, [[MOV32rm9]] :: (store 4 into %ir.gstoreptridx11)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 48, $noreg, [[MOV32rm10]] :: (store 4 into %ir.gstoreptridx12)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 52, $noreg, [[MOV32rm11]] :: (store 4 into %ir.gstoreptridx13)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 56, $noreg, [[MOV32rm12]] :: (store 4 into %ir.gstoreptridx14)
  ; CHECK:   MOV32mr [[COPY1]], 1, $noreg, 60, $noreg, [[ADD32rr1]] :: (store 4 into %ir.gstoreptridx15)
  ; CHECK:   RET 0
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $rdi, $rsi, $rdx

    %23:gr64 = COPY $rdx
    %22:gr64 = COPY $rsi
    %21:gr64 = COPY $rdi
    undef %267.sub_32bit:gr64_with_sub_8bit = MOV32rm %21, 1, $noreg, 0, $noreg :: (load 4 from %ir.gloadptridx0)
    undef %268.sub_32bit:gr64_nosp = MOV32rm %21, 1, $noreg, 4, $noreg :: (load 4 from %ir.gloadptridx1)
    %2:gr32 = MOV32rm %21, 1, $noreg, 8, $noreg :: (load 4 from %ir.gloadptridx2)
    %3:gr32 = MOV32rm %21, 1, $noreg, 12, $noreg :: (load 4 from %ir.gloadptridx3)
    %4:gr32 = MOV32rm %21, 1, $noreg, 16, $noreg :: (load 4 from %ir.gloadptridx4)
    %5:gr32 = MOV32rm %21, 1, $noreg, 20, $noreg :: (load 4 from %ir.gloadptridx5)
    %6:gr32 = MOV32rm %21, 1, $noreg, 24, $noreg :: (load 4 from %ir.gloadptridx6)
    %7:gr32 = MOV32rm %21, 1, $noreg, 28, $noreg :: (load 4 from %ir.gloadptridx7)
    %8:gr32 = MOV32rm %21, 1, $noreg, 32, $noreg :: (load 4 from %ir.gloadptridx8)
    %9:gr32 = MOV32rm %21, 1, $noreg, 36, $noreg :: (load 4 from %ir.gloadptridx9)
    %10:gr32 = MOV32rm %21, 1, $noreg, 40, $noreg :: (load 4 from %ir.gloadptridx10)
    %11:gr32 = MOV32rm %21, 1, $noreg, 44, $noreg :: (load 4 from %ir.gloadptridx11)
    %12:gr32 = MOV32rm %21, 1, $noreg, 48, $noreg :: (load 4 from %ir.gloadptridx12)
    %13:gr32 = MOV32rm %21, 1, $noreg, 52, $noreg :: (load 4 from %ir.gloadptridx13)
    %14:gr32 = MOV32rm %21, 1, $noreg, 56, $noreg :: (load 4 from %ir.gloadptridx14)
    %15:gr32 = MOV32rm %21, 1, $noreg, 60, $noreg :: (load 4 from %ir.gloadptridx15)
    %16:gr64 = MOV64rm %23, 1, $noreg, 0, $noreg :: (load 8 from %ir.fptridx0)
    %17:gr64 = MOV64rm %23, 1, $noreg, 8, $noreg :: (load 8 from %ir.fptridx1)
    %19:gr64 = MOV64rm %23, 1, $noreg, 16, $noreg :: (load 8 from %ir.fptridx2)

  bb.1.body:
    successors: %bb.2(0x80000000)

    %26:gr32 = LEA64_32r %267, 1, %268, 0, $noreg
    %26:gr32 = ADD32rr %26, %2, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %3, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %4, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %5, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %6, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %7, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %8, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %9, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %10, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %11, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %12, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %13, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %14, implicit-def dead $eflags
    %26:gr32 = ADD32rr %26, %15, implicit-def dead $eflags
    %38:gr64 = MOVSX64rr32 %267.sub_32bit
    %49:gr64 = MOVSX64rr32 %268.sub_32bit
    %40:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 0, $noreg :: (load 4 from %ir.aptridx0)
    %41:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 0, $noreg :: (load 4 from %ir.bptridx0)
    %42:fr32x = nofpexcept VADDSSZrr %40, %40, implicit $mxcsr
    %43:fr32x = nofpexcept VMULSSZrr %40, %40, implicit $mxcsr
    %44:fr32x = nofpexcept VADDSSZrr %41, %41, implicit $mxcsr
    %45:fr32x = nofpexcept VMULSSZrr %41, %41, implicit $mxcsr
    %46:fr32x = nofpexcept VSUBSSZrr %40, %41, implicit $mxcsr
    %48:gr64_nosp = IMUL64rri32 %38, 3211264, implicit-def dead $eflags
    %48:gr64_nosp = ADD64rr %48, %19, implicit-def dead $eflags
    %49:gr64 = SHL64ri %49, 12, implicit-def dead $eflags
    VMOVSSZmr %49, 1, %48, 0, $noreg, %46 :: (store 4 into %ir.fstoreptridx0)
    %50:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 4, $noreg :: (load 4 from %ir.aptridx1)
    %51:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 4, $noreg :: (load 4 from %ir.bptridx1)
    %52:fr32x = nofpexcept VADDSSZrr %42, %50, implicit $mxcsr
    %53:fr32x = nofpexcept VMULSSZrr %50, %43, implicit $mxcsr
    %54:fr32x = nofpexcept VADDSSZrr %44, %51, implicit $mxcsr
    %55:fr32x = nofpexcept VMULSSZrr %51, %45, implicit $mxcsr
    %56:fr32x = nofpexcept VSUBSSZrr %50, %51, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 4, $noreg, %56 :: (store 4 into %ir.fstoreptridx1)
    %57:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 8, $noreg :: (load 4 from %ir.aptridx2)
    %58:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 8, $noreg :: (load 4 from %ir.bptridx2)
    %59:fr32x = nofpexcept VADDSSZrr %52, %57, implicit $mxcsr
    %60:fr32x = nofpexcept VMULSSZrr %57, %53, implicit $mxcsr
    %61:fr32x = nofpexcept VADDSSZrr %54, %58, implicit $mxcsr
    %62:fr32x = nofpexcept VMULSSZrr %58, %55, implicit $mxcsr
    %63:fr32x = nofpexcept VSUBSSZrr %57, %58, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 8, $noreg, %63 :: (store 4 into %ir.fstoreptridx2)
    %64:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 12, $noreg :: (load 4 from %ir.aptridx3)
    %65:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 12, $noreg :: (load 4 from %ir.bptridx3)
    %66:fr32x = nofpexcept VADDSSZrr %59, %64, implicit $mxcsr
    %67:fr32x = nofpexcept VMULSSZrr %64, %60, implicit $mxcsr
    %68:fr32x = nofpexcept VADDSSZrr %61, %65, implicit $mxcsr
    %69:fr32x = nofpexcept VMULSSZrr %65, %62, implicit $mxcsr
    %70:fr32x = nofpexcept VSUBSSZrr %64, %65, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 12, $noreg, %70 :: (store 4 into %ir.fstoreptridx3)
    %71:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 16, $noreg :: (load 4 from %ir.aptridx4)
    %72:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 16, $noreg :: (load 4 from %ir.bptridx4)
    %73:fr32x = nofpexcept VADDSSZrr %66, %71, implicit $mxcsr
    %74:fr32x = nofpexcept VMULSSZrr %71, %67, implicit $mxcsr
    %75:fr32x = nofpexcept VADDSSZrr %68, %72, implicit $mxcsr
    %76:fr32x = nofpexcept VMULSSZrr %72, %69, implicit $mxcsr
    %77:fr32x = nofpexcept VSUBSSZrr %71, %72, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 16, $noreg, %77 :: (store 4 into %ir.fstoreptridx4)
    %78:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 20, $noreg :: (load 4 from %ir.aptridx5)
    %79:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 20, $noreg :: (load 4 from %ir.bptridx5)
    %80:fr32x = nofpexcept VADDSSZrr %73, %78, implicit $mxcsr
    %81:fr32x = nofpexcept VMULSSZrr %78, %74, implicit $mxcsr
    %82:fr32x = nofpexcept VADDSSZrr %75, %79, implicit $mxcsr
    %83:fr32x = nofpexcept VMULSSZrr %79, %76, implicit $mxcsr
    %84:fr32x = nofpexcept VSUBSSZrr %78, %79, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 20, $noreg, %84 :: (store 4 into %ir.fstoreptridx5)
    %85:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 24, $noreg :: (load 4 from %ir.aptridx6)
    %86:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 24, $noreg :: (load 4 from %ir.bptridx6)
    %87:fr32x = nofpexcept VADDSSZrr %80, %85, implicit $mxcsr
    %88:fr32x = nofpexcept VMULSSZrr %85, %81, implicit $mxcsr
    %89:fr32x = nofpexcept VADDSSZrr %82, %86, implicit $mxcsr
    %90:fr32x = nofpexcept VMULSSZrr %86, %83, implicit $mxcsr
    %91:fr32x = nofpexcept VSUBSSZrr %85, %86, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 24, $noreg, %91 :: (store 4 into %ir.fstoreptridx6)
    %92:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 28, $noreg :: (load 4 from %ir.aptridx7)
    %93:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 28, $noreg :: (load 4 from %ir.bptridx7)
    %94:fr32x = nofpexcept VADDSSZrr %87, %92, implicit $mxcsr
    %95:fr32x = nofpexcept VMULSSZrr %92, %88, implicit $mxcsr
    %96:fr32x = nofpexcept VADDSSZrr %89, %93, implicit $mxcsr
    %97:fr32x = nofpexcept VMULSSZrr %93, %90, implicit $mxcsr
    %98:fr32x = nofpexcept VSUBSSZrr %92, %93, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 28, $noreg, %98 :: (store 4 into %ir.fstoreptridx7)
    %99:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 32, $noreg :: (load 4 from %ir.aptridx8)
    %100:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 32, $noreg :: (load 4 from %ir.bptridx8)
    %101:fr32x = nofpexcept VADDSSZrr %94, %99, implicit $mxcsr
    %102:fr32x = nofpexcept VMULSSZrr %99, %95, implicit $mxcsr
    %103:fr32x = nofpexcept VADDSSZrr %96, %100, implicit $mxcsr
    %104:fr32x = nofpexcept VMULSSZrr %100, %97, implicit $mxcsr
    %105:fr32x = nofpexcept VSUBSSZrr %99, %100, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 32, $noreg, %105 :: (store 4 into %ir.fstoreptridx8)
    %106:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 36, $noreg :: (load 4 from %ir.aptridx9)
    %107:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 36, $noreg :: (load 4 from %ir.bptridx9)
    %108:fr32x = nofpexcept VADDSSZrr %101, %106, implicit $mxcsr
    %109:fr32x = nofpexcept VMULSSZrr %106, %102, implicit $mxcsr
    %110:fr32x = nofpexcept VADDSSZrr %103, %107, implicit $mxcsr
    %111:fr32x = nofpexcept VMULSSZrr %107, %104, implicit $mxcsr
    %112:fr32x = nofpexcept VSUBSSZrr %106, %107, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 36, $noreg, %112 :: (store 4 into %ir.fstoreptridx9)
    %113:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 40, $noreg :: (load 4 from %ir.aptridx10)
    %114:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 40, $noreg :: (load 4 from %ir.bptridx10)
    %115:fr32x = nofpexcept VADDSSZrr %108, %113, implicit $mxcsr
    %116:fr32x = nofpexcept VMULSSZrr %113, %109, implicit $mxcsr
    %117:fr32x = nofpexcept VADDSSZrr %110, %114, implicit $mxcsr
    %118:fr32x = nofpexcept VMULSSZrr %114, %111, implicit $mxcsr
    %119:fr32x = nofpexcept VSUBSSZrr %113, %114, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 40, $noreg, %119 :: (store 4 into %ir.fstoreptridx10)
    %120:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 44, $noreg :: (load 4 from %ir.aptridx11)
    %121:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 44, $noreg :: (load 4 from %ir.bptridx11)
    %122:fr32x = nofpexcept VADDSSZrr %115, %120, implicit $mxcsr
    %123:fr32x = nofpexcept VMULSSZrr %120, %116, implicit $mxcsr
    %124:fr32x = nofpexcept VADDSSZrr %117, %121, implicit $mxcsr
    %125:fr32x = nofpexcept VMULSSZrr %121, %118, implicit $mxcsr
    %126:fr32x = nofpexcept VSUBSSZrr %120, %121, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 44, $noreg, %126 :: (store 4 into %ir.fstoreptridx11)
    %127:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 48, $noreg :: (load 4 from %ir.aptridx12)
    %128:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 48, $noreg :: (load 4 from %ir.bptridx12)
    %129:fr32x = nofpexcept VADDSSZrr %122, %127, implicit $mxcsr
    %130:fr32x = nofpexcept VMULSSZrr %127, %123, implicit $mxcsr
    %131:fr32x = nofpexcept VADDSSZrr %124, %128, implicit $mxcsr
    %132:fr32x = nofpexcept VMULSSZrr %128, %125, implicit $mxcsr
    %133:fr32x = nofpexcept VSUBSSZrr %127, %128, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 48, $noreg, %133 :: (store 4 into %ir.fstoreptridx12)
    %134:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 52, $noreg :: (load 4 from %ir.aptridx13)
    %135:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 52, $noreg :: (load 4 from %ir.bptridx13)
    %136:fr32x = nofpexcept VADDSSZrr %129, %134, implicit $mxcsr
    %137:fr32x = nofpexcept VMULSSZrr %134, %130, implicit $mxcsr
    %138:fr32x = nofpexcept VADDSSZrr %131, %135, implicit $mxcsr
    %139:fr32x = nofpexcept VMULSSZrr %135, %132, implicit $mxcsr
    %140:fr32x = nofpexcept VSUBSSZrr %134, %135, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 52, $noreg, %140 :: (store 4 into %ir.fstoreptridx13)
    %141:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 56, $noreg :: (load 4 from %ir.aptridx14)
    %142:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 56, $noreg :: (load 4 from %ir.bptridx14)
    %143:fr32x = nofpexcept VADDSSZrr %136, %141, implicit $mxcsr
    %144:fr32x = nofpexcept VMULSSZrr %141, %137, implicit $mxcsr
    %145:fr32x = nofpexcept VADDSSZrr %138, %142, implicit $mxcsr
    %146:fr32x = nofpexcept VMULSSZrr %142, %139, implicit $mxcsr
    %147:fr32x = nofpexcept VSUBSSZrr %141, %142, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 56, $noreg, %147 :: (store 4 into %ir.fstoreptridx14)
    %148:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 60, $noreg :: (load 4 from %ir.aptridx15)
    %149:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 60, $noreg :: (load 4 from %ir.bptridx15)
    %150:fr32x = nofpexcept VADDSSZrr %143, %148, implicit $mxcsr
    %151:fr32x = nofpexcept VMULSSZrr %148, %144, implicit $mxcsr
    %152:fr32x = nofpexcept VADDSSZrr %145, %149, implicit $mxcsr
    %153:fr32x = nofpexcept VMULSSZrr %149, %146, implicit $mxcsr
    %154:fr32x = nofpexcept VSUBSSZrr %148, %149, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 60, $noreg, %154 :: (store 4 into %ir.fstoreptridx15)
    %155:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 64, $noreg :: (load 4 from %ir.aptridx16)
    %156:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 64, $noreg :: (load 4 from %ir.bptridx16)
    %157:fr32x = nofpexcept VADDSSZrr %150, %155, implicit $mxcsr
    %158:fr32x = nofpexcept VMULSSZrr %155, %151, implicit $mxcsr
    %159:fr32x = nofpexcept VADDSSZrr %152, %156, implicit $mxcsr
    %160:fr32x = nofpexcept VMULSSZrr %156, %153, implicit $mxcsr
    %161:fr32x = nofpexcept VSUBSSZrr %155, %156, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 64, $noreg, %161 :: (store 4 into %ir.fstoreptridx16)
    %162:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 68, $noreg :: (load 4 from %ir.aptridx17)
    %163:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 68, $noreg :: (load 4 from %ir.bptridx17)
    %164:fr32x = nofpexcept VADDSSZrr %157, %162, implicit $mxcsr
    %165:fr32x = nofpexcept VMULSSZrr %162, %158, implicit $mxcsr
    %166:fr32x = nofpexcept VADDSSZrr %159, %163, implicit $mxcsr
    %167:fr32x = nofpexcept VMULSSZrr %163, %160, implicit $mxcsr
    %168:fr32x = nofpexcept VSUBSSZrr %162, %163, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 68, $noreg, %168 :: (store 4 into %ir.fstoreptridx17)
    %169:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 72, $noreg :: (load 4 from %ir.aptridx18)
    %170:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 72, $noreg :: (load 4 from %ir.bptridx18)
    %171:fr32x = nofpexcept VADDSSZrr %164, %169, implicit $mxcsr
    %172:fr32x = nofpexcept VMULSSZrr %169, %165, implicit $mxcsr
    %173:fr32x = nofpexcept VADDSSZrr %166, %170, implicit $mxcsr
    %174:fr32x = nofpexcept VMULSSZrr %170, %167, implicit $mxcsr
    %175:fr32x = nofpexcept VSUBSSZrr %169, %170, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 72, $noreg, %175 :: (store 4 into %ir.fstoreptridx18)
    %176:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 76, $noreg :: (load 4 from %ir.aptridx19)
    %177:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 76, $noreg :: (load 4 from %ir.bptridx19)
    %178:fr32x = nofpexcept VADDSSZrr %171, %176, implicit $mxcsr
    %179:fr32x = nofpexcept VMULSSZrr %176, %172, implicit $mxcsr
    %180:fr32x = nofpexcept VADDSSZrr %173, %177, implicit $mxcsr
    %181:fr32x = nofpexcept VMULSSZrr %177, %174, implicit $mxcsr
    %182:fr32x = nofpexcept VSUBSSZrr %176, %177, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 76, $noreg, %182 :: (store 4 into %ir.fstoreptridx19)
    %183:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 80, $noreg :: (load 4 from %ir.aptridx20)
    %184:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 80, $noreg :: (load 4 from %ir.bptridx20)
    %185:fr32x = nofpexcept VADDSSZrr %178, %183, implicit $mxcsr
    %186:fr32x = nofpexcept VMULSSZrr %183, %179, implicit $mxcsr
    %187:fr32x = nofpexcept VADDSSZrr %180, %184, implicit $mxcsr
    %188:fr32x = nofpexcept VMULSSZrr %184, %181, implicit $mxcsr
    %189:fr32x = nofpexcept VSUBSSZrr %183, %184, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 80, $noreg, %189 :: (store 4 into %ir.fstoreptridx20)
    %190:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 84, $noreg :: (load 4 from %ir.aptridx21)
    %191:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 84, $noreg :: (load 4 from %ir.bptridx21)
    %192:fr32x = nofpexcept VADDSSZrr %185, %190, implicit $mxcsr
    %193:fr32x = nofpexcept VMULSSZrr %190, %186, implicit $mxcsr
    %194:fr32x = nofpexcept VADDSSZrr %187, %191, implicit $mxcsr
    %195:fr32x = nofpexcept VMULSSZrr %191, %188, implicit $mxcsr
    %196:fr32x = nofpexcept VSUBSSZrr %190, %191, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 84, $noreg, %196 :: (store 4 into %ir.fstoreptridx21)
    %197:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 88, $noreg :: (load 4 from %ir.aptridx22)
    %198:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 88, $noreg :: (load 4 from %ir.bptridx22)
    %199:fr32x = nofpexcept VADDSSZrr %192, %197, implicit $mxcsr
    %200:fr32x = nofpexcept VMULSSZrr %197, %193, implicit $mxcsr
    %201:fr32x = nofpexcept VADDSSZrr %194, %198, implicit $mxcsr
    %202:fr32x = nofpexcept VMULSSZrr %198, %195, implicit $mxcsr
    %203:fr32x = nofpexcept VSUBSSZrr %197, %198, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 88, $noreg, %203 :: (store 4 into %ir.fstoreptridx22)
    %204:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 92, $noreg :: (load 4 from %ir.aptridx23)
    %205:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 92, $noreg :: (load 4 from %ir.bptridx23)
    %206:fr32x = nofpexcept VADDSSZrr %199, %204, implicit $mxcsr
    %207:fr32x = nofpexcept VMULSSZrr %204, %200, implicit $mxcsr
    %208:fr32x = nofpexcept VADDSSZrr %201, %205, implicit $mxcsr
    %209:fr32x = nofpexcept VMULSSZrr %205, %202, implicit $mxcsr
    %210:fr32x = nofpexcept VSUBSSZrr %204, %205, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 92, $noreg, %210 :: (store 4 into %ir.fstoreptridx23)
    %211:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 96, $noreg :: (load 4 from %ir.aptridx24)
    %212:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 96, $noreg :: (load 4 from %ir.bptridx24)
    %213:fr32x = nofpexcept VADDSSZrr %206, %211, implicit $mxcsr
    %214:fr32x = nofpexcept VMULSSZrr %211, %207, implicit $mxcsr
    %215:fr32x = nofpexcept VADDSSZrr %208, %212, implicit $mxcsr
    %216:fr32x = nofpexcept VMULSSZrr %212, %209, implicit $mxcsr
    %217:fr32x = nofpexcept VSUBSSZrr %211, %212, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 96, $noreg, %217 :: (store 4 into %ir.fstoreptridx24)
    %218:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 100, $noreg :: (load 4 from %ir.aptridx25)
    %219:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 100, $noreg :: (load 4 from %ir.bptridx25)
    %220:fr32x = nofpexcept VADDSSZrr %213, %218, implicit $mxcsr
    %221:fr32x = nofpexcept VMULSSZrr %218, %214, implicit $mxcsr
    %222:fr32x = nofpexcept VADDSSZrr %215, %219, implicit $mxcsr
    %223:fr32x = nofpexcept VMULSSZrr %219, %216, implicit $mxcsr
    %224:fr32x = nofpexcept VSUBSSZrr %218, %219, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 100, $noreg, %224 :: (store 4 into %ir.fstoreptridx25)
    %225:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 104, $noreg :: (load 4 from %ir.aptridx26)
    %226:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 104, $noreg :: (load 4 from %ir.bptridx26)
    %227:fr32x = nofpexcept VADDSSZrr %220, %225, implicit $mxcsr
    %228:fr32x = nofpexcept VMULSSZrr %225, %221, implicit $mxcsr
    %229:fr32x = nofpexcept VADDSSZrr %222, %226, implicit $mxcsr
    %230:fr32x = nofpexcept VMULSSZrr %226, %223, implicit $mxcsr
    %231:fr32x = nofpexcept VSUBSSZrr %225, %226, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 104, $noreg, %231 :: (store 4 into %ir.fstoreptridx26)
    %232:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 108, $noreg :: (load 4 from %ir.aptridx27)
    %233:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 108, $noreg :: (load 4 from %ir.bptridx27)
    %234:fr32x = nofpexcept VADDSSZrr %227, %232, implicit $mxcsr
    %235:fr32x = nofpexcept VMULSSZrr %232, %228, implicit $mxcsr
    %236:fr32x = nofpexcept VADDSSZrr %229, %233, implicit $mxcsr
    %237:fr32x = nofpexcept VMULSSZrr %233, %230, implicit $mxcsr
    %238:fr32x = nofpexcept VSUBSSZrr %232, %233, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 108, $noreg, %238 :: (store 4 into %ir.fstoreptridx27)
    %239:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 112, $noreg :: (load 4 from %ir.aptridx28)
    %240:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 112, $noreg :: (load 4 from %ir.bptridx28)
    %241:fr32x = nofpexcept VADDSSZrr %234, %239, implicit $mxcsr
    %242:fr32x = nofpexcept VMULSSZrr %239, %235, implicit $mxcsr
    %243:fr32x = nofpexcept VADDSSZrr %236, %240, implicit $mxcsr
    %244:fr32x = nofpexcept VMULSSZrr %240, %237, implicit $mxcsr
    %245:fr32x = nofpexcept VSUBSSZrr %239, %240, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 112, $noreg, %245 :: (store 4 into %ir.fstoreptridx28)
    %246:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 116, $noreg :: (load 4 from %ir.aptridx29)
    %247:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 116, $noreg :: (load 4 from %ir.bptridx29)
    %248:fr32x = nofpexcept VADDSSZrr %241, %246, implicit $mxcsr
    %249:fr32x = nofpexcept VMULSSZrr %246, %242, implicit $mxcsr
    %250:fr32x = nofpexcept VADDSSZrr %243, %247, implicit $mxcsr
    %251:fr32x = nofpexcept VMULSSZrr %247, %244, implicit $mxcsr
    %252:fr32x = nofpexcept VSUBSSZrr %246, %247, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 116, $noreg, %252 :: (store 4 into %ir.fstoreptridx29)
    %253:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 120, $noreg :: (load 4 from %ir.aptridx30)
    %254:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 120, $noreg :: (load 4 from %ir.bptridx30)
    %255:fr32x = nofpexcept VADDSSZrr %248, %253, implicit $mxcsr
    %256:fr32x = nofpexcept VMULSSZrr %253, %249, implicit $mxcsr
    %257:fr32x = nofpexcept VADDSSZrr %250, %254, implicit $mxcsr
    %258:fr32x = nofpexcept VMULSSZrr %254, %251, implicit $mxcsr
    %259:fr32x = nofpexcept VSUBSSZrr %253, %254, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 120, $noreg, %259 :: (store 4 into %ir.fstoreptridx30)
    %260:fr32x = VMOVSSZrm_alt %16, 1, $noreg, 124, $noreg :: (load 4 from %ir.aptridx31)
    %261:fr32x = VMOVSSZrm_alt %17, 1, $noreg, 124, $noreg :: (load 4 from %ir.bptridx31)
    %262:fr32x = nofpexcept VADDSSZrr %255, %260, implicit $mxcsr
    %263:fr32x = nofpexcept VMULSSZrr %260, %256, implicit $mxcsr
    %264:fr32x = nofpexcept VADDSSZrr %257, %261, implicit $mxcsr
    %265:fr32x = nofpexcept VMULSSZrr %261, %258, implicit $mxcsr
    %266:fr32x = nofpexcept VSUBSSZrr %260, %261, implicit $mxcsr
    VMOVSSZmr %49, 1, %48, 124, $noreg, %266 :: (store 4 into %ir.fstoreptridx31)
    VMOVSSZmr %19, 1, $noreg, 0, $noreg, %262 :: (store 4 into %ir.fstoreptr2idx0)
    VMOVSSZmr %19, 1, $noreg, 4, $noreg, %263 :: (store 4 into %ir.fstoreptr2idx1)
    VMOVSSZmr %19, 1, $noreg, 8, $noreg, %264 :: (store 4 into %ir.fstoreptr2idx2)
    VMOVSSZmr %19, 1, $noreg, 12, $noreg, %265 :: (store 4 into %ir.fstoreptr2idx3)

  bb.2.exit:
    MOV32mr %22, 1, $noreg, 0, $noreg, %267.sub_32bit :: (store 4 into %ir.gstoreptridx0)
    MOV32mr %22, 1, $noreg, 4, $noreg, %268.sub_32bit :: (store 4 into %ir.gstoreptridx1)
    MOV32mr %22, 1, $noreg, 8, $noreg, %2 :: (store 4 into %ir.gstoreptridx2)
    MOV32mr %22, 1, $noreg, 12, $noreg, %3 :: (store 4 into %ir.gstoreptridx3)
    MOV32mr %22, 1, $noreg, 16, $noreg, %4 :: (store 4 into %ir.gstoreptridx4)
    MOV32mr %22, 1, $noreg, 20, $noreg, %5 :: (store 4 into %ir.gstoreptridx5)
    MOV32mr %22, 1, $noreg, 24, $noreg, %6 :: (store 4 into %ir.gstoreptridx6)
    MOV32mr %22, 1, $noreg, 28, $noreg, %7 :: (store 4 into %ir.gstoreptridx7)
    MOV32mr %22, 1, $noreg, 32, $noreg, %8 :: (store 4 into %ir.gstoreptridx8)
    MOV32mr %22, 1, $noreg, 36, $noreg, %9 :: (store 4 into %ir.gstoreptridx9)
    MOV32mr %22, 1, $noreg, 40, $noreg, %10 :: (store 4 into %ir.gstoreptridx10)
    MOV32mr %22, 1, $noreg, 44, $noreg, %11 :: (store 4 into %ir.gstoreptridx11)
    MOV32mr %22, 1, $noreg, 48, $noreg, %12 :: (store 4 into %ir.gstoreptridx12)
    MOV32mr %22, 1, $noreg, 52, $noreg, %13 :: (store 4 into %ir.gstoreptridx13)
    MOV32mr %22, 1, $noreg, 56, $noreg, %14 :: (store 4 into %ir.gstoreptridx14)
    MOV32mr %22, 1, $noreg, 60, $noreg, %26 :: (store 4 into %ir.gstoreptridx15)
    RET 0

...
