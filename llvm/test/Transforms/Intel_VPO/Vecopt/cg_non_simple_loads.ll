; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -S -VPlanDriver -vplan-force-vf=2 | FileCheck %s

; Check that non-simple loads are properly serialized.

declare token @llvm.directive.region.entry() nounwind
declare void @llvm.directive.region.exit(token) nounwind

define void @test1(i64 %n, i64 addrspace(4)* %arr) {
; CHECK-LABEL: @test1(
; CHECK:       vector.ph:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <2 x i64 addrspace(4)*> undef, i64 addrspace(4)* [[ARR:%.*]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[BROADCAST_SPLATINSERT]], <2 x i64 addrspace(4)*> undef, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT5:%.*]] = insertelement <2 x i64> undef, i64 [[N:%.*]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT6:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT5]], <2 x i64> undef, <2 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK:         [[MM_VECTORGEP:%.*]] = getelementptr inbounds i64, <2 x i64 addrspace(4)*> [[BROADCAST_SPLAT]], <2 x i64> [[VEC_PHI:%.*]]
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_1_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP]], i32 1
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_0_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP]], i32 0
; CHECK-NEXT:    [[MM_VECTORGEP1:%.*]] = getelementptr inbounds i64, i64 addrspace(4)* [[ARR]], i64 42

; TODO: SVA should make the computation scalar for the uniform GEP

; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <2 x i64 addrspace(4)*> undef, i64 addrspace(4)* [[MM_VECTORGEP1]], i32 0
; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[DOTSPLATINSERT]], <2 x i64 addrspace(4)*> undef, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[DOTSPLAT_EXTRACT_0_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[DOTSPLAT]], i32 0

; Volatile loads - serialized for all the lanes
; CHECK-NEXT:    [[TMP1:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]]
; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <2 x i64> undef, i64 [[TMP1]], i32 0
; CHECK-NEXT:    [[TMP3:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]]
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> [[TMP2]], i64 [[TMP3]], i32 1

; Atomic load from uniform ptr - ok to perform single atomic load
; CHECK-NEXT:    [[TMP5:%.*]] = load atomic i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]] unordered, align 4
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT2:%.*]] = insertelement <2 x i64> undef, i64 [[TMP5]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT3:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT2]], <2 x i64> undef, <2 x i32> zeroinitializer

; Atomic load from consecutive memory - serialize all, target might not have
; wide enough atomic load for the whole vector iteration.
; CHECK-NEXT:    [[TMP6:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_0_]] unordered, align 4
; CHECK-NEXT:    [[TMP7:%.*]] = insertelement <2 x i64> undef, i64 [[TMP6]], i32 0
; CHECK-NEXT:    [[TMP8:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_1_]] unordered, align 4
; CHECK-NEXT:    [[TMP9:%.*]] = insertelement <2 x i64> [[TMP7]], i64 [[TMP8]], i32 1
; CHECK-NEXT:    [[TMP10:%.*]] = add <2 x i64> [[TMP4]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP11:%.*]] = add <2 x i64> [[BROADCAST_SPLAT3]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP12:%.*]] = add <2 x i64> [[TMP9]], [[VEC_PHI]]
; CHECK-NEXT:    [[PREDICATE:%.*]] = extractelement <2 x i1> [[TMP0:%.*]], i64 0
; CHECK-NEXT:    [[TMP13:%.*]] = icmp eq i1 [[PREDICATE]], true
; CHECK-NEXT:    br i1 [[TMP13]], label [[PRED_LOAD_IF:%.*]], label [[TMP16:%.*]]
; CHECK:       pred.load.if:
; CHECK-NEXT:    [[TMP14:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]]
; CHECK-NEXT:    [[TMP15:%.*]] = insertelement <2 x i64> undef, i64 [[TMP14]], i32 0
; CHECK-NEXT:    br label [[TMP16]]
; CHECK:       16:
; CHECK-NEXT:    [[TMP17:%.*]] = phi <2 x i64> [ undef, [[VECTOR_BODY]] ], [ [[TMP15]], [[PRED_LOAD_IF]] ]
; CHECK-NEXT:    br label [[PRED_LOAD_CONTINUE:%.*]]
; CHECK:       pred.load.continue:
; CHECK-NEXT:    [[PREDICATE4:%.*]] = extractelement <2 x i1> [[TMP0]], i64 1
; CHECK-NEXT:    [[TMP18:%.*]] = icmp eq i1 [[PREDICATE4]], true
; CHECK-NEXT:    br i1 [[TMP18]], label [[PRED_LOAD_IF7:%.*]], label [[TMP21:%.*]]
; CHECK:       pred.load.if7:
; CHECK-NEXT:    [[TMP19:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]]
; CHECK-NEXT:    [[TMP20:%.*]] = insertelement <2 x i64> [[TMP17]], i64 [[TMP19]], i32 1
; CHECK-NEXT:    br label [[TMP21]]
;
entry:
  %cmp = icmp sgt i64 %n, 0
  br i1 %cmp, label %for.body.lr.ph, label %exit

for.body.lr.ph:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %for.body.lr.ph ], [ %iv.next, %for.latch ]
  %cond = icmp eq i64 %iv, 42
  %var.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 %iv
  %uni.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 42
  %load.uni = load volatile i64, i64 addrspace(4)* %uni.gep
  %load.uni.atomic = load atomic i64, i64 addrspace(4)* %uni.gep unordered, align 4
  %load.atomic = load atomic i64, i64 addrspace(4)* %var.gep unordered, align 4
  %use1 = add i64 %load.uni, %iv
  %use2 = add i64 %load.uni.atomic, %iv
  %use3 = add i64 %load.atomic, %iv
  br i1 %cond, label %if.then, label %for.latch

if.then:
  %load.uni.pred = load volatile i64, i64 addrspace(4)* %uni.gep
  %var.use = add i64 %load.uni.pred, %iv
  br label %for.latch

for.latch:
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %exit

exit:
  ret void
}
