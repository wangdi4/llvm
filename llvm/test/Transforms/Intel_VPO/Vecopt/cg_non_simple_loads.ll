; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -S -VPlanDriver -vplan-force-vf=2 | FileCheck %s

; Check that non-simple loads are properly serialized.

declare token @llvm.directive.region.entry() nounwind
declare void @llvm.directive.region.exit(token) nounwind

define void @test1(i64 %n, i64 addrspace(4)* %arr) {
; CHECK-LABEL: @test1(
; CHECK:       vector.ph:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <2 x i64 addrspace(4)*> poison, i64 addrspace(4)* [[ARR:%.*]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[BROADCAST_SPLATINSERT]], <2 x i64 addrspace(4)*> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP26:%.*]], [[VPLANNEDBB8:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH]] ], [ [[TMP25:%.*]], [[VPLANNEDBB8]] ]
; CHECK-NEXT:    [[TMP1:%.*]] = icmp eq <2 x i64> [[VEC_PHI]], <i64 42, i64 42>
; CHECK-NEXT:    [[MM_VECTORGEP:%.*]] = getelementptr inbounds i64, <2 x i64 addrspace(4)*> [[BROADCAST_SPLAT]], <2 x i64> [[VEC_PHI]]
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_1_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP]], i32 1
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_0_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP]], i32 0
; CHECK-NEXT:    [[MM_VECTORGEP3:%.*]] = getelementptr inbounds i64, i64 addrspace(4)* [[ARR]], i64 42

; TODO: SVA should make the computation scalar for the uniform GEP

; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <2 x i64 addrspace(4)*> poison, i64 addrspace(4)* [[MM_VECTORGEP3]], i32 0
; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[DOTSPLATINSERT]], <2 x i64 addrspace(4)*> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[DOTSPLAT_EXTRACT_0_:%.*]] = extractelement <2 x i64 addrspace(4)*> [[DOTSPLAT]], i32 0

; Volatile loads - serialized for all the lanes
; CHECK-NEXT:    [[TMP2:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]], align 4
; CHECK-NEXT:    [[TMP3:%.*]] = insertelement <2 x i64> undef, i64 [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP4:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]], align 4
; CHECK-NEXT:    [[TMP5:%.*]] = insertelement <2 x i64> [[TMP3]], i64 [[TMP4]], i32 1

; Atomic load from uniform ptr - ok to perform single atomic load
; CHECK-NEXT:    [[TMP6:%.*]] = load atomic i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]] unordered, align 4
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT4:%.*]] = insertelement <2 x i64> poison, i64 [[TMP6]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT5:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT4]], <2 x i64> poison, <2 x i32> zeroinitializer

; Atomic load from consecutive memory - serialize all, target might not have
; wide enough atomic load for the whole vector iteration.
; CHECK-NEXT:    [[TMP7:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_0_]] unordered, align 4
; CHECK-NEXT:    [[TMP8:%.*]] = insertelement <2 x i64> undef, i64 [[TMP7]], i32 0
; CHECK-NEXT:    [[TMP9:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_1_]] unordered, align 4
; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <2 x i64> [[TMP8]], i64 [[TMP9]], i32 1
; CHECK-NEXT:    [[TMP11:%.*]] = add <2 x i64> [[TMP5]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP12:%.*]] = add <2 x i64> [[BROADCAST_SPLAT5]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP13:%.*]] = add <2 x i64> [[TMP10]], [[VEC_PHI]]
; CHECK-NEXT:    br label [[VPLANNEDBB6:%.*]]
; CHECK:       VPlannedBB6:
; CHECK-NEXT:    [[PREDICATE:%.*]] = extractelement <2 x i1> [[TMP1]], i64 0
; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i1 [[PREDICATE]], true
; CHECK-NEXT:    br i1 [[TMP14]], label [[PRED_LOAD_IF:%.*]], label [[TMP17:%.*]]
; CHECK:       pred.load.if:
; CHECK-NEXT:    [[TMP15:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]], align 4
; CHECK-NEXT:    [[TMP16:%.*]] = insertelement <2 x i64> undef, i64 [[TMP15]], i32 0
; CHECK-NEXT:    br label [[TMP17]]
; CHECK:       17:
; CHECK-NEXT:    [[TMP18:%.*]] = phi <2 x i64> [ undef, [[VPLANNEDBB6]] ], [ [[TMP16]], [[PRED_LOAD_IF]] ]
; CHECK-NEXT:    br label [[PRED_LOAD_CONTINUE:%.*]]
; CHECK:       pred.load.continue:
; CHECK-NEXT:    [[PREDICATE7:%.*]] = extractelement <2 x i1> [[TMP1]], i64 1
; CHECK-NEXT:    [[TMP19:%.*]] = icmp eq i1 [[PREDICATE7]], true
; CHECK-NEXT:    br i1 [[TMP19]], label [[PRED_LOAD_IF15:%.*]], label [[TMP22:%.*]]
; CHECK:       pred.load.if15:
; CHECK-NEXT:    [[TMP20:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_]], align 4
; CHECK-NEXT:    [[TMP21:%.*]] = insertelement <2 x i64> [[TMP18]], i64 [[TMP20]], i32 1
; CHECK-NEXT:    br label [[TMP22]]
;
entry:
  %cmp = icmp sgt i64 %n, 0
  br i1 %cmp, label %for.body.lr.ph, label %exit

for.body.lr.ph:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %for.body.lr.ph ], [ %iv.next, %for.latch ]
  %cond = icmp eq i64 %iv, 42
  %var.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 %iv
  %uni.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 42
  %load.uni = load volatile i64, i64 addrspace(4)* %uni.gep
  %load.uni.atomic = load atomic i64, i64 addrspace(4)* %uni.gep unordered, align 4
  %load.atomic = load atomic i64, i64 addrspace(4)* %var.gep unordered, align 4
  %use1 = add i64 %load.uni, %iv
  %use2 = add i64 %load.uni.atomic, %iv
  %use3 = add i64 %load.atomic, %iv
  br i1 %cond, label %if.then, label %for.latch

if.then:
  %load.uni.pred = load volatile i64, i64 addrspace(4)* %uni.gep
  %var.use = add i64 %load.uni.pred, %iv
  br label %for.latch

for.latch:
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %exit

exit:
  ret void
}
