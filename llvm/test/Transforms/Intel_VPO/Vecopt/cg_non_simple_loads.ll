; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt < %s -S -VPlanDriver -vplan-force-vf=2 | FileCheck %s

; Check that non-simple loads are properly serialized.

declare token @llvm.directive.region.entry() nounwind
declare void @llvm.directive.region.exit(token) nounwind

define void @test1(i64 %n, i64 addrspace(4)* %arr) {
; CHECK:       vector.ph:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT0:%.*]] = insertelement <2 x i64 addrspace(4)*> poison, i64 addrspace(4)* [[ARR0:%.*]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT0:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[BROADCAST_SPLATINSERT0]], <2 x i64 addrspace(4)*> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VECTOR_BODY0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VECTOR_PH0:%.*]] ], [ [[TMP27:%.*]], [[VPLANNEDBB80:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH0]] ], [ [[TMP26:%.*]], [[VPLANNEDBB80]] ]
; CHECK-NEXT:    [[TMP2:%.*]] = icmp eq <2 x i64> [[VEC_PHI0]], <i64 42, i64 42>
; CHECK-NEXT:    [[MM_VECTORGEP0:%.*]] = getelementptr inbounds i64, <2 x i64 addrspace(4)*> [[BROADCAST_SPLAT0]], <2 x i64> [[VEC_PHI0]]
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_1_0:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP0]], i32 1
; CHECK-NEXT:    [[MM_VECTORGEP_EXTRACT_0_0:%.*]] = extractelement <2 x i64 addrspace(4)*> [[MM_VECTORGEP0]], i32 0
; CHECK-NEXT:    [[MM_VECTORGEP30:%.*]] = getelementptr inbounds i64, i64 addrspace(4)* [[ARR0]], i64 42

; TODO: SVA should make the computation scalar for the uniform GEP

; CHECK-NEXT:    [[DOTSPLATINSERT0:%.*]] = insertelement <2 x i64 addrspace(4)*> poison, i64 addrspace(4)* [[MM_VECTORGEP30]], i32 0
; CHECK-NEXT:    [[DOTSPLAT0:%.*]] = shufflevector <2 x i64 addrspace(4)*> [[DOTSPLATINSERT0]], <2 x i64 addrspace(4)*> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[DOTSPLAT_EXTRACT_0_0:%.*]] = extractelement <2 x i64 addrspace(4)*> [[DOTSPLAT0]], i32 0

; Volatile loads - serialized for all the lanes

; CHECK-NEXT:    [[TMP3:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_0]], align 4
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x i64> undef, i64 [[TMP3]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_0]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x i64> [[TMP4]], i64 [[TMP5]], i32 1

; Atomic load from uniform ptr - ok to perform single atomic load

; CHECK-NEXT:    [[TMP7:%.*]] = load atomic i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_0]] unordered, align 4
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT40:%.*]] = insertelement <2 x i64> poison, i64 [[TMP7]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT50:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT40]], <2 x i64> poison, <2 x i32> zeroinitializer

; Atomic load from consecutive memory - serialize all, target might not have
; wide enough atomic load for the whole vector iteration.

; CHECK-NEXT:    [[TMP8:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_0_0]] unordered, align 4
; CHECK-NEXT:    [[TMP9:%.*]] = insertelement <2 x i64> undef, i64 [[TMP8]], i32 0
; CHECK-NEXT:    [[TMP10:%.*]] = load atomic i64, i64 addrspace(4)* [[MM_VECTORGEP_EXTRACT_1_0]] unordered, align 4
; CHECK-NEXT:    [[TMP11:%.*]] = insertelement <2 x i64> [[TMP9]], i64 [[TMP10]], i32 1
; CHECK-NEXT:    [[TMP12:%.*]] = add <2 x i64> [[TMP6]], [[VEC_PHI0]]
; CHECK-NEXT:    [[TMP13:%.*]] = add <2 x i64> [[BROADCAST_SPLAT50]], [[VEC_PHI0]]
; CHECK-NEXT:    [[TMP14:%.*]] = add <2 x i64> [[TMP11]], [[VEC_PHI0]]
; CHECK-NEXT:    br label [[VPLANNEDBB60:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB6:
; CHECK-NEXT:    [[PREDICATE0:%.*]] = extractelement <2 x i1> [[TMP2]], i64 0
; CHECK-NEXT:    [[TMP15:%.*]] = icmp eq i1 [[PREDICATE0]], true
; CHECK-NEXT:    br i1 [[TMP15]], label [[PRED_LOAD_IF0:%.*]], label [[TMP18:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  pred.load.if:
; CHECK-NEXT:    [[TMP16:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_0]], align 4
; CHECK-NEXT:    [[TMP17:%.*]] = insertelement <2 x i64> undef, i64 [[TMP16]], i32 0
; CHECK-NEXT:    br label [[TMP18]]
; CHECK-EMPTY:
; CHECK-NEXT:  18:
; CHECK-NEXT:    [[TMP19:%.*]] = phi <2 x i64> [ undef, [[VPLANNEDBB60]] ], [ [[TMP17]], [[PRED_LOAD_IF0]] ]
; CHECK-NEXT:    br label [[PRED_LOAD_CONTINUE0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  pred.load.continue:
; CHECK-NEXT:    [[PREDICATE70:%.*]] = extractelement <2 x i1> [[TMP2]], i64 1
; CHECK-NEXT:    [[TMP20:%.*]] = icmp eq i1 [[PREDICATE70]], true
; CHECK-NEXT:    br i1 [[TMP20]], label [[PRED_LOAD_IF150:%.*]], label [[TMP23:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  pred.load.if15:
; CHECK-NEXT:    [[TMP21:%.*]] = load volatile i64, i64 addrspace(4)* [[DOTSPLAT_EXTRACT_0_0]], align 4
; CHECK-NEXT:    [[TMP22:%.*]] = insertelement <2 x i64> [[TMP19]], i64 [[TMP21]], i32 1
; CHECK-NEXT:    br label [[TMP23]]
; CHECK-EMPTY:
; CHECK-NEXT:  23:
; CHECK-NEXT:    [[TMP24:%.*]] = phi <2 x i64> [ [[TMP19]], [[PRED_LOAD_CONTINUE0]] ], [ [[TMP22]], [[PRED_LOAD_IF150]] ]
; CHECK-NEXT:    br label [[PRED_LOAD_CONTINUE160:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  pred.load.continue16:
; CHECK-NEXT:    [[TMP25:%.*]] = add <2 x i64> [[TMP24]], [[VEC_PHI0]]
; CHECK-NEXT:    br label [[VPLANNEDBB80]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB8:
; CHECK-NEXT:    [[TMP26]] = add nuw nsw <2 x i64> [[VEC_PHI0]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP27]] = add nuw nsw i64 [[UNI_PHI0]], 2
; CHECK-NEXT:    [[TMP28:%.*]] = icmp eq i64 [[TMP27]], [[TMP0:%.*]]
; CHECK-NEXT:    br i1 [[TMP28]], label [[VPLANNEDBB90:%.*]], label [[VECTOR_BODY0]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB9:
; CHECK-NEXT:    [[TMP29:%.*]] = mul i64 1, [[TMP0]]
; CHECK-NEXT:    [[TMP30:%.*]] = add i64 0, [[TMP29]]
; CHECK-NEXT:    br label [[MIDDLE_BLOCK0:%.*]]
; CHECK-EMPTY:
;
entry:
  %cmp = icmp sgt i64 %n, 0
  br i1 %cmp, label %for.body.lr.ph, label %exit

for.body.lr.ph:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %for.body.lr.ph ], [ %iv.next, %for.latch ]
  %cond = icmp eq i64 %iv, 42
  %var.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 %iv
  %uni.gep = getelementptr inbounds i64, i64 addrspace(4)* %arr, i64 42
  %load.uni = load volatile i64, i64 addrspace(4)* %uni.gep
  %load.uni.atomic = load atomic i64, i64 addrspace(4)* %uni.gep unordered, align 4
  %load.atomic = load atomic i64, i64 addrspace(4)* %var.gep unordered, align 4
  %use1 = add i64 %load.uni, %iv
  %use2 = add i64 %load.uni.atomic, %iv
  %use3 = add i64 %load.atomic, %iv
  br i1 %cond, label %if.then, label %for.latch

if.then:
  %load.uni.pred = load volatile i64, i64 addrspace(4)* %uni.gep
  %var.use = add i64 %load.uni.pred, %iv
  br label %for.latch

for.latch:
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %exit

exit:
  ret void
}
