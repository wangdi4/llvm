; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -enable-new-pm=0 -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec -vplan-force-vf=4 -hir-cg -simplifycfg -sroa -print-after=hir-vplan-vec -print-after=sroa -disable-output < %s 2>&1 | FileCheck %s
; RUN: opt -passes="hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec,print<hir>,hir-cg,simplifycfg,sroa" -vplan-force-vf=4 -print-after=sroa -disable-output < %s 2>&1 | FileCheck %s

;
; LIT test to check that we keep canon expressions for unit strided memory refs
; linear when bitcasts are involved. The test here checks that we do not
; generate a gather/scatter. The arrays arr1/arr2 are of type float. Due to LLVM
; canonicalization, the pointers are bitcast to i32 *.
;
; Scalar HIR:
;               DO i1 = 0, 99, 1   <DO_LOOP>
;                 %1 = (i32*)(@arr2)[0][i1];
;                 (i32*)(@arr1)[0][i1] = %1;
;               END LOOP
;

@arr2 = dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16
@arr1 = dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16

define void @foo() {
; CHECK:               + DO i1 = 0, 99, 4   <DO_LOOP> <auto-vectorized> <novectorize>
; CHECK-NEXT:          |   %.vec = (<4 x i32>*)(@arr2)[0][i1];
; CHECK-NEXT:          |   (<4 x i32>*)(@arr1)[0][i1] = %.vec;
; CHECK-NEXT:          + END LOOP
;
; CHECK:  *** IR Dump After SROA{{.*}} ***
;
; CHECK:       define void @foo() {
; CHECK:       loop.17:
; CHECK-NEXT:    [[I1_I64_00:%.*]] = phi i64 [ 0, [[ENTRY0:%.*]] ], [ [[NEXTIVLOOP_170:%.*]], [[LOOP_170:%.*]] ]
; CHECK-NEXT:    [[ARRAYIDX0:%.*]] = getelementptr inbounds [100 x float], [100 x float]* @arr2, i64 0, i64 [[I1_I64_00]]
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARRAYIDX0]] to <4 x i32>*
; CHECK-NEXT:    [[GEPLOAD0:%.*]] = load <4 x i32>, <4 x i32>* [[TMP0]], align 4
; CHECK-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds [100 x float], [100 x float]* @arr1, i64 0, i64 [[I1_I64_00]]
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast float* [[ARRAYIDX10]] to <4 x i32>*
; CHECK-NEXT:    store <4 x i32> [[GEPLOAD0]], <4 x i32>* [[TMP1]], align 4
; CHECK-NEXT:    [[NEXTIVLOOP_170]] = add nuw nsw i64 [[I1_I64_00]], 4
; CHECK-NEXT:    [[CONDLOOP_170:%.*]] = icmp sle i64 [[NEXTIVLOOP_170]], 99
; CHECK-NEXT:    br i1 [[CONDLOOP_170]], label [[LOOP_170]], label [[AFTERLOOP_170:%.*]], !llvm.loop !0
;
entry:
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %l1.06 = phi i64 [ 0, %entry ], [ %inc, %for.body ]
  %arrayidx = getelementptr inbounds [100 x float], [100 x float]* @arr2, i64 0, i64 %l1.06
  %0 = bitcast float* %arrayidx to i32*
  %1 = load i32, i32* %0, align 4
  %arrayidx1 = getelementptr inbounds [100 x float], [100 x float]* @arr1, i64 0, i64 %l1.06
  %2 = bitcast float* %arrayidx1 to i32*
  store i32 %1, i32* %2, align 4
  %inc = add nuw nsw i64 %l1.06, 1
  %cmp = icmp ult i64 %l1.06, 99
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  ret void
}
