; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Test that VPlan DA computes vector shape and stride of VPSubscriptInst correctly
; accounting for stride of index in 0th dimension. Also check for generated vector
; HIR code.

; Incoming HIR
;     BEGIN REGION { }
;           %entry.region = @llvm.directive.region.entry(); [ DIR.VPO.AUTO.VEC() ]
;
;           + DO i1 = 0, 29, 1   <DO_LOOP>
;           |   %p1 = &((@A)[0:0:2048([256 x double]*:0)][0:2:8([256 x double]:256)]);
;           |   (%p1)[0:i1:32(double*:0)] = 1.000000e+00;
;           |   (%B)[0:2 * i1:32(double*:0)] = 2.000000e+00;
;           + END LOOP
;
;           @llvm.directive.region.exit(%entry.region); [ DIR.VPO.END.AUTO.VEC() ]
;     END REGION


; REQUIRES: asserts
; RUN: opt -passes="hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec,print<hir>" -vplan-dump-da -vplan-dump-subscript-details -vplan-force-vf=4 -disable-output < %s 2>&1 | FileCheck %s


; Check for results from DA.
; CHECK:       Printing Divergence info for Loop at depth 1 containing: [[BB0:BB[0-9]+]]<header><latch><exiting>
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT:%.*]], [[BB1:BB[0-9]+]] ],  [ i64 [[VP1:%.*]], [[BB0]] ]
; CHECK-NEXT:  Uniform: [Shape: Uniform] ptr [[VP_SUBSCRIPT:%.*]] = subscript inbounds ptr @A {i64 0 : i64 0 : i64 2048 : ptr([256 x double])} {i64 0 : i64 2 : i64 8 : [256 x double](double)}
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 32] ptr [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds ptr [[VP_SUBSCRIPT]] {i64 0 : i64 [[VP0]] : i64 32 : ptr(double)}
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 32] store double 1.000000e+00 ptr [[VP_SUBSCRIPT_1]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 2] i64 [[VP2:%.*]] = mul i64 2 i64 [[VP0]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 64] ptr [[VP_SUBSCRIPT_2:%.*]] = subscript inbounds ptr [[B0:%.*]] {i64 0 : i64 [[VP2]] : i64 32 : ptr(double)}
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 64] store double 2.000000e+00 ptr [[VP_SUBSCRIPT_2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP1]] = add i64 [[VP0]] i64 [[VP__IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP3:%.*]] = icmp slt i64 [[VP1]] i64 [[VP_VECTOR_TRIP_COUNT:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] br i1 [[VP3]], [[BB0]], [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] br [[BB3:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB3]]

; Checks for vector HIR generated.
; CHECK:       + DO i1 = 0, 27, 4   <DO_LOOP> <auto-vectorized> <novectorize>
; CHECK-NEXT:  |   [[NSBGEPCOPY0:%.*]] = &((<4 x ptr>)(@A)[0][2])
; CHECK-NEXT:  |   [[NSBGEPCOPY1:%.*]] = &((double*)(@A)[0][2]);
; CHECK-NEXT:  |   (<4 x double>*)([[NSBGEPCOPY0]])[i1 + <i64 0, i64 1, i64 2, i64 3>] = 1.000000e+00
; CHECK-NEXT:  |   (<4 x double>*)([[B0]])[2 * i1 + 2 * <i64 0, i64 1, i64 2, i64 3>] = 2.000000e+00
; CHECK-NEXT:  + END LOOP

@A = global [256 x double] zeroinitializer

define void @main() {
entry:
  %B = alloca double
  br label %loop

loop:
  %i = phi i64 [0, %entry], [%ip, %loop]
  %i2 = mul i64 %i, 2

  %p = getelementptr inbounds [256 x double], ptr @A, i64 0, i64 1
  %p1 = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr elementtype(double) nonnull %p, i64 1)
  %p2 = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 32, ptr elementtype(double) nonnull %p1, i64 %i)
  store double 1.0, ptr %p2

  %p3 = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 32, ptr elementtype(double) nonnull %B, i64 %i2)
  store double 2.0, ptr %p3

  %ip = add nsw nuw i64 %i, 1
  %cmp = icmp ult i64 %ip, 30
  br i1 %cmp, label %loop, label %exit

exit:
  ret void
}

declare ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8, i64, i64, ptr, i64)

