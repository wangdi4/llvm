; NOTE: Assertions have been autogenerated by utils/update_test_checks.py

; Test to check that VPlan LLVM-IR CG generates correct addrspacecast
; instructions when they operate on privates optimized for SOA layout.
; This is opaque pointers version of existing LIT test ir_cg_soa_addrspace_cast.ll

; RUN: opt < %s -S -passes=vplan-vec -vplan-force-vf=2 | FileCheck %s

; Test case to simulate simple scenario where we have a SOA unit-strided
; addrspacecast that is kept scalar for SOA layout in outgoing code.
define dso_local void @test1(ptr %a) {
; CHECK-LABEL: @test1(
; CHECK:       vector.body:
; CHECK:         [[SOA_SCALAR_GEP:%.*]] = getelementptr inbounds [275 x <2 x i8>], ptr [[ARR_PRIV_SOA_VEC:%.*]], i64 0, i64 23
; CHECK-NEXT:    store <2 x i8> <i8 123, i8 123>, ptr [[SOA_SCALAR_GEP]], align 1
; CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast ptr [[SOA_SCALAR_GEP]] to ptr addrspace(4)
; CHECK-NEXT:    store <2 x i8> <i8 124, i8 124>, ptr addrspace(4) [[TMP0]], align 1
;
omp.inner.for.body.lr.ph:
  %arr.priv = alloca [275 x i8], align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %omp.inner.for.body.lr.ph
%entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE:TYPED"(ptr %arr.priv, i8 0, i32 275)]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %iv.next, %for.body ]
  %gep = getelementptr inbounds [275 x i8], ptr %arr.priv, i64 0, i64 23
  store i8 123, ptr %gep
  %ascast = addrspacecast ptr %gep to ptr addrspace(4)
  store i8 124, ptr addrspace(4) %ascast
  %iv.next = add nuw nsw i64 %iv, 1
  %cmp = icmp ult i64 %iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}


; Test case to simulate a scenario where we have a SOA unit-strided
; addrspacecast that is used in a PHI emitted by AZB. Both cast and
; PHI are kept scalar for SOA layout in outgoing code.
define dso_local void @test2(ptr %a) {
; CHECK-LABEL: @test2(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP15:%.*]], [[VPLANNEDBB15:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VPLANNEDBB1]] ], [ [[TMP14:%.*]], [[VPLANNEDBB15]] ]
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, ptr [[A:%.*]], i64 [[UNI_PHI]]
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x i32>, ptr [[SCALAR_GEP]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = icmp sgt <2 x i32> [[WIDE_LOAD]], <i32 3, i32 3>
; CHECK-NEXT:    br label [[ALL_ZERO_BYPASS_BEGIN42:%.*]]
; CHECK:       all.zero.bypass.begin42:
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i1> [[TMP1]] to i2
; CHECK-NEXT:    [[TMP3:%.*]] = icmp eq i2 [[TMP2]], 0
; CHECK-NEXT:    br i1 [[TMP3]], label [[ALL_ZERO_BYPASS_END44:%.*]], label [[VPLANNEDBB3:%.*]]
; CHECK:       VPlannedBB3:
; CHECK-NEXT:    [[SOA_SCALAR_GEP:%.*]] = getelementptr inbounds [275 x <2 x i8>], ptr [[ARR_PRIV_SOA_VEC:%.*]], i64 0, i64 23
; CHECK-NEXT:    call void @llvm.masked.store.v2i8.p0(<2 x i8> <i8 123, i8 123>, ptr [[SOA_SCALAR_GEP]], i32 1, <2 x i1> [[TMP1]])
; CHECK-NEXT:    [[TMP4:%.*]] = addrspacecast ptr [[SOA_SCALAR_GEP]] to ptr addrspace(4)
; CHECK-NEXT:    br label [[VPLANNEDBB4:%.*]]
; CHECK:       VPlannedBB4:
; CHECK-NEXT:    [[UNI_PHI5:%.*]] = phi i64 [ 0, [[VPLANNEDBB3]] ], [ [[TMP5:%.*]], [[VPLANNEDBB4]] ]
; CHECK-NEXT:    [[TMP5]] = add i64 [[UNI_PHI5]], 1
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <2 x i64> poison, i64 [[TMP5]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT]], <2 x i64> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ult <2 x i64> [[BROADCAST_SPLAT]], <i64 125, i64 125>
; CHECK-NEXT:    [[DOTEXTRACT_0_8:%.*]] = extractelement <2 x i1> [[TMP6]], i32 0
; CHECK-NEXT:    [[TMP7:%.*]] = and <2 x i1> [[TMP1]], [[TMP1]]
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast <2 x i1> [[TMP7]] to i2
; CHECK-NEXT:    [[TMP9:%.*]] = icmp eq i2 [[TMP8]], 0
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT6:%.*]] = insertelement <2 x i1> poison, i1 [[TMP9]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT7:%.*]] = shufflevector <2 x i1> [[BROADCAST_SPLATINSERT6]], <2 x i1> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP10:%.*]] = xor <2 x i1> [[BROADCAST_SPLAT7]], <i1 true, i1 true>
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <2 x i1> [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP11:%.*]] = and i1 [[DOTEXTRACT_0_]], [[DOTEXTRACT_0_8]]
; CHECK-NEXT:    br i1 [[TMP11]], label [[VPLANNEDBB4]], label [[VPLANNEDBB9:%.*]]
; CHECK:       VPlannedBB9:
; CHECK-NEXT:    br label [[VPLANNEDBB10:%.*]]
; CHECK:       VPlannedBB10:
; CHECK-NEXT:    [[TMP12:%.*]] = icmp sgt <2 x i32> [[WIDE_LOAD]], <i32 42, i32 42>
; CHECK-NEXT:    br label [[ALL_ZERO_BYPASS_END44]]
; CHECK:       all.zero.bypass.end44:
; CHECK-NEXT:    [[UNI_PHI11:%.*]] = phi ptr addrspace(4) [ [[TMP4]], [[VPLANNEDBB10]] ], [ null, [[ALL_ZERO_BYPASS_BEGIN42]] ]
; CHECK-NEXT:    [[VEC_PHI12:%.*]] = phi <2 x i1> [ [[TMP12]], [[VPLANNEDBB10]] ], [ zeroinitializer, [[ALL_ZERO_BYPASS_BEGIN42]] ]
; CHECK-NEXT:    br label [[VPLANNEDBB13:%.*]]
; CHECK:       VPlannedBB13:
; CHECK-NEXT:    [[TMP13:%.*]] = select <2 x i1> [[TMP1]], <2 x i1> [[VEC_PHI12]], <2 x i1> zeroinitializer
; CHECK-NEXT:    br label [[VPLANNEDBB14:%.*]]
; CHECK:       VPlannedBB14:
; CHECK-NEXT:    call void @llvm.masked.store.v2i8.p4(<2 x i8> <i8 124, i8 124>, ptr addrspace(4) [[UNI_PHI11]], i32 1, <2 x i1> [[TMP13]])
; CHECK-NEXT:    br label [[VPLANNEDBB15]]
; CHECK:       VPlannedBB15:
; CHECK-NEXT:    [[TMP14]] = add nuw nsw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP15]] = add nuw nsw i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ult i64 [[TMP15]], 1024
; CHECK-NEXT:    br i1 [[TMP16]], label [[VECTOR_BODY:%.*]], label [[VPLANNEDBB16:%.*]], !llvm.loop [[LOOP2:![0-9]+]]
;
omp.inner.for.body.lr.ph:
  %arr.priv = alloca [275 x i8], align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %omp.inner.for.body.lr.ph
%entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE:TYPED"(ptr %arr.priv, i8 0, i32 275)]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %iv.next, %skip.inner ]
  %idx = getelementptr inbounds i32, ptr %a, i64 %iv
  %ld = load i32, ptr %idx, align 4
  %cond = icmp sgt i32 %ld, 3
  br i1 %cond, label %inner.ph, label %skip.inner

inner.ph:
  %gep = getelementptr inbounds [275 x i8], ptr %arr.priv, i64 0, i64 23
  store i8 123, ptr %gep
  %ascast = addrspacecast ptr %gep to ptr addrspace(4)
  br label %inner.loop

inner.loop:
  %inner.iv = phi i64 [ 0, %inner.ph ], [ %inner.iv.next, %inner.loop ]
  %inner.iv.next = add nuw nsw i64 %inner.iv, 1
  %inner.cmp = icmp ult i64 %inner.iv.next, 125
  br i1 %inner.cmp, label %inner.loop, label %inner.exit

inner.exit:
  br label %store.check.bb

store.check.bb:
  %check = icmp sgt i32 %ld, 42
  br i1 %check, label %liveout.use.bb, label %skip.inner

liveout.use.bb:
  store i8 124, ptr addrspace(4) %ascast
  br label %skip.inner

skip.inner:
  %iv.next = add nuw nsw i64 %iv, 1
  %cmp = icmp ult i64 %iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token %0)
