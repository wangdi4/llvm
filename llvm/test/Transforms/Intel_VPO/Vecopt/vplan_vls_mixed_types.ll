; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -S -vplan-vec < %s | FileCheck %s
; REQUIRES: asserts

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.test_00 = type { i32, float }

;  for (i = 0; i < 1024; i += 1) {
;    dst[i] = src[i];
;  }
;
define void @test_00(%struct.test_00* nocapture %src, %struct.test_00* nocapture %dst) {
; CHECK-LABEL: @test_00(
; CHECK:         [[SCALAR_GEP:%.*]] = getelementptr inbounds [[STRUCT_TEST_00:%.*]], %struct.test_00* [[SRC:%.*]], i64 [[UNI_PHI:%.*]], i32 0
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[SCALAR_GEP]] to <8 x i32>*
; CHECK-NEXT:    [[VLS_LOAD:%.*]] = load <8 x i32>, <8 x i32>* [[TMP0]], align 4
; CHECK-NEXT:    [[VP_VAL_FST:%.*]] = shufflevector <8 x i32> [[VLS_LOAD]], <8 x i32> [[VLS_LOAD]], <4 x i32> <i32 0, i32 2, i32 4, i32 6>
; CHECK-NEXT:    [[TMP1:%.*]] = shufflevector <8 x i32> [[VLS_LOAD]], <8 x i32> [[VLS_LOAD]], <4 x i32> <i32 1, i32 3, i32 5, i32 7>
; CHECK-NEXT:    [[VP_VAL_SND:%.*]] = bitcast <4 x i32> [[TMP1]] to <4 x float>
; CHECK-NEXT:    [[SCALAR_GEP3:%.*]] = getelementptr inbounds [[STRUCT_TEST_00]], %struct.test_00* [[DST:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[EXTENDED_:%.*]] = shufflevector <4 x i32> [[VP_VAL_FST]], <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP2:%.*]] = shufflevector <8 x i32> undef, <8 x i32> [[EXTENDED_]], <8 x i32> <i32 8, i32 1, i32 9, i32 3, i32 10, i32 5, i32 11, i32 7>
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x float> [[VP_VAL_SND]] to <4 x i32>
; CHECK-NEXT:    [[EXTENDED_4:%.*]] = shufflevector <4 x i32> [[TMP3]], <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> [[TMP2]], <8 x i32> [[EXTENDED_4]], <8 x i32> <i32 0, i32 8, i32 2, i32 9, i32 4, i32 10, i32 6, i32 11>
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32* [[SCALAR_GEP3]] to <8 x i32>*
; CHECK-NEXT:    store <8 x i32> [[TMP4]], <8 x i32>* [[TMP5]], align 4
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]

  %ptr.src.fst = getelementptr inbounds %struct.test_00, %struct.test_00* %src, i64 %indvars.iv, i32 0
  %ptr.src.snd = getelementptr inbounds %struct.test_00, %struct.test_00* %src, i64 %indvars.iv, i32 1

  %val.fst = load i32, i32* %ptr.src.fst, align 4
  %val.snd = load float, float* %ptr.src.snd, align 4

  %ptr.dst.fst = getelementptr inbounds %struct.test_00, %struct.test_00* %dst, i64 %indvars.iv, i32 0
  %ptr.dst.snd = getelementptr inbounds %struct.test_00, %struct.test_00* %dst, i64 %indvars.iv, i32 1

  store i32 %val.fst, i32* %ptr.dst.fst, align 4
  store float %val.snd, float* %ptr.dst.snd, align 4

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

;  for (i = 0; i < 1024; i += 1) {
;    { t0, t1 } = src[i];
;    dst[i] = { t0 + 42, t1 * t1 };
;  }
;
%struct.test_01 = type { i32, float }
define void @test_01(%struct.test_01* nocapture %src, %struct.test_01* nocapture %dst) {
; CHECK-LABEL: @test_01(
; CHECK:         [[SCALAR_GEP:%.*]] = getelementptr inbounds [[STRUCT_TEST_01:%.*]], %struct.test_01* [[SRC:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[SCALAR_GEP]] to <8 x i32>*
; CHECK-NEXT:    [[VLS_LOAD:%.*]] = load <8 x i32>, <8 x i32>* [[TMP0]], align 4
; CHECK-NEXT:    [[VP_VAL_SRC_FST:%.*]] = shufflevector <8 x i32> [[VLS_LOAD]], <8 x i32> [[VLS_LOAD]], <4 x i32> <i32 0, i32 2, i32 4, i32 6>
; CHECK-NEXT:    [[TMP1:%.*]] = shufflevector <8 x i32> [[VLS_LOAD]], <8 x i32> [[VLS_LOAD]], <4 x i32> <i32 1, i32 3, i32 5, i32 7>
; CHECK-NEXT:    [[VP_VAL_SRC_SND:%.*]] = bitcast <4 x i32> [[TMP1]] to <4 x float>
; CHECK-NEXT:    [[TMP2:%.*]] = add nuw nsw <4 x i32> [[VP_VAL_SRC_FST]], <i32 42, i32 42, i32 42, i32 42>
; CHECK-NEXT:    [[TMP3:%.*]] = fmul fast <4 x float> [[VP_VAL_SRC_SND]], [[VP_VAL_SRC_SND]]
; CHECK-NEXT:    [[SCALAR_GEP3:%.*]] = getelementptr inbounds [[STRUCT_TEST_01]], %struct.test_01* [[DST:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[EXTENDED_:%.*]] = shufflevector <4 x i32> [[TMP2]], <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x i32> undef, <8 x i32> [[EXTENDED_]], <8 x i32> <i32 8, i32 1, i32 9, i32 3, i32 10, i32 5, i32 11, i32 7>
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast <4 x float> [[TMP3]] to <4 x i32>
; CHECK-NEXT:    [[EXTENDED_4:%.*]] = shufflevector <4 x i32> [[TMP5]], <4 x i32> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x i32> [[TMP4]], <8 x i32> [[EXTENDED_4]], <8 x i32> <i32 0, i32 8, i32 2, i32 9, i32 4, i32 10, i32 6, i32 11>
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast i32* [[SCALAR_GEP3]] to <8 x i32>*
; CHECK-NEXT:    store <8 x i32> [[TMP6]], <8 x i32>* [[TMP7]], align 4
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]

  %ptr.src.fst = getelementptr inbounds %struct.test_01, %struct.test_01* %src, i64 %indvars.iv, i32 0
  %ptr.src.snd = getelementptr inbounds %struct.test_01, %struct.test_01* %src, i64 %indvars.iv, i32 1

  %val.src.fst = load i32, i32* %ptr.src.fst, align 4
  %val.src.snd = load float, float* %ptr.src.snd, align 4

  %val.dst.fst = add nuw nsw i32 %val.src.fst, 42
  %val.dst.snd = fmul fast float %val.src.snd, %val.src.snd

  %ptr.dst.fst = getelementptr inbounds %struct.test_01, %struct.test_01* %dst, i64 %indvars.iv, i32 0
  %ptr.dst.snd = getelementptr inbounds %struct.test_01, %struct.test_01* %dst, i64 %indvars.iv, i32 1

  store i32 %val.dst.fst, i32* %ptr.dst.fst, align 4
  store float %val.dst.snd, float* %ptr.dst.snd, align 4

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}


;  for (i = 0; i < 1024; i += 1) {
;    { t0, t1 } = src[i];
;    dst[i] = { t0 + <11, 17>, t1 * t1 };
;  }
;
%struct.test_02 = type { <2 x i32>, double }
define void @test_02(%struct.test_02* nocapture %src, %struct.test_02* nocapture %dst) {
; CHECK-LABEL: @test_02(
; CHECK:         [[SCALAR_GEP:%.*]] = getelementptr inbounds [[STRUCT_TEST_02:%.*]], %struct.test_02* [[SRC:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i32>* [[SCALAR_GEP]] to <8 x double>*
; CHECK-NEXT:    [[VLS_LOAD:%.*]] = load <8 x double>, <8 x double>* [[TMP0]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = shufflevector <8 x double> [[VLS_LOAD]], <8 x double> [[VLS_LOAD]], <4 x i32> <i32 0, i32 2, i32 4, i32 6>
; CHECK-NEXT:    [[VP_VAL_SRC_FST:%.*]] = bitcast <4 x double> [[TMP1]] to <8 x i32>
; CHECK-NEXT:    [[VP_VAL_SRC_SND:%.*]] = shufflevector <8 x double> [[VLS_LOAD]], <8 x double> [[VLS_LOAD]], <4 x i32> <i32 1, i32 3, i32 5, i32 7>
; CHECK-NEXT:    [[TMP2:%.*]] = add nuw nsw <8 x i32> [[VP_VAL_SRC_FST]], <i32 11, i32 17, i32 11, i32 17, i32 11, i32 17, i32 11, i32 17>
; CHECK-NEXT:    [[TMP3:%.*]] = fmul fast <4 x double> [[VP_VAL_SRC_SND]], [[VP_VAL_SRC_SND]]
; CHECK-NEXT:    [[SCALAR_GEP3:%.*]] = getelementptr inbounds [[STRUCT_TEST_02]], %struct.test_02* [[DST:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP2]] to <4 x double>
; CHECK-NEXT:    [[EXTENDED_:%.*]] = shufflevector <4 x double> [[TMP4]], <4 x double> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <8 x double> undef, <8 x double> [[EXTENDED_]], <8 x i32> <i32 8, i32 1, i32 9, i32 3, i32 10, i32 5, i32 11, i32 7>
; CHECK-NEXT:    [[EXTENDED_4:%.*]] = shufflevector <4 x double> [[TMP3]], <4 x double> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x double> [[TMP5]], <8 x double> [[EXTENDED_4]], <8 x i32> <i32 0, i32 8, i32 2, i32 9, i32 4, i32 10, i32 6, i32 11>
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast <2 x i32>* [[SCALAR_GEP3]] to <8 x double>*
; CHECK-NEXT:    store <8 x double> [[TMP6]], <8 x double>* [[TMP7]], align 4
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]

  %ptr.src.fst = getelementptr inbounds %struct.test_02, %struct.test_02* %src, i64 %indvars.iv, i32 0
  %ptr.src.snd = getelementptr inbounds %struct.test_02, %struct.test_02* %src, i64 %indvars.iv, i32 1

  %val.src.fst = load <2 x i32>, <2 x i32>* %ptr.src.fst, align 4
  %val.src.snd = load double, double* %ptr.src.snd, align 4

  %val.dst.fst = add nuw nsw <2 x i32> %val.src.fst, <i32 11, i32 17>
  %val.dst.snd = fmul fast double %val.src.snd, %val.src.snd

  %ptr.dst.fst = getelementptr inbounds %struct.test_02, %struct.test_02* %dst, i64 %indvars.iv, i32 0
  %ptr.dst.snd = getelementptr inbounds %struct.test_02, %struct.test_02* %dst, i64 %indvars.iv, i32 1

  store <2 x i32> %val.dst.fst, <2 x i32>* %ptr.dst.fst, align 4
  store double %val.dst.snd, double* %ptr.dst.snd, align 4

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}


;  for (i = 0; i < 1024; i += 1) {
;    { t0, t1 } = src[i];
;    dst[i] = { t0 * t0, t1 + <11, 17> };
;  }
;
%struct.test_03 = type { double, <2 x i32> }
define void @test_03(%struct.test_03* nocapture %src, %struct.test_03* nocapture %dst) {
; CHECK-LABEL: @test_03(
; CHECK:         [[SCALAR_GEP:%.*]] = getelementptr inbounds [[STRUCT_TEST_03:%.*]], %struct.test_03* [[SRC:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast double* [[SCALAR_GEP]] to <8 x double>*
; CHECK-NEXT:    [[VLS_LOAD:%.*]] = load <8 x double>, <8 x double>* [[TMP0]], align 4
; CHECK-NEXT:    [[VP_VAL_SRC_FST:%.*]] = shufflevector <8 x double> [[VLS_LOAD]], <8 x double> [[VLS_LOAD]], <4 x i32> <i32 0, i32 2, i32 4, i32 6>
; CHECK-NEXT:    [[TMP1:%.*]] = shufflevector <8 x double> [[VLS_LOAD]], <8 x double> [[VLS_LOAD]], <4 x i32> <i32 1, i32 3, i32 5, i32 7>
; CHECK-NEXT:    [[VP_VAL_SRC_SND:%.*]] = bitcast <4 x double> [[TMP1]] to <8 x i32>
; CHECK-NEXT:    [[TMP2:%.*]] = fmul fast <4 x double> [[VP_VAL_SRC_FST]], [[VP_VAL_SRC_FST]]
; CHECK-NEXT:    [[TMP3:%.*]] = add nuw nsw <8 x i32> [[VP_VAL_SRC_SND]], <i32 11, i32 17, i32 11, i32 17, i32 11, i32 17, i32 11, i32 17>
; CHECK-NEXT:    [[SCALAR_GEP3:%.*]] = getelementptr inbounds [[STRUCT_TEST_03]], %struct.test_03* [[DST:%.*]], i64 [[UNI_PHI]], i32 0
; CHECK-NEXT:    [[EXTENDED_:%.*]] = shufflevector <4 x double> [[TMP2]], <4 x double> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <8 x double> undef, <8 x double> [[EXTENDED_]], <8 x i32> <i32 8, i32 1, i32 9, i32 3, i32 10, i32 5, i32 11, i32 7>
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i32> [[TMP3]] to <4 x double>
; CHECK-NEXT:    [[EXTENDED_4:%.*]] = shufflevector <4 x double> [[TMP5]], <4 x double> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[TMP6:%.*]] = shufflevector <8 x double> [[TMP4]], <8 x double> [[EXTENDED_4]], <8 x i32> <i32 0, i32 8, i32 2, i32 9, i32 4, i32 10, i32 6, i32 11>
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[SCALAR_GEP3]] to <8 x double>*
; CHECK-NEXT:    store <8 x double> [[TMP6]], <8 x double>* [[TMP7]], align 4
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]

  %ptr.src.fst = getelementptr inbounds %struct.test_03, %struct.test_03* %src, i64 %indvars.iv, i32 0
  %ptr.src.snd = getelementptr inbounds %struct.test_03, %struct.test_03* %src, i64 %indvars.iv, i32 1

  %val.src.fst = load double, double* %ptr.src.fst, align 4
  %val.src.snd = load <2 x i32>, <2 x i32>* %ptr.src.snd, align 4

  %val.dst.fst = fmul fast double %val.src.fst, %val.src.fst
  %val.dst.snd = add nuw nsw <2 x i32> %val.src.snd, <i32 11, i32 17>

  %ptr.dst.fst = getelementptr inbounds %struct.test_03, %struct.test_03* %dst, i64 %indvars.iv, i32 0
  %ptr.dst.snd = getelementptr inbounds %struct.test_03, %struct.test_03* %dst, i64 %indvars.iv, i32 1

  store double %val.dst.fst, double* %ptr.dst.fst, align 4
  store <2 x i32> %val.dst.snd, <2 x i32>* %ptr.dst.snd, align 4

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
