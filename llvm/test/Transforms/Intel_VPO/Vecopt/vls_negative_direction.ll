; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -S -vplan-vec -disable-output -vplan-print-after-vls < %s | FileCheck %s

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%S1 = type { i64, i64, i64 }

define void @foo(%S1 *%p) {
; CHECK-LABEL:  VPlan after VPlan-to-VPlan VLS transformation:
; CHECK:            [DA: Div] i64 [[VP_IV:%.*]] = phi  [ i64 [[VP_IV_IND_INIT:%.*]], vector.ph ],  [ i64 [[VP_IV_NEXT:%.*]], [[BB2:.*]] ]
; CHECK-NEXT:       [DA: Div] i64 [[VP_NEG:%.*]] = sub i64 0 i64 [[VP_IV]]
; CHECK-NEXT:       [DA: Div] i64* [[VP_P0:%.*]] = getelementptr inbounds %S1* [[P0:%.*]] i64 [[VP_NEG]] i32 0
; CHECK-NEXT:       [DA: Div] i64* [[VP_P0_REVERSE_ADJUST:%.*]] = getelementptr i64* [[VP_P0]] i64 -9
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_LOAD:%.*]] = vls-load i64* [[VP_P0_REVERSE_ADJUST]], group_size=3, align=8
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_LOAD_REVERSE:%.*]] = shufflevector <16 x i64> [[VP_VLS_LOAD]] <16 x i64> [[VP_VLS_LOAD]] <16 x i64> <i64 9, i64 10, i64 11, i64 6, i64 7, i64 8, i64 3, i64 4, i64 5, i64 0, i64 1, i64 2, i64 undef, i64 undef, i64 undef, i64 undef>
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD0:%.*]] = vls-extract <16 x i64> [[VP_VLS_LOAD_REVERSE]], group_size=3, offset=0
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD1:%.*]] = vls-extract <16 x i64> [[VP_VLS_LOAD_REVERSE]], group_size=3, offset=1
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD2:%.*]] = vls-extract <16 x i64> [[VP_VLS_LOAD_REVERSE]], group_size=3, offset=2
; CHECK-NEXT:       [DA: Div] i64 [[VP_ADD:%.*]] = add i64 [[VP_LD0]] i64 [[VP_LD1]]
; CHECK-NEXT:       [DA: Div] i64 [[VP_SUB:%.*]] = sub i64 [[VP_LD0]] i64 [[VP_LD1]]
; CHECK-NEXT:       [DA: Div] i64 [[VP_MUL:%.*]] = mul i64 [[VP_LD1]] i64 [[VP_LD2]]
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_INSERT:%.*]] = vls-insert <16 x i64> undef i64 [[VP_ADD]], group_size=3, offset=0
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_INSERT_1:%.*]] = vls-insert <16 x i64> [[VP_VLS_INSERT]] i64 [[VP_SUB]], group_size=3, offset=1
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_INSERT_2:%.*]] = vls-insert <16 x i64> [[VP_VLS_INSERT_1]] i64 [[VP_MUL]], group_size=3, offset=2
; CHECK-NEXT:       [DA: Uni] <16 x i64> [[VP_VLS_INSERT_REVERSE:%.*]] = shufflevector <16 x i64> [[VP_VLS_INSERT_2]] <16 x i64> [[VP_VLS_INSERT_2]] <16 x i64> <i64 9, i64 10, i64 11, i64 6, i64 7, i64 8, i64 3, i64 4, i64 5, i64 0, i64 1, i64 2, i64 undef, i64 undef, i64 undef, i64 undef>
; CHECK-NEXT:       [DA: Div] i64* [[VP_P0_REVERSE_ADJUST_1:%.*]] = getelementptr i64* [[VP_P0]] i64 -9
; CHECK-NEXT:       [DA: Div] vls-store <16 x i64> [[VP_VLS_INSERT_REVERSE]] i64* [[VP_P0_REVERSE_ADJUST_1]], group_size=3, align=8
; CHECK-NEXT:       [DA: Div] i64 [[VP_IV_NEXT]] = add i64 [[VP_IV]] i64 [[VP_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:       [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp uge i64 [[VP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT:%.*]]
; CHECK-NEXT:       [DA: Uni] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB3:BB[0-9]+]], [[BB2]]
;
entry:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %header

header:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %header ]

  %neg = sub i64 0, %iv
  %p0 = getelementptr inbounds %S1, %S1* %p, i64 %neg, i32 0
  %p1 = getelementptr inbounds %S1, %S1* %p, i64 %neg, i32 1
  %p2 = getelementptr inbounds %S1, %S1* %p, i64 %neg, i32 2

  %ld0 = load i64, i64 *%p0
  %ld1 = load i64, i64 *%p1
  %ld2 = load i64, i64 *%p2

  %add = add i64 %ld0, %ld1
  %sub = sub i64 %ld0, %ld1
  %mul = mul i64 %ld1, %ld2

  store i64 %add, i64 *%p0
  store i64 %sub, i64 *%p1
  store i64 %mul, i64 *%p2

  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 128
  br i1 %exitcond, label %exit, label %header

exit:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

%S2 = type { <2 x i32>, i64 }

define void @test_vec(%S2 *%p) {
; CHECK-LABEL:  VPlan after VPlan-to-VPlan VLS transformation:
; CHECK-NEXT:  VPlan IR for: test_vec:header
; CHECK:            [DA: Div] i64 [[VP_IV:%.*]] = phi  [ i64 [[VP_IV_IND_INIT:%.*]], vector.ph ],  [ i64 [[VP_IV_NEXT:%.*]], [[BB2:.*]] ]
; CHECK-NEXT:       [DA: Div] i64 [[VP_NEG:%.*]] = sub i64 0 i64 [[VP_IV]]
; CHECK-NEXT:       [DA: Div] <2 x i32>* [[VP_P0:%.*]] = getelementptr inbounds %S2* [[P0:%.*]] i64 [[VP_NEG]] i32 0
; CHECK-NEXT:       [DA: Div] <2 x i32>* [[VP_P0_REVERSE_ADJUST:%.*]] = getelementptr <2 x i32>* [[VP_P0]] i64 -1
; CHECK-NEXT:       [DA: Uni] <4 x i64> [[VP_VLS_LOAD:%.*]] = vls-load <2 x i32>* [[VP_P0_REVERSE_ADJUST]], group_size=2, align=8
; CHECK-NEXT:       [DA: Uni] <4 x i64> [[VP_VLS_LOAD_REVERSE:%.*]] = shufflevector <4 x i64> [[VP_VLS_LOAD]] <4 x i64> [[VP_VLS_LOAD]] <4 x i64> <i64 2, i64 3, i64 0, i64 1>
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD0:%.*]] = vls-extract <4 x i64> [[VP_VLS_LOAD_REVERSE]], group_size=2, offset=0
; CHECK-NEXT:       [DA: Div] <2 x i32> [[VP0:%.*]] = bitcast i64 [[VP_LD0]]
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD1:%.*]] = vls-extract <4 x i64> [[VP_VLS_LOAD_REVERSE]], group_size=2, offset=1
; CHECK-NEXT:       [DA: Div] i64 [[VP_IV_NEXT]] = add i64 [[VP_IV]] i64 [[VP_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:       [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp uge i64 [[VP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT:%.*]]
; CHECK-NEXT:       [DA: Uni] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB3:BB[0-9]+]], [[BB2]]
;
entry:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 2) ]
  br label %header

header:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %header ]

  %neg = sub i64 0, %iv
  %p0 = getelementptr inbounds %S2, %S2* %p, i64 %neg, i32 0
  %p1 = getelementptr inbounds %S2, %S2* %p, i64 %neg, i32 1

  %ld0 = load <2 x i32>, <2 x i32> *%p0
  %ld1 = load i64, i64 *%p1

  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 128
  br i1 %exitcond, label %exit, label %header

exit:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
