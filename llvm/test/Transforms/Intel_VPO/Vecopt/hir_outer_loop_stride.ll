; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -hir-ssa-deconstruction -hir-framework -hir-vplan-vec -vplan-print-after-all-zero-bypass -disable-output -enable-vplan-outer-loop-hir < %s 2>&1 | FileCheck %s
; RUN: opt -passes="hir-ssa-deconstruction,hir-vplan-vec" -vplan-print-after-all-zero-bypass -disable-output -enable-vplan-outer-loop-hir < %s 2>&1 | FileCheck %s
;
; LIT test to demonstrate issue with stride information being calculated
; incorrectly when using underlying HIR. Incoming HIR is shown below.
; Note that the store is not unit strided at the i1 loop level that we are
; trying to vectorize at. However, IR based stride information improvement
; works for inner loops only and we incorrectly conclude that the access
; is unit strided(which is true at the inner loop level). SVA results as
; a result show only first scalar value as required for the subscript
; instruction.
;
;     BEGIN REGION { }
;           %tok = @llvm.directive.region.entry(); [ DIR.OMP.SIMD() ]
;
;          + DO i1 = 0, 127, 1   <DO_LOOP> <simd>
;          |   + DO i2 = 0, 127, 1   <DO_LOOP>
;          |   |   (@arr)[0][i1][i2] = i1 + i2;
;          |   + END LOOP
;          + END LOOP
;
;          @llvm.directive.region.exit(%tok); [ DIR.OMP.END.SIMD() ]
;          ret ;
;     END REGION
;
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@arr = dso_local local_unnamed_addr global [128 x [128 x i64]] zeroinitializer, align 16

; Function Attrs: nofree norecurse nosync nounwind uwtable writeonly
define dso_local void @foo() local_unnamed_addr #0 {
; CHECK-LABEL:  VPlan after all zero bypass insertion:
; CHECK-NEXT:  VPlan IR for: Initial VPlan for VF=2
; CHECK-NEXT:  External Defs Start:
; CHECK-DAG:     [[VP0:%.*]] = {@arr}
; CHECK-NEXT:  External Defs End:
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 127, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP1:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP2:%.*]], [[BB3]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB5:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB5]]: # preds: [[BB4]], [[BB5]]
; CHECK-NEXT:     [DA: Uni, SVA: (FV )] i64 [[VP3:%.*]] = phi  [ i64 0, [[BB4]] ],  [ i64 [[VP4:%.*]], [[BB5]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP5:%.*]] = add i64 [[VP1]] i64 [[VP3]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_SUBSCRIPT:%.*]] = subscript inbounds [128 x [128 x i64]]* @arr i64 0 i64 [[VP1]] i64 [[VP3]] (SVAOpBits 0->V 1->V 2->V 3->V 4->V 5->V 6->V 7->V 8->V 9->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i64 [[VP5]] i64* [[VP_SUBSCRIPT]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP4]] = add i64 [[VP3]] i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP6:%.*]] = icmp sle i64 [[VP4]] i64 127 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP6]], [[BB5]], [[BB3]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB5]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP2]] = add i64 [[VP1]] i64 [[VP__IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP7:%.*]] = icmp sle i64 [[VP2]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP7]], [[BB2]], [[BB6:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB6]]: # preds: [[BB3]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB7:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB7]]: # preds: [[BB6]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br <External Block> (SVAOpBits )
; CHECK-EMPTY:
; CHECK-NEXT:  External Uses:
; CHECK-NEXT:  Id: 0   no underlying for i64 [[VP__IND_FINAL]]
;
entry:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.cond1.preheader

for.cond1.preheader:                              ; preds = %entry, %for.inc5
  %l1.017 = phi i64 [ 0, %entry ], [ %inc6, %for.inc5 ]
  br label %for.body3

for.body3:                                        ; preds = %for.cond1.preheader, %for.body3
  %l2.016 = phi i64 [ 0, %for.cond1.preheader ], [ %inc, %for.body3 ]
  %add = add nuw nsw i64 %l2.016, %l1.017
  %arrayidx4 = getelementptr inbounds [128 x [128 x i64]], [128 x [128 x i64]]* @arr, i64 0, i64 %l1.017, i64 %l2.016
  store i64 %add, i64* %arrayidx4, align 8
  %inc = add nuw nsw i64 %l2.016, 1
  %exitcond.not = icmp eq i64 %inc, 128
  br i1 %exitcond.not, label %for.inc5, label %for.body3

for.inc5:                                         ; preds = %for.body3
  %inc6 = add nuw nsw i64 %l1.017, 1
  %exitcond18.not = icmp eq i64 %inc6, 128
  br i1 %exitcond18.not, label %for.end7, label %for.cond1.preheader

for.end7:                                         ; preds = %for.inc5
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
