; Test that masked private temps are undef initialized every vector loop iteration in HIR vector CG.
; We also check that no backedge flow value is introduced for these temps when they are lowered to
; LLVM-IR.

; HIR before vectorizer
; BEGIN REGION { }
;       %entry.region = @llvm.directive.region.entry(); [ DIR.VPO.AUTO.VEC() ]
;
;       + DO i1 = 0, 1023, 1   <DO_LOOP>
;       |   %0 = (@a)[0][i1];
;       |   if (%0 * i1 + %constant >= (@b)[0][i1])
;       |   {
;       |      %3 = (@c)[0][i1];
;       |      if (i1 + -1 * %constant + 2 * %3 < (@d)[0][i1])
;       |      {
;       |         (@e)[0][i1] = %0 * i1 + 2 * %3;
;       |      }
;       |   }
;       + END LOOP
;
;       @llvm.directive.region.exit(%entry.region); [ DIR.VPO.END.AUTO.VEC() ]
; END REGION

; RUN: opt -hir-ssa-deconstruction -hir-temp-cleanup -hir-vec-dir-insert -VPlanDriverHIR -vplan-force-vf=4 -print-after=VPlanDriverHIR -hir-cg -sroa -enable-vp-value-codegen-hir=0 -S < %s 2>&1 | FileCheck %s --check-prefixes=CHECK,IRCHECK
; RUN: opt -hir-ssa-deconstruction -hir-temp-cleanup -hir-vec-dir-insert -VPlanDriverHIR -vplan-force-vf=4 -print-after=VPlanDriverHIR -hir-cg -sroa -enable-vp-value-codegen-hir -S < %s 2>&1 | FileCheck %s --check-prefixes=VPCHECK,IRCHECK

; Check HIR generated by VPlan.
; CHECK-LABEL:    BEGIN REGION { modified }
; CHECK-NEXT:     + DO i1 = 0, 1023, 4   <DO_LOOP> <novectorize>
; CHECK-NEXT:     |   %wide.cmp.2 = undef;
; CHECK-NEXT:     |   %.vec1 = undef;
; CHECK-NEXT:     |   %.vec = (<4 x i32>*)(@a)[0][i1];
; CHECK-NEXT:     |   %.BlobMul = %.vec  *  <i32 0, i32 1, i32 2, i32 3>;
; CHECK-NEXT:     |   %wide.cmp. = %.vec * i1 + %constant + %.BlobMul >= (<4 x i32>*)(@b)[0][i1];
; CHECK-NEXT:     |   %.vec1 = (<4 x i32>*)(@c)[0][i1]; Mask = @{%wide.cmp.}
; CHECK-NEXT:     |   %wide.cmp.2 = i1 + -1 * %constant + <i32 0, i32 1, i32 2, i32 3> + 2 * %.vec1 < (<4 x i32>*)(@d)[0][i1]; Mask = @{%wide.cmp.}
; CHECK-NEXT:     |   %.vec4 = %wide.cmp.  &  %wide.cmp.2;
; CHECK-NEXT:     |   %.BlobMul5 = %.vec  *  <i32 0, i32 1, i32 2, i32 3>;
; CHECK-NEXT:     |   (<4 x i32>*)(@e)[0][i1] = %.vec * i1 + 2 * %.vec1 + %.BlobMul5; Mask = @{%.vec4}
; CHECK-NEXT:     + END LOOP

; Checks for VPValue based code generation.
; VPCHECK:        + DO i1 = 0, 1023, 4   <DO_LOOP> <novectorize>
; VPCHECK-NEXT:   |   %.vec8 = undef;
; VPCHECK-NEXT:   |   %.vec4 = undef;
; VPCHECK-NEXT:   |   %.vec = (<4 x i32>*)(@a)[0][i1];
; VPCHECK-NEXT:   |   %.vec1 = %.vec  *  i1 + <i64 0, i64 1, i64 2, i64 3>;
; VPCHECK-NEXT:   |   %.vec2 = (<4 x i32>*)(@b)[0][i1];
; VPCHECK-NEXT:   |   %.vec3 = %constant + %.vec1 >= %.vec2;
; VPCHECK-NEXT:   |   %.vec4 = (<4 x i32>*)(@c)[0][i1]; Mask = @{%.vec3}
; VPCHECK-NEXT:   |   %.vec5 = %constant  *  -1;
; VPCHECK-NEXT:   |   %.vec6 = %.vec4  *  2;
; VPCHECK-NEXT:   |   %.vec7 = %.vec5 + %.vec6  +  i1 + <i64 0, i64 1, i64 2, i64 3>;
; VPCHECK-NEXT:   |   %.vec8 = (<4 x i32>*)(@d)[0][i1]; Mask = @{%.vec3}
; VPCHECK-NEXT:   |   %.vec10 = %.vec7 < %.vec8;
; VPCHECK-NEXT:   |   %.vec11 = %.vec3  &  %.vec10;
; VPCHECK-NEXT:   |   %.vec12 = %.vec4  *  2;
; VPCHECK-NEXT:   |   %.vec13 = %.vec  *  i1 + <i64 0, i64 1, i64 2, i64 3>;
; VPCHECK-NEXT:   |   (<4 x i32>*)(@e)[0][i1] = %.vec12 + %.vec13; Mask = @{%.vec11}
; VPCHECK-NEXT:   + END LOOP

; Check LLVM-IR after HIR CG and SROA to ensure no PHIs are added for masked temps.
; IRCHECK-LABEL: void @foo(i32 %constant)
; IRCHECK:       loop.40:
; IRCHECK-NEXT:    [[LOOP_IV:%.*]] = phi i64 [ 0, %region.0 ], [ [[IV_UPDATE:%.*]], %loop.40 ]
; IRCHECK-NOT:     phi <4 x i32>
; IRCHECK-NOT:     phi <4 x i1>


target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@a = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
@b = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
@c = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
@d = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
@e = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16

; Function Attrs: nofree norecurse nounwind uwtable
define dso_local void @foo(i32 %constant) local_unnamed_addr {
entry:
  br label %for.body

for.body:                                         ; preds = %if.end16, %entry
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %if.end16 ]
  %arrayidx = getelementptr inbounds [1024 x i32], [1024 x i32]* @a, i64 0, i64 %indvars.iv
  %0 = load i32, i32* %arrayidx, align 4
  %1 = trunc i64 %indvars.iv to i32
  %mul = mul nsw i32 %0, %1
  %add = add nsw i32 %mul, %constant
  %arrayidx2 = getelementptr inbounds [1024 x i32], [1024 x i32]* @b, i64 0, i64 %indvars.iv
  %2 = load i32, i32* %arrayidx2, align 4
  %cmp3 = icmp slt i32 %add, %2
  br i1 %cmp3, label %if.end16, label %if.then

if.then:                                          ; preds = %for.body
  %arrayidx5 = getelementptr inbounds [1024 x i32], [1024 x i32]* @c, i64 0, i64 %indvars.iv
  %3 = load i32, i32* %arrayidx5, align 4
  %mul6 = shl nsw i32 %3, 1
  %add7 = add nsw i32 %mul6, %1
  %sub = sub nsw i32 %add7, %constant
  %arrayidx9 = getelementptr inbounds [1024 x i32], [1024 x i32]* @d, i64 0, i64 %indvars.iv
  %4 = load i32, i32* %arrayidx9, align 4
  %cmp10 = icmp slt i32 %sub, %4
  br i1 %cmp10, label %if.then11, label %if.end16

if.then11:                                        ; preds = %if.then
  %sub13 = add i32 %mul6, %mul
  %arrayidx15 = getelementptr inbounds [1024 x i32], [1024 x i32]* @e, i64 0, i64 %indvars.iv
  store i32 %sub13, i32* %arrayidx15, align 4
  br label %if.end16

if.end16:                                         ; preds = %if.then, %if.then11, %for.body
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %if.end16
  ret void
}
