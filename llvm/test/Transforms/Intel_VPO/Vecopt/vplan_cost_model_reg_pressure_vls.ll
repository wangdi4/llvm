; INTEL_FEATURE_SW_ADVANCED
; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; REQUIRES: intel_feature_sw_advanced
; RUN: opt < %s -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec \
; RUN:     -mtriple=x86_64-unknown-unknown -mattr=+avx2 -enable-intel-advanced-opts \
; RUN:     -disable-output -vplan-cost-model-print-analysis-for-vf=8 \
; RUN:     -vplan-enable-new-cfg-merge-hir=0 | FileCheck %s
; RUN: opt < %s -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec \
; RUN:     -mtriple=x86_64-unknown-unknown -mattr=+avx2 -enable-intel-advanced-opts \
; RUN:     -disable-output -vplan-cost-model-print-analysis-for-vf=8 \
; RUN:     -vplan-enable-new-cfg-merge-hir=1 | FileCheck %s

define void @test(i8 *%p) local_unnamed_addr {
; CHECK-LABEL:  Cost Model for VPlan test:HIR.#{{[0-9]+}} with VF = 8:
; CHECK-NEXT:  Analyzing VPBasicBlock [[BB0:BB[0-9]+]]
; CHECK-NEXT:    Cost 0 for br [[BB1:BB[0-9]+]]
; CHECK-NEXT:  [[BB0]]: base cost: 0
; CHECK-NEXT:  Analyzing VPBasicBlock [[BB1]]
; CHECK-NEXT:    Cost Unknown for i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1
; CHECK-NEXT:    Cost 0 for i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:    Cost 0 for i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:    Cost 0 for br [[BB2:BB[0-9]+]]
; CHECK-NEXT:  [[BB1]]: base cost: 0
; CHECK-NEXT:  Cost Model for Loop preheader [[BB0]] : [[BB1]] for VF = 8 resulted Cost = 0
; CHECK-NEXT:  Analyzing VPBasicBlock [[BB2]]
; CHECK-NEXT:    Cost Unknown for i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP1:%.*]], [[BB2]] ]
; CHECK-NEXT:    Cost 12 for i64 [[VP2:%.*]] = mul i64 4 i64 [[VP0]]
; CHECK-NEXT:    Cost 1 for i8 [[VP3:%.*]] = trunc i64 [[VP2]] to i8
; CHECK-NEXT:    Cost 0 for i8* [[VP_SUBSCRIPT:%.*]] = subscript i8* [[P0:%.*]] i64 [[VP2]]
; CHECK-NEXT:    Cost 26 for store i8 [[VP3]] i8* [[VP_SUBSCRIPT]] *OVLS*(-26) AdjCost: 0
; CHECK-NEXT:    Cost 2 for i64 [[VP6:%.*]] = add i64 [[VP2]] i64 1
; CHECK-NEXT:    Cost 1 for i8 [[VP7:%.*]] = trunc i64 [[VP6]] to i8
; CHECK-NEXT:    Cost 0 for i8* [[VP_SUBSCRIPT_1:%.*]] = subscript i8* [[P0]] i64 [[VP6]]
; CHECK-NEXT:    Cost 26 for store i8 [[VP7]] i8* [[VP_SUBSCRIPT_1]] *OVLS*(-26) AdjCost: 0
; CHECK-NEXT:    Cost 2 for i64 [[VP11:%.*]] = add i64 [[VP2]] i64 2
; CHECK-NEXT:    Cost 1 for i8 [[VP12:%.*]] = trunc i64 [[VP11]] to i8
; CHECK-NEXT:    Cost 0 for i8* [[VP_SUBSCRIPT_2:%.*]] = subscript i8* [[P0]] i64 [[VP11]]
; CHECK-NEXT:    Cost 26 for store i8 [[VP12]] i8* [[VP_SUBSCRIPT_2]] *OVLS*(-26) AdjCost: 0
; CHECK-NEXT:    Cost 2 for i64 [[VP16:%.*]] = add i64 [[VP2]] i64 3
; CHECK-NEXT:    Cost 1 for i8 [[VP17:%.*]] = trunc i64 [[VP16]] to i8
; CHECK-NEXT:    Cost 0 for i8* [[VP_SUBSCRIPT_3:%.*]] = subscript i8* [[P0]] i64 [[VP16]]
; CHECK-NEXT:    Cost 26 for store i8 [[VP17]] i8* [[VP_SUBSCRIPT_3]] *OVLS*(-21) AdjCost: 5
; CHECK-NEXT:    Cost 2 for i64 [[VP1]] = add i64 [[VP0]] i64 [[VP__IND_INIT_STEP]]
; CHECK-NEXT:    Cost 8 for i1 [[VP20:%.*]] = icmp slt i64 [[VP1]] i64 [[VP_VECTOR_TRIP_COUNT]]
; CHECK-NEXT:    Cost 0 for br i1 [[VP20]], [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:  [[BB2]]: base cost: 37
; CHECK-NEXT:  Base Cost: 37
; CHECK-NEXT:  Analyzing VPBasicBlock [[BB3]]
; CHECK-NEXT:    Cost 0 for i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1
; CHECK-NEXT:    Cost 0 for br [[BB4:BB[0-9]+]]
; CHECK-NEXT:  [[BB3]]: base cost: 0
; CHECK-NEXT:  Analyzing VPBasicBlock [[BB4]]
; CHECK-NEXT:    Cost 0 for br <External Block>
; CHECK-NEXT:  [[BB4]]: base cost: 0
; CHECK-NEXT:  Cost Model for Loop postexit [[BB3]] : [[BB4]] for VF = 8 resulted Cost = 0
;
entry:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %header

header:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %header ]

  %idx0 = mul i64 %iv, 4
  %idx1 = add i64 %idx0, 1
  %idx2 = add i64 %idx0, 2
  %idx3 = add i64 %idx0, 3

  %gep0 = getelementptr i8, i8 *%p, i64 %idx0
  %gep1 = getelementptr i8, i8 *%p, i64 %idx1
  %gep2 = getelementptr i8, i8 *%p, i64 %idx2
  %gep3 = getelementptr i8, i8 *%p, i64 %idx3

  %v0 = trunc i64 %idx0 to i8
  %v1 = trunc i64 %idx1 to i8
  %v2 = trunc i64 %idx2 to i8
  %v3 = trunc i64 %idx3 to i8

  ; Byte scatter isn't legal, so Spill/Fill heuristic used to assume those will
  ; be scalarized when VLS wasn't accounted for.
  store i8 %v0, i8* %gep0
  store i8 %v1, i8* %gep1
  store i8 %v2, i8* %gep2
  store i8 %v3, i8* %gep3



  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 1024
  br i1 %exitcond, label %exit, label %header

exit:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
; end INTEL_FEATURE_SW_ADVANCED
