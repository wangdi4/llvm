; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -enable-new-pm=0 -vplan-vec -vplan-vec-scenario="n0;m4;n0" -print-after=vplan-vec -vplan-enable-peeling -disable-output %s 2>&1 | FileCheck %s
; RUN: opt -passes='vplan-vec' -vplan-vec-scenario="n0;m4;n0" -print-after=vplan-vec -vplan-enable-peeling -disable-output %s 2>&1 | FileCheck %s

target datalayout = "e-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024"
target triple = "x86_64-unknown-linux-gnu"

define void @test_store(i64* nocapture %ary, i32 %c) {
;
; CHECK-LABEL:  Single loop scenario:
; CHECK-NEXT:   MainLoop: masked, VF=4
; CHECK-NEXT:   PeelLoop: none
; CHECK-NEXT:   Remainders: none,
;
; CHECK:  define void @test_store(i64* nocapture [[ARY0:%.*]], i32 [[C0:%.*]]) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[VPLANNEDBB0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT0:%.*]] = insertelement <4 x i32> poison, i32 [[C0]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT0:%.*]] = shufflevector <4 x i32> [[BROADCAST_SPLATINSERT0]], <4 x i32> poison, <4 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VPLANNEDBB10:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB1:
; CHECK-NEXT:    br label [[VECTOR_BODY0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB10]] ], [ [[TMP5:%.*]], [[NEW_LATCH0:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB10]] ], [ [[TMP4:%.*]], [[NEW_LATCH0]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = icmp ult <4 x i64> [[VEC_PHI0]], <i64 1024, i64 1024, i64 1024, i64 1024>
; CHECK-NEXT:    br label [[VPLANNEDBB30:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB3:
; CHECK-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i64, i64* [[ARY0]], i64 [[UNI_PHI0]]
; CHECK-NEXT:    [[TMP1:%.*]] = sext <4 x i32> [[BROADCAST_SPLAT0]] to <4 x i64>
; CHECK-NEXT:    [[TMP2:%.*]] = add <4 x i64> [[TMP1]], [[VEC_PHI0]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i64* [[SCALAR_GEP0]] to <4 x i64>*
; CHECK-NEXT:    call void @llvm.masked.store.v4i64.p0v4i64(<4 x i64> [[TMP2]], <4 x i64>* [[TMP3]], i32 8, <4 x i1> [[TMP0]])
; CHECK-NEXT:    br label [[NEW_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  new_latch:
; CHECK-NEXT:    [[TMP4]] = add nuw nsw <4 x i64> [[VEC_PHI0]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-NEXT:    [[TMP5]] = add nuw nsw i64 [[UNI_PHI0]], 4
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ult <4 x i64> [[TMP4]], <i64 1024, i64 1024, i64 1024, i64 1024>
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x i1> [[TMP6]] to i4
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq i4 [[TMP7]], 0
; CHECK-NEXT:    br i1 [[TMP8]], label [[VPLANNEDBB40:%.*]], label [[VECTOR_BODY0]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB4:
; CHECK-NEXT:    br label [[VPLANNEDBB50:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB5:
; CHECK-NEXT:    br label [[FINAL_MERGE0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  final.merge:
; CHECK-NEXT:    [[UNI_PHI60:%.*]] = phi i64 [ 1024, [[VPLANNEDBB50]] ]
; CHECK-NEXT:    br label [[FOR_END0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.body:
; CHECK-NEXT:    [[INDVARS_IV0:%.*]] = phi i64 [ [[INDVARS_IV_NEXT0:%.*]], [[FOR_BODY0:%.*]] ]
; CHECK-NEXT:    [[PTR0:%.*]] = getelementptr inbounds i64, i64* [[ARY0]], i64 [[INDVARS_IV0]]
; CHECK-NEXT:    [[CC0:%.*]] = sext i32 [[C0]] to i64
; CHECK-NEXT:    [[ADD0:%.*]] = add i64 [[CC0]], [[INDVARS_IV0]]
; CHECK-NEXT:    store i64 [[ADD0]], i64* [[PTR0]], align 8
; CHECK-NEXT:    [[INDVARS_IV_NEXT0]] = add nuw nsw i64 [[INDVARS_IV0]], 1
; CHECK-NEXT:    [[CMP0:%.*]] = icmp ult i64 [[INDVARS_IV_NEXT0]], 1024
; CHECK-NEXT:    br label [[FOR_BODY0]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.end:
; CHECK-NEXT:    ret void
; CHECK-NEXT:  }
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %ptr = getelementptr inbounds i64, i64* %ary, i64 %indvars.iv
  %cc = sext i32 %c to i64
  %add = add i64 %cc, %indvars.iv
  store i64 %add, i64* %ptr, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
