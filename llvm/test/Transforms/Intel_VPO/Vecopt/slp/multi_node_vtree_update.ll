; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=slp-vectorizer -enable-intel-advanced-opts -slp-multinode -mtriple="x86_64" -mcpu=skylake-avx512 -S | FileCheck %s

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"

@A = external global [4 x i64], align 16
@B = external global [4 x i64], align 16
@C = external global [4 x i64], align 16

; Compiler crash test (CMPLRLLVM-39322), requires MultiNode extension enabled.
;
; The issue related to applying MN reordering back to vectorizable tree.
; Here MN is created rooted with %c0.%c1 pair when we process right operands
; of %shB.0...%shB.3 scalars. %c1 gets its opcode reversed which in initial
; design is done via replacing the instruction (with reversed opcode) directly
; on LLVM IR. At that time  operands of  %shC.0.. %shC.3 has already been
; processed and right operand of the shift set form a broadcast node. The crash
; happened because  the node was not updated and still referenced the original
; instruction (which was deleted).

define void @crash_condition(i64 %c) {
; CHECK-LABEL: @crash_condition(
; CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i64> poison, i64 [[C:%.*]], i32 0
; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <2 x i64> [[TMP1]], i64 [[C]], i32 1
; CHECK-NEXT:    [[TMP3:%.*]] = add <2 x i64> [[TMP2]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP4:%.*]] = sub nsw <2 x i64> [[TMP3]], <i64 1, i64 1>
; CHECK-NEXT:    [[SHUFFLE1:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> poison, <4 x i32> <i32 0, i32 1, i32 1, i32 1>
; CHECK-NEXT:    [[TMP5:%.*]] = load <4 x i64>, <4 x i64>* bitcast ([4 x i64]* @C to <4 x i64>*), align 8
; CHECK-NEXT:    [[TMP6:%.*]] = load <4 x i64>, <4 x i64>* bitcast ([4 x i64]* @B to <4 x i64>*), align 8
; CHECK-NEXT:    [[TMP7:%.*]] = shl <4 x i64> [[TMP6]], [[SHUFFLE1]]
; CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> poison, <4 x i32> <i32 1, i32 undef, i32 undef, i32 undef>
; CHECK-NEXT:    [[SHUFFLE:%.*]] = shufflevector <4 x i64> [[TMP8]], <4 x i64> poison, <4 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP9:%.*]] = shl <4 x i64> [[TMP5]], [[SHUFFLE]]
; CHECK-NEXT:    [[TMP10:%.*]] = add <4 x i64> [[TMP9]], [[TMP7]]
; CHECK-NEXT:    store <4 x i64> [[TMP10]], <4 x i64>* bitcast ([4 x i64]* @A to <4 x i64>*), align 8
; CHECK-NEXT:    ret void
;
  %t0 = add i64 %c, 2
  %c0 = sub nsw i64 %t0, 1
  %t1 = add i64 %c, 2
  %c1 = sub nsw i64 %t1, 1

  %ldC.0 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @C, i64 0, i64 0) , align 8
  %ldC.1 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @C, i64 0, i64 1) , align 8
  %ldC.2 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @C, i64 0, i64 2) , align 8
  %ldC.3 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @C, i64 0, i64 3) , align 8

  %ldB.0 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @B, i64 0, i64 0) , align 8
  %ldB.1 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @B, i64 0, i64 1) , align 8
  %ldB.2 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @B, i64 0, i64 2) , align 8
  %ldB.3 = load i64, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @B, i64 0, i64 3) , align 8

  %shB.0 = shl i64 %ldB.0, %c0
  %shB.1 = shl i64 %ldB.1, %c1
  %shB.2 = shl i64 %ldB.2, %c1
  %shB.3 = shl i64 %ldB.3, %c1

  %shC.0 = shl i64 %ldC.0, %c1
  %shC.1 = shl i64 %ldC.1, %c1
  %shC.2 = shl i64 %ldC.2, %c1
  %shC.3 = shl i64 %ldC.3, %c1

  %add0 = add i64 %shC.0, %shB.0
  %add1 = add i64 %shC.1, %shB.1
  %add2 = add i64 %shC.2, %shB.2
  %add3 = add i64 %shC.3, %shB.3

  store i64 %add0, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @A, i64 0, i64 0), align 8
  store i64 %add1, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @A, i64 0, i64 1), align 8
  store i64 %add2, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @A, i64 0, i64 2), align 8
  store i64 %add3, i64* getelementptr inbounds ([4 x i64], [4 x i64]* @A, i64 0, i64 3), align 8

  ret void
}
