; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Test to check VPValue-based CG for incoming LLVM-IR with unit-strided vector loads/stores.

; RUN: opt -S -VPlanDriver < %s | FileCheck %s

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(<4 x i32>* nocapture %ary) {
;  void foo(int4 *ary) {
;    for (i = 0; i < 1024; i += 1)
;      ary[i] += 7;
;  }
;
; CHECK-LABEL: @foo(
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP4:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VECTOR_PH]] ], [ [[TMP3:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds <4 x i32>, <4 x i32>* [[ARY:%.*]], i64 [[UNI_PHI]]
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x i32>* [[SCALAR_GEP]] to <16 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <16 x i32>, <16 x i32>* [[TMP0]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw <16 x i32> [[WIDE_LOAD]], <i32 7, i32 8, i32 9, i32 10, i32 7, i32 8, i32 9, i32 10, i32 7, i32 8, i32 9, i32 10, i32 7, i32 8, i32 9, i32 10>
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32>* [[SCALAR_GEP]] to <16 x i32>*
; CHECK-NEXT:    store <16 x i32> [[TMP1]], <16 x i32>* [[TMP2]], align 4
; CHECK-NEXT:    [[TMP3]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-NEXT:    [[TMP4]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ult <4 x i64> [[TMP3]], <i64 1024, i64 1024, i64 1024, i64 1024>
; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <4 x i1> [[TMP5]], i32 0
; CHECK-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; CHECK-NEXT:    br i1 [[TMP7]], label [[VPLANNEDBB:%.*]], label [[VECTOR_BODY]]
;
entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %arrayidx = getelementptr inbounds <4 x i32>, <4 x i32>* %ary, i64 %indvars.iv
  %0 = load <4 x i32>, <4 x i32>* %arrayidx, align 4
  %add7 = add nsw <4 x i32> %0, <i32 7, i32 8, i32 9, i32 10>
  store <4 x i32> %add7, <4 x i32>* %arrayidx, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
