; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Test to verify correctness of vectorized LLVM-IR generated
; by VPlan for a loop containing array type inscan inclusive
; reduction entity.

; int A[3][8] = { {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7} };
; int B[3][8] = {0};
;
; void foo() {
;   int x[3] = {0, 0, 0};
; #pragma omp simd reduction(inscan,+:x)
; #pragma nounroll
;   for (int i = 0; i < 8; i++) {
;     x[0] += A[0][i];
;     x[1] += A[1][i];
;     x[2] += A[2][i];
; #pragma omp scan inclusive(x)
;     B[0][i] = x[0];
;     B[1][i] = x[1];
;     B[2][i] = x[2];
;   }
; }

; RUN: opt -passes="vplan-vec" -vplan-force-vf=4 -S < %s 2>&1 | FileCheck %s

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@A = dso_local local_unnamed_addr global [3 x [8 x i32]] [[8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7]], align 16
@B = dso_local local_unnamed_addr global [3 x [8 x i32]] zeroinitializer, align 16

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @_Z3foov() local_unnamed_addr {
; CHECK-LABEL: @_Z3foov(
; CHECK-NEXT:  DIR.OMP.SIMD.1.split46.split:
; CHECK-NEXT:    [[X_RED:%.*]] = alloca [3 x i32], align 4
; CHECK:         [[X_RED_SOA_VEC:%.*]] = alloca [3 x <4 x i32>], align 16
; CHECK-NEXT:    br label [[DIR_OMP_SIMD_1:%.*]]

; CHECK:       VPlannedBB1:
; CHECK:         br label [[VECTOR_BODY:%.*]]

; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP24:%.*]], [[VPLANNEDBB39:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB1]] ], [ [[TMP23:%.*]], [[VPLANNEDBB39]] ]
; CHECK-NEXT:    [[UNI_PHI3:%.*]] = phi i32 [ [[TMP2:%.*]], [[VPLANNEDBB1]] ], [ [[TMP26:%.*]], [[VPLANNEDBB39]] ]
; CHECK-NEXT:    [[VEC_PHI4:%.*]] = phi <4 x i32> [ [[TMP6:%.*]], [[VPLANNEDBB1]] ], [ [[TMP25:%.*]], [[VPLANNEDBB39]] ]
; CHECK-NEXT:    store <4 x i32> [[VEC_PHI4]], ptr [[I_LINEAR_IV_VEC:%.*]], align 1
; CHECK-NEXT:    br label [[ARRAY_REDN_INIT_LOOP:%.*]]

; CHECK:       array.redn.init.loop:
; CHECK-NEXT:    [[CUR_ELEM_IDX:%.*]] = phi i64 [ 0, [[VECTOR_BODY]] ], [ [[NEXT_ELEM_IDX:%.*]], [[ARRAY_REDN_INIT_LOOP]] ]
; CHECK-NEXT:    [[CUR_ELEM_PTR:%.*]] = getelementptr i32, ptr [[X_RED_SOA_VEC]], i64 [[CUR_ELEM_IDX]]
; CHECK-NEXT:    store i32 0, ptr [[CUR_ELEM_PTR]], align 4
; CHECK-NEXT:    [[NEXT_ELEM_IDX]] = add i64 [[CUR_ELEM_IDX]], 1
; CHECK-NEXT:    [[INITLOOP_COND:%.*]] = icmp ult i64 [[NEXT_ELEM_IDX]], 12
; CHECK-NEXT:    br i1 [[INITLOOP_COND]], label [[ARRAY_REDN_INIT_LOOP]], label [[ARRAY_REDN_INIT_LOOPEXIT:%.*]]

; CHECK:       array.redn.init.loop.exit:
; CHECK-NEXT:    br label [[VPLANNEDBB5:%.*]]

; CHECK:       VPlannedBB5:
; CHECK-NEXT:    br label [[VPLANNEDBB6:%.*]]

; CHECK:       VPlannedBB6:
; CHECK-NEXT:    [[TMP7:%.*]] = trunc <4 x i64> [[VEC_PHI]] to <4 x i32>
; CHECK-NEXT:    store <4 x i32> [[TMP7]], ptr [[I_LINEAR_IV_VEC]], align 4
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds [8 x i32], ptr @A, i64 0, i64 [[UNI_PHI]]
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP]], align 16
; CHECK-NEXT:    [[WIDE_LOAD8:%.*]] = load <4 x i32>, ptr [[X_RED_SOA_VEC]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = add nsw <4 x i32> [[WIDE_LOAD8]], [[WIDE_LOAD]]
; CHECK-NEXT:    store <4 x i32> [[TMP8]], ptr [[X_RED_SOA_VEC]], align 4
; CHECK-NEXT:    [[SCALAR_GEP9:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 1, i64 [[UNI_PHI]]
; CHECK-NEXT:    [[WIDE_LOAD10:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP9]], align 16
; CHECK-NEXT:    [[SOA_SCALAR_GEP11:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 1
; CHECK-NEXT:    [[WIDE_LOAD12:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP11]], align 4
; CHECK-NEXT:    [[TMP9:%.*]] = add nsw <4 x i32> [[WIDE_LOAD12]], [[WIDE_LOAD10]]
; CHECK-NEXT:    store <4 x i32> [[TMP9]], ptr [[SOA_SCALAR_GEP11]], align 4
; CHECK-NEXT:    [[SCALAR_GEP13:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 2, i64 [[UNI_PHI]]
; CHECK-NEXT:    [[WIDE_LOAD14:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP13]], align 16
; CHECK-NEXT:    [[SOA_SCALAR_GEP15:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 2
; CHECK-NEXT:    [[WIDE_LOAD16:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP15]], align 4
; CHECK-NEXT:    [[TMP10:%.*]] = add nsw <4 x i32> [[WIDE_LOAD16]], [[WIDE_LOAD14]]
; CHECK-NEXT:    store <4 x i32> [[TMP10]], ptr [[SOA_SCALAR_GEP15]], align 4
; CHECK-NEXT:    br label [[VPLANNEDBB17:%.*]]

; CHECK:       VPlannedBB15:
; CHECK-NEXT:    br label [[VPLANNEDBB18:%.*]]

; CHECK:       VPlannedBB16:
; CHECK-NEXT:    br label [[VPLANNEDBB19:%.*]]

; CHECK:       VPlannedBB17:
; CHECK-NEXT:    [[UNI_PHI20:%.*]] = phi i64 [ 0, [[VPLANNEDBB18]] ], [ [[TMP20:%.*]], [[VPLANNEDBB19]] ]
; CHECK-NEXT:    [[SOA_SCALAR_GEP21:%.*]] = getelementptr [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 [[UNI_PHI20]]
; CHECK-NEXT:    [[WIDE_LOAD22:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP21]], align 1
; CHECK-NEXT:    [[SCALAR_GEP23:%.*]] = getelementptr [3 x i32], ptr [[X_RED]], i64 0, i64 [[UNI_PHI20]]
; CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[SCALAR_GEP23]], align 1
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x i32> poison, i32 [[TMP11]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x i32> [[BROADCAST_SPLATINSERT]], <4 x i32> poison, <4 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[SCALAR_GEP23]], align 1
; CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[SCALAR_GEP23]], align 1
; CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[SCALAR_GEP23]], align 1
; CHECK-NEXT:    [[TMP15:%.*]] = shufflevector <4 x i32> [[WIDE_LOAD22]], <4 x i32> zeroinitializer, <4 x i32> <i32 4, i32 0, i32 1, i32 2>
; CHECK-NEXT:    [[TMP16:%.*]] = add <4 x i32> [[WIDE_LOAD22]], [[TMP15]]
; CHECK-NEXT:    [[TMP17:%.*]] = shufflevector <4 x i32> [[TMP16]], <4 x i32> zeroinitializer, <4 x i32> <i32 4, i32 5, i32 0, i32 1>
; CHECK-NEXT:    [[TMP18:%.*]] = add <4 x i32> [[TMP16]], [[TMP17]]
; CHECK-NEXT:    [[TMP19:%.*]] = add <4 x i32> [[TMP18]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[DOTEXTRACT_3_:%.*]] = extractelement <4 x i32> [[TMP19]], i32 3
; CHECK-NEXT:    store <4 x i32> [[TMP19]], ptr [[SOA_SCALAR_GEP21]], align 1
; CHECK-NEXT:    store i32 [[DOTEXTRACT_3_]], ptr [[SCALAR_GEP23]], align 1
; CHECK-NEXT:    [[TMP20]] = add i64 [[UNI_PHI20]], 1
; CHECK-NEXT:    [[TMP21:%.*]] = icmp eq i64 [[TMP20]], 3
; CHECK-NEXT:    br i1 [[TMP21]], label [[VPLANNEDBB24:%.*]], label [[VPLANNEDBB19]]

; CHECK:       VPlannedBB22:
; CHECK-NEXT:    br label [[VPLANNEDBB25:%.*]]

; CHECK:       VPlannedBB23:
; CHECK-NEXT:    br label [[VPLANNEDBB26:%.*]]

; CHECK:       VPlannedBB24:
; CHECK-NEXT:    br label [[VPLANNEDBB27:%.*]]

; CHECK:       VPlannedBB25:
; CHECK-NEXT:    [[WIDE_LOAD29:%.*]] = load <4 x i32>, ptr [[X_RED_SOA_VEC]], align 4
; CHECK-NEXT:    [[WIDE_LOAD30:%.*]] = load <4 x i32>, ptr [[I_LINEAR_IV_VEC]], align 4
; CHECK-NEXT:    [[TMP22:%.*]] = sext <4 x i32> [[WIDE_LOAD30]] to <4 x i64>
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <4 x i64> [[TMP22]], i32 0
; CHECK-NEXT:    [[SCALAR_GEP31:%.*]] = getelementptr inbounds [8 x i32], ptr @B, i64 0, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD29]], ptr [[SCALAR_GEP31]], align 4
; CHECK-NEXT:    [[SOA_SCALAR_GEP32:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 1
; CHECK-NEXT:    [[WIDE_LOAD33:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP32]], align 4
; CHECK-NEXT:    [[SCALAR_GEP34:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 1, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD33]], ptr [[SCALAR_GEP34]], align 4
; CHECK-NEXT:    [[SOA_SCALAR_GEP35:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 2
; CHECK-NEXT:    [[WIDE_LOAD36:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP35]], align 4
; CHECK-NEXT:    [[SCALAR_GEP37:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 2, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD36]], ptr [[SCALAR_GEP37]], align 4
; CHECK-NEXT:    br label [[VPLANNEDBB38:%.*]]

; CHECK:       VPlannedBB35:
; CHECK-NEXT:    br label [[VPLANNEDBB39]]

; CHECK:       VPlannedBB36:
; CHECK-NEXT:    [[TMP23]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-NEXT:    [[TMP24]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK-NEXT:    [[TMP25]] = add <4 x i32> [[VEC_PHI4]], <i32 4, i32 4, i32 4, i32 4>
; CHECK-NEXT:    [[TMP26]] = add i32 [[UNI_PHI3]], 4
; CHECK-NEXT:    [[TMP27:%.*]] = icmp uge i64 [[TMP24]], 8
; CHECK-NEXT:    br i1 [[TMP27]], label [[VPLANNEDBB40:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
;
DIR.OMP.SIMD.1.split46.split:
  %x.red = alloca [3 x i32], align 4
  %i.linear.iv = alloca i32, align 4
  store i32 0, ptr %x.red, align 4
  %0 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  store i32 0, ptr %0, align 4
  %1 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  store i32 0, ptr %1, align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %DIR.OMP.SIMD.1.split46.split
  %2 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr %x.red, i32 0, i64 3, i64 1), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5

DIR.VPO.END.GUARD.MEM.MOTION.5:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %DIR.VPO.END.GUARD.MEM.MOTION.5100 ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1.split

DIR.VPO.GUARD.MEM.MOTION.1.split:                 ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5
  %pre.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1

DIR.VPO.GUARD.MEM.MOTION.1:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.1.split
  %3 = trunc i64 %indvars.iv to i32
  store i32 %3, ptr %i.linear.iv, align 4
  %arrayidx = getelementptr inbounds [8 x i32], ptr @A, i64 0, i64 %indvars.iv
  %4 = load i32, ptr %arrayidx, align 4
  %5 = load i32, ptr %x.red, align 4
  %add2 = add nsw i32 %5, %4
  store i32 %add2, ptr %x.red, align 4
  %arrayidx4 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 1, i64 %indvars.iv
  %6 = load i32, ptr %arrayidx4, align 4
  %arrayidx5 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  %7 = load i32, ptr %arrayidx5, align 4
  %add6 = add nsw i32 %7, %6
  store i32 %add6, ptr %arrayidx5, align 4
  %arrayidx8 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 2, i64 %indvars.iv
  %8 = load i32, ptr %arrayidx8, align 4
  %arrayidx9 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  %9 = load i32, ptr %arrayidx9, align 4
  %add10 = add nsw i32 %9, %8
  store i32 %add10, ptr %arrayidx9, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.3

DIR.VPO.END.GUARD.MEM.MOTION.3:                   ; preds = %DIR.VPO.GUARD.MEM.MOTION.1
  call void @llvm.directive.region.exit(token %pre.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.4

DIR.VPO.END.GUARD.MEM.MOTION.4:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.3
  %10 = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr %x.red, i64 1) ]
  br label %DIR.OMP.SCAN.2

DIR.OMP.SCAN.2:                                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.4
  fence acq_rel
  br label %DIR.OMP.END.SCAN.6

DIR.OMP.END.SCAN.6:                               ; preds = %DIR.OMP.SCAN.2
  call void @llvm.directive.region.exit(token %10) [ "DIR.OMP.END.SCAN"() ]
  br label %DIR.OMP.END.SCAN.3

DIR.OMP.END.SCAN.3:                               ; preds = %DIR.OMP.END.SCAN.6
  br label %DIR.VPO.GUARD.MEM.MOTION.8.split

DIR.VPO.GUARD.MEM.MOTION.8.split:                 ; preds = %DIR.OMP.END.SCAN.3
  %post.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.4

DIR.VPO.GUARD.MEM.MOTION.4:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.8.split
  %11 = load i32, ptr %x.red, align 4
  %12 = load i32, ptr %i.linear.iv, align 4
  %idxprom12 = sext i32 %12 to i64
  %arrayidx13 = getelementptr inbounds [8 x i32], ptr @B, i64 0, i64 %idxprom12
  store i32 %11, ptr %arrayidx13, align 4
  %arrayidx14 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  %13 = load i32, ptr %arrayidx14, align 4
  %arrayidx16 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 1, i64 %idxprom12
  store i32 %13, ptr %arrayidx16, align 4
  %arrayidx17 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  %14 = load i32, ptr %arrayidx17, align 4
  %arrayidx19 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 2, i64 %idxprom12
  store i32 %14, ptr %arrayidx19, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.10

DIR.VPO.END.GUARD.MEM.MOTION.10:                  ; preds = %DIR.VPO.GUARD.MEM.MOTION.4
  call void @llvm.directive.region.exit(token %post.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5100

DIR.VPO.END.GUARD.MEM.MOTION.5100:                ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.10
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 8
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.12, label %DIR.VPO.END.GUARD.MEM.MOTION.5, !llvm.loop !0

DIR.OMP.END.SIMD.12:                              ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100
  call void @llvm.directive.region.exit(token %2) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.6

DIR.OMP.END.SIMD.6:                               ; preds = %DIR.OMP.END.SIMD.12
  ret void
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.vectorize.enable", i1 true}
!2 = !{!"llvm.loop.vectorize.ivdep_loop", i32 0}
