; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Check that VPlan's LLVM-IR and HIR vector codegen can handle trivial MetadataAsValue operands
; represented via VPMetadataAsValue in VPlan CFG.

; RUN: opt < %s -S -vplan-vec -vplan-force-vf=2 | FileCheck %s --check-prefix=LLVM-IR
; RUN: opt -hir-ssa-deconstruction -hir-framework -hir-vplan-vec -print-after=hir-vplan-vec -vplan-force-vf=2 -disable-output < %s 2>&1 | FileCheck %s --check-prefix=HIR

define void @mod_gauss_hermite_mp_derquadgausshermite_(double* %ptr, double %T_fetch, i64 %N) local_unnamed_addr {
; LLVM-IR-LABEL: @mod_gauss_hermite_mp_derquadgausshermite_(
; LLVM-IR:       vector.body:
; LLVM-IR-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY:%.*]] ]
; LLVM-IR-NEXT:    [[UNI_PHI1:%.*]] = phi i64 [ 1, [[VECTOR_PH]] ], [ [[TMP7:%.*]], [[VECTOR_BODY]] ]
; LLVM-IR-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 1, i64 2>, [[VECTOR_PH]] ], [ [[TMP6:%.*]], [[VECTOR_BODY]] ]
; LLVM-IR-NEXT:    [[TMP2:%.*]] = add nsw <2 x i64> [[VEC_PHI]], <i64 -1, i64 -1>
; LLVM-IR-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <2 x i64> [[TMP2]], i32 0
; LLVM-IR-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds double, double* [[PTR:%.*]], i64 [[DOTEXTRACT_0_]]
; LLVM-IR-NEXT:    [[TMP3:%.*]] = bitcast double* [[SCALAR_GEP]] to <2 x double>*
; LLVM-IR-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP3]], align 8
; LLVM-IR-NEXT:    [[WIDE_LOAD_EXTRACT_1_:%.*]] = extractelement <2 x double> [[WIDE_LOAD]], i32 1
; LLVM-IR-NEXT:    [[WIDE_LOAD_EXTRACT_0_:%.*]] = extractelement <2 x double> [[WIDE_LOAD]], i32 0
; LLVM-IR-NEXT:    [[TMP4:%.*]] = call double @llvm.experimental.constrained.fmul.f64(double [[WIDE_LOAD_EXTRACT_0_]], double [[T_FETCH:%.*]], metadata !"round.dynamic", metadata !"fpexcept.strict")
; LLVM-IR-NEXT:    [[TMP5:%.*]] = call double @llvm.experimental.constrained.fmul.f64(double [[WIDE_LOAD_EXTRACT_1_]], double [[T_FETCH]], metadata !"round.dynamic", metadata !"fpexcept.strict")
; LLVM-IR-NEXT:    [[TMP6]] = add nuw nsw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; LLVM-IR-NEXT:    [[TMP7]] = add nuw nsw i64 [[UNI_PHI1]], 2
; LLVM-IR-NEXT:    [[TMP8]] = add i64 [[UNI_PHI]], 2
; LLVM-IR-NEXT:    [[TMP9:%.*]] = icmp uge i64 [[TMP8]], [[N_VEC:%.*]]
; LLVM-IR-NEXT:    br i1 [[TMP9]], label [[VPLANNEDBB:%.*]], label [[VECTOR_BODY]], [[LOOP0:!llvm.loop !.*]]
;
; HIR-LABEL:Function: mod_gauss_hermite_mp_derquadgausshermite_
; HIR-LABEL:          BEGIN REGION { modified }
; HIR-NEXT:               %tgu = (smax(0, %N) + 1)/u2;
; HIR-NEXT:               if (0 <u 2 * %tgu)
; HIR-NEXT:               {
; HIR-NEXT:                  + DO i1 = 0, 2 * %tgu + -1, 2   <DO_LOOP> <simd-vectorized> <nounroll> <novectorize>
; HIR-NEXT:                  |   %.vec = (<2 x double>*)(%ptr)[i1];
; HIR-NEXT:                  |   %serial.temp = undef;
; HIR-NEXT:                  |   %extract.0. = extractelement %.vec,  0;
; HIR-NEXT:                  |   %llvm.experimental.constrained.fmul.f64 = @llvm.experimental.constrained.fmul.f64(%extract.0.,  %T_fetch,  !"round.dynamic",  !"fpexcept.strict");
; HIR-NEXT:                  |   %serial.temp = insertelement %serial.temp,  %llvm.experimental.constrained.fmul.f64,  0;
; HIR-NEXT:                  |   %extract.1. = extractelement %.vec,  1;
; HIR-NEXT:                  |   %llvm.experimental.constrained.fmul.f642 = @llvm.experimental.constrained.fmul.f64(%extract.1.,  %T_fetch,  !"round.dynamic",  !"fpexcept.strict");
; HIR-NEXT:                  |   %serial.temp = insertelement %serial.temp,  %llvm.experimental.constrained.fmul.f642,  1;
; HIR-NEXT:                  + END LOOP
; HIR-NEXT:               }
; HIR-LABEL:               + DO i1 = 2 * %tgu, smax(0, %N), 1   <DO_LOOP>  <MAX_TC_EST = 1>   <LEGAL_MAX_TC = 1> <nounroll> <novectorize> <max_trip_count = 1>
; HIR-NEXT:               |   %"mod_gauss_hermite_mp_h2d_[][]_fetch" = (%ptr)[i1];
; HIR-NEXT:               |   %mul = @llvm.experimental.constrained.fmul.f64(%"mod_gauss_hermite_mp_h2d_[][]_fetch",  %T_fetch,  !"round.dynamic",  !"fpexcept.strict");
; HIR-NEXT:               + END LOOP
; HIR-NEXT:          END REGION
;
DIR.OMP.SIMD.1:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %bb9

bb9:
  %indvars.iv = phi i64 [ 1, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %bb9 ]
  %1 = add nsw i64 %indvars.iv, -1
  %2 = getelementptr inbounds double, double* %ptr, i64 %1
  %"mod_gauss_hermite_mp_h2d_[][]_fetch" = load double, double* %2, align 8
  %mul = call double @llvm.experimental.constrained.fmul.f64(double %"mod_gauss_hermite_mp_h2d_[][]_fetch", double %T_fetch, metadata !"round.dynamic", metadata !"fpexcept.strict") #4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %rel41 = icmp sgt i64 %indvars.iv, %N
  br i1 %rel41, label %DIR.OMP.END.SIMD.3, label %bb9

DIR.OMP.END.SIMD.3:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.454

DIR.OMP.END.SIMD.454:
  ret void
}

; Function Attrs: inaccessiblememonly nounwind willreturn
declare double @llvm.experimental.constrained.fmul.f64(double, double, metadata, metadata) #2

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)

attributes #2 = { inaccessiblememonly nounwind willreturn }
attributes #3 = { nounwind }
attributes #4 = { strictfp }
