; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -vplan-enable-soa=false -VPlanDriver -disable-vplan-predicator -vplan-force-vf=4 -S %s | FileCheck %s

; This test checks for widened allocas for the privates and wide-stores being generated to
; the widened allocas for stores to the privates (including stores through a bitcasted
; private). The test also checks for appropriate arguments to serialized calls using the
; privates as arguments and that we do not attempt to setup last value out for the privates.

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(i64 %n1, i32 %k1, float* nocapture %accumulated_grid, i32* nocapture readonly %iarr) {
; CHECK-LABEL: @foo(
; CHECK-NEXT:  entry:
; CHECK:         [[COUNT_VEC:%.*]] = alloca <4 x i64>, align 32
; CHECK-NEXT:    [[COUNT_VEC_BC:%.*]] = bitcast <4 x i64>* [[COUNT_VEC]] to i64*
; CHECK-NEXT:    [[COUNT_VEC_BASE_ADDR:%.*]] = getelementptr i64, i64* [[COUNT_VEC_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BC:%.*]] = bitcast <4 x float>* [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC]] to float*
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BASE_ADDR:%.*]] = getelementptr float, float* [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[A2_VEC:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[A2_VEC_BC:%.*]] = bitcast <4 x i32>* [[A2_VEC]] to i32*
; CHECK-NEXT:    [[A2_VEC_BASE_ADDR:%.*]] = getelementptr i32, i32* [[A2_VEC_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BC:%.*]] = bitcast <4 x float>* [[ACCUMULATED_OCCUPANCY_INPUT_VEC]] to float*
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BASE_ADDR:%.*]] = getelementptr float, float* [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK:       vector.ph:
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x float*> [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BASE_ADDR]] to <4 x i32*>
; CHECK-NEXT:    [[DOTEXTRACT_0_3:%.*]] = extractelement <4 x i32*> [[TMP1]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x float*> undef, float* [[ACCUMULATED_GRID:%.*]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x float*> [[BROADCAST_SPLATINSERT]], <4 x float*> undef, <4 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP13:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VECTOR_PH]] ], [ [[TMP11:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    store <4 x i64> [[VEC_PHI]], <4 x i64>* [[COUNT_VEC]], align 8
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, i32* [[IARR:%.*]], i64 [[UNI_PHI1:%.*]]
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32* [[SCALAR_GEP]] to <4 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i32>, <4 x i32>* [[TMP2]], align 4
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_3_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 3
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_2_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 2
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_1_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 1
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_0_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 0
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD]], <4 x i32>* [[A2_VEC]], align 4
; CHECK-NEXT:    [[MM_VECTORGEP:%.*]] = getelementptr inbounds float, <4 x float*> [[BROADCAST_SPLAT]], <4 x i64> [[VEC_PHI]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x float*> [[MM_VECTORGEP]] to <4 x i32*>
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <4 x i32*> [[TMP3]], i32 0
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32* [[DOTEXTRACT_0_]] to <4 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <4 x i32>, <4 x i32>* [[TMP4]], align 4
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32* [[DOTEXTRACT_0_3]] to <4 x i32>*
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD2]], <4 x i32>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[WIDE_LOAD2]] to <4 x float>
; CHECK-NEXT:    [[DOTEXTRACT_3_:%.*]] = extractelement <4 x float> [[TMP6]], i32 3
; CHECK-NEXT:    [[DOTEXTRACT_2_:%.*]] = extractelement <4 x float> [[TMP6]], i32 2
; CHECK-NEXT:    [[DOTEXTRACT_1_:%.*]] = extractelement <4 x float> [[TMP6]], i32 1
; CHECK-NEXT:    [[DOTEXTRACT_0_4:%.*]] = extractelement <4 x float> [[TMP6]], i32 0
; CHECK-NEXT:    [[TMP7:%.*]] = call float @baz(float [[DOTEXTRACT_0_4]], i32 [[WIDE_LOAD_EXTRACT_0_]])
; CHECK-NEXT:    [[TMP8:%.*]] = call float @baz(float [[DOTEXTRACT_1_]], i32 [[WIDE_LOAD_EXTRACT_1_]])
; CHECK-NEXT:    [[TMP9:%.*]] = call float @baz(float [[DOTEXTRACT_2_]], i32 [[WIDE_LOAD_EXTRACT_2_]])
; CHECK-NEXT:    [[TMP10:%.*]] = call float @baz(float [[DOTEXTRACT_3_]], i32 [[WIDE_LOAD_EXTRACT_3_]])
; CHECK-NOT:     LastUpdatedLanePtr
entry:
  %count = alloca i64, align 8
  %accumulated_occupancy_input = alloca float, align 4
  %accumulated_occupancy_output = alloca float, align 4
  %a2 = alloca i32, align 4
  %0 = bitcast i64* %count to i8*
  %cmp = icmp sgt i64 %n1, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"(i64* %count, float* %accumulated_occupancy_output, i32 *%a2, float* %accumulated_occupancy_input) ]
  br label %DIR.QUAL.LIST.END.1

DIR.QUAL.LIST.END.1:                              ; preds = %omp.precond.then
  %x3 = bitcast float* %accumulated_occupancy_input to i32*
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.QUAL.LIST.END.1
  %.omp.iv.018 = phi i64 [ 0, %DIR.QUAL.LIST.END.1 ], [ %add9, %omp.inner.for.body ]
  store i64 %.omp.iv.018, i64* %count, align 8
  %arrayidx = getelementptr inbounds i32, i32* %iarr, i64 %.omp.iv.018
  %x5 = load i32, i32* %arrayidx, align 4
  store i32 %x5, i32* %a2, align 4
  %arrayidx7 = getelementptr inbounds float, float* %accumulated_grid, i64 %.omp.iv.018
  %x6 = bitcast float* %arrayidx7 to i32*
  %x7 = load i32, i32* %x6, align 4
  store i32 %x7, i32* %x3, align 4
  %.cast = bitcast i32 %x7 to float
  %call = call float @baz(float %.cast, i32 %x5)
  %add9 = add nuw nsw i64 %.omp.iv.018, 1
  %exitcond = icmp eq i64 %add9, %n1
  br i1 %exitcond, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}
declare float @baz(float, i32)

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token)
