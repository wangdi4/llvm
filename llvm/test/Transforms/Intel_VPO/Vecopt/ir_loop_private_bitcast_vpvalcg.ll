; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -VPlanDriver -disable-vplan-predicator -disable-vplan-subregions -vplan-force-vf=4 -S -enable-vp-value-codegen=true %s | FileCheck %s

; This test checks for widened allocas for the privates and scatters being generated to
; the widened allocas for stores to the privates (including stores through a bitcasted
; private). The test also checks for appropriate arguments to serialized calls using the
; privates as arguments and that we do not attempt to setup last value out for the privates.
; TODO - The scatters can be replaced with wide stores. This functionality will be added
; later.
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(i64 %n1, i32 %k1, float* nocapture %accumulated_grid, i32* nocapture readonly %iarr) {
; CHECK:       entry:
; CHECK-NEXT:    [[PRIVATE_MEM3:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[PRIVATE_MEM3_BC:%.*]] = bitcast <4 x float>* [[PRIVATE_MEM3]] to float*
; CHECK-NEXT:    [[PRIVATE_MEM3_BASE_ADDR:%.*]] = getelementptr float, float* [[PRIVATE_MEM3_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[PRIVATE_MEM2:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[PRIVATE_MEM2_BC:%.*]] = bitcast <4 x i32>* [[PRIVATE_MEM2]] to i32*
; CHECK-NEXT:    [[PRIVATE_MEM2_BASE_ADDR:%.*]] = getelementptr i32, i32* [[PRIVATE_MEM2_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[PRIVATE_MEM1:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[PRIVATE_MEM1_BC:%.*]] = bitcast <4 x float>* [[PRIVATE_MEM1]] to float*
; CHECK-NEXT:    [[PRIVATE_MEM1_BASE_ADDR:%.*]] = getelementptr float, float* [[PRIVATE_MEM1_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[PRIVATE_MEM:%.*]] = alloca <4 x i64>, align 32
; CHECK-NEXT:    [[PRIVATE_MEM_BC:%.*]] = bitcast <4 x i64>* [[PRIVATE_MEM]] to i64*
; CHECK-NEXT:    [[PRIVATE_MEM_BASE_ADDR:%.*]] = getelementptr i64, i64* [[PRIVATE_MEM_BC]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK:       vector.ph:
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x float*> [[PRIVATE_MEM3_BASE_ADDR]] to <4 x i32*>
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x float*> undef, float* %accumulated_grid, i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x float*> [[BROADCAST_SPLATINSERT]], <4 x float*> undef, <4 x i32> zeroinitializer
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %vector.ph ], [ [[INDEX_NEXT:%.*]], %vector.body ]
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, %vector.ph ], [ [[TMP10:%.*]], %vector.body ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, %vector.ph ], [ [[TMP9:%.*]], %vector.body ]
; CHECK-NEXT:    call void @llvm.masked.scatter.v4i64.v4p0i64(<4 x i64> [[VEC_PHI]], <4 x i64*> [[PRIVATE_MEM_BASE_ADDR]], i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>)
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, i32* [[IARR:%.*]], i64 [[UNI_PHI]]
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32* [[SCALAR_GEP]] to <4 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i32>, <4 x i32>* [[TMP2]], align 4
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_3_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 3
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_2_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 2
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_1_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 1
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_0_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD]], i32 0
; CHECK-NEXT:    call void @llvm.masked.scatter.v4i32.v4p0i32(<4 x i32> [[WIDE_LOAD]], <4 x i32*> [[PRIVATE_MEM2_BASE_ADDR]], i32 4, <4 x i1> <i1 true, i1 true, i1 true, i1 true>)
; CHECK-NEXT:    [[MM_VECTORGEP:%.*]] = getelementptr inbounds float, <4 x float*> [[BROADCAST_SPLAT]], <4 x i64> [[VEC_PHI]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x float*> [[MM_VECTORGEP]] to <4 x i32*>
; CHECK-NEXT:    [[_0:%.*]] = extractelement <4 x i32*> [[TMP3]], i64 0
; CHECK-NEXT:    [[GROUPPTR:%.*]] = bitcast i32* [[_0]] to <4 x i32>*
; CHECK-NEXT:    [[GROUPLOAD:%.*]] = load <4 x i32>, <4 x i32>* [[GROUPPTR]], align 4
; CHECK-NEXT:    call void @llvm.masked.scatter.v4i32.v4p0i32(<4 x i32> [[GROUPLOAD]], <4 x i32*> [[TMP1]], i32 4, <4 x i1> <i1 true, i1 true, i1 true, i1 true>)
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[GROUPLOAD]] to <4 x float>
; CHECK-NEXT:    [[DOTEXTRACT_3_:%.*]] = extractelement <4 x float> [[TMP4]], i32 3
; CHECK-NEXT:    [[DOTEXTRACT_2_:%.*]] = extractelement <4 x float> [[TMP4]], i32 2
; CHECK-NEXT:    [[DOTEXTRACT_1_:%.*]] = extractelement <4 x float> [[TMP4]], i32 1
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <4 x float> [[TMP4]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = call float @baz(float [[DOTEXTRACT_0_]], i32 [[WIDE_LOAD_EXTRACT_0_]])
; CHECK-NEXT:    [[TMP6:%.*]] = call float @baz(float [[DOTEXTRACT_1_]], i32 [[WIDE_LOAD_EXTRACT_1_]])
; CHECK-NEXT:    [[TMP7:%.*]] = call float @baz(float [[DOTEXTRACT_2_]], i32 [[WIDE_LOAD_EXTRACT_2_]])
; CHECK-NEXT:    [[TMP8:%.*]] = call float @baz(float [[DOTEXTRACT_3_]], i32 [[WIDE_LOAD_EXTRACT_3_]])
; CHECK-NOT:     LastUpdatedLanePtr
entry:
  %count = alloca i64, align 8
  %accumulated_occupancy_input = alloca float, align 4
  %accumulated_occupancy_output = alloca float, align 4
  %a2 = alloca i32, align 4
  %0 = bitcast i64* %count to i8*
  %cmp = icmp sgt i64 %n1, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"(i64* %count, float* %accumulated_occupancy_output, i32 *%a2, float* %accumulated_occupancy_input) ]
  br label %DIR.QUAL.LIST.END.1

DIR.QUAL.LIST.END.1:                              ; preds = %omp.precond.then
  %x3 = bitcast float* %accumulated_occupancy_input to i32*
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.QUAL.LIST.END.1
  %.omp.iv.018 = phi i64 [ 0, %DIR.QUAL.LIST.END.1 ], [ %add9, %omp.inner.for.body ]
  store i64 %.omp.iv.018, i64* %count, align 8
  %arrayidx = getelementptr inbounds i32, i32* %iarr, i64 %.omp.iv.018
  %x5 = load i32, i32* %arrayidx, align 4
  store i32 %x5, i32* %a2, align 4
  %arrayidx7 = getelementptr inbounds float, float* %accumulated_grid, i64 %.omp.iv.018
  %x6 = bitcast float* %arrayidx7 to i32*
  %x7 = load i32, i32* %x6, align 4
  store i32 %x7, i32* %x3, align 4
  %.cast = bitcast i32 %x7 to float
  %call = call float @baz(float %.cast, i32 %x5) #3
  %add9 = add nuw nsw i64 %.omp.iv.018, 1
  %exitcond = icmp eq i64 %add9, %n1
  br i1 %exitcond, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}
declare float @baz(float, i32) #1

; Function Attrs: nounwind
declare token @llvm.directive.region.entry() #1

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token) #1
