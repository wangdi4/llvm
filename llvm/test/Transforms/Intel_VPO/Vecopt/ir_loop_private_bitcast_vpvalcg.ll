; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -vplan-enable-soa=false -VPlanDriver -disable-vplan-predicator -vplan-force-vf=4 -S %s | FileCheck %s

; This test checks for widened allocas for the privates and wide-stores being generated to
; the widened allocas for stores to the privates (including stores through a bitcasted
; private). The test also checks for appropriate arguments to serialized calls using the
; privates as arguments and that we do not attempt to setup last value out for the privates.

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(i64 %n1, i32 %k1, float* nocapture %accumulated_grid, i32* nocapture readonly %iarr) {
; CHECK:       target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
; CHECK-NEXT:  target triple = "x86_64-unknown-linux-gnu"
;
; CHECK:  define void @foo(i64 [[N10:%.*]], i32 [[K10:%.*]], float* nocapture [[ACCUMULATED_GRID0:%.*]], i32* nocapture readonly [[IARR0:%.*]]) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[COUNT0:%.*]] = alloca i64, align 8
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT0:%.*]] = alloca float, align 4
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT0:%.*]] = alloca float, align 4
; CHECK-NEXT:    [[A20:%.*]] = alloca i32, align 4
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64* [[COUNT0]] to i8*
; CHECK-NEXT:    [[CMP0:%.*]] = icmp sgt i64 [[N10]], 0
; CHECK-NEXT:    [[COUNT_VEC0:%.*]] = alloca <4 x i64>, align 32
; CHECK-NEXT:    [[COUNT_VEC_BC0:%.*]] = bitcast <4 x i64>* [[COUNT_VEC0]] to i64*
; CHECK-NEXT:    [[COUNT_VEC_BASE_ADDR0:%.*]] = getelementptr i64, i64* [[COUNT_VEC_BC0]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC0:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BC0:%.*]] = bitcast <4 x float>* [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC0]] to float*
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BASE_ADDR0:%.*]] = getelementptr float, float* [[ACCUMULATED_OCCUPANCY_OUTPUT_VEC_BC0]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[A2_VEC0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[A2_VEC_BC0:%.*]] = bitcast <4 x i32>* [[A2_VEC0]] to i32*
; CHECK-NEXT:    [[A2_VEC_BASE_ADDR0:%.*]] = getelementptr i32, i32* [[A2_VEC_BC0]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC0:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BC0:%.*]] = bitcast <4 x float>* [[ACCUMULATED_OCCUPANCY_INPUT_VEC0]] to float*
; CHECK-NEXT:    [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BASE_ADDR0:%.*]] = getelementptr float, float* [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BC0]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    br i1 [[CMP0]], label [[OMP_PRECOND_THEN0:%.*]], label [[OMP_PRECOND_END0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  omp.precond.then:
; CHECK-NEXT:    br label [[DIR_QUAL_LIST_END_10:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  DIR.QUAL.LIST.END.1:
; CHECK-NEXT:    [[X30:%.*]] = bitcast float* [[ACCUMULATED_OCCUPANCY_INPUT0]] to i32*
; CHECK-NEXT:    br label [[VPLANNEDBB0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB:
; CHECK-NEXT:    br label [[VPLANNEDBB10:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  VPlannedBB1:
; CHECK-NEXT:    [[TMP1:%.*]] = and i64 [[N10]], 4294967292
; CHECK-NEXT:    [[TMP2:%.*]] = icmp eq i64 0, [[TMP1]]
; CHECK-NEXT:    br i1 [[TMP2]], label [[SCALAR_PH0:%.*]], label [[VECTOR_PH0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  vector.ph:
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x float*> [[ACCUMULATED_OCCUPANCY_INPUT_VEC_BASE_ADDR0]] to <4 x i32*>
; CHECK-NEXT:    [[DOTEXTRACT_0_40:%.*]] = extractelement <4 x i32*> [[TMP3]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT0:%.*]] = insertelement <4 x float*> poison, float* [[ACCUMULATED_GRID0]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT0:%.*]] = shufflevector <4 x float*> [[BROADCAST_SPLATINSERT0]], <4 x float*> poison, <4 x i32> zeroinitializer
; CHECK-NEXT:    br label [[VECTOR_BODY0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VECTOR_PH0]] ], [ [[TMP14:%.*]], [[VECTOR_BODY0]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VECTOR_PH0]] ], [ [[TMP13:%.*]], [[VECTOR_BODY0]] ]
; CHECK-NEXT:    store <4 x i64> [[VEC_PHI0]], <4 x i64>* [[COUNT_VEC0]], align 8
; CHECK-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i32, i32* [[IARR0]], i64 [[UNI_PHI0]]
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32* [[SCALAR_GEP0]] to <4 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD0:%.*]] = load <4 x i32>, <4 x i32>* [[TMP4]], align 4
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_3_0:%.*]] = extractelement <4 x i32> [[WIDE_LOAD0]], i32 3
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_2_0:%.*]] = extractelement <4 x i32> [[WIDE_LOAD0]], i32 2
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_1_0:%.*]] = extractelement <4 x i32> [[WIDE_LOAD0]], i32 1
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_0_0:%.*]] = extractelement <4 x i32> [[WIDE_LOAD0]], i32 0
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD0]], <4 x i32>* [[A2_VEC0]], align 4
; CHECK-NEXT:    [[MM_VECTORGEP0:%.*]] = getelementptr inbounds float, <4 x float*> [[BROADCAST_SPLAT0]], <4 x i64> [[VEC_PHI0]]
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast <4 x float*> [[MM_VECTORGEP0]] to <4 x i32*>
; CHECK-NEXT:    [[DOTEXTRACT_0_0:%.*]] = extractelement <4 x i32*> [[TMP5]], i32 0
; CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32* [[DOTEXTRACT_0_0]] to <4 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD30:%.*]] = load <4 x i32>, <4 x i32>* [[TMP6]], align 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast i32* [[DOTEXTRACT_0_40]] to <4 x i32>*
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD30]], <4 x i32>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x i32> [[WIDE_LOAD30]] to <4 x float>
; CHECK-NEXT:    [[DOTEXTRACT_3_0:%.*]] = extractelement <4 x float> [[TMP8]], i32 3
; CHECK-NEXT:    [[DOTEXTRACT_2_0:%.*]] = extractelement <4 x float> [[TMP8]], i32 2
; CHECK-NEXT:    [[DOTEXTRACT_1_0:%.*]] = extractelement <4 x float> [[TMP8]], i32 1
; CHECK-NEXT:    [[DOTEXTRACT_0_50:%.*]] = extractelement <4 x float> [[TMP8]], i32 0
; CHECK-NEXT:    [[TMP9:%.*]] = call float @baz(float [[DOTEXTRACT_0_50]], i32 [[WIDE_LOAD_EXTRACT_0_0]])
; CHECK-NEXT:    [[TMP10:%.*]] = call float @baz(float [[DOTEXTRACT_1_0]], i32 [[WIDE_LOAD_EXTRACT_1_0]])
; CHECK-NEXT:    [[TMP11:%.*]] = call float @baz(float [[DOTEXTRACT_2_0]], i32 [[WIDE_LOAD_EXTRACT_2_0]])
; CHECK-NEXT:    [[TMP12:%.*]] = call float @baz(float [[DOTEXTRACT_3_0]], i32 [[WIDE_LOAD_EXTRACT_3_0]])
; CHECK-NEXT:    [[TMP13]] = add nuw nsw <4 x i64> [[VEC_PHI0]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-NEXT:    [[TMP14]] = add nuw nsw i64 [[UNI_PHI0]], 4
; CHECK-NEXT:    [[TMP15:%.*]] = icmp uge i64 [[TMP14]], [[TMP1]]
; CHECK-NEXT:    br i1 [[TMP15]], label [[VPLANNEDBB60:%.*]], label [[VECTOR_BODY0]], !llvm.loop !0
; CHECK-NOT:     LastUpdatedLanePtr
entry:
  %count = alloca i64, align 8
  %accumulated_occupancy_input = alloca float, align 4
  %accumulated_occupancy_output = alloca float, align 4
  %a2 = alloca i32, align 4
  %0 = bitcast i64* %count to i8*
  %cmp = icmp sgt i64 %n1, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"(i64* %count, float* %accumulated_occupancy_output, i32 *%a2, float* %accumulated_occupancy_input) ]
  br label %DIR.QUAL.LIST.END.1

DIR.QUAL.LIST.END.1:                              ; preds = %omp.precond.then
  %x3 = bitcast float* %accumulated_occupancy_input to i32*
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.QUAL.LIST.END.1
  %.omp.iv.018 = phi i64 [ 0, %DIR.QUAL.LIST.END.1 ], [ %add9, %omp.inner.for.body ]
  store i64 %.omp.iv.018, i64* %count, align 8
  %arrayidx = getelementptr inbounds i32, i32* %iarr, i64 %.omp.iv.018
  %x5 = load i32, i32* %arrayidx, align 4
  store i32 %x5, i32* %a2, align 4
  %arrayidx7 = getelementptr inbounds float, float* %accumulated_grid, i64 %.omp.iv.018
  %x6 = bitcast float* %arrayidx7 to i32*
  %x7 = load i32, i32* %x6, align 4
  store i32 %x7, i32* %x3, align 4
  %.cast = bitcast i32 %x7 to float
  %call = call float @baz(float %.cast, i32 %x5)
  %add9 = add nuw nsw i64 %.omp.iv.018, 1
  %exitcond = icmp eq i64 %add9, %n1
  br i1 %exitcond, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"()]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}
declare float @baz(float, i32)

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token)
