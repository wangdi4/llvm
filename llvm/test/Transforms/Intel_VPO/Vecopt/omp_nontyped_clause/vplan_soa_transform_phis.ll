; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -vplan-vec -vplan-force-vf=2 -vplan-enable-soa -disable-output\
; RUN: -vplan-print-after-transformed-soa-geps -S %s 2>&1 | FileCheck %s

define void @merge_uniform_soa_geps() {
; CHECK-LABEL:  VPlan after Dump Transformed SOA GEPs:
; CHECK-NEXT:  VPlan IR for: merge_uniform_soa_geps:simd.loop
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni] br [[BB1:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Div] [1024 x i64]* [[VP_ARR_SOA_PRIV64:%.*]] = allocate-priv [1024 x i64]*, OrigAlign = 4
; CHECK-NEXT:     [DA: Div] i8* [[VP_ARR_SOA_PRIV64_BCAST:%.*]] = bitcast [1024 x i64]* [[VP_ARR_SOA_PRIV64]]
; CHECK-NEXT:     [DA: Div] call i64 8192 i8* [[VP_ARR_SOA_PRIV64_BCAST]] void (i64, i8*)* @llvm.lifetime.start.p0i8 [Serial] 
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:     [DA: Uni] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:     [DA: Uni] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1
; CHECK-NEXT:     [DA: Uni] br [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB3]] ]
; CHECK-NEXT:     [DA: Div] i64* [[VP_UNI_GEP:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 0
; CHECK-NEXT:     [DA: Div] i64 [[VP_LDSTD:%.*]] = load i64* [[VP_UNI_GEP]]
; CHECK-NEXT:     [DA: Uni] br i1 true, [[BB4:BB[0-9]+]], [[BB5:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB5]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div] i64* [[VP_UNI_ELSE:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 1
; CHECK-NEXT:       [DA: Uni] br [[BB3]]
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div] i64* [[VP_UNI_IF:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 0
; CHECK-NEXT:       [DA: Uni] br [[BB3]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB5]], [[BB4]]
; CHECK-NEXT:     [DA: Div] i64* [[VP_PHI_MIX_UNI:%.*]] = phi  [ i64* [[VP_UNI_ELSE]], [[BB5]] ],  [ i64* [[VP_UNI_IF]], [[BB4]] ]
; CHECK-NEXT:     [DA: Div] i64 [[VP_LD:%.*]] = load i64* [[VP_PHI_MIX_UNI]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_CONST_STEP:%.*]] = const-step-vector: { Start:0, Step:1, NumSteps:2}
; CHECK-NEXT:     [DA: Div] i64* [[VP_GEP_MIX_UNI:%.*]] = getelementptr inbounds i64* [[VP_PHI_MIX_UNI]] i64 [[VP_LD]] i64 [[VP_CONST_STEP]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_LD_PHI_DERIVED:%.*]] = load i64* [[VP_GEP_MIX_UNI]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:     [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ult i64 [[VP_IV1_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]
; CHECK-NEXT:     [DA: Uni] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB2]], [[BB6:BB[0-9]+]]
;
entry:
  %arr.soa.priv64 = alloca [1024 x i64], align 4
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i64]* %arr.soa.priv64)]
  br label %simd.loop.preheader

simd.loop.preheader:
  br label %simd.loop

simd.loop:
  %iv1 = phi i64 [ 0, %simd.loop.preheader ], [ %iv1.next, %simd.check.phi]
  %uni.gep = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 0
  %ldstd = load i64, i64* %uni.gep, align 4
  br i1 true, label %bb1, label %bb2

bb1:
  %uni.if = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 0
  br label %simd.check.phi

bb2:
  %uni.else = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 1
  br label %simd.check.phi

simd.check.phi:
  %phi.mix.uni = phi i64* [%uni.else, %bb2], [%uni.if, %bb1]
  %ld = load i64, i64* %phi.mix.uni, align 4
  %gep.mix.uni = getelementptr inbounds i64, i64* %phi.mix.uni, i64 %ld
  %ld.phi.derived = load i64, i64* %gep.mix.uni, align 4
  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %simd.loop, label %simd.end

simd.end:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @merge_uniform_strided_soa_geps() {
; CHECK-LABEL:  VPlan after Dump Transformed SOA GEPs:
; CHECK-NEXT:  VPlan IR for: merge_uniform_strided_soa_geps:simd.loop
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni] br [[BB1:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Div] [1024 x i64]* [[VP_ARR_SOA_PRIV64:%.*]] = allocate-priv [1024 x i64]*, OrigAlign = 4
; CHECK-NEXT:     [DA: Div] i8* [[VP_ARR_SOA_PRIV64_BCAST:%.*]] = bitcast [1024 x i64]* [[VP_ARR_SOA_PRIV64]]
; CHECK-NEXT:     [DA: Div] call i64 8192 i8* [[VP_ARR_SOA_PRIV64_BCAST]] void (i64, i8*)* @llvm.lifetime.start.p0i8 [Serial] 
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:     [DA: Uni] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:     [DA: Uni] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1
; CHECK-NEXT:     [DA: Uni] br [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB3]] ]
; CHECK-NEXT:     [DA: Div] i64* [[VP_UNI_GEP:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 0
; CHECK-NEXT:     [DA: Div] i64 [[VP_LDSTD:%.*]] = load i64* [[VP_UNI_GEP]]
; CHECK-NEXT:     [DA: Uni] br i1 true, [[BB4:BB[0-9]+]], [[BB5:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB5]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div] i64* [[VP_UNI_ELSE:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 1
; CHECK-NEXT:       [DA: Div] i64* [[VP_UNI_ELSE_1:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 1
; CHECK-NEXT:       [DA: Div] i64 [[VP_CONST_STEP:%.*]] = const-step-vector: { Start:0, Step:1, NumSteps:2}
; CHECK-NEXT:       [DA: Div] i64* [[VP0:%.*]] = getelementptr i64* [[VP_UNI_ELSE_1]] i64 0 i64 [[VP_CONST_STEP]]
; CHECK-NEXT:       [DA: Div] i64 [[VP_LD_ELSE:%.*]] = load i64* [[VP_UNI_ELSE]]
; CHECK-NEXT:       [DA: Uni] br [[BB3]]
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div] i64 [[VP_CONST_STEP_1:%.*]] = const-step-vector: { Start:0, Step:1, NumSteps:2}
; CHECK-NEXT:       [DA: Div] i64* [[VP_STR_IF:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 [[VP_IV1]] i64 [[VP_CONST_STEP_1]]
; CHECK-NEXT:       [DA: Uni] br [[BB3]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB5]], [[BB4]]
; CHECK-NEXT:     [DA: Div] i64* [[VP_PHI_MIX_UNI:%.*]] = phi  [ i64* [[VP0]], [[BB5]] ],  [ i64* [[VP_STR_IF]], [[BB4]] ]
; CHECK-NEXT:     [DA: Div] i64 [[VP_LD:%.*]] = load i64* [[VP_PHI_MIX_UNI]]
; CHECK-NEXT:     [DA: Div] i64* [[VP_GEP_MIX_UNI:%.*]] = getelementptr inbounds i64* [[VP_PHI_MIX_UNI]] i64 [[VP_LD]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_LD_PHI_DERIVED:%.*]] = load i64* [[VP_GEP_MIX_UNI]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:     [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ult i64 [[VP_IV1_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]
; CHECK-NEXT:     [DA: Uni] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB2]], [[BB6:BB[0-9]+]]
;
entry:
  %arr.soa.priv64 = alloca [1024 x i64], align 4
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i64]* %arr.soa.priv64)]
  br label %simd.loop.preheader

simd.loop.preheader:
  br label %simd.loop

simd.loop:
  %iv1 = phi i64 [ 0, %simd.loop.preheader ], [ %iv1.next, %simd.check.phi]
  %uni.gep = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 0
  %ldstd = load i64, i64* %uni.gep, align 4
  br i1 true, label %bb1, label %bb2

bb1:
  %str.if = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 %iv1
  br label %simd.check.phi

bb2:
  %uni.else = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 1
  %ld.else = load i64, i64* %uni.else, align 4
  br label %simd.check.phi

simd.check.phi:
  %phi.mix.uni = phi i64* [%uni.else, %bb2], [%str.if, %bb1]
  %ld = load i64, i64* %phi.mix.uni, align 4
  %gep.mix.uni = getelementptr inbounds i64, i64* %phi.mix.uni, i64 %ld
  %ld.phi.derived = load i64, i64* %gep.mix.uni, align 4
  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %simd.loop, label %simd.end

simd.end:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @merge_str_soa_geps() {
; CHECK-LABEL:  VPlan after Dump Transformed SOA GEPs:
; CHECK-NEXT:  VPlan IR for: merge_str_soa_geps:simd.loop
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] [1024 x i64]* [[VP_ARR_SOA_PRIV64:%.*]] = allocate-priv [1024 x i64]*, OrigAlign = 4 (SVAOpBits )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i8* [[VP_ARR_SOA_PRIV64_BCAST:%.*]] = bitcast [1024 x i64]* [[VP_ARR_SOA_PRIV64]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] call i64 8192 i8* [[VP_ARR_SOA_PRIV64_BCAST]] void (i64, i8*)* @llvm.lifetime.start.p0i8 [Serial] 
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB3]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_UNI_GEP:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 0 (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 true, [[BB4:BB[0-9]+]], [[BB5:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB5]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div, SVA: ( V )] i64* [[VP_STR_ELSE:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 [[VP_IV1]] (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:       [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div, SVA: ( V )] i64* [[VP_STR_IF:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 [[VP_IV1]] (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:       [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB5]], [[BB4]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_PHI_MIX_UNI:%.*]] = phi  [ i64* [[VP_STR_ELSE]], [[BB5]] ],  [ i64* [[VP_STR_IF]], [[BB4]] ] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LD:%.*]] = load i64* [[VP_PHI_MIX_UNI]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_GEP_MIX_UNI:%.*]] = getelementptr inbounds i64* [[VP_PHI_MIX_UNI]] i64 [[VP_LD]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LD_PHI_DERIVED:%.*]] = load i64* [[VP_GEP_MIX_UNI]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ult i64 [[VP_IV1_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB2]], [[BB6:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
;
entry:
  %arr.soa.priv64 = alloca [1024 x i64], align 4
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i64]* %arr.soa.priv64)]
  br label %simd.loop.preheader

simd.loop.preheader:
  br label %simd.loop

simd.loop:
  %iv1 = phi i64 [ 0, %simd.loop.preheader ], [ %iv1.next, %simd.check.phi]
  %uni.gep = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 0
  br i1 true, label %bb1, label %bb2

bb1:
  %str.if = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 %iv1
  br label %simd.check.phi

bb2:
  %str.else = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 %iv1
  br label %simd.check.phi

simd.check.phi:
  %phi.mix.uni = phi i64* [%str.else, %bb2], [%str.if, %bb1]
  %ld = load i64, i64* %phi.mix.uni, align 4
  %gep.mix.uni = getelementptr inbounds i64, i64* %phi.mix.uni, i64 %ld
  %ld.phi.derived = load i64, i64* %gep.mix.uni, align 4
  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %simd.loop, label %simd.end

simd.end:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @merge_aos_soa_geps() {
; CHECK-LABEL:  VPlan after Dump Transformed SOA GEPs:
; CHECK-NEXT:  VPlan IR for: merge_aos_soa_geps:simd.loop
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] [1024 x i64]* [[VP_ARR_AOS_PRIV64:%.*]] = allocate-priv [1024 x i64]*, OrigAlign = 4 (SVAOpBits )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i8* [[VP_ARR_AOS_PRIV64_BCAST:%.*]] = bitcast [1024 x i64]* [[VP_ARR_AOS_PRIV64]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] call i64 8192 i8* [[VP_ARR_AOS_PRIV64_BCAST]] void (i64, i8*)* @llvm.lifetime.start.p0i8 [Serial] 
; CHECK-NEXT:     [DA: Div, SVA: ( V )] [1024 x i64]* [[VP_ARR_SOA_PRIV64:%.*]] = allocate-priv [1024 x i64]*, OrigAlign = 4 (SVAOpBits )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i8* [[VP_ARR_SOA_PRIV64_BCAST:%.*]] = bitcast [1024 x i64]* [[VP_ARR_SOA_PRIV64]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] call i64 8192 i8* [[VP_ARR_SOA_PRIV64_BCAST]] void (i64, i8*)* @llvm.lifetime.start.p0i8 [Serial] 
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB3]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_UNI_GEP:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 0 (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_DIV_GEP:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_AOS_PRIV64]] i64 0 i64 0 (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_CALL:%.*]] = call i64* [[VP_DIV_GEP]] i64 (i64*)* @helper [Serial] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 true, [[BB4:BB[0-9]+]], [[BB5:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB5]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div, SVA: ( V )] i64* [[VP_UNI_ELSE:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_SOA_PRIV64]] i64 0 i64 [[VP_IV1]] (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:       [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:      [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:       [DA: Div, SVA: ( V )] i64* [[VP_AOS_IF:%.*]] = getelementptr inbounds [1024 x i64]* [[VP_ARR_AOS_PRIV64]] i64 0 i64 [[VP_IV1]] (SVAOpBits 0->V 1->V 2->V )
; CHECK-NEXT:       [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB5]], [[BB4]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_PHI_MIX_UNI:%.*]] = phi  [ i64* [[VP_DIV_GEP]], [[BB4]] ],  [ i64* [[VP_UNI_ELSE]], [[BB5]] ] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LD:%.*]] = load i64* [[VP_PHI_MIX_UNI]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64* [[VP_GEP_MIX_UNI:%.*]] = getelementptr inbounds i64* [[VP_PHI_MIX_UNI]] i64 [[VP_LD]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LD_PHI_DERIVED:%.*]] = load i64* [[VP_GEP_MIX_UNI]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ult i64 [[VP_IV1_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB2]], [[BB6:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
;
entry:
  %arr.soa.priv64 = alloca [1024 x i64], align 4
  %arr.aos.priv64 = alloca [1024 x i64], align 4
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i64]* %arr.soa.priv64, [1024 x i64]* %arr.aos.priv64)]
  br label %simd.loop.preheader

simd.loop.preheader:
  br label %simd.loop

simd.loop:
  %iv1 = phi i64 [ 0, %simd.loop.preheader ], [ %iv1.next, %simd.check.phi]
  %uni.gep = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 0
  %div.gep = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.aos.priv64, i64 0, i64 0
  %call = call i64 @helper(i64* nonnull %div.gep)
  br i1 true, label %bb1, label %bb2

bb1:
  %aos.if = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.aos.priv64, i64 0, i64 %iv1
  br label %simd.check.phi

bb2:
  %uni.else = getelementptr inbounds [1024 x i64], [1024 x i64]* %arr.soa.priv64, i64 0, i64 %iv1
  br label %simd.check.phi

simd.check.phi:
  %phi.mix.uni = phi i64* [%div.gep, %bb1], [%uni.else, %bb2]
  %ld = load i64, i64* %phi.mix.uni, align 4
  %gep.mix.uni = getelementptr inbounds i64, i64* %phi.mix.uni, i64 %ld
  %ld.phi.derived = load i64, i64* %gep.mix.uni, align 4
  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %simd.loop, label %simd.end

simd.end:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}



declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token %0)
declare dso_local i64 @helper(i64*)
