; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -passes='hir-ssa-deconstruction,hir-temp-cleanup,hir-vplan-vec,print<hir>' -disable-output -vplan-print-after-vpentity-instrs < %s 2>&1 | FileCheck %s

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; <0>          BEGIN REGION { }
; <2>                %0 = @llvm.directive.region.entry(); [ DIR.OMP.SIMD(),  QUAL.OMP.LINEAR(&((%lp.addr.linear)[0])2),  QUAL.OMP.LINEAR:IV(&((%l1.linear.iv)[0])1),  QUAL.OMP.NORMALIZED.IV(null),  QUAL.OMP.NORMALIZED.UB(null) ]
; <19>
; <19>               + DO i1 = 0, 1023, 1   <DO_LOOP> <simd>
; <6>                |   @_Z3bazPPl(&((%lp.addr.linear)[0]));
; <7>                |   %1 = (%lp.addr.linear)[0];
; <9>                |   (%lp.addr.linear)[0] = &((%1)[2]);
; <19>               + END LOOP
; <19>
; <17>               @llvm.directive.region.exit(%0); [ DIR.OMP.END.SIMD() ]
; <0>          END REGION

define void @_Z3fooPl(i64* noundef %lp) {
; CHECK-LABEL:  VPlan after insertion of VPEntities instructions:
; CHECK-NEXT:  VPlan IR for: _Z3fooPl:HIR.#{{[0-9]+}}
; CHECK-NEXT:  External Defs Start:
; CHECK-DAG:     [[VP0:%.*]] = {%lp.addr.linear}
; CHECK-DAG:     [[VP1:%.*]] = {%l1.linear.iv}
; CHECK-NEXT:  External Defs End:
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     br [[BB1:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     i64** [[VP_LP_ADDR_LINEAR:%.*]] = allocate-priv i64*, OrigAlign = 8
; CHECK-NEXT:     i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 0 i64 1
; CHECK-NEXT:     i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:     i64* [[VP_LOAD:%.*]] = load i64** [[LP_ADDR_LINEAR0:%.*]]
; CHECK-NEXT:     i64* [[VP_LP_ADDR_LINEAR_IND_INIT:%.*]] = induction-init{getelementptr} i64* [[VP_LOAD]] i64 2
; CHECK-NEXT:     store i64* [[VP_LP_ADDR_LINEAR_IND_INIT]] i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     i64 [[VP_LP_ADDR_LINEAR_IND_INIT_STEP:%.*]] = induction-init-step{getelementptr} i64 2
; CHECK-NEXT:     br [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     i64 [[VP2:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP3:%.*]], [[BB2]] ]
; CHECK-NEXT:     i64* [[VP4:%.*]] = phi  [ i64* [[VP_LP_ADDR_LINEAR_IND_INIT]], [[BB1]] ],  [ i64* [[VP5:%.*]], [[BB2]] ]
; CHECK-NEXT:     store i64* [[VP4]] i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     i64** [[VP_SUBSCRIPT:%.*]] = subscript inbounds i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     call i64** [[VP_SUBSCRIPT]] void (i64**)* @_Z3bazPPl
; CHECK-NEXT:     i64** [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     i64* [[VP_LOAD_1:%.*]] = load i64** [[VP_SUBSCRIPT_1]]
; CHECK-NEXT:     i64* [[VP_SUBSCRIPT_2:%.*]] = subscript inbounds i64* [[VP_LOAD_1]] i64 2
; CHECK-NEXT:     i64** [[VP_SUBSCRIPT_3:%.*]] = subscript inbounds i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     store i64* [[VP_SUBSCRIPT_2]] i64** [[VP_SUBSCRIPT_3]]
; CHECK-NEXT:     i64 [[VP3]] = add i64 [[VP2]] i64 [[VP__IND_INIT_STEP]]
; CHECK-NEXT:     i64* [[VP5]] = getelementptr inbounds i64* [[VP4]] i64 [[VP_LP_ADDR_LINEAR_IND_INIT_STEP]]
; CHECK-NEXT:     i1 [[VP6:%.*]] = icmp slt i64 [[VP3]] i64 1024
; CHECK-NEXT:     br i1 [[VP6]], [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1
; CHECK-NEXT:     i64* [[VP_LOAD_2:%.*]] = load i64** [[VP_LP_ADDR_LINEAR]]
; CHECK-NEXT:     i64* [[VP_LP_ADDR_LINEAR_IND_FINAL:%.*]] = induction-final{getelementptr} i64* [[VP_LOAD]] i64 2
; CHECK-NEXT:     store i64* [[VP_LP_ADDR_LINEAR_IND_FINAL]] i64** [[LP_ADDR_LINEAR0]]
; CHECK-NEXT:     br [[BB4:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB3]]
; CHECK-NEXT:     br <External Block>
;
DIR.OMP.SIMD.1:
  %lp.addr.linear = alloca i64*, align 8
  %l1.linear.iv = alloca i64, align 8
  store i64* %lp, i64** %lp.addr.linear, align 8
  br label %DIR.OMP.SIMD.116

DIR.OMP.SIMD.116:                                 ; preds = %DIR.OMP.SIMD.1
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.LINEAR"(i64** %lp.addr.linear, i32 2), "QUAL.OMP.LINEAR:IV"(i64* %l1.linear.iv, i32 1), "QUAL.OMP.NORMALIZED.IV"(i8* null), "QUAL.OMP.NORMALIZED.UB"(i8* null) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.116
  %.omp.iv.local.010 = phi i64 [ 0, %DIR.OMP.SIMD.116 ], [ %add1, %omp.inner.for.body ]
  call void @_Z3bazPPl(i64** noundef nonnull %lp.addr.linear)
  %1 = load i64*, i64** %lp.addr.linear, align 8
  %add.ptr = getelementptr inbounds i64, i64* %1, i64 2
  store i64* %add.ptr, i64** %lp.addr.linear, align 8
  %add1 = add nuw nsw i64 %.omp.iv.local.010, 1
  %exitcond.not = icmp eq i64 %add1, 1024
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.115, label %omp.inner.for.body

DIR.OMP.END.SIMD.115:                             ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.2

DIR.OMP.END.SIMD.2:                               ; preds = %DIR.OMP.END.SIMD.115
  ret void
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

declare dso_local void @_Z3bazPPl(i64** noundef) #0

attributes #0 = { nounwind }
