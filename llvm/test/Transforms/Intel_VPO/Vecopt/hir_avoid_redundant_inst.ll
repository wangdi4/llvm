; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -passes="hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec,print<hir>" -vplan-avoid-redundant-inst -vplan-print-after-plain-cfg -vplan-force-vf=4 -disable-output < %s 2>&1 | FileCheck %s
;
; LIT test to check that we do not generate redundant instructions during canon
; expression decomposition. Incoming HIR looks like the following:
;
;       + DO i64 i1 = 0, 1023, 1   <DO_LOOP>
;       |   (%lp1)[i1 + 2] = i1;
;       |   <LVAL-REG> {al:4}(LINEAR i32* %lp1)[LINEAR i64 i1 + 2] inbounds  {sb:12}
;       |      <BLOB> LINEAR i32* %lp1 {sb:7}
;       |   <RVAL-REG> LINEAR trunc.i64.i32(i1) {sb:2}
;       |
;       |   (%lp2)[i1 + 2] = i1;
;       |   <LVAL-REG> {al:4}(LINEAR i32* %lp2)[LINEAR i64 i1 + 2] inbounds  {sb:13}
;       |      <BLOB> LINEAR i32* %lp2 {sb:9}
;       |   <RVAL-REG> LINEAR trunc.i64.i32(i1) {sb:2}
;       |
;       + END LOOP
;
; Test checks that we do not generate a duplicate instruction for either i1 + 2
; or for truncation of of i1 to i32.
;
;
define void @foo(ptr noalias %lp1, ptr noalias %lp2) {
; CHECK-LABEL:  VPlan after importing plain CFG:
; CHECK-NEXT:  VPlan IR for: foo:HIR.#{{[0-9]+}}
; CHECK-NEXT:  External Defs Start:
; CHECK-DAG:     [[VP0:%.*]] = {%lp2}
; CHECK-DAG:     [[VP1:%.*]] = {%lp1}
; CHECK-NEXT:  External Defs End:
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     br [[BB1:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     br [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     i64 [[VP2:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP3:%.*]], [[BB2]] ]
; CHECK-NEXT:     i32 [[VP4:%.*]] = trunc i64 [[VP2]] to i32
; CHECK-NEXT:     i64 [[VP5:%.*]] = add i64 [[VP2]] i64 2
; CHECK-NEXT:     ptr [[VP_SUBSCRIPT:%.*]] = subscript inbounds ptr [[LP10:%.*]] i64 [[VP5]]
; CHECK-NEXT:     store i32 [[VP4]] ptr [[VP_SUBSCRIPT]]
; CHECK-NEXT:     ptr [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds ptr [[LP20:%.*]] i64 [[VP5]]
; CHECK-NEXT:     store i32 [[VP4]] ptr [[VP_SUBSCRIPT_1]]
; CHECK-NEXT:     i64 [[VP3]] = add i64 [[VP2]] i64 1
; CHECK-NEXT:     i1 [[VP6:%.*]] = icmp slt i64 [[VP3]] i64 1024
; CHECK-NEXT:     br i1 [[VP6]], [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     br [[BB4:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB3]]
; CHECK-NEXT:     br <External Block>
;
;
; CHECK:             + DO i1 = 0, 1023, 4   <DO_LOOP> <auto-vectorized> <novectorize>
; CHECK-NEXT:        |   (<4 x i32>*)(%lp1)[i1 + 2] = i1 + <i64 0, i64 1, i64 2, i64 3>
; CHECK-NEXT:        |   (<4 x i32>*)(%lp2)[i1 + 2] = i1 + <i64 0, i64 1, i64 2, i64 3>
; CHECK-NEXT:        + END LOOP
; CHECK-NEXT:  END REGION
;
entry:
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %l1.010 = phi i64 [ 0, %entry ], [ %inc, %for.body ]
  %conv = trunc i64 %l1.010 to i32
  %add = add nuw nsw i64 %l1.010, 2
  %arrayidx = getelementptr inbounds i32, ptr %lp1, i64 %add
  store i32 %conv, ptr %arrayidx, align 4
  %arrayidx3 = getelementptr inbounds i32, ptr %lp2, i64 %add
  store i32 %conv, ptr %arrayidx3, align 4
  %inc = add nuw nsw i64 %l1.010, 1
  %exitcond.not = icmp eq i64 %inc, 1024
  br i1 %exitcond.not, label %for.end, label %for.body

for.end:                                          ; preds = %for.body
  ret void
}
