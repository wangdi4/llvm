; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py UTC_ARGS: --version 2
; RUN: opt -passes='hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec' \
; RUN:     -vplan-force-vf=2 \
; RUN:     -vplan-enable-peeling \
; RUN:     -debug-only=vplan-scalar-evolution,vplan-alignment-analysis \
; RUN:     -print-before=hir-vplan-vec -disable-output < %s 2>&1 \
; RUN: | FileCheck %s

; In this test there are 3 nested loops in the HIR region, and only the
; innermost loop is vectorized by VPlan. The purpose of this test is to check
; that higher dimensions (> 1) are handled properly.
;
;     ! A  - 3D array of double [NX * NY * NZ]
;     DO K = 1, NZ
;        DO J = 1, NY
;           DO I = 1, NX
;              TMP = A(I-1,J-1,K) + A(I-1,J,K) + A(I-1,J+1,K) + &
;                    A(I  ,J-1,K) + A(I  ,J,K) + A(I  ,J+1,K) + &
;                    A(I+1,J-1,K) + A(I+1,J,K) + A(I+1,J+1,K)
;              ...
;           END DO
;        END DO
;     END DO
;
; In the above loop-nest, each memref in each column is congruent to one
; another; for example, A(I-1,J-1,K) is congruent to A(I,J-1,K) and A(I+1,J-1,K).
;
; However, none of the memrefs in any column are congruent to a memref in
; another column; for example, A(I-1,J-1,K) is *not* congruent to A(I-1,J,K).
;
; Additionally, for the purposes of alignment analysis, only the top-most
; memory ref in each column is considered congruent to the bottom-most: in the
; sense that aligning one pointer will transfer an alignment of 16 (since the
; distance between refs in bytes is 2 * 8) to the other.

define void @foo(ptr noalias %A, ptr noalias %OUT, i64 %NX, i64 %NY, i64 %NZ, i64 %K_STRIDE, i64 %J_STRIDE) {
; CHECK-LABEL: Function: foo
; CHECK-EMPTY:
; CHECK-NEXT:  BEGIN REGION { }
; CHECK-NEXT:        + DO i1 = 0, [[NZ0:%.*]] + -1, 1   <DO_LOOP>
; CHECK-NEXT:        |   + DO i2 = 0, [[NY0:%.*]] + -1, 1   <DO_LOOP>
; CHECK-NEXT:        |   |   [[ENTRY_REGION0:%.*]] = @llvm.directive.region.entry()
; CHECK-NEXT:        |   |
; CHECK-NEXT:        |   |   + DO i3 = 0, [[NX0:%.*]] + -1, 1   <DO_LOOP>
; CHECK-NEXT:        |   |   |   [[TL0:%.*]] = ([[IN0:%.*]])[i1][i2 + -1][i3 + -1]
; CHECK-NEXT:        |   |   |   [[CL0:%.*]] = ([[IN0]])[i1][i2 + -1][i3]
; CHECK-NEXT:        |   |   |   [[BL0:%.*]] = ([[IN0]])[i1][i2 + -1][i3 + 1]
; CHECK-NEXT:        |   |   |   [[TC0:%.*]] = ([[IN0]])[i1][i2][i3 + -1]
; CHECK-NEXT:        |   |   |   [[CC0:%.*]] = ([[IN0]])[i1][i2][i3]
; CHECK-NEXT:        |   |   |   [[BC0:%.*]] = ([[IN0]])[i1][i2][i3 + 1]
; CHECK-NEXT:        |   |   |   [[TR0:%.*]] = ([[IN0]])[i1][i2 + 1][i3 + -1]
; CHECK-NEXT:        |   |   |   [[CR0:%.*]] = ([[IN0]])[i1][i2 + 1][i3]
; CHECK-NEXT:        |   |   |   [[BR0:%.*]] = ([[IN0]])[i1][i2 + 1][i3 + 1]
; CHECK-NEXT:        |   |   + END LOOP
; CHECK-NEXT:        |   |
; CHECK-NEXT:        |   |   @llvm.directive.region.exit([[ENTRY_REGION0]])
; CHECK-NEXT:        |   + END LOOP
; CHECK-NEXT:        + END LOOP
; CHECK-NEXT:  END REGION
; CHECK-EMPTY:
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD:%.*]] = load ptr [[VP_SUBSCRIPT:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_1:%.*]] = load ptr [[VP_SUBSCRIPT_1:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_2:%.*]] = load ptr [[VP_SUBSCRIPT_2:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_3:%.*]] = load ptr [[VP_SUBSCRIPT_3:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_4:%.*]] = load ptr [[VP_SUBSCRIPT_4:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_5:%.*]] = load ptr [[VP_SUBSCRIPT_5:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_6:%.*]] = load ptr [[VP_SUBSCRIPT_6:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_7:%.*]] = load ptr [[VP_SUBSCRIPT_7:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP_LOAD_8:%.*]] = load ptr [[VP_SUBSCRIPT_8:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP0:%.*]] = load ptr [[VP1:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP2:%.*]] = load ptr [[VP3:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP4:%.*]] = load ptr [[VP5:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP6:%.*]] = load ptr [[VP7:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP8:%.*]] = load ptr [[VP9:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP10:%.*]] = load ptr [[VP11:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP12:%.*]] = load ptr [[VP13:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + -8),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP14:%.*]] = load ptr [[VP15:%.*]])
; CHECK-NEXT:    -> {([[IN0]]),+,8}
; CHECK-NEXT:  computeAddressSCEV(double [[VP16:%.*]] = load ptr [[VP17:%.*]])
; CHECK-NEXT:    -> {([[IN0]] + 8),+,8}
;
; CHECK-LABEL: VPlanPeelingAnalysis Candidate Memrefs:
; CHECK-NEXT:  double [[VP_LOAD]] = load ptr [[VP_SUBSCRIPT]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD_2]] = load ptr [[VP_SUBSCRIPT_2]] (A16)
; CHECK-NEXT:  double [[VP_LOAD_1]] = load ptr [[VP_SUBSCRIPT_1]] | KB = 0b????????
; CHECK-NEXT:    (none)
; CHECK-NEXT:  double [[VP_LOAD_2]] = load ptr [[VP_SUBSCRIPT_2]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD]] = load ptr [[VP_SUBSCRIPT]] (A16)
; CHECK-NEXT:  double [[VP_LOAD_3]] = load ptr [[VP_SUBSCRIPT_3]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD_5]] = load ptr [[VP_SUBSCRIPT_5]] (A16)
; CHECK-NEXT:  double [[VP_LOAD_4]] = load ptr [[VP_SUBSCRIPT_4]] | KB = 0b????????
; CHECK-NEXT:    (none)
; CHECK-NEXT:  double [[VP_LOAD_5]] = load ptr [[VP_SUBSCRIPT_5]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD_3]] = load ptr [[VP_SUBSCRIPT_3]] (A16)
; CHECK-NEXT:  double [[VP_LOAD_6]] = load ptr [[VP_SUBSCRIPT_6]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD_8]] = load ptr [[VP_SUBSCRIPT_8]] (A16)
; CHECK-NEXT:  double [[VP_LOAD_7]] = load ptr [[VP_SUBSCRIPT_7]] | KB = 0b????????
; CHECK-NEXT:    (none)
; CHECK-NEXT:  double [[VP_LOAD_8]] = load ptr [[VP_SUBSCRIPT_8]] | KB = 0b????????
; CHECK-NEXT:    -> double [[VP_LOAD_6]] = load ptr [[VP_SUBSCRIPT_6]] (A16)
;

entry:
  br label %body.1

body.1:
  %iv.k = phi i64 [ 0, %entry ], [ %iv.k.next, %latch.1 ]
  %A.k = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 2, i64 0, i64 %K_STRIDE, ptr nonnull elementtype(double) %A, i64 %iv.k)
  br label %body.2

body.2:
  %iv.j = phi i64 [ 0, %body.1 ], [ %iv.j.next, %latch.2 ]
  %A.j.k = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 1, i64 0, i64 %J_STRIDE, ptr nonnull elementtype(double) %A.k, i64 %iv.j)

  %iv.j.sub.1 = sub nuw nsw i64 %iv.j, 1
  %A.j.sub.1.k = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 1, i64 0, i64 %J_STRIDE, ptr nonnull elementtype(double) %A.k, i64 %iv.j.sub.1)

  %iv.j.add.1 = add nuw nsw i64 %iv.j, 1
  %A.j.add.1.k = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 1, i64 0, i64 %J_STRIDE, ptr nonnull elementtype(double) %A.k, i64 %iv.j.add.1)

  br label %body.3

body.3:
  %iv.i = phi i64 [ 0, %body.2 ], [ %iv.i.next, %body.3 ]
  %iv.i.sub.1 = sub nuw nsw i64 %iv.i, 1
  %iv.i.add.1 = add nuw nsw i64 %iv.i, 1

  ; Key:
  ;  TL TC TR
  ;  CL CC CR
  ;  BL BC BR

  %TL.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.sub.1.k, i64 %iv.i.sub.1)
  %TL = load double, ptr %TL.p, align 8
  %CL.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.sub.1.k, i64 %iv.i)
  %CL = load double, ptr %CL.p, align 8
  %BL.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.sub.1.k, i64 %iv.i.add.1)
  %BL = load double, ptr %BL.p, align 8

  %TC.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.k, i64 %iv.i.sub.1)
  %TC = load double, ptr %TC.p, align 8
  %CC.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.k, i64 %iv.i)
  %CC = load double, ptr %CC.p, align 8
  %BC.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.k, i64 %iv.i.add.1)
  %BC = load double, ptr %BC.p, align 8

  %TR.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.add.1.k, i64 %iv.i.sub.1)
  %TR = load double, ptr %TR.p, align 8
  %CR.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.add.1.k, i64 %iv.i)
  %CR = load double, ptr %CR.p, align 8
  %BR.p = call ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8 0, i64 0, i64 8, ptr nonnull elementtype(double) %A.j.add.1.k, i64 %iv.i.add.1)
  %BR = load double, ptr %BR.p, align 8

  %iv.i.next = add nuw nsw i64 %iv.i, 1
  %exitcond.3 = icmp eq i64 %iv.i.next, %NX
  br i1 %exitcond.3, label %latch.2, label %body.3

latch.2:
  %iv.j.next = add nuw nsw i64 %iv.j, 1
  %exitcond.2 = icmp eq i64 %iv.j.next, %NY
  br i1 %exitcond.2, label %latch.1, label %body.2

latch.1:
  %iv.k.next = add nuw nsw i64 %iv.k, 1
  %exitcond.1 = icmp eq i64 %iv.k.next, %NZ
  br i1 %exitcond.1, label %exit, label %body.1

exit:
  ret void
}

; Function Attrs: nounwind
declare token @llvm.directive.region.entry() #0

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token) #0

; Function Attrs: mustprogress nocallback nofree norecurse nosync nounwind speculatable willreturn memory(none)
declare ptr @llvm.intel.subscript.p0.i64.i64.p0.i64(i8, i64, i64, ptr, i64) #1

attributes #0 = { nounwind }
attributes #1 = { mustprogress nocallback nofree norecurse nosync nounwind speculatable willreturn memory(none) }
