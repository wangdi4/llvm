; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; REQUIRES: asserts
; RUN: opt -S < %s -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec -disable-output -vplan-dump-external-defs-hir=false -vplan-enable-scalvec-analysis -vplan-print-scalvec-results -vplan-enable-new-cfg-merge-hir=0 | FileCheck %s
; RUN: opt -S < %s -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec -disable-output -vplan-dump-external-defs-hir=false -vplan-enable-scalvec-analysis -vplan-print-scalvec-results -vplan-enable-new-cfg-merge-hir=1 | FileCheck %s

; One of the purposes of this test is to confirm that we don't store any stale
; SVA results in the final copy presented to CG. Thus SVA tests are not
; enforcing any VFs because we want to let SVA run for all different VFs and
; have different results, but still end up storing only the best VF results
; ultimately.

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind uwtable
define dso_local void @uniformDivergent(i32* nocapture %a, i32* nocapture %b, i64* nocapture %c) local_unnamed_addr {
; CHECK-LABEL:  VPlan after ScalVec analysis:
; CHECK-NEXT:  VPlan IR for: Initial VPlan for VF={{[2|4|8|16|32]}}
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK:          [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP1:%.*]], [[BB2]] ] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_LOAD:%.*]] = load i32* [[A0:%.*]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_LOAD_1:%.*]] = load i32* [[B0:%.*]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64* [[VP_SUBSCRIPT:%.*]] = subscript inbounds i64* [[C0:%.*]] i64 [[VP0]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LOAD_2:%.*]] = load i64* [[VP_SUBSCRIPT]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP2:%.*]] = add i32 [[VP_LOAD_1]] i32 [[VP_LOAD]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP3:%.*]] = sext i32 [[VP2]] to i64 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP4:%.*]] = mul i64 [[VP_LOAD_2]] i64 [[VP3]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64* [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds i64* [[C0]] i64 [[VP0]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i64 [[VP4]] i64* [[VP_SUBSCRIPT_1]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP1]] = add i64 [[VP0]] i64 [[VP__IND_INIT_STEP]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP5:%.*]] = icmp slt i64 [[VP1]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP5]], [[BB2]], [[BB3:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK:       External Uses:
; CHECK-NEXT:  Id: 0   no underlying for i64 [[VP__IND_FINAL]]
;
entry:
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %omp.inner.for.body ]

  ; Uniform instruction sequence later used in vector context.
  %a.load = load i32, i32* %a, align 4
  %b.load = load i32, i32* %b, align 4
  %uniform.add = add i32 %a.load, %b.load
  %add.sext = sext i32 %uniform.add to i64
  %c.gep = getelementptr inbounds i64, i64* %c, i64 %indvars.iv
  %c.load = load i64, i64* %c.gep, align 4
  %divergent.mul = mul i64 %add.sext, %c.load
  store i64 %divergent.mul, i64* %c.gep, align 4

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %DIR.OMP.END.SIMD.3, label %omp.inner.for.body

DIR.OMP.END.SIMD.3:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define dso_local void @gatherScatter(i32* nocapture %a, i32* nocapture %b) local_unnamed_addr {
; CHECK-LABEL:  VPlan after ScalVec analysis:
; CHECK-NEXT:  VPlan IR for: Initial VPlan for VF={{[2|4|8|16|32]}}
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK:          [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP1:%.*]], [[BB2]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP2:%.*]] = mul i64 2 i64 [[VP0]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32* [[VP_SUBSCRIPT:%.*]] = subscript inbounds i32* [[A0:%.*]] i64 [[VP2]] (SVAOpBits 0->V 1->V 2->V 3->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_LOAD:%.*]] = load i32* [[VP_SUBSCRIPT]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP3:%.*]] = add i32 [[VP_LOAD]] i32 42 (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32* [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds i32* [[B0:%.*]] i64 [[VP2]] (SVAOpBits 0->V 1->V 2->V 3->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP3]] i32* [[VP_SUBSCRIPT_1]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP1]] = add i64 [[VP0]] i64 [[VP__IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP5:%.*]] = icmp slt i64 [[VP1]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP5]], [[BB2]], [[BB3:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK:       External Uses:
; CHECK-NEXT:  Id: 0   no underlying for i64 [[VP__IND_FINAL]]
;
entry:
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %omp.inner.for.body ]
  %iv.x.2 = mul i64 %indvars.iv, 2
  %a.gep = getelementptr inbounds i32, i32* %a, i64 %iv.x.2
  %a.load = load i32, i32* %a.gep, align 4
  %add = add i32 %a.load, 42
  %b.gep = getelementptr inbounds i32, i32* %b, i64 %iv.x.2
  store i32 %add, i32* %b.gep
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %DIR.OMP.END.SIMD.3, label %omp.inner.for.body

DIR.OMP.END.SIMD.3:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define dso_local i32 @simpleReduction(i32* nocapture %a, i32 %b) local_unnamed_addr {
; CHECK-LABEL:  VPlan after ScalVec analysis:
; CHECK-NEXT:  VPlan IR for: Initial VPlan for VF={{[2|4|8|16|32]}}
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK:          [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_RED_INIT:%.*]] = reduction-init i32 0 i32 [[REDUCTION_PHI0:%.*]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP0:%.*]] = hir-copy i32 [[VP_RED_INIT]] , OriginPhiId: 0 (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP1:%.*]] = phi  [ i32 [[VP0]], [[BB1]] ],  [ i32 [[VP2:%.*]], [[BB2]] ] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP3:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP4:%.*]], [[BB2]] ] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_LOAD:%.*]] = load i32* [[A0:%.*]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP5:%.*]] = add i32 [[VP1]] i32 [[VP_LOAD]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP4]] = add i64 [[VP3]] i64 [[VP__IND_INIT_STEP]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP6:%.*]] = icmp slt i64 [[VP4]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP2]] = hir-copy i32 [[VP5]] , OriginPhiId: 0 (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP6]], [[BB2]], [[BB3:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     [DA: Uni, SVA: RetVal:(F  ), Inst:( V )] i32 [[VP_RED_FINAL:%.*]] = reduction-final{u_add} i32 [[VP5]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK:       External Uses:
; CHECK-NEXT:  Id: 0   i32 {{.*}} -> [[VP7:%.*]] = {%reduction.phi}
; CHECK-EMPTY:
; CHECK-NEXT:  Id: 1   no underlying for i64 [[VP__IND_FINAL]]
;
entry:
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %omp.inner.for.body ]
  %reduction.phi = phi i32 [ %b, %DIR.OMP.SIMD.1], [ %reduction.add, %omp.inner.for.body ]
  %uniform.a = load i32, i32* %a, align 4
  %reduction.add = add i32 %reduction.phi, %uniform.a
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %DIR.OMP.END.SIMD.3, label %omp.inner.for.body

DIR.OMP.END.SIMD.3:                               ; preds = %omp.inner.for.body
  %lcssa.red.phi = phi i32 [ %reduction.add, %omp.inner.for.body ]
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret i32 %lcssa.red.phi
}

define dso_local void @phiUsedByPhi(i64* nocapture %a, i64* nocapture %b) local_unnamed_addr {
; CHECK-LABEL:  VPlan after ScalVec analysis:
; CHECK-NEXT:  VPlan IR for: Initial VPlan for VF={{[2|4|8|16|32]}}
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK:          [DA: Uni, SVA: (F  )] br [[BB1:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 1024, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP__IND_INIT:%.*]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT]], [[BB1]] ],  [ i64 [[VP1:%.*]], [[BB3]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP2:%.*]] = hir-copy i64 [[VP0]] , OriginPhiId: -1 (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i1 [[VP3:%.*]] = icmp eq i64 [[VP0]] i64 42 (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB2]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i1 [[VP4:%.*]] = block-predicate i1 [[VP3]] (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64* [[VP_SUBSCRIPT:%.*]] = subscript inbounds i64* [[A0:%.*]] i64 [[VP0]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_LOAD:%.*]] = load i64* [[VP_SUBSCRIPT]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP5:%.*]] = hir-copy i64 [[VP_LOAD]] , OriginPhiId: -1 (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB4]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP__BLEND_BB4:%.*]] = blend [ i64 [[VP2]], i1 true ], [ i64 [[VP5]], i1 [[VP3]] ] (SVAOpBits 0->V 1->V 2->V 3->V )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64* [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds i64* [[B0:%.*]] i64 [[VP0]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i64 [[VP__BLEND_BB4]] i64* [[VP_SUBSCRIPT_1]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP1]] = add i64 [[VP0]] i64 [[VP__IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP6:%.*]] = icmp slt i64 [[VP1]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP6]], [[BB2]], [[BB5:BB[0-9]+]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB5]]: # preds: [[BB3]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP__IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB6:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK:       External Uses:
; CHECK-NEXT:  Id: 0   no underlying for i64 [[VP__IND_FINAL]]
;
entry:
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %if.end, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %if.end ]
  %cmp = icmp eq i64 %indvars.iv, 42
  br i1 %cmp, label %if.then, label %if.end

if.then:                                          ; preds = %omp.inner.for.body
  %a.gep = getelementptr inbounds i64, i64* %a, i64 %indvars.iv
  %a.load = load i64, i64* %a.gep, align 4
  br label %if.end

if.end:                                           ; preds = %if.then, %omp.inner.for.body
  %use.phi = phi i64 [ %a.load, %if.then], [ %indvars.iv, %omp.inner.for.body ]
  %b.gep = getelementptr inbounds i64, i64* %b, i64 %indvars.iv
  store i64 %use.phi, i64* %b.gep
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %DIR.OMP.END.SIMD.3, label %omp.inner.for.body

DIR.OMP.END.SIMD.3:                               ; preds = %if.end
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @no_side_effect_insts(<8 x i32> %val, i64 %n, i32 *%p) {
; FIXME: No CHECKs currently because loop is not handled due to
; insert/extractelement with variable index. The crash was happenning
; in SVA invocation inside CM, not the one peformed during CG that has
; dumps.
;
entry:
  br label %preheader

preheader:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %header

header:
  %iv = phi i64 [ 0, %preheader ], [ %iv.next, %header ]

  %uni.ld = load i32, i32 *%p
; SVA used to assert for it to not be side-effect free when trying to keep scalar.
  %extract = extractelement <8 x i32> %val, i32 %uni.ld
  store i32 %extract, i32 *%p

  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %exit, label %header

exit:
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token)

