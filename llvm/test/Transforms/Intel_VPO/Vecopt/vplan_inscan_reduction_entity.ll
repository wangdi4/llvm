; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
;; Test this for IR path only at this point, as HIR framework bails out both
;; due to unrecognized pragmas and a present atomic instruction.
; RUN: opt -disable-output -vplan-vec -vplan-print-after-final-cond-transform -vplan-entities-dump -disable-vplan-codegen  -vplan-force-vf=2 -vplan-force-inscan-reduction-vectorization=true -S < %s 2>&1 | FileCheck %s
; RUN: opt -disable-output -passes="vplan-vec" -vplan-print-after-final-cond-transform -vplan-entities-dump -disable-vplan-codegen  -vplan-force-vf=2 -vplan-force-inscan-reduction-vectorization=true -S < %s 2>&1 | FileCheck %s
; REQUIRES: asserts

;; void foo(float *A, float *B) {
;;   float x = 0.0f;
;; #pragma omp simd reduction(inscan, + : x)
;;   for (int i=0; i<1024; i++) {
;;     x += A[i];
;; #pragma omp scan inclusive(x)
;;     B[i] = x;
;;   }
;; }

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @omp_scan(float* %A, float* %B) {
;
; CHECK-LABEL:  Reduction list
; CHECK-NEXT:   (+) Start: float* [[X_RED0:%.*]]
; CHECK-NEXT:    Linked values:
; CHECK-NEXT:    inscan ReductionKind: inclusive
; CHECK-NEXT:   Memory: float* [[X_RED0]]
; CHECK-EMPTY:
; CHECK:         [[BB2:BB[0-9]+]]: # preds: [[BB1:BB[0-9]+]]
; CHECK:          [DA: Div] float* [[VP_X_RED:%.*]] = allocate-priv float*, OrigAlign = 4
; CHECK:          [DA: Uni] float [[VP_LOAD:%.*]] = load float* [[X_RED0]]
; CHECK-NEXT:     [DA: Uni] float [[VP_X_REDINSCAN_REDINSCAN_INIT:%.*]] = reduction-init-scalar float 0.000000e+00 float [[VP_LOAD]]
; CHECK:          [DA: Uni] br [[BB0:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB0]]: # preds: [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_INDVARS_IV:%.*]] = phi  [ i64 [[VP_INDVARS_IV_IND_INIT:%.*]], [[BB2]] ],  [ i64 [[VP_INDVARS_IV_NEXT:%.*]], [[BB3]] ]
; CHECK-NEXT:     [DA: Uni] float [[VP_INSCAN_ACCUM:%.*]] = phi  [ float [[VP_X_REDINSCAN_REDINSCAN_INIT]], [[BB2]] ],  [ float [[VP3:%.*]], [[BB3]] ]
; CHECK-NEXT:     [DA: Div] i32 [[VP4:%.*]] = phi  [ i32 [[VP_I_LINEAR_IV_IND_INIT:%.*]], [[BB2]] ],  [ i32 [[VP5:%.*]], [[BB3]] ]
; CHECK-NEXT:     [DA: Div] store i32 [[VP4]] i32* [[VP_I_LINEAR_IV:%.*]]
; CHECK-NEXT:     [DA: Div] store float 0.000000e+00 float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Div] i32 [[VP0:%.*]] = trunc i64 [[VP_INDVARS_IV]] to i32
; CHECK-NEXT:     [DA: Div] store i32 [[VP0]] i32* [[VP_I_LINEAR_IV]]
; CHECK-NEXT:     [DA: Div] float* [[VP_ARRAYIDX:%.*]] = getelementptr inbounds float* [[A0:%.*]] i64 [[VP_INDVARS_IV]]
; CHECK-NEXT:     [DA: Div] float [[VP6:%.*]] = load float* [[VP_ARRAYIDX]]
; CHECK-NEXT:     [DA: Div] float [[VP7:%.*]] = load float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Div] float [[VP_ADD5:%.*]] = fadd float [[VP7]] float [[VP6]]
; CHECK-NEXT:     [DA: Div] store float [[VP_ADD5]] float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Uni] br [[BB4:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni] br [[BB5:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB5]]: # preds: [[BB4]]
; CHECK-NEXT:     [DA: Div] float [[VP_LOAD_2:%.*]] = load float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Div] float [[VP8:%.*]] = running-inclusive-reduction float [[VP_LOAD_2]] float [[VP_INSCAN_ACCUM]] float 0.000000e+00
; CHECK-NEXT:     [DA: Div] store float [[VP8]] float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Uni] float [[VP3]] = extract-last-vector-lane float [[VP8]]
; CHECK-NEXT:     [DA: Uni] br [[BB6:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB6]]: # preds: [[BB5]]
; CHECK-NEXT:     [DA: Uni] br [[BB3]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB6]]
; CHECK-NEXT:     [DA: Div] float [[VP9:%.*]] = load float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Div] i32 [[VP10:%.*]] = load i32* [[VP_I_LINEAR_IV]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_IDXPROM6:%.*]] = sext i32 [[VP10]] to i64
; CHECK-NEXT:     [DA: Div] float* [[VP_ARRAYIDX7:%.*]] = getelementptr inbounds float* [[B0:%.*]] i64 [[VP_IDXPROM6]]
; CHECK-NEXT:     [DA: Div] store float [[VP9]] float* [[VP_ARRAYIDX7]]
; CHECK-NEXT:     [DA: Div] i64 [[VP_INDVARS_IV_NEXT]] = add i64 [[VP_INDVARS_IV]] i64 [[VP_INDVARS_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:     [DA: Div] i32 [[VP5]] = add i32 [[VP4]] i32 [[VP_I_LINEAR_IV_IND_INIT_STEP:%.*]]
; CHECK:          [DA: Uni] br i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]], [[BB7:BB[0-9]+]], [[BB0]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB7]]: # preds: [[BB3]]
; CHECK-NEXT:     [DA: Div] float [[VP_LOAD_3:%.*]] = load float* [[VP_X_RED]]
; CHECK-NEXT:     [DA: Uni] float [[VP_X_REDINSCAN_RED_FINAL:%.*]] = reduction-final-inscan float [[VP_LOAD_3]]
; CHECK-NEXT:     [DA: Uni] store float [[VP_X_REDINSCAN_RED_FINAL]] float* [[X_RED0]]
; CHECK:          [DA: Uni] br [[BB8:BB[0-9]+]]
;

entry:
  %x.red = alloca float, align 4
  %i.linear.iv = alloca i32, align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %entry
  store float 0.000000e+00, float* %x.red, align 4
  br label %DIR.OMP.SIMD.138

DIR.OMP.SIMD.138:                                 ; preds = %DIR.OMP.SIMD.1
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN"(float* %x.red, i64 1), "QUAL.OMP.NORMALIZED.IV"(i8* null), "QUAL.OMP.NORMALIZED.UB"(i8* null), "QUAL.OMP.LINEAR:IV"(i32* %i.linear.iv, i32 1) ]
  br label %DIR.OMP.SIMD.139

DIR.OMP.SIMD.139:                                 ; preds = %DIR.OMP.SIMD.138
  br label %DIR.OMP.END.SCAN.335

DIR.OMP.END.SCAN.335:                             ; preds = %DIR.OMP.END.SCAN.3, %DIR.OMP.SIMD.139
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.139 ], [ %indvars.iv.next, %DIR.OMP.END.SCAN.3 ]
  %1 = trunc i64 %indvars.iv to i32
  store i32 %1, i32* %i.linear.iv, align 4
  %arrayidx = getelementptr inbounds float, float* %A, i64 %indvars.iv
  %2 = load float, float* %arrayidx, align 4
  %3 = load float, float* %x.red, align 4
  %add5 = fadd fast float %3, %2
  store float %add5, float* %x.red, align 4
  br label %DIR.OMP.SCAN.3

DIR.OMP.SCAN.3:                                   ; preds = %DIR.OMP.END.SCAN.335
  %4 = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(float* %x.red, i64 1) ]
  br label %DIR.OMP.SCAN.2

DIR.OMP.SCAN.2:                                   ; preds = %DIR.OMP.SCAN.3
  fence acq_rel
  br label %DIR.OMP.END.SCAN.5

DIR.OMP.END.SCAN.5:                               ; preds = %DIR.OMP.SCAN.2
  call void @llvm.directive.region.exit(token %4) [ "DIR.OMP.END.SCAN"() ]
  br label %DIR.OMP.END.SCAN.3

DIR.OMP.END.SCAN.3:                               ; preds = %DIR.OMP.END.SCAN.5
  %5 = load float, float* %x.red, align 4
  %6 = load i32, i32* %i.linear.iv, align 4
  %idxprom6 = sext i32 %6 to i64
  %arrayidx7 = getelementptr inbounds float, float* %B, i64 %idxprom6
  store float %5, float* %arrayidx7, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.1, label %DIR.OMP.END.SCAN.335, !llvm.loop !0

DIR.OMP.END.SIMD.1:                               ; preds = %DIR.OMP.END.SIMD.7
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %DIR.OMP.END.SIMD.4, %entry
  ret void
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.vectorize.enable", i1 true}
!2 = !{!"llvm.loop.vectorize.ivdep_loop", i32 0}
