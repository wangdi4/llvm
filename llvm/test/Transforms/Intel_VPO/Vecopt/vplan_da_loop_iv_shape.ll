; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Test to check that DA initializes correct VectorShape for different types of IVs in loop.

; REQUIRES: asserts
; RUN: opt -VPlanDriver -vplan-dump-da -disable-output %s 2>&1 | FileCheck %s
; RUN: opt -passes="vplan-driver" -vplan-dump-da -disable-output %s 2>&1 | FileCheck %s

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(i32* nocapture %ary, i32 %x, i32 %y) {
; CHECK:       Printing Divergence info for Loop at depth 1 containing: [[BB0:BB[0-9]+]]<header><latch><exiting>
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1:BB[0-9]+]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i64 [[VP_STRIDED_IV:%.*]] = phi  [ i64 [[VP_STRIDED_IV_IND_INIT:%.*]], [[BB1]] ],  [ i64 [[VP_STRIDED_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_UNIT_STRIDE_IV:%.*]] = phi  [ i64 [[VP_UNIT_STRIDE_IV_IND_INIT:%.*]], [[BB1]] ],  [ i64 [[VP_UNIT_STRIDE_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 -1] i64 [[VP_UNIT_NEG_STRIDE_IV:%.*]] = phi  [ i64 [[VP_UNIT_NEG_STRIDE_IV_IND_INIT:%.*]], [[BB1]] ],  [ i64 [[VP_UNIT_NEG_STRIDE_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_VAR_STRIDE_IV:%.*]] = phi  [ i64 [[VP_VAR_STRIDE_IV_IND_INIT:%.*]], [[BB1]] ],  [ i64 [[VP_VAR_STRIDE_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 16] i32* [[VP_STRIDED_GEP:%.*]] = getelementptr inbounds i32* [[ARY0:%.*]] i64 [[VP_STRIDED_IV]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP0:%.*]] = load i32* [[VP_STRIDED_GEP]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i32* [[VP_UNIT_STRIDE_GEP:%.*]] = getelementptr inbounds i32* [[ARY0]] i64 [[VP_UNIT_STRIDE_IV]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP1:%.*]] = load i32* [[VP_UNIT_STRIDE_GEP]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 -4] i32* [[VP_UNIT_NEG_STRIDE_GEP:%.*]] = getelementptr inbounds i32* [[ARY0]] i64 [[VP_UNIT_NEG_STRIDE_IV]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP2:%.*]] = load i32* [[VP_UNIT_NEG_STRIDE_GEP]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_VAR_STRIDE_GEP:%.*]] = getelementptr inbounds i32* [[ARY0]] i64 [[VP_VAR_STRIDE_IV]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP3:%.*]] = load i32* [[VP_VAR_STRIDE_GEP]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i64 [[VP_STRIDED_IV_NEXT]] = add i64 [[VP_STRIDED_IV]] i64 [[VP_STRIDED_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_UNIT_STRIDE_IV_NEXT]] = add i64 [[VP_UNIT_STRIDE_IV]] i64 [[VP_UNIT_STRIDE_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 -1] i64 [[VP_UNIT_NEG_STRIDE_IV_NEXT]] = add i64 [[VP_UNIT_NEG_STRIDE_IV]] i64 [[VP_UNIT_NEG_STRIDE_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_VAR_STRIDE_IV_NEXT]] = add i64 [[VP_VAR_STRIDE_IV]] i64 [[VP_VAR_STRIDE_IV_IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ne i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2:BB[0-9]+]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_STRIDED_IV_IND_FINAL:%.*]] = induction-final{add} i64 live-in0 i64 4
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_UNIT_STRIDE_IV_IND_FINAL:%.*]] = induction-final{add} i64 live-in1 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_UNIT_NEG_STRIDE_IV_IND_FINAL:%.*]] = induction-final{add} i64 live-in2 i64 -1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VAR_STRIDE_IV_IND_FINAL:%.*]] = induction-final{add} i64 live-in3 i64 [[VAR_STEP0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB3:BB[0-9]+]]
;
pre.entry:
  %i = alloca i64, align 4
  br label %entry

entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4), "QUAL.OMP.LINEAR"(i64* %i, i32 -1) ]
  br label %for.preheader

for.preheader:
  %add.neg = sub i32 2, %x
  %sub = sub i32 %add.neg, %y
  %mul = mul nsw i32 %y, %x
  %var.step = sext i32 %sub to i64
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %strided.iv = phi i64 [ 0, %for.preheader ], [ %strided.iv.next, %for.body ]
  %unit.stride.iv = phi i64 [ 0, %for.preheader ], [ %unit.stride.iv.next, %for.body ]
  %unit.neg.stride.iv = phi i64 [ 0, %for.preheader ], [ %unit.neg.stride.iv.next, %for.body ]
  %var.stride.iv = phi i64 [ 0, %for.preheader ], [ %var.stride.iv.next, %for.body ]

  ; Users of IVs
  %strided.gep = getelementptr inbounds i32, i32* %ary, i64 %strided.iv
  %0 = load i32, i32* %strided.gep, align 4
  %unit.stride.gep = getelementptr inbounds i32, i32* %ary, i64 %unit.stride.iv
  %1 = load i32, i32* %unit.stride.gep, align 4
  %unit.neg.stride.gep = getelementptr inbounds i32, i32* %ary, i64 %unit.neg.stride.iv
  %2 = load i32, i32* %unit.neg.stride.gep, align 4
  %var.stride.gep = getelementptr inbounds i32, i32* %ary, i64 %var.stride.iv
  %3 = load i32, i32* %var.stride.gep, align 4

  ; Stride updates for IVs
  %strided.iv.next = add nuw nsw i64 %strided.iv, 4
  %unit.stride.iv.next = add nuw nsw i64 %unit.stride.iv, 1
  %unit.neg.stride.iv.next = add nuw nsw i64 %unit.neg.stride.iv, -1
  %var.stride.iv.next = add nuw nsw i64 %var.stride.iv, %var.step

  %cmp = icmp ult i64 %strided.iv.next, 1024
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
