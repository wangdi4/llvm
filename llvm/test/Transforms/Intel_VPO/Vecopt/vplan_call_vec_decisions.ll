; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Test to check correctness of CallVecDecisions analysis for different scenarios.

; RUN: opt < %s -VPlanDriver -disable-output -vplan-print-after-call-vec-decisions -vector-library=SVML | FileCheck %s --check-prefix=CALLVECDEC
; RUN: opt < %s -VPlanDriver -disable-output -vplan-print-scalvec-results -vector-library=SVML | FileCheck %s --check-prefix=SVA

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@src = common dso_local local_unnamed_addr global [1024 x float] zeroinitializer, align 16
@dst = common dso_local local_unnamed_addr global [1024 x float] zeroinitializer, align 16
; Function Attrs: nounwind uwtable
define dso_local void @foo() local_unnamed_addr #0 {
; CALLVECDEC-LABEL:  VPlan after CallVecDecisions analysis for VF=4:
; CALLVECDEC-NEXT:    [[BB0:BB[0-9]+]]:
; CALLVECDEC-NEXT:     <Empty Block>
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB1:BB[0-9]+]]
; CALLVECDEC-NEXT:    no PREDECESSORS
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB1]]:
; CALLVECDEC-NEXT:     [DA: Div] i64 [[VP_INDVARS_IV_IND_INIT:%.*]] = induction-init{add} i64 0 i64 1
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_INDVARS_IV_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_VF:%.*]] = induction-init-step{add} i64 1
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop omp.inner.for.body
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB2:BB[0-9]+]]
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB0]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB2]]:
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB3:BB[0-9]+]] ]
; CALLVECDEC-NEXT:     [DA: Div] i64 [[VP_INDVARS_IV:%.*]] = phi  [ i64 [[VP_INDVARS_IV_NEXT:%.*]], [[BB3]] ],  [ i64 [[VP_INDVARS_IV_IND_INIT]], [[BB1]] ]
; CALLVECDEC-NEXT:     [DA: Div] float* [[VP_ARRAYIDX:%.*]] = getelementptr inbounds [1024 x float]* @src i64 0 i64 [[VP_INDVARS_IV]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP0:%.*]] = load float* [[VP_ARRAYIDX]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_SERIAL_CALL:%.*]] = call float [[VP0]] float (float)* @bar [Serial]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_VEC_VARIANT:%.*]] = call float [[VP0]] _ZGVbM4v_simdBar [x 1] [@CurrMask]
; CALLVECDEC-NEXT:     [DA: Div] double [[VP_CONV:%.*]] = fpext float [[VP0]] to double
; CALLVECDEC-NEXT:     [DA: Div] double [[VP_LIB_CALL:%.*]] = call double [[VP_CONV]] __svml_sin4 [x 1]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_LIB_TRUNC:%.*]] = fptrunc double [[VP_LIB_CALL]] to float
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_INTRIN:%.*]] = call float [[VP0]] float [[VP_SERIAL_CALL]] float [[VP_VEC_VARIANT]] llvm.fmuladd.v4f32 [x 1]
; CALLVECDEC-NEXT:     [DA: Div] i64 [[VP_COND:%.*]] = and i64 [[VP_INDVARS_IV]] i64 1
; CALLVECDEC-NEXT:     [DA: Div] i1 [[VP_CMP13:%.*]] = icmp i64 [[VP_COND]] i64 0
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB4:BB[0-9]+]]
; CALLVECDEC-NEXT:    PREDECESSORS(2): [[BB1]] [[BB3]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB4]]:
; CALLVECDEC-NEXT:     [DA: Div] i1 [[VP1:%.*]] = block-predicate i1 [[VP_CMP13]]
; CALLVECDEC-NEXT:     [DA: Div] double [[VP_MASK_LIB_CALL:%.*]] = call double [[VP_CONV]] __svml_log4_mask [x 1] [@CurrMask]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_MASK_LIB_TRUNC:%.*]] = fptrunc double [[VP_MASK_LIB_CALL]] to float
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_MASK_VEC_VARIANT:%.*]] = call float [[VP0]] _ZGVbM4v_simdBar [x 1] [@CurrMask]
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB3]]
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB2]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB3]]:
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_PHI_MASK_LIB_BLEND_BB3:%.*]] = blend [ float 0.000000e+00, i1 true ], [ float [[VP_MASK_LIB_TRUNC]], i1 [[VP_CMP13]] ]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_PHI_MASK_VV_BLEND_BB3:%.*]] = blend [ float 0.000000e+00, i1 true ], [ float [[VP_MASK_VEC_VARIANT]], i1 [[VP_CMP13]] ]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP2:%.*]] = fadd float [[VP_SERIAL_CALL]] float [[VP_VEC_VARIANT]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP3:%.*]] = fadd float [[VP2]] float [[VP_PHI_MASK_VV_BLEND_BB3]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP4:%.*]] = fadd float [[VP3]] float [[VP_LIB_TRUNC]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP5:%.*]] = fadd float [[VP4]] float [[VP_PHI_MASK_LIB_BLEND_BB3]]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP6:%.*]] = fadd float [[VP5]] float [[VP_INTRIN]]
; CALLVECDEC-NEXT:     [DA: Div] float* [[VP_ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x float]* @dst i64 0 i64 [[VP_INDVARS_IV]]
; CALLVECDEC-NEXT:     [DA: Div] store float [[VP6]] float* [[VP_ARRAYIDX2]]
; CALLVECDEC-NEXT:     [DA: Div] i64 [[VP_INDVARS_IV_NEXT]] = add i64 [[VP_INDVARS_IV]] i64 [[VP_INDVARS_IV_IND_INIT_STEP]]
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CALLVECDEC-NEXT:     [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]
; CALLVECDEC-NEXT:    SUCCESSORS(2):[[BB5:BB[0-9]+]](i1 [[VP_VECTOR_LOOP_EXITCOND]]), [[BB2]](!i1 [[VP_VECTOR_LOOP_EXITCOND]])
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB4]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB5]]:
; CALLVECDEC-NEXT:     [DA: Uni] i64 [[VP_INDVARS_IV_IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB6:BB[0-9]+]]
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB3]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB6]]:
; CALLVECDEC-NEXT:     <Empty Block>
; CALLVECDEC-NEXT:    no SUCCESSORS
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB5]]
;
; SVA-LABEL:  VPlan after ScalVec analysis:
; SVA-NEXT:    [[BB0:BB[0-9]+]]:
; SVA-NEXT:     <Empty Block>
; SVA-NEXT:    SUCCESSORS(1):[[BB1:BB[0-9]+]]
; SVA-NEXT:    no PREDECESSORS
; SVA-EMPTY:
; SVA-NEXT:    [[BB1]]:
; SVA-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV_IND_INIT:%.*]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_INDVARS_IV_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VF:%.*]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop omp.inner.for.body (SVAOpBits )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1 (SVAOpBits 0->F )
; SVA-NEXT:    SUCCESSORS(1):[[BB2:BB[0-9]+]]
; SVA-NEXT:    PREDECESSORS(1): [[BB0]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB2]]:
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB3:BB[0-9]+]] ] (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV:%.*]] = phi  [ i64 [[VP_INDVARS_IV_NEXT:%.*]], [[BB3]] ],  [ i64 [[VP_INDVARS_IV_IND_INIT]], [[BB1]] ] (SVAOpBits 0->FV 1->FV )
; SVA-NEXT:     [DA: Div, SVA: (F  )] float* [[VP_ARRAYIDX:%.*]] = getelementptr inbounds [1024 x float]* @src i64 0 i64 [[VP_INDVARS_IV]] (SVAOpBits 0->F 1->F 2->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP0:%.*]] = load float* [[VP_ARRAYIDX]] (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_SERIAL_CALL:%.*]] = call float [[VP0]] float (float)* @bar [Serial] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_VEC_VARIANT:%.*]] = call float [[VP0]] _ZGVbM4v_simdBar [x 1] [@CurrMask] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] double [[VP_CONV:%.*]] = fpext float [[VP0]] to double (SVAOpBits 0->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] double [[VP_LIB_CALL:%.*]] = call double [[VP_CONV]] __svml_sin4 [x 1] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_LIB_TRUNC:%.*]] = fptrunc double [[VP_LIB_CALL]] to float (SVAOpBits 0->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_INTRIN:%.*]] = call float [[VP0]] float [[VP_SERIAL_CALL]] float [[VP_VEC_VARIANT]] llvm.fmuladd.v4f32 [x 1] (SVAOpBits 0->V 1->V 2->V 3->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] i64 [[VP_COND:%.*]] = and i64 [[VP_INDVARS_IV]] i64 1 (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] i1 [[VP_CMP13:%.*]] = icmp i64 [[VP_COND]] i64 0 (SVAOpBits 0->V 1->V )
; SVA-NEXT:    SUCCESSORS(1):[[BB4:BB[0-9]+]]
; SVA-NEXT:    PREDECESSORS(2): [[BB1]] [[BB3]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB4]]:
; SVA-NEXT:     [DA: Div, SVA: ( V )] i1 [[VP1:%.*]] = block-predicate i1 [[VP_CMP13]] (SVAOpBits 0->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] double [[VP_MASK_LIB_CALL:%.*]] = call double [[VP_CONV]] __svml_log4_mask [x 1] [@CurrMask] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_MASK_LIB_TRUNC:%.*]] = fptrunc double [[VP_MASK_LIB_CALL]] to float (SVAOpBits 0->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_MASK_VEC_VARIANT:%.*]] = call float [[VP0]] _ZGVbM4v_simdBar [x 1] [@CurrMask] (SVAOpBits 0->V 1->F )
; SVA-NEXT:    SUCCESSORS(1):[[BB3]]
; SVA-NEXT:    PREDECESSORS(1): [[BB2]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB3]]:
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_PHI_MASK_LIB_BLEND_BB3:%.*]] = blend [ float 0.000000e+00, i1 true ], [ float [[VP_MASK_LIB_TRUNC]], i1 [[VP_CMP13]] ] (SVAOpBits 0->V 1->V 2->V 3->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_PHI_MASK_VV_BLEND_BB3:%.*]] = blend [ float 0.000000e+00, i1 true ], [ float [[VP_MASK_VEC_VARIANT]], i1 [[VP_CMP13]] ] (SVAOpBits 0->V 1->V 2->V 3->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP2:%.*]] = fadd float [[VP_SERIAL_CALL]] float [[VP_VEC_VARIANT]] (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP3:%.*]] = fadd float [[VP2]] float [[VP_PHI_MASK_VV_BLEND_BB3]] (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP4:%.*]] = fadd float [[VP3]] float [[VP_LIB_TRUNC]] (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP5:%.*]] = fadd float [[VP4]] float [[VP_PHI_MASK_LIB_BLEND_BB3]] (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP6:%.*]] = fadd float [[VP5]] float [[VP_INTRIN]] (SVAOpBits 0->V 1->V )
; SVA-NEXT:     [DA: Div, SVA: (F  )] float* [[VP_ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x float]* @dst i64 0 i64 [[VP_INDVARS_IV]] (SVAOpBits 0->F 1->F 2->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] store float [[VP6]] float* [[VP_ARRAYIDX2]] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV_NEXT]] = add i64 [[VP_INDVARS_IV]] i64 [[VP_INDVARS_IV_IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]] (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; SVA-NEXT:    SUCCESSORS(2):[[BB5:BB[0-9]+]](i1 [[VP_VECTOR_LOOP_EXITCOND]]), [[BB2]](!i1 [[VP_VECTOR_LOOP_EXITCOND]])
; SVA-NEXT:    PREDECESSORS(1): [[BB4]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB5]]:
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_INDVARS_IV_IND_FINAL:%.*]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; SVA-NEXT:    SUCCESSORS(1):[[BB6:BB[0-9]+]]
; SVA-NEXT:    PREDECESSORS(1): [[BB3]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB6]]:
; SVA-NEXT:     <Empty Block>
; SVA-NEXT:    no SUCCESSORS
; SVA-NEXT:    PREDECESSORS(1): [[BB5]]
;
omp.inner.for.body.lr.ph:
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %omp.inner.for.body.lr.ph
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.inc, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.inc ], [ 0, %DIR.OMP.SIMD.1 ]
  %arrayidx = getelementptr inbounds [1024 x float], [1024 x float]* @src, i64 0, i64 %indvars.iv
  %1 = load float, float* %arrayidx, align 4
  ; User call without SIMD variants, should be serialized always.
  %serial.call = call float @bar(float %1) #1
  ; User call with SIMD variant available for VF=4 but masked, should use
  ; masked variant with all-zero mask.
  %vec.variant = call float @simdBar(float %1) #1
  %conv = fpext float %1 to double
  ; Unmasked vector library call.
  %lib.call = call double @sin(double %conv) #1
  %lib.trunc = fptrunc double %lib.call to float
  ; Unmasked trivially vectorizable intrinsic call.
  %intrin = call float @llvm.fmuladd.f32(float %1, float %serial.call, float %vec.variant)
  %cond = and i64 %indvars.iv, 1
  %cmp13 = icmp eq i64 %cond, 0
  br i1 %cmp13, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  ; Masked vector library call.
  %mask.lib.call = call double @log(double %conv) #1
  %mask.lib.trunc = fptrunc double %mask.lib.call to float
  ; User call with SIMD variant available for VF=4 and masked.
  %mask.vec.variant = call float @simdBar(float %1) #1
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %phi.mask.lib = phi float [ %mask.lib.trunc, %if.then ], [ 0.0, %omp.inner.for.body ]
  %phi.mask.vv = phi float [ %mask.vec.variant, %if.then ], [ 0.0, %omp.inner.for.body ]

  ; Users of calls
  %2 = fadd float %serial.call, %vec.variant
  %3 = fadd float %2, %phi.mask.vv
  %4 = fadd float %3, %lib.trunc
  %5 = fadd float %4, %phi.mask.lib
  %6 = fadd float %5, %intrin
  %arrayidx2 = getelementptr inbounds [1024 x float], [1024 x float]* @dst, i64 0, i64 %indvars.iv
  store float %6, float* %arrayidx2

  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %DIR.OMP.END.SIMD.4, label %omp.inner.for.body

DIR.OMP.END.SIMD.4:                               ; preds = %omp.inner.for.inc
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.2

DIR.OMP.END.SIMD.2:                               ; preds = %DIR.OMP.END.SIMD.4
  ret void
}

; Function Attrs: nounwind uwtable
define dso_local void @foo_pumping(float* nocapture %A, float* nocapture %B, i32 %N) {
; CALLVECDEC-LABEL:  VPlan after CallVecDecisions analysis for VF=128:
; CALLVECDEC-NEXT:    [[BB0:BB[0-9]+]]:
; CALLVECDEC-NEXT:     <Empty Block>
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB1:BB[0-9]+]]
; CALLVECDEC-NEXT:    no PREDECESSORS
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB1]]:
; CALLVECDEC-NEXT:     [DA: Div] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT:%.*]] = induction-init{add} i32 0 i32 1
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT_STEP:%.*]] = induction-init-step{add} i32 1
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP_VF:%.*]] = induction-init-step{add} i32 1
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop omp.inner.for.body
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i32 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB2:BB[0-9]+]]
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB0]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB2]]:
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i32 0, [[BB1]] ],  [ i32 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB2]] ]
; CALLVECDEC-NEXT:     [DA: Div] i32 [[VP__OMP_IV_LOCAL_014:%.*]] = phi  [ i32 [[VP_ADD6:%.*]], [[BB2]] ],  [ i32 [[VP__OMP_IV_LOCAL_014_IND_INIT]], [[BB1]] ]
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_CONV:%.*]] = sitofp i32 [[VP__OMP_IV_LOCAL_014]] to float
; CALLVECDEC-NEXT:     [DA: Div] float [[VP_PUMP_CALL:%.*]] = call float [[VP_CONV]] __svml_sinf64 [x 2]
; CALLVECDEC-NEXT:     [DA: Div] i64 [[VP_IDXPROM:%.*]] = sext i32 [[VP__OMP_IV_LOCAL_014]] to i64
; CALLVECDEC-NEXT:     [DA: Div] float* [[VP_ARRAYIDXA:%.*]] = getelementptr inbounds float* [[A0:%.*]] i64 [[VP_IDXPROM]]
; CALLVECDEC-NEXT:     [DA: Div] store float [[VP_PUMP_CALL]] float* [[VP_ARRAYIDXA]]
; CALLVECDEC-NEXT:     [DA: Div] i32 [[VP_ADD6]] = add i32 [[VP__OMP_IV_LOCAL_014]] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT_STEP]]
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP_VECTOR_LOOP_IV_NEXT]] = add i32 [[VP_VECTOR_LOOP_IV]] i32 [[VP_VF]]
; CALLVECDEC-NEXT:     [DA: Uni] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp i32 [[VP_VECTOR_LOOP_IV_NEXT]] i32 [[VP_VECTOR_TRIP_COUNT]]
; CALLVECDEC-NEXT:    SUCCESSORS(2):[[BB3:BB[0-9]+]](i1 [[VP_VECTOR_LOOP_EXITCOND]]), [[BB2]](!i1 [[VP_VECTOR_LOOP_EXITCOND]])
; CALLVECDEC-NEXT:    PREDECESSORS(2): [[BB1]] [[BB2]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB3]]:
; CALLVECDEC-NEXT:     [DA: Uni] i32 [[VP__OMP_IV_LOCAL_014_IND_FINAL:%.*]] = induction-final{add} i32 0 i32 1
; CALLVECDEC-NEXT:    SUCCESSORS(1):[[BB4:BB[0-9]+]]
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB2]]
; CALLVECDEC-EMPTY:
; CALLVECDEC-NEXT:    [[BB4]]:
; CALLVECDEC-NEXT:     <Empty Block>
; CALLVECDEC-NEXT:    no SUCCESSORS
; CALLVECDEC-NEXT:    PREDECESSORS(1): [[BB3]]
;
; SVA-LABEL:  VPlan after ScalVec analysis:
; SVA-NEXT:    [[BB0:BB[0-9]+]]:
; SVA-NEXT:     <Empty Block>
; SVA-NEXT:    SUCCESSORS(1):[[BB1:BB[0-9]+]]
; SVA-NEXT:    no PREDECESSORS
; SVA-EMPTY:
; SVA-NEXT:    [[BB1]]:
; SVA-NEXT:     [DA: Div, SVA: (FV )] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT:%.*]] = induction-init{add} i32 0 i32 1 (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT_STEP:%.*]] = induction-init-step{add} i32 1 (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_VF:%.*]] = induction-init-step{add} i32 1 (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop omp.inner.for.body (SVAOpBits )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i32 [[VP_ORIG_TRIP_COUNT]], UF = 1 (SVAOpBits 0->F )
; SVA-NEXT:    SUCCESSORS(1):[[BB2:BB[0-9]+]]
; SVA-NEXT:    PREDECESSORS(1): [[BB0]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB2]]:
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i32 0, [[BB1]] ],  [ i32 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB2]] ] (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Div, SVA: (FV )] i32 [[VP__OMP_IV_LOCAL_014:%.*]] = phi  [ i32 [[VP_ADD6:%.*]], [[BB2]] ],  [ i32 [[VP__OMP_IV_LOCAL_014_IND_INIT]], [[BB1]] ] (SVAOpBits 0->FV 1->FV )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_CONV:%.*]] = sitofp i32 [[VP__OMP_IV_LOCAL_014]] to float (SVAOpBits 0->V )
; SVA-NEXT:     [DA: Div, SVA: ( V )] float [[VP_PUMP_CALL:%.*]] = call float [[VP_CONV]] __svml_sinf64 [x 2] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP_IDXPROM:%.*]] = sext i32 [[VP__OMP_IV_LOCAL_014]] to i64 (SVAOpBits 0->F )
; SVA-NEXT:     [DA: Div, SVA: (F  )] float* [[VP_ARRAYIDXA:%.*]] = getelementptr inbounds float* [[A0:%.*]] i64 [[VP_IDXPROM]] (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Div, SVA: ( V )] store float [[VP_PUMP_CALL]] float* [[VP_ARRAYIDXA]] (SVAOpBits 0->V 1->F )
; SVA-NEXT:     [DA: Div, SVA: (FV )] i32 [[VP_ADD6]] = add i32 [[VP__OMP_IV_LOCAL_014]] i32 [[VP__OMP_IV_LOCAL_014_IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_VECTOR_LOOP_IV_NEXT]] = add i32 [[VP_VECTOR_LOOP_IV]] i32 [[VP_VF]] (SVAOpBits 0->F 1->F )
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp i32 [[VP_VECTOR_LOOP_IV_NEXT]] i32 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; SVA-NEXT:    SUCCESSORS(2):[[BB3:BB[0-9]+]](i1 [[VP_VECTOR_LOOP_EXITCOND]]), [[BB2]](!i1 [[VP_VECTOR_LOOP_EXITCOND]])
; SVA-NEXT:    PREDECESSORS(2): [[BB1]] [[BB2]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB3]]:
; SVA-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP__OMP_IV_LOCAL_014_IND_FINAL:%.*]] = induction-final{add} i32 0 i32 1 (SVAOpBits 0->F 1->F )
; SVA-NEXT:    SUCCESSORS(1):[[BB4:BB[0-9]+]]
; SVA-NEXT:    PREDECESSORS(1): [[BB2]]
; SVA-EMPTY:
; SVA-NEXT:    [[BB4]]:
; SVA-NEXT:     <Empty Block>
; SVA-NEXT:    no SUCCESSORS
; SVA-NEXT:    PREDECESSORS(1): [[BB3]]
;
entry:
  %cmp = icmp sgt i32 %N, 0
  br i1 %cmp, label %DIR.OMP.SIMD.2, label %omp.precond.end

DIR.OMP.SIMD.2:                                   ; preds = %entry
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %DIR.OMP.SIMD.2
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 128) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.1
  %.omp.iv.local.014 = phi i32 [ %add6, %omp.inner.for.body ], [ 0, %DIR.OMP.SIMD.1 ]
  %conv = sitofp i32 %.omp.iv.local.014 to float
  %pump.call = call float @sinf(float %conv)
  %idxprom = sext i32 %.omp.iv.local.014 to i64
  %arrayidxA = getelementptr inbounds float, float* %A, i64 %idxprom
  store float %pump.call, float* %arrayidxA, align 4
  %add6 = add nuw nsw i32 %.omp.iv.local.014, 1
  %exitcond = icmp eq i32 %add6, %N
  br i1 %exitcond, label %DIR.OMP.END.SIMD.3, label %omp.inner.for.body

DIR.OMP.END.SIMD.3:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %DIR.OMP.END.SIMD.3, %entry
  ret void
}

; Function Attrs: nounwind
declare token @llvm.directive.region.entry() #1
; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token) #1
declare dso_local float @bar(float) local_unnamed_addr #2
declare dso_local float @simdBar(float) local_unnamed_addr #3
; Function Attrs: nofree nounwind
declare dso_local double @sin(double) local_unnamed_addr #4
; Function Attrs: nofree nounwind
declare dso_local double @log(double) local_unnamed_addr #4
; Function Attrs: nounwind readnone
declare float @sinf(float) local_unnamed_addr
declare float @llvm.fmuladd.f32(float %a, float %b, float %c) nounwind readnone

attributes #0 = { nounwind uwtable "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "may-have-openmp-directive"="true" "min-legal-vector-width"="0" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind }
attributes #2 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" "vector-variants"="_ZGVbM4v_simdBar,_ZGVcM4v_simdBar,_ZGVdM4v_simdBar,_ZGVeM4v_simdBar" }
attributes #4 = { nofree nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "frame-pointer"="none" "less-precise-fpmad"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "unsafe-fp-math"="false" "use-soft-float"="false" }

