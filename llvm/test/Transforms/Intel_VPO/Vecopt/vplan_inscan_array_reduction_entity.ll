; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Test to verify correctness of VPlan IR generated to handle
; array type inscan inclusive reduction entity.

; int A[3][8] = { {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7} };
; int B[3][8] = {0};
;
; void foo() {
;   int x[3] = {0, 0, 0};
; #pragma omp simd reduction(inscan,+:x)
; #pragma nounroll
;   for (int i = 0; i < 8; i++) {
;     x[0] += A[0][i];
;     x[1] += A[1][i];
;     x[2] += A[2][i];
; #pragma omp scan inclusive(x)
;     B[0][i] = x[0];
;     B[1][i] = x[1];
;     B[2][i] = x[2];
;   }
; }

; RUN: opt -disable-output -passes="vplan-vec" -vplan-print-scalvec-results -vplan-entities-dump -vplan-force-vf=2 -S < %s 2>&1 | FileCheck %s
; REQUIRES : asserts

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@A = dso_local local_unnamed_addr global [3 x [8 x i32]] [[8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7]], align 16
@B = dso_local local_unnamed_addr global [3 x [8 x i32]] zeroinitializer, align 16

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @_Z3foov() local_unnamed_addr {
; CHECK-LABEL:  VPlan after ScalVec analysis:
; CHECK-NEXT:  VPlan IR for: _Z3foov:DIR.VPO.END.GUARD.MEM.MOTION.5.#{{[0-9]+}}
; CHECK-NEXT:  Loop Entities of the loop with header [[BB0:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:  Reduction list
; CHECK-NEXT:   (+) Start: ptr [[X_RED0:%.*]]
; CHECK-NEXT:    Linked values: ptr [[VP_X_RED:%.*]],
; CHECK-NEXT:    inscan ReductionKind: inclusive
; CHECK-NEXT:   Memory: ptr [[X_RED0]]
; CHECK-EMPTY:
; CHECK-NEXT:  Induction list
; CHECK-NEXT:   IntInduction(+) Start: i64 0 Step: i64 1 StartVal: i64 0 EndVal: i64 8 BinOp: [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV_NEXT:%.*]] = add i64 [[VP_INDVARS_IV:%.*]] i64 [[VP_INDVARS_IV_IND_INIT_STEP:%.*]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:    Linked values: i64 [[VP_INDVARS_IV]], i64 [[VP_INDVARS_IV_NEXT]], i64 [[VP_INDVARS_IV_IND_INIT:%.*]], i64 [[VP_INDVARS_IV_IND_INIT_STEP]], i64 [[VP_INDVARS_IV_IND_FINAL:%.*]],
; CHECK-EMPTY:
; CHECK-NEXT:   IntInduction(+) Start: i32 [[VP0:undef]] Step: i32 1 StartVal: ? EndVal: ? need close form
; CHECK-NEXT:    Linked values: ptr [[VP_I_LINEAR_IV:%.*]], i32 [[VP_I_LINEAR_IV_IND_INIT:%.*]], i32 [[VP_I_LINEAR_IV_IND_INIT_STEP:%.*]], void [[VP_STORE:%.*]], i32 [[VP_I_LINEAR_IV_IND_FINAL:%.*]],
; CHECK-NEXT:   Memory: ptr [[I_LINEAR_IV0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1:BB[0-9]+]]: # preds:
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] pushvf VF=2 UF=1 (SVAOpBits )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] pushvf VF=2 UF=1 (SVAOpBits )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB2:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]]
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_I_LINEAR_IV]] = allocate-priv i32, OrigAlign = 4 (SVAOpBits )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] call i64 4 ptr [[VP_I_LINEAR_IV]] ptr @llvm.lifetime.start.p0 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] ptr [[VP_X_RED]] = allocate-priv [3 x i32], OrigAlign = 4 (SVAOpBits )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] call i64 12 ptr [[VP_X_RED]] ptr @llvm.lifetime.start.p0 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV_IND_INIT]] = induction-init{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_INDVARS_IV_IND_INIT_STEP]] = induction-init-step{add} i64 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_LOAD:%.*]] = load ptr [[I_LINEAR_IV0]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_I_LINEAR_IV_IND_INIT]] = induction-init{add} i32 [[VP_LOAD]] i32 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP_I_LINEAR_IV_IND_INIT]] ptr [[VP_I_LINEAR_IV]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_I_LINEAR_IV_IND_INIT_STEP]] = induction-init-step{add} i32 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 8, UF = 1 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB0]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB0]]: # preds: [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV]] = phi  [ i64 [[VP_INDVARS_IV_IND_INIT]], [[BB2]] ],  [ i64 [[VP_INDVARS_IV_NEXT]], [[BB3]] ] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP1:%.*]] = phi  [ i32 [[VP_I_LINEAR_IV_IND_INIT]], [[BB2]] ],  [ i32 [[VP2:%.*]], [[BB3]] ] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP1]] ptr [[VP_I_LINEAR_IV]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: ( V )] reduction-init-arr i32 0 ptr [[VP_X_RED]] (SVAOpBits 0->F 1->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB4:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB0]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB5:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB5]]: # preds: [[BB4]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP0_1:%.*]] = trunc i64 [[VP_INDVARS_IV]] to i32 (SVAOpBits 0->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP0_1]] ptr [[VP_I_LINEAR_IV]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX:%.*]] = getelementptr inbounds [8 x i32], ptr @A i64 0 i64 [[VP_INDVARS_IV]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP3:%.*]] = load ptr [[VP_ARRAYIDX]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP4:%.*]] = load ptr [[VP_X_RED]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_ADD2:%.*]] = add i32 [[VP4]] i32 [[VP3]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP_ADD2]] ptr [[VP_X_RED]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX4:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A i64 0 i64 1 i64 [[VP_INDVARS_IV]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP5:%.*]] = load ptr [[VP_ARRAYIDX4]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX5:%.*]] = getelementptr inbounds [3 x i32], ptr [[VP_X_RED]] i64 0 i64 1 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP6:%.*]] = load ptr [[VP_ARRAYIDX5]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_ADD6:%.*]] = add i32 [[VP6]] i32 [[VP5]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP_ADD6]] ptr [[VP_ARRAYIDX5]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX8:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A i64 0 i64 2 i64 [[VP_INDVARS_IV]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP7:%.*]] = load ptr [[VP_ARRAYIDX8]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX9:%.*]] = getelementptr inbounds [3 x i32], ptr [[VP_X_RED]] i64 0 i64 2 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP8:%.*]] = load ptr [[VP_ARRAYIDX9]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_ADD10:%.*]] = add i32 [[VP8]] i32 [[VP7]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP_ADD10]] ptr [[VP_ARRAYIDX9]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB6:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB6]]: # preds: [[BB5]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB7:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB7]]: # preds: [[BB6]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB8:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB8]]: # preds: [[BB7]], [[BB8]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_SCAN_ARR_IDX:%.*]] = phi  [ i64 0, [[BB7]] ],  [ i64 [[VP_SCAN_ARR_IDX_NXT:%.*]], [[BB8]] ] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_SCAN_ARR_IP_PTR:%.*]] = getelementptr [3 x i32], ptr [[VP_X_RED]] i64 0 i64 [[VP_SCAN_ARR_IDX]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_SCAN_ARR_IP:%.*]] = load ptr [[VP_SCAN_ARR_IP_PTR]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] ptr [[VP_SCAN_ARR_ACCUM_PTR:%.*]] = getelementptr [3 x i32], ptr [[X_RED0]] i64 0 i64 [[VP_SCAN_ARR_IDX]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_SCAN_ARR_ACCUM:%.*]] = load ptr [[VP_SCAN_ARR_ACCUM_PTR]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_ARR_SCAN_INCL:%.*]] = running-inclusive-reduction{add} i32 [[VP_SCAN_ARR_IP]] i32 [[VP_SCAN_ARR_ACCUM]] i32 0 (SVAOpBits 0->V 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP_ARR_SCAN_INCL]] ptr [[VP_SCAN_ARR_IP_PTR]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (  L)] i32 [[VP9:%.*]] = extract-last-vector-lane i32 [[VP_ARR_SCAN_INCL]] (SVAOpBits 0->L )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] store i32 [[VP9]] ptr [[VP_SCAN_ARR_ACCUM_PTR]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_SCAN_ARR_IDX_NXT]] = add i64 [[VP_SCAN_ARR_IDX]] i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_SCAN_ARR_RUNNING_DONE:%.*]] = icmp eq i64 [[VP_SCAN_ARR_IDX_NXT]] i64 3 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP_SCAN_ARR_RUNNING_DONE]], [[BB9:BB[0-9]+]], [[BB8]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB9]]: # preds: [[BB8]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB10:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB10]]: # preds: [[BB9]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB11:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB11]]: # preds: [[BB10]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB12:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB12]]: # preds: [[BB11]]
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP10:%.*]] = load ptr [[VP_X_RED]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP11:%.*]] = load ptr [[VP_I_LINEAR_IV]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] i64 [[VP_IDXPROM12:%.*]] = sext i32 [[VP11]] to i64 (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX13:%.*]] = getelementptr inbounds [8 x i32], ptr @B i64 0 i64 [[VP_IDXPROM12]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP10]] ptr [[VP_ARRAYIDX13]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX14:%.*]] = getelementptr inbounds [3 x i32], ptr [[VP_X_RED]] i64 0 i64 1 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP12:%.*]] = load ptr [[VP_ARRAYIDX14]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX16:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B i64 0 i64 1 i64 [[VP_IDXPROM12]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP12]] ptr [[VP_ARRAYIDX16]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX17:%.*]] = getelementptr inbounds [3 x i32], ptr [[VP_X_RED]] i64 0 i64 2 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP13:%.*]] = load ptr [[VP_ARRAYIDX17]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] ptr [[VP_ARRAYIDX19:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B i64 0 i64 2 i64 [[VP_IDXPROM12]] (SVAOpBits 0->F 1->F 2->F 3->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] store i32 [[VP13]] ptr [[VP_ARRAYIDX19]] (SVAOpBits 0->V 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB13:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB13]]: # preds: [[BB12]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB3]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB13]]
; CHECK-NEXT:     [DA: Div, SVA: (FV )] i64 [[VP_INDVARS_IV_NEXT]] = add i64 [[VP_INDVARS_IV]] i64 [[VP_INDVARS_IV_IND_INIT_STEP]] (SVAOpBits 0->FV 1->FV )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP2]] = add i32 [[VP1]] i32 [[VP_I_LINEAR_IV_IND_INIT_STEP]] (SVAOpBits 0->V 1->V )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp uge i64 [[VP_INDVARS_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br i1 [[VP_VECTOR_LOOP_EXITCOND]], [[BB14:BB[0-9]+]], [[BB0]] (SVAOpBits 0->F 1->F 2->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB14]]: # preds: [[BB3]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP_INDVARS_IV_IND_FINAL]] = induction-final{add} i64 0 i64 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: ( V )] i32 [[VP_LOAD_1:%.*]] = load ptr [[VP_I_LINEAR_IV]] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i32 [[VP_I_LINEAR_IV_IND_FINAL]] = induction-final{add} i32 [[VP_LOAD]] i32 1 (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] store i32 [[VP_I_LINEAR_IV_IND_FINAL]] ptr [[I_LINEAR_IV0]] (SVAOpBits 0->F 1->F )
; CHECK-NEXT:     [DA: Div, SVA: (F  )] call i64 4 ptr [[VP_I_LINEAR_IV]] ptr @llvm.lifetime.end.p0 (SVAOpBits 0->F 1->F 2->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br [[BB15:BB[0-9]+]] (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB15]]: # preds: [[BB14]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] popvf (SVAOpBits )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br final.merge (SVAOpBits 0->F )
; CHECK-EMPTY:
; CHECK-NEXT:    final.merge: # preds: [[BB15]]
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] i64 [[VP14:%.*]] = phi-merge  [ i64 live-out0, [[BB15]] ] (SVAOpBits 0->F )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] popvf (SVAOpBits )
; CHECK-NEXT:     [DA: Uni, SVA: (F  )] br <External Block> (SVAOpBits )
; CHECK-EMPTY:
; CHECK-NEXT:  External Uses:
; CHECK-NEXT:  Id: 0   no underlying for i64 [[VP_INDVARS_IV_IND_FINAL]]
;
DIR.OMP.SIMD.1.split46.split:
  %x.red = alloca [3 x i32], align 4
  %i.linear.iv = alloca i32, align 4
  store i32 0, ptr %x.red, align 4
  %0 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  store i32 0, ptr %0, align 4
  %1 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  store i32 0, ptr %1, align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %DIR.OMP.SIMD.1.split46.split
  %2 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr %x.red, i32 0, i64 3, i64 1), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5

DIR.VPO.END.GUARD.MEM.MOTION.5:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %DIR.VPO.END.GUARD.MEM.MOTION.5100 ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1.split

DIR.VPO.GUARD.MEM.MOTION.1.split:                 ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5
  %pre.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1

DIR.VPO.GUARD.MEM.MOTION.1:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.1.split
  %3 = trunc i64 %indvars.iv to i32
  store i32 %3, ptr %i.linear.iv, align 4
  %arrayidx = getelementptr inbounds [8 x i32], ptr @A, i64 0, i64 %indvars.iv
  %4 = load i32, ptr %arrayidx, align 4
  %5 = load i32, ptr %x.red, align 4
  %add2 = add nsw i32 %5, %4
  store i32 %add2, ptr %x.red, align 4
  %arrayidx4 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 1, i64 %indvars.iv
  %6 = load i32, ptr %arrayidx4, align 4
  %arrayidx5 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  %7 = load i32, ptr %arrayidx5, align 4
  %add6 = add nsw i32 %7, %6
  store i32 %add6, ptr %arrayidx5, align 4
  %arrayidx8 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 2, i64 %indvars.iv
  %8 = load i32, ptr %arrayidx8, align 4
  %arrayidx9 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  %9 = load i32, ptr %arrayidx9, align 4
  %add10 = add nsw i32 %9, %8
  store i32 %add10, ptr %arrayidx9, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.3

DIR.VPO.END.GUARD.MEM.MOTION.3:                   ; preds = %DIR.VPO.GUARD.MEM.MOTION.1
  call void @llvm.directive.region.exit(token %pre.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.4

DIR.VPO.END.GUARD.MEM.MOTION.4:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.3
  %10 = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr %x.red, i64 1) ]
  br label %DIR.OMP.SCAN.2

DIR.OMP.SCAN.2:                                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.4
  fence acq_rel
  br label %DIR.OMP.END.SCAN.6

DIR.OMP.END.SCAN.6:                               ; preds = %DIR.OMP.SCAN.2
  call void @llvm.directive.region.exit(token %10) [ "DIR.OMP.END.SCAN"() ]
  br label %DIR.OMP.END.SCAN.3

DIR.OMP.END.SCAN.3:                               ; preds = %DIR.OMP.END.SCAN.6
  br label %DIR.VPO.GUARD.MEM.MOTION.8.split

DIR.VPO.GUARD.MEM.MOTION.8.split:                 ; preds = %DIR.OMP.END.SCAN.3
  %post.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.4

DIR.VPO.GUARD.MEM.MOTION.4:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.8.split
  %11 = load i32, ptr %x.red, align 4
  %12 = load i32, ptr %i.linear.iv, align 4
  %idxprom12 = sext i32 %12 to i64
  %arrayidx13 = getelementptr inbounds [8 x i32], ptr @B, i64 0, i64 %idxprom12
  store i32 %11, ptr %arrayidx13, align 4
  %arrayidx14 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  %13 = load i32, ptr %arrayidx14, align 4
  %arrayidx16 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 1, i64 %idxprom12
  store i32 %13, ptr %arrayidx16, align 4
  %arrayidx17 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  %14 = load i32, ptr %arrayidx17, align 4
  %arrayidx19 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 2, i64 %idxprom12
  store i32 %14, ptr %arrayidx19, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.10

DIR.VPO.END.GUARD.MEM.MOTION.10:                  ; preds = %DIR.VPO.GUARD.MEM.MOTION.4
  call void @llvm.directive.region.exit(token %post.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5100

DIR.VPO.END.GUARD.MEM.MOTION.5100:                ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.10
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 8
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.12, label %DIR.VPO.END.GUARD.MEM.MOTION.5, !llvm.loop !0

DIR.OMP.END.SIMD.12:                              ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100
  call void @llvm.directive.region.exit(token %2) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.6

DIR.OMP.END.SIMD.6:                               ; preds = %DIR.OMP.END.SIMD.12
  ret void
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.vectorize.enable", i1 true}
!2 = !{!"llvm.loop.vectorize.ivdep_loop", i32 0}
