; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py UTC_ARGS: --version 2
; RUN: opt -mtriple=x86_64 -mcpu=skylake-avx512 -passes="vplan-vec" -vec-clone-legalize-enabled -S < %s | FileCheck %s
; RUN: opt -mtriple=x86_64 -mcpu=skylake-avx512 -passes="hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec,print<hir>" -vec-clone-legalize-enabled -disable-output < %s 2>&1 | FileCheck %s --check-prefix=HIR

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)

; Check that VPlan correctly legalizes arguments and return value for
; a vector variant _ZGVyN8vul_foo of "foo".
; Check that first vector argument is legalized while 2dn and 3d arguments
; (uniform and linear respectively) do not require legalization .
; The return value does require legalization.
; The test is simlar to vplan_legalize_vecvariant_ulv.ll with only difference the arguments order.

declare i32 @foo(i32 %x, i32 %n, i32 %i) local_unnamed_addr #0

define i32 @caller() local_unnamed_addr #1 {
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ [[TMP5:%.*]], [[VECTOR_BODY0:%.*]] ], [ 0, [[VPLANNEDBB10:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <8 x i64> [ [[TMP4:%.*]], [[VECTOR_BODY0]] ], [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7>, [[VPLANNEDBB10]] ]
; CHECK-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds [256 x i32], ptr [[B0:%.*]], i64 0, i64 [[UNI_PHI0]]
; CHECK-NEXT:    [[WIDE_LOAD0:%.*]] = load <8 x i32>, ptr [[SCALAR_GEP0]], align 16
; CHECK-NEXT:    [[TMP0:%.*]] = trunc <8 x i64> [[VEC_PHI0]] to <8 x i32>
; CHECK-NEXT:    [[DOTEXTRACT_0_0:%.*]] = extractelement <8 x i32> [[TMP0]], i32 0
; CHECK-NEXT:    [[WIDE_LOAD_PART_0_OF_2_0:%.*]] = shufflevector <8 x i32> [[WIDE_LOAD0]], <8 x i32> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
; CHECK-NEXT:    [[WIDE_LOAD_PART_1_OF_2_0:%.*]] = shufflevector <8 x i32> [[WIDE_LOAD0]], <8 x i32> undef, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
; CHECK-NEXT:    [[TMP1:%.*]] = call x86_regcallcc { <4 x i32>, <4 x i32> } @_ZGVyN8vul_foo(<4 x i32> [[WIDE_LOAD_PART_0_OF_2_0]], <4 x i32> [[WIDE_LOAD_PART_1_OF_2_0]], i32 256, i32 [[DOTEXTRACT_0_0]])
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[TMP1]], 0
; CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[TMP1]], 1
; CHECK-NEXT:    [[COMBINED0:%.*]] = shufflevector <4 x i32> [[TMP2]], <4 x i32> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; CHECK-NEXT:    [[SCALAR_GEP30:%.*]] = getelementptr inbounds [256 x i32], ptr [[A0:%.*]], i64 0, i64 [[UNI_PHI0]]
; CHECK-NEXT:    store <8 x i32> [[COMBINED0]], ptr [[SCALAR_GEP30]], align 16
; CHECK-NEXT:    [[TMP4]] = add nuw nsw <8 x i64> [[VEC_PHI0]], <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
; CHECK-NEXT:    [[TMP5]] = add nuw nsw i64 [[UNI_PHI0]], 8
; CHECK-NEXT:    [[TMP6:%.*]] = icmp uge i64 [[TMP5]], 256
; CHECK-NEXT:    br i1 [[TMP6]], label [[VPLANNEDBB40:%.*]], label [[VECTOR_BODY0]], !llvm.loop !0
;
; HIR-LABEL:  Function: caller
; HIR-EMPTY:
; HIR-NEXT:  BEGIN REGION { modified }
; HIR-NEXT:        + DO i1 = 0, 255, 8   <DO_LOOP> <simd-vectorized> <novectorize>
; HIR-NEXT:        |   [[DOTVEC0:%.*]] = (<8 x i32>*)([[B0:%.*]])[0][i1]
; HIR-NEXT:        |   [[DOTEXTRACTED_SUBVEC0:%.*]] = shufflevector [[DOTVEC0]],  undef,  <i32 0, i32 1, i32 2, i32 3>
; HIR-NEXT:        |   [[DOTEXTRACTED_SUBVEC20:%.*]] = shufflevector [[DOTVEC0]],  undef,  <i32 4, i32 5, i32 6, i32 7>
; HIR-NEXT:        |   [[_ZGVYN8VUL_FOO0:%.*]] = @_ZGVyN8vul_foo([[DOTEXTRACTED_SUBVEC0]],  [[DOTEXTRACTED_SUBVEC20]],  256,  i1)
; HIR-NEXT:        |   [[EXTRACT_RESULT0:%.*]] = extractvalue [[_ZGVYN8VUL_FOO0]], 0
; HIR-NEXT:        |   [[EXTRACT_RESULT30:%.*]] = extractvalue [[_ZGVYN8VUL_FOO0]], 1
; HIR-NEXT:        |   [[COMB_SHUF0:%.*]] = shufflevector [[EXTRACT_RESULT0]],  [[EXTRACT_RESULT30]],  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; HIR-NEXT:        |   (<8 x i32>*)([[A0:%.*]])[0][i1] = [[COMB_SHUF0]]
; HIR-NEXT:        + END LOOP
; HIR-NEXT:  END REGION
;
DIR.OMP.SIMD.2:
  %a = alloca [256 x i32], align 16
  %b = alloca [256 x i32], align 16
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %DIR.OMP.SIMD.2
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 8) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ 0, %DIR.OMP.SIMD.1 ]
  %arrayidx = getelementptr inbounds [256 x i32], ptr %b, i64 0, i64 %indvars.iv
  %1 = load i32, ptr %arrayidx, align 4
  %2 = trunc i64 %indvars.iv to i32
  %call = call i32 @foo(i32 %1, i32 256, i32 %2) #3
  %arrayidx2 = getelementptr inbounds [256 x i32], ptr %a, i64 0, i64 %indvars.iv
  store i32 %call, ptr %arrayidx2, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 256
  br i1 %exitcond, label %DIR.OMP.END.SIMD.4, label %omp.inner.for.body

DIR.OMP.END.SIMD.4:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.2

DIR.OMP.END.SIMD.2:                               ; preds = %DIR.OMP.END.SIMD.4
  ret i32 0
}

attributes #0 = { "vector-variants"="_ZGVyN8vul_foo" }
attributes #1 = { "may-have-openmp-directive"="true" }
