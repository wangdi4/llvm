; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Test VPCallVecDecision analysis and LLVM-IR vector codegen for a loop
; containing a uniform call with no side effects.

; RUN: opt -VPlanDriver -vplan-print-scalvec-results -S -vplan-force-vf=2 %s | FileCheck %s

; Check results from VPCallVecDecisions.
; FIXME: Call should be marked as DoNotWiden instead.
; CHECK: [DA: Uni, SVA: ( V )] i64 [[VPCALL:%vp.*]] = call i32 0 i64 (i32)* @uniform_call [Serial] (SVAOpBits 0->V 1->F )

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux"

; attributes #1 guarantees this function to be uniform with no side effects.
declare i64 @uniform_call(i32) local_unnamed_addr #1

define void @_ZGVeN2u_testKernel(i64 addrspace(1)* noalias %results) local_unnamed_addr {
; CHECK-LABEL: @_ZGVeN2u_testKernel(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP17:%.*]], [[PRED_STORE_CONTINUE14:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI1:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP16:%.*]], [[PRED_STORE_CONTINUE14]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH]] ], [ [[TMP15:%.*]], [[PRED_STORE_CONTINUE14]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add nuw <2 x i64> [[VEC_PHI]], [[BROADCAST_SPLAT:%.*]]
; CHECK-NEXT:    [[TMP1:%.*]] = icmp eq <2 x i64> [[TMP0]], [[BROADCAST_SPLAT3:%.*]]
; CHECK-NEXT:    [[PREDICATE:%.*]] = extractelement <2 x i1> [[TMP1]], i64 0
; CHECK-NEXT:    [[TMP2:%.*]] = icmp eq i1 [[PREDICATE]], true
; CHECK-NEXT:    br i1 [[TMP2]], label [[PRED_CALL_IF:%.*]], label [[TMP4:%.*]]
; FIXME: Call should be kept as single scalar copy instead of serialization for all lanes.
; CHECK:       pred.call.if:
; CHECK-NEXT:    [[TMP3:%.*]] = call i64 @uniform_call(i32 0)
; CHECK-NEXT:    br label [[TMP4]]
; CHECK:       4:
; CHECK-NEXT:    [[TMP5:%.*]] = phi i64 [ undef, [[VECTOR_BODY:%.*]] ], [ [[TMP3]], [[PRED_CALL_IF]] ]
; CHECK-NEXT:    br label [[PRED_CALL_CONTINUE:%.*]]
; CHECK:       pred.call.continue:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT7:%.*]] = insertelement <2 x i64> undef, i64 [[TMP5]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT8:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT7]], <2 x i64> undef, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[PREDICATE4:%.*]] = extractelement <2 x i1> [[TMP1]], i64 1
; CHECK-NEXT:    [[TMP6:%.*]] = icmp eq i1 [[PREDICATE4]], true
; CHECK-NEXT:    br i1 [[TMP6]], label [[PRED_CALL_IF11:%.*]], label [[TMP8:%.*]]
; CHECK:       pred.call.if11:
; CHECK-NEXT:    [[TMP7:%.*]] = call i64 @uniform_call(i32 0)
; CHECK-NEXT:    br label [[TMP8]]
; CHECK:       8:
; CHECK-NEXT:    [[TMP9:%.*]] = phi i64 [ undef, [[PRED_CALL_CONTINUE]] ], [ [[TMP7]], [[PRED_CALL_IF11]] ]
; CHECK-NEXT:    br label [[PRED_CALL_CONTINUE12:%.*]]
; CHECK:       pred.call.continue12:
; CHECK-NEXT:    [[TMP10:%.*]] = add i64 [[TMP5]], [[GID:%.*]]
; CHECK-NEXT:    [[PREDICATE5:%.*]] = extractelement <2 x i1> [[TMP1]], i64 0
; CHECK-NEXT:    [[TMP11:%.*]] = icmp eq i1 [[PREDICATE5]], true
; CHECK-NEXT:    br i1 [[TMP11]], label [[PRED_STORE_IF:%.*]], label [[TMP12:%.*]]
; CHECK:       pred.store.if:
; CHECK-NEXT:    store volatile i64 [[TMP5]], i64 addrspace(1)* [[RESULTS:%.*]], align 8
; CHECK-NEXT:    br label [[TMP12]]
; CHECK:       12:
; CHECK-NEXT:    br label [[PRED_STORE_CONTINUE:%.*]]
; CHECK:       pred.store.continue:
; CHECK-NEXT:    [[PREDICATE6:%.*]] = extractelement <2 x i1> [[TMP1]], i64 1
; CHECK-NEXT:    [[TMP13:%.*]] = icmp eq i1 [[PREDICATE6]], true
; CHECK-NEXT:    br i1 [[TMP13]], label [[PRED_STORE_IF13:%.*]], label [[TMP14:%.*]]
; CHECK:       pred.store.if13:
; CHECK-NEXT:    store volatile i64 [[TMP5]], i64 addrspace(1)* [[RESULTS]], align 8
; CHECK-NEXT:    br label [[TMP14]]
; CHECK:       14:
; CHECK-NEXT:    br label [[PRED_STORE_CONTINUE14]]
; CHECK:       pred.store.continue14:
; CHECK-NEXT:    call void @llvm.masked.scatter.v2i64.v2p1i64(<2 x i64> [[BROADCAST_SPLAT8]], <2 x i64 addrspace(1)*> [[BROADCAST_SPLAT10:%.*]], i32 8, <2 x i1> [[TMP1]])
; CHECK-NEXT:    [[TMP15]] = add nuw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP16]] = add nuw i64 [[UNI_PHI1]], 2
; CHECK-NEXT:    [[TMP17]] = add i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne i64 [[TMP17]], 2
; CHECK-NEXT:    br i1 false, label [[VECTOR_BODY]], label [[VPLANNEDBB:%.*]], !llvm.loop !0
;
entry:
  %alloca.results = alloca i64 addrspace(1)*
  store i64 addrspace(1)* %results, i64 addrspace(1)** %alloca.results
  %gid = tail call i64 @_Z13get_global_idj(i32 0)
  %bound = tail call i64 @foo(i32 0)
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 2), "QUAL.OMP.UNIFORM"(i64 addrspace(1)** %alloca.results) ]
  br label %simd.loop.preheader

simd.loop.preheader:
  %load.results = load i64 addrspace(1)*, i64 addrspace(1)** %alloca.results
  br label %simd.loop

simd.loop:
  %index = phi i64 [ 0, %simd.loop.preheader ], [ %indvar, %simd.loop.exit ]
  %add = add nuw i64 %index, %gid
  %cmp = icmp eq i64 %add, %bound
  br i1 %cmp, label %if.end, label %simd.loop.exit

if.end:
  %result = tail call i64 @uniform_call(i32 0)
  %scal.user = add i64 %result, %gid
  store volatile i64 %result, i64 addrspace(1)* %results, align 8
  store i64 %result, i64 addrspace(1)* %load.results, align 8
  br label %simd.loop.exit

simd.loop.exit:
  %indvar = add nuw i64 %index, 1
  %vl.cond = icmp ult i64 %indvar, 2
  br i1 %vl.cond, label %simd.loop, label %simd.end.region

simd.end.region:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
declare i64 @_Z13get_global_idj(i32) local_unnamed_addr #1
declare i64 @foo(i32) local_unnamed_addr #1

attributes #1 = { nounwind readnone  }

