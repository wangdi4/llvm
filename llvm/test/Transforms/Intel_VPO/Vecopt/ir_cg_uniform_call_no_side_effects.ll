; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Test VPCallVecDecision analysis and LLVM-IR vector codegen for a loop
; containing a uniform call with no side effects.

; RUN: opt -vplan-vec -vplan-print-scalvec-results -S -vplan-force-vf=2 %s | FileCheck %s

; Check results from VPCallVecDecisions.
; CHECK: [DA: Uni, SVA: (F  )] i64 [[VPCALL:%vp.*]] = call i32 0 i64 (i32)* @uniform_call (SVAOpBits 0->F 1->F )

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux"

; attributes #1 guarantees this function to be uniform with no side effects.
declare i64 @uniform_call(i32) local_unnamed_addr #1

define void @_ZGVeN2u_testKernel(i64 addrspace(1)* noalias %results) local_unnamed_addr {
; CHECK-LABEL: @_ZGVeN2u_testKernel(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP13:%.*]], [[VPLANNEDBB11:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VPLANNEDBB1]] ], [ [[TMP12:%.*]], [[VPLANNEDBB11]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add nuw <2 x i64> [[VEC_PHI]], [[BROADCAST_SPLAT:%.*]]
; CHECK-NEXT:    [[TMP1:%.*]] = icmp eq <2 x i64> [[TMP0]], [[BROADCAST_SPLAT4:%.*]]
; CHECK-NEXT:    br label [[VPLANNEDBB5:%.*]]
; CHECK:       VPlannedBB5:
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i1> [[TMP1]] to i2
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne i2 [[TMP2]], 0
; CHECK-NEXT:    br i1 [[TMP3]], label [[PRED_CALL_IF:%.*]], label [[TMP5:%.*]]
; CHECK:       pred.call.if:
; CHECK-NEXT:    [[TMP4:%.*]] = tail call i64 @uniform_call(i32 0)
; CHECK-NEXT:    br label [[TMP5]]
; CHECK:       5:
; CHECK-NEXT:    [[TMP6:%.*]] = phi i64 [ undef, [[VPLANNEDBB5]] ], [ [[TMP4]], [[PRED_CALL_IF]] ]
; CHECK-NEXT:    br label [[PRED_CALL_CONTINUE:%.*]]
; CHECK:       pred.call.continue:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT7:%.*]] = insertelement <2 x i64> poison, i64 [[TMP6]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT8:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT7]], <2 x i64> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = add i64 [[TMP6]], [[GID:%.*]]
; CHECK-NEXT:    [[PREDICATE:%.*]] = extractelement <2 x i1> [[TMP1]], i64 0
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq i1 [[PREDICATE]], true
; CHECK-NEXT:    br i1 [[TMP8]], label [[PRED_STORE_IF:%.*]], label [[TMP9:%.*]]
; CHECK:       pred.store.if:
; CHECK-NEXT:    store volatile i64 [[TMP6]], i64 addrspace(1)* [[RESULTS:%.*]], align 8
; CHECK-NEXT:    br label [[TMP9]]
; CHECK:       9:
; CHECK-NEXT:    br label [[PRED_STORE_CONTINUE:%.*]]
; CHECK:       pred.store.continue:
; CHECK-NEXT:    [[PREDICATE6:%.*]] = extractelement <2 x i1> [[TMP1]], i64 1
; CHECK-NEXT:    [[TMP10:%.*]] = icmp eq i1 [[PREDICATE6]], true
; CHECK-NEXT:    br i1 [[TMP10]], label [[PRED_STORE_IF15:%.*]], label [[TMP11:%.*]]
; CHECK:       pred.store.if15:
; CHECK-NEXT:    store volatile i64 [[TMP6]], i64 addrspace(1)* [[RESULTS]], align 8
; CHECK-NEXT:    br label [[TMP11]]
; CHECK:       11:
; CHECK-NEXT:    br label [[PRED_STORE_CONTINUE16:%.*]]
; CHECK:       pred.store.continue16:
; CHECK-NEXT:    call void @llvm.masked.scatter.v2i64.v2p1i64(<2 x i64> [[BROADCAST_SPLAT8]], <2 x i64 addrspace(1)*> [[BROADCAST_SPLAT10:%.*]], i32 8, <2 x i1> [[TMP1]])
; CHECK-NEXT:    br label [[VPLANNEDBB11]]
; CHECK:       VPlannedBB11:
; CHECK-NEXT:    [[TMP12]] = add nuw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP13]] = add nuw i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ult i64 [[TMP13]], 2
; CHECK-NEXT:    br i1 false, label [[VECTOR_BODY:%.*]], label [[VPLANNEDBB12:%.*]], !llvm.loop [[LOOP0:![0-9]+]]
;
entry:
  %alloca.results = alloca i64 addrspace(1)*
  store i64 addrspace(1)* %results, i64 addrspace(1)** %alloca.results
  %gid = tail call i64 @_Z13get_global_idj(i32 0)
  %bound = tail call i64 @foo(i32 0)
  br label %simd.begin.region

simd.begin.region:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 2), "QUAL.OMP.UNIFORM:PTR_TO_PTR.TYPED"(i64 addrspace(1)** %alloca.results, i64 0, i32 1) ]
  br label %simd.loop.preheader

simd.loop.preheader:
  %load.results = load i64 addrspace(1)*, i64 addrspace(1)** %alloca.results
  br label %simd.loop

simd.loop:
  %index = phi i64 [ 0, %simd.loop.preheader ], [ %indvar, %simd.loop.exit ]
  %add = add nuw i64 %index, %gid
  %cmp = icmp eq i64 %add, %bound
  br i1 %cmp, label %if.end, label %simd.loop.exit

if.end:
  %result = tail call i64 @uniform_call(i32 0)
  %scal.user = add i64 %result, %gid
  store volatile i64 %result, i64 addrspace(1)* %results, align 8
  store i64 %result, i64 addrspace(1)* %load.results, align 8
  br label %simd.loop.exit

simd.loop.exit:
  %indvar = add nuw i64 %index, 1
  %vl.cond = icmp ult i64 %indvar, 2
  br i1 %vl.cond, label %simd.loop, label %simd.end.region

simd.end.region:
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
declare i64 @_Z13get_global_idj(i32) local_unnamed_addr #1
declare i64 @foo(i32) local_unnamed_addr #1

attributes #1 = { nounwind readnone willreturn }

