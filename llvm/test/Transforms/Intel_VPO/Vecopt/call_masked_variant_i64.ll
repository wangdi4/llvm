; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py UTC_ARGS: --version 2
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=16 -vplan-cm-prohibit-zmm-low-pumping=0 -S < %s | FileCheck %s -check-prefix=VPLANCGVF16
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=8 -vplan-cm-prohibit-zmm-low-pumping=0 -S < %s | FileCheck %s -check-prefix=VPLANCGVF8
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=16 -vplan-cm-prohibit-zmm-low-pumping=0 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF16
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=8 -vplan-cm-prohibit-zmm-low-pumping=0 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF8

; Test case to test mask argument lowering at a call site for avx512
; according to VecABI specification.
; Test with VF16 checks that mask parameter is split into two i32
; arguments and each provides effective 8 bits of mask, so we need
; to generate only required set of instruction to pack logical type
; (which is the characteristic type) into the variant argument.
; Since characteristic data type is i64 we can skip one bitcast instruction.
; VF8 case is similar but entire mask is passed via single data chunk.


@ARRAY_SIZE = external dso_local  constant i32, align 4

declare token @llvm.directive.region.entry() #0
declare void @llvm.directive.region.exit(token) #0
declare i64 @vfunc(i64 noundef ) #1

define void @test(ptr nocapture noundef readonly %src, ptr nocapture noundef writeonly %dst) #2 {
; VPLANCGVF16:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF16:       vector.body:
; VPLANCGVF16-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF16-NEXT:    [[VEC_PHI0:%.*]] = phi <16 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15>, [[VPLANNEDBB20]] ], [ [[TMP7:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF16-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i64, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF16-NEXT:    [[WIDE_LOAD0:%.*]] = load <16 x i64>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF16-NEXT:    [[WIDE_LOAD_PART_0_OF_2_0:%.*]] = shufflevector <16 x i64> [[WIDE_LOAD0]], <16 x i64> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; VPLANCGVF16-NEXT:    [[WIDE_LOAD_PART_1_OF_2_0:%.*]] = shufflevector <16 x i64> [[WIDE_LOAD0]], <16 x i64> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; VPLANCGVF16-NEXT:    [[ZEXT0:%.*]] = zext i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32
; VPLANCGVF16-NEXT:    [[ZEXT1:%.*]] = zext i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32
; VPLANCGVF16-NEXT:    [[TMP4:%.*]] = call x86_regcallcc { <8 x i64>, <8 x i64> } @_ZGVZM16v_vfunc(<8 x i64> noundef [[WIDE_LOAD_PART_0_OF_2_0]], <8 x i64> noundef [[WIDE_LOAD_PART_1_OF_2_0]], i32 [[ZEXT0]], i32 [[ZEXT1]]) #0
;
; VPLANCGVF8:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF8:       vector.body:
; VPLANCGVF8-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP6:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF8-NEXT:    [[VEC_PHI0:%.*]] = phi <8 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7>, [[VPLANNEDBB20]] ], [ [[TMP5:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF8-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i64, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF8-NEXT:    [[WIDE_LOAD0:%.*]] = load <8 x i64>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF8-NEXT:    [[ZEXT0:%.*]] = zext i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32
; VPLANCGVF8-NEXT:    [[TMP4:%.*]] = call x86_regcallcc <8 x i64> @_ZGVZM8v_vfunc(<8 x i64> noundef [[WIDE_LOAD0]], i32 [[ZEXT0]]) #0
;
; HIRVF16-LABEL:  Function: test
; HIRVF16:             + DO i1 = 0, [[LOOP_UB0:%.*]], 16   <DO_LOOP>  <MAX_TC_EST = 134217727>  <LEGAL_MAX_TC = 134217727> <simd-vectorized> <nounroll> <novectorize>
; HIRVF16-NEXT:        |   [[DOTVEC200:%.*]] = undef
; HIRVF16-NEXT:        |   [[DOTVEC40:%.*]] = (<16 x i64>*)([[SRC0:%.*]])[i1]
; HIRVF16-NEXT:        |   [[SEXT0:%.*]] = sext.<16 x i1>.<16 x i64>(1)
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC0:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC50:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC60:%.*]] = shufflevector [[SEXT0]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; HIRVF16-NEXT:        |   [[TRUNC0:%.*]] = trunc.<8 x i64>.<8 x i1>([[DOTEXTRACTED_SUBVEC60]])
; HIRVF16-NEXT:        |   [[TMP1:%.*]] = bitcast.<8 x i1>.i8([[TRUNC0]])
; HIRVF16-NEXT:        |   [[ZEXT0:%.*]] = zext.i8.i32([[TMP1]])
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC70:%.*]] = shufflevector [[SEXT0]],  undef,  <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   [[TRUNC80:%.*]] = trunc.<8 x i64>.<8 x i1>([[DOTEXTRACTED_SUBVEC70]])
; HIRVF16-NEXT:        |   [[TMP2:%.*]] = bitcast.<8 x i1>.i8([[TRUNC80]])
; HIRVF16-NEXT:        |   [[ZEXT90:%.*]] = zext.i8.i32([[TMP2]])
; HIRVF16-NEXT:        |   [[_ZGVZM16V_VFUNC0:%.*]] = @_ZGVZM16v_vfunc([[DOTEXTRACTED_SUBVEC0]],  [[DOTEXTRACTED_SUBVEC50]],  [[ZEXT0]],  [[ZEXT90]])
; HIRVF16-NEXT:        |   [[EXTRACT_RESULT0:%.*]] = extractvalue [[_ZGVZM16V_VFUNC0]], 0
; HIRVF16-NEXT:        |   [[EXTRACT_RESULT100:%.*]] = extractvalue [[_ZGVZM16V_VFUNC0]], 1
; HIRVF16-NEXT:        |   [[COMB_SHUF0:%.*]] = shufflevector [[EXTRACT_RESULT0]],  [[EXTRACT_RESULT100]],  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   (<16 x i64>*)([[DST0:%.*]])[i1] = [[COMB_SHUF0]]
; HIRVF16-NEXT:        + END LOOP
;
; HIRVF8-LABEL:  Function: test
; HIRVF8:             + DO i1 = 0, [[LOOP_UB0:%.*]], 8   <DO_LOOP>  <MAX_TC_EST = 268435455>  <LEGAL_MAX_TC = 268435455> <simd-vectorized> <nounroll> <novectorize>
; HIRVF8-NEXT:        |   [[DOTVEC140:%.*]] = undef
; HIRVF8-NEXT:        |   [[DOTVEC40:%.*]] = (<8 x i64>*)([[SRC0:%.*]])[i1]
; HIRVF8-NEXT:        |   [[SEXT0:%.*]] = sext.<8 x i1>.<8 x i64>(1)
; HIRVF8-NEXT:        |   [[TRUNC0:%.*]] = trunc.<8 x i64>.<8 x i1>([[SEXT0]])
; HIRVF8-NEXT:        |   [[TMP1:%.*]] = bitcast.<8 x i1>.i8([[TRUNC0]])
; HIRVF8-NEXT:        |   [[ZEXT0:%.*]] = zext.i8.i32([[TMP1]])
; HIRVF8-NEXT:        |   [[_ZGVZM8V_VFUNC0:%.*]] = @_ZGVZM8v_vfunc([[DOTVEC40]],  [[ZEXT0]])
; HIRVF8-NEXT:        |   (<8 x i64>*)([[DST0:%.*]])[i1] = [[_ZGVZM8V_VFUNC0]]
; HIRVF8-NEXT:        + END LOOP
;
entry:
  %i.linear.iv = alloca i32, align 4
  %0 = load i32, ptr @ARRAY_SIZE, align 4
  %cmp = icmp sgt i32 %0, 0
  br i1 %cmp, label %DIR.OMP.SIMD.1, label %omp.precond.end

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %1 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr null, i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr null, i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.OMP.SIMD.117

DIR.OMP.SIMD.117:                                 ; preds = %DIR.OMP.SIMD.1
  %wide.trip.count = zext i32 %0 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.117
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.117 ], [ %indvars.iv.next, %omp.inner.for.body ]
  %arrayidx = getelementptr inbounds i64, ptr %src, i64 %indvars.iv
  %2 = load i64, ptr %arrayidx, align 8
  %callret = call i64 @vfunc(i64 noundef %2) #0
  %arrayidx6 = getelementptr inbounds i64, ptr %dst, i64 %indvars.iv
  store i64 %callret, ptr %arrayidx6, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge: ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %1) [ "DIR.OMP.END.SIMD"() ]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, %entry
  ret void
}

attributes #0 = { nounwind }
attributes #1 = { "vector-variants"="_ZGVZM8v_vfunc,_ZGVZM16v_vfunc" }
attributes #2 = { "target-cpu"="skylake-avx512" }
