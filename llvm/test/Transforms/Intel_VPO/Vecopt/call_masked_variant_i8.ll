; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py UTC_ARGS: --version 2
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=64 -vplan-cm-prohibit-zmm-low-pumping=0 -S < %s | FileCheck %s -check-prefix=VPLANCGVF64
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=128 -vplan-cm-prohibit-zmm-low-pumping=0 -S < %s | FileCheck %s -check-prefix=VPLANCGVF128
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=64 -vplan-cm-prohibit-zmm-low-pumping=0 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF64
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=128 -vplan-cm-prohibit-zmm-low-pumping=0 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF128

; Test case to test mask argument lowering at a call site for avx512
; according to VecABI specification.
; Test checks that mask parameter is passed via i64 argument and since
; the argument provides effective 64 bits of mask, we do not need to
; generate final zext instruction as a part of instructions sequence
; which packs logical type (which is the characteristic type) into the
; variant argument. Since characteristic data type is i16 we can skip
; one bitcast instruction too.
; Note that IR builder is smart enough to optimize the sequence into
; a const expression.
; In VF=64 case each arguments needs only single data chunk to pass
; each logical argument (two for VF=128 case).
; Test also checks that i8 data type is not promoted to i32.

@ARRAY_SIZE = external dso_local  constant i32, align 4

declare token @llvm.directive.region.entry() #0
declare void @llvm.directive.region.exit(token) #0
declare i8 @vfunc(i8 noundef) #1

define void @test(ptr nocapture noundef readonly %src, ptr nocapture noundef writeonly %dst) #2 {
; VPLANCGVF64:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF64:       vector.body:
; VPLANCGVF64-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP6:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF64-NEXT:    [[VEC_PHI0:%.*]] = phi <64 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31, i64 32, i64 33, i64 34, i64 35, i64 36, i64 37, i64 38, i64 39, i64 40, i64 41, i64 42, i64 43, i64 44, i64 45, i64 46, i64 47, i64 48, i64 49, i64 50, i64 51, i64 52, i64 53, i64 54, i64 55, i64 56, i64 57, i64 58, i64 59, i64 60, i64 61, i64 62, i64 63>, [[VPLANNEDBB20]] ], [ [[TMP5:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF64-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i8, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF64-NEXT:    [[WIDE_LOAD0:%.*]] = load <64 x i8>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF64-NEXT:    [[TMP4:%.*]] = call x86_regcallcc <64 x i8> @_ZGVZM64v_vfunc(<64 x i8> noundef [[WIDE_LOAD0]], i64 bitcast (<64 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i64)) #0
;
; VPLANCGVF128:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF128:       vector.body:
; VPLANCGVF128-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF128-NEXT:    [[VEC_PHI0:%.*]] = phi <128 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15, i64 16, i64 17, i64 18, i64 19, i64 20, i64 21, i64 22, i64 23, i64 24, i64 25, i64 26, i64 27, i64 28, i64 29, i64 30, i64 31, i64 32, i64 33, i64 34, i64 35, i64 36, i64 37, i64 38, i64 39, i64 40, i64 41, i64 42, i64 43, i64 44, i64 45, i64 46, i64 47, i64 48, i64 49, i64 50, i64 51, i64 52, i64 53, i64 54, i64 55, i64 56, i64 57, i64 58, i64 59, i64 60, i64 61, i64 62, i64 63, i64 64, i64 65, i64 66, i64 67, i64 68, i64 69, i64 70, i64 71, i64 72, i64 73, i64 74, i64 75, i64 76, i64 77, i64 78, i64 79, i64 80, i64 81, i64 82, i64 83, i64 84, i64 85, i64 86, i64 87, i64 88, i64 89, i64 90, i64 91, i64 92, i64 93, i64 94, i64 95, i64 96, i64 97, i64 98, i64 99, i64 100, i64 101, i64 102, i64 103, i64 104, i64 105, i64 106, i64 107, i64 108, i64 109, i64 110, i64 111, i64 112, i64 113, i64 114, i64 115, i64 116, i64 117, i64 118, i64 119, i64 120, i64 121, i64 122, i64 123, i64 124, i64 125, i64 126, i64 127>, [[VPLANNEDBB20]] ], [ [[TMP7:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF128-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds i8, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF128-NEXT:    [[WIDE_LOAD0:%.*]] = load <128 x i8>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF128-NEXT:    [[WIDE_LOAD_PART_0_OF_2_0:%.*]] = shufflevector <128 x i8> [[WIDE_LOAD0]], <128 x i8> undef, <64 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39, i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47, i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55, i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63>
; VPLANCGVF128-NEXT:    [[WIDE_LOAD_PART_1_OF_2_0:%.*]] = shufflevector <128 x i8> [[WIDE_LOAD0]], <128 x i8> undef, <64 x i32> <i32 64, i32 65, i32 66, i32 67, i32 68, i32 69, i32 70, i32 71, i32 72, i32 73, i32 74, i32 75, i32 76, i32 77, i32 78, i32 79, i32 80, i32 81, i32 82, i32 83, i32 84, i32 85, i32 86, i32 87, i32 88, i32 89, i32 90, i32 91, i32 92, i32 93, i32 94, i32 95, i32 96, i32 97, i32 98, i32 99, i32 100, i32 101, i32 102, i32 103, i32 104, i32 105, i32 106, i32 107, i32 108, i32 109, i32 110, i32 111, i32 112, i32 113, i32 114, i32 115, i32 116, i32 117, i32 118, i32 119, i32 120, i32 121, i32 122, i32 123, i32 124, i32 125, i32 126, i32 127>
; VPLANCGVF128-NEXT:    [[TMP4:%.*]] = call x86_regcallcc { <64 x i8>, <64 x i8> } @_ZGVZM128v_vfunc(<64 x i8> noundef [[WIDE_LOAD_PART_0_OF_2_0]], <64 x i8> noundef [[WIDE_LOAD_PART_1_OF_2_0]], i64 bitcast (<64 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i64), i64 bitcast (<64 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i64)) #0
;
; HIRVF64-LABEL:  Function: test
; HIRVF64:             + DO i1 = 0, [[LOOP_UB0:%.*]], 64   <DO_LOOP>  <MAX_TC_EST = 33554431>  <LEGAL_MAX_TC = 33554431> <simd-vectorized> <nounroll> <novectorize>
; HIRVF64-NEXT:        |   [[DOTVEC140:%.*]] = undef
; HIRVF64-NEXT:        |   [[DOTVEC40:%.*]] = (<64 x i8>*)([[SRC0:%.*]])[i1]
; HIRVF64-NEXT:        |   [[SEXT0:%.*]] = sext.<64 x i1>.<64 x i8>(1)
; HIRVF64-NEXT:        |   [[TRUNC0:%.*]] = trunc.<64 x i8>.<64 x i1>([[SEXT0]])
; HIRVF64-NEXT:        |   [[TMP1:%.*]] = bitcast.<64 x i1>.i64([[TRUNC0]])
; HIRVF64-NEXT:        |   [[_ZGVZM64V_VFUNC0:%.*]] = @_ZGVZM64v_vfunc([[DOTVEC40]],  [[TMP1]])
; HIRVF64-NEXT:        |   (<64 x i8>*)([[DST0:%.*]])[i1] = [[_ZGVZM64V_VFUNC0]]
; HIRVF64-NEXT:        + END LOOP
;
; HIRVF128-LABEL:  Function: test
; HIRVF128:             + DO i1 = 0, [[LOOP_UB0:%.*]], 128   <DO_LOOP>  <MAX_TC_EST = 16777215>  <LEGAL_MAX_TC = 16777215> <simd-vectorized> <nounroll> <novectorize>
; HIRVF128-NEXT:        |   [[DOTVEC190:%.*]] = undef
; HIRVF128-NEXT:        |   [[DOTVEC40:%.*]] = (<128 x i8>*)([[SRC0:%.*]])[i1]
; HIRVF128-NEXT:        |   [[SEXT0:%.*]] = sext.<128 x i1>.<128 x i8>(1)
; HIRVF128-NEXT:        |   [[DOTEXTRACTED_SUBVEC0:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39, i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47, i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55, i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63>
; HIRVF128-NEXT:        |   [[DOTEXTRACTED_SUBVEC50:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 64, i32 65, i32 66, i32 67, i32 68, i32 69, i32 70, i32 71, i32 72, i32 73, i32 74, i32 75, i32 76, i32 77, i32 78, i32 79, i32 80, i32 81, i32 82, i32 83, i32 84, i32 85, i32 86, i32 87, i32 88, i32 89, i32 90, i32 91, i32 92, i32 93, i32 94, i32 95, i32 96, i32 97, i32 98, i32 99, i32 100, i32 101, i32 102, i32 103, i32 104, i32 105, i32 106, i32 107, i32 108, i32 109, i32 110, i32 111, i32 112, i32 113, i32 114, i32 115, i32 116, i32 117, i32 118, i32 119, i32 120, i32 121, i32 122, i32 123, i32 124, i32 125, i32 126, i32 127>
; HIRVF128-NEXT:        |   [[DOTEXTRACTED_SUBVEC60:%.*]] = shufflevector [[SEXT0]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39, i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47, i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55, i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63>
; HIRVF128-NEXT:        |   [[TRUNC0:%.*]] = trunc.<64 x i8>.<64 x i1>([[DOTEXTRACTED_SUBVEC60]])
; HIRVF128-NEXT:        |   [[TMP1:%.*]] = bitcast.<64 x i1>.i64([[TRUNC0]])
; HIRVF128-NEXT:        |   [[DOTEXTRACTED_SUBVEC70:%.*]] = shufflevector [[SEXT0]],  undef,  <i32 64, i32 65, i32 66, i32 67, i32 68, i32 69, i32 70, i32 71, i32 72, i32 73, i32 74, i32 75, i32 76, i32 77, i32 78, i32 79, i32 80, i32 81, i32 82, i32 83, i32 84, i32 85, i32 86, i32 87, i32 88, i32 89, i32 90, i32 91, i32 92, i32 93, i32 94, i32 95, i32 96, i32 97, i32 98, i32 99, i32 100, i32 101, i32 102, i32 103, i32 104, i32 105, i32 106, i32 107, i32 108, i32 109, i32 110, i32 111, i32 112, i32 113, i32 114, i32 115, i32 116, i32 117, i32 118, i32 119, i32 120, i32 121, i32 122, i32 123, i32 124, i32 125, i32 126, i32 127>
; HIRVF128-NEXT:        |   [[TRUNC80:%.*]] = trunc.<64 x i8>.<64 x i1>([[DOTEXTRACTED_SUBVEC70]])
; HIRVF128-NEXT:        |   [[TMP2:%.*]] = bitcast.<64 x i1>.i64([[TRUNC80]])
; HIRVF128-NEXT:        |   [[_ZGVZM128V_VFUNC0:%.*]] = @_ZGVZM128v_vfunc([[DOTEXTRACTED_SUBVEC0]],  [[DOTEXTRACTED_SUBVEC50]],  [[TMP1]],  [[TMP2]])
; HIRVF128-NEXT:        |   [[EXTRACT_RESULT0:%.*]] = extractvalue [[_ZGVZM128V_VFUNC0]], 0
; HIRVF128-NEXT:        |   [[EXTRACT_RESULT90:%.*]] = extractvalue [[_ZGVZM128V_VFUNC0]], 1
; HIRVF128-NEXT:        |   [[COMB_SHUF0:%.*]] = shufflevector [[EXTRACT_RESULT0]],  [[EXTRACT_RESULT90]],  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31, i32 32, i32 33, i32 34, i32 35, i32 36, i32 37, i32 38, i32 39, i32 40, i32 41, i32 42, i32 43, i32 44, i32 45, i32 46, i32 47, i32 48, i32 49, i32 50, i32 51, i32 52, i32 53, i32 54, i32 55, i32 56, i32 57, i32 58, i32 59, i32 60, i32 61, i32 62, i32 63, i32 64, i32 65, i32 66, i32 67, i32 68, i32 69, i32 70, i32 71, i32 72, i32 73, i32 74, i32 75, i32 76, i32 77, i32 78, i32 79, i32 80, i32 81, i32 82, i32 83, i32 84, i32 85, i32 86, i32 87, i32 88, i32 89, i32 90, i32 91, i32 92, i32 93, i32 94, i32 95, i32 96, i32 97, i32 98, i32 99, i32 100, i32 101, i32 102, i32 103, i32 104, i32 105, i32 106, i32 107, i32 108, i32 109, i32 110, i32 111, i32 112, i32 113, i32 114, i32 115, i32 116, i32 117, i32 118, i32 119, i32 120, i32 121, i32 122, i32 123, i32 124, i32 125, i32 126, i32 127>
; HIRVF128-NEXT:        |   (<128 x i8>*)([[DST0:%.*]])[i1] = [[COMB_SHUF0]]
; HIRVF128-NEXT:        + END LOOP
;
entry:
  %i.linear.iv = alloca i32, align 4
  %0 = load i32, ptr @ARRAY_SIZE, align 4
  %cmp = icmp sgt i32 %0, 0
  br i1 %cmp, label %DIR.OMP.SIMD.1, label %omp.precond.end

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %1 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr null, i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr null, i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.OMP.SIMD.117

DIR.OMP.SIMD.117:                                 ; preds = %DIR.OMP.SIMD.1
  %wide.trip.count = zext i32 %0 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.117
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.117 ], [ %indvars.iv.next, %omp.inner.for.body ]
  %arrayidx = getelementptr inbounds i8, ptr %src, i64 %indvars.iv
  %2 = load i8, ptr %arrayidx, align 8
  %callret = call i8 @vfunc(i8 noundef %2) #0
  %arrayidx6 = getelementptr inbounds i8, ptr %dst, i64 %indvars.iv
  store i8 %callret, ptr %arrayidx6, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge: ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %1) [ "DIR.OMP.END.SIMD"() ]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, %entry
  ret void
}

attributes #0 = { nounwind }
attributes #1 = { "vector-variants"="_ZGVZM128v_vfunc,_ZGVZM64v_vfunc" }
attributes #2 = { "target-cpu"="skylake-avx512" }
