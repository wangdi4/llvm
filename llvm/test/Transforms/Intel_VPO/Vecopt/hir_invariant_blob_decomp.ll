; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -hir-ssa-deconstruction -hir-vec-dir-insert -hir-vplan-vec -disable-output -vplan-print-after-plain-cfg -vplan-force-invariant-decomposition=0 -vplan-dump-external-defs-hir < %s 2>&1 | FileCheck %s
; RUN: opt -passes="hir-ssa-deconstruction,hir-vec-dir-insert,hir-vplan-vec" -disable-output -vplan-print-after-plain-cfg -vplan-force-invariant-decomposition=0 -vplan-dump-external-defs-hir < %s 2>&1 | FileCheck %s

;
; LIT test to check that invariant blob(SCEV) expressions are not decomposed and
; get treated as external defs. The loop being vectorized is the following:
;    for (i1 = 0; i1 < 100; i1++) {
;      arr2[n1 * n2 + arr[i1]] = n1 * n2 + arr[i1];
;    }
;
; The blob (n1 * n2) in the canon expressions (n1 * n2 + arr[i1]) is invariant.
; The test checks that (n1 * n2) is treated as one external def and that this
; external def is used appropriately during decomposition.
;
; The auto generated checks for external defs are done using CHECK-DAG
; as the order of printing of external defs is non-deterministic. Auto
; generated checks that were unnecessary were also removed.

@arr = dso_local local_unnamed_addr global [100 x i64] zeroinitializer, align 16
@arr2 = dso_local local_unnamed_addr global [100 x i64] zeroinitializer, align 16

define void @foo(i64 %n1, i64 %n2) {
; CHECK-LABEL:  VPlan after importing plain CFG:
; CHECK-NEXT:  VPlan IR for: foo:HIR.#{{[0-9]+}}
; CHECK-NEXT:  External Defs Start:
; CHECK-DAG:     [[VP0:%.*]] = {@arr2}
; CHECK-DAG:     [[VP1:%.*]] = {@arr}
; CHECK-DAG:     [[VP2:%.*]] = {(%n1 * %n2)<nsw>}
; CHECK-NEXT:  External Defs End:
; CHECK-NEXT:    [[BB0:BB[0-9]+]]: # preds:
; CHECK-NEXT:     br [[BB1:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB1]]: # preds: [[BB0]]
; CHECK-NEXT:     br [[BB2:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB2]]: # preds: [[BB1]], [[BB2]]
; CHECK-NEXT:     i64 [[VP3:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP4:%.*]], [[BB2]] ]
; CHECK-NEXT:     i64* [[VP_SUBSCRIPT:%.*]] = subscript inbounds [100 x i64]* @arr i64 0 i64 [[VP3]]
; CHECK-NEXT:     i64 [[VP_LOAD:%.*]] = load i64* [[VP_SUBSCRIPT]]
; CHECK-NEXT:     i64 [[VP5:%.*]] = add i64 [[VP_LOAD]] i64 [[VP2]]
; CHECK-NEXT:     i64* [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds [100 x i64]* @arr2 i64 0 i64 [[VP5]]
; CHECK-NEXT:     store i64 [[VP5]] i64* [[VP_SUBSCRIPT_1]]
; CHECK-NEXT:     i64 [[VP4]] = add i64 [[VP3]] i64 1
; CHECK-NEXT:     i1 [[VP7:%.*]] = icmp slt i64 [[VP4]] i64 100
; CHECK-NEXT:     br i1 [[VP7]], [[BB2]], [[BB3:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB3]]: # preds: [[BB2]]
; CHECK-NEXT:     br [[BB4:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:    [[BB4]]: # preds: [[BB3]]
; CHECK-NEXT:     br <External Block>
;
entry:
  %mul = mul nsw i64 %n2, %n1
  br label %for.body

for.body:                                         ; preds = %for.body, %entry
  %i1.011 = phi i64 [ 0, %entry ], [ %inc, %for.body ]
  %arrayidx = getelementptr inbounds [100 x i64], [100 x i64]* @arr, i64 0, i64 %i1.011
  %0 = load i64, i64* %arrayidx, align 8
  %add = add nsw i64 %0, %mul
  %arrayidx4 = getelementptr inbounds [100 x i64], [100 x i64]* @arr2, i64 0, i64 %add
  store i64 %add, i64* %arrayidx4, align 8
  %inc = add nuw nsw i64 %i1.011, 1
  %exitcond = icmp eq i64 %inc, 100
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body
  ret void
}
