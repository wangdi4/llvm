; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py UTC_ARGS: --version 2
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=16 -S < %s | FileCheck %s -check-prefix=VPLANCGVF16
; RUN: opt -mtriple=x86_64 -passes=vplan-vec -vplan-force-vf=8 -S < %s | FileCheck %s -check-prefix=VPLANCGVF8
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=16 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF16
; RUN: opt -mtriple=x86_64 -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -vplan-force-vf=8 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=HIRVF8

; Test case to test mask argument lowering at a call site for avx512
; according to VecABI specification.
; Test with VF16 checks that mask parameter is split into two i32
; arguments and each provides effective 8 bits of mask, so we need
; to generate whole set of instruction to pack logical type (which is
; the characteristic type) into the variant argument.
; VF8 case is similar but entire mask is passed via single data chunk.

@ARRAY_SIZE = external dso_local  constant i32, align 4

declare token @llvm.directive.region.entry() #0
declare void @llvm.directive.region.exit(token) #0
declare nofpclass(nan inf) double @vfunc(double noundef nofpclass(nan inf)) #1

define void @test(ptr nocapture noundef readonly %src, ptr nocapture noundef writeonly %dst) #2 {
; VPLANCGVF16:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF16:       vector.body:
; VPLANCGVF16-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF16-NEXT:    [[VEC_PHI0:%.*]] = phi <16 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7, i64 8, i64 9, i64 10, i64 11, i64 12, i64 13, i64 14, i64 15>, [[VPLANNEDBB20]] ], [ [[TMP7:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF16-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds double, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF16-NEXT:    [[WIDE_LOAD0:%.*]] = load <16 x double>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF16-NEXT:    [[WIDE_LOAD_PART_0_OF_2_0:%.*]] = shufflevector <16 x double> [[WIDE_LOAD0]], <16 x double> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; VPLANCGVF16-NEXT:    [[WIDE_LOAD_PART_1_OF_2_0:%.*]] = shufflevector <16 x double> [[WIDE_LOAD0]], <16 x double> undef, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; VPLANCGVF16-NEXT:    [[TMP4:%.*]] = call x86_regcallcc { <8 x double>, <8 x double> } @_ZGVZM16v_vfunc(<8 x double> noundef nofpclass(nan inf) [[WIDE_LOAD_PART_0_OF_2_0]], <8 x double> noundef nofpclass(nan inf) [[WIDE_LOAD_PART_1_OF_2_0]], i32 zext (i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32), i32 zext (i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32)) #0
;
; VPLANCGVF8:  define void @test(ptr nocapture noundef readonly [[SRC0:%.*]], ptr nocapture noundef writeonly [[DST0:%.*]]) #2 {
; VPLANCGVF8:       vector.body:
; VPLANCGVF8-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VPLANNEDBB20:%.*]] ], [ [[TMP6:%.*]], [[VECTOR_BODY0:%.*]] ]
; VPLANCGVF8-NEXT:    [[VEC_PHI0:%.*]] = phi <8 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7>, [[VPLANNEDBB20]] ], [ [[TMP5:%.*]], [[VECTOR_BODY0]] ]
; VPLANCGVF8-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds double, ptr [[SRC0]], i64 [[UNI_PHI0]]
; VPLANCGVF8-NEXT:    [[WIDE_LOAD0:%.*]] = load <8 x double>, ptr [[SCALAR_GEP0]], align 8
; VPLANCGVF8-NEXT:    [[TMP4:%.*]] = call fast x86_regcallcc nofpclass(nan inf) <8 x double> @_ZGVZM8v_vfunc(<8 x double> noundef nofpclass(nan inf) [[WIDE_LOAD0]], i32 zext (i8 bitcast (<8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true> to i8) to i32)) #0
;
; HIRVF16-LABEL:  Function: test
; HIRVF16:             + DO i1 = 0, [[LOOP_UB0:%.*]], 16   <DO_LOOP>  <MAX_TC_EST = 134217727>  <LEGAL_MAX_TC = 134217727> <simd-vectorized> <nounroll> <novectorize>
; HIRVF16-NEXT:        |   [[DOTVEC200:%.*]] = undef
; HIRVF16-NEXT:        |   [[DOTVEC40:%.*]] = (<16 x double>*)([[SRC0:%.*]])[i1]
; HIRVF16-NEXT:        |   [[SEXT0:%.*]] = sext.<16 x i1>.<16 x i64>(1)
; HIRVF16-NEXT:        |   [[TMP1:%.*]] = bitcast.<16 x i64>.<16 x double>([[SEXT0]])
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC0:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC50:%.*]] = shufflevector [[DOTVEC40]],  undef,  <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC60:%.*]] = shufflevector [[TMP1]],  undef,  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
; HIRVF16-NEXT:        |   [[TMP2:%.*]] = bitcast.<8 x double>.<8 x i64>([[DOTEXTRACTED_SUBVEC60]])
; HIRVF16-NEXT:        |   [[TRUNC0:%.*]] = trunc.<8 x i64>.<8 x i1>([[TMP2]])
; HIRVF16-NEXT:        |   [[TMP3:%.*]] = bitcast.<8 x i1>.i8([[TRUNC0]])
; HIRVF16-NEXT:        |   [[ZEXT0:%.*]] = zext.i8.i32([[TMP3]])
; HIRVF16-NEXT:        |   [[DOTEXTRACTED_SUBVEC70:%.*]] = shufflevector [[TMP1]],  undef,  <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   [[TMP4:%.*]] = bitcast.<8 x double>.<8 x i64>([[DOTEXTRACTED_SUBVEC70]])
; HIRVF16-NEXT:        |   [[TRUNC80:%.*]] = trunc.<8 x i64>.<8 x i1>([[TMP4]])
; HIRVF16-NEXT:        |   [[TMP5:%.*]] = bitcast.<8 x i1>.i8([[TRUNC80]])
; HIRVF16-NEXT:        |   [[ZEXT90:%.*]] = zext.i8.i32([[TMP5]])
; HIRVF16-NEXT:        |   [[_ZGVZM16V_VFUNC0:%.*]] = @_ZGVZM16v_vfunc([[DOTEXTRACTED_SUBVEC0]],  [[DOTEXTRACTED_SUBVEC50]],  [[ZEXT0]],  [[ZEXT90]])
; HIRVF16-NEXT:        |   [[EXTRACT_RESULT0:%.*]] = extractvalue [[_ZGVZM16V_VFUNC0]], 0
; HIRVF16-NEXT:        |   [[EXTRACT_RESULT100:%.*]] = extractvalue [[_ZGVZM16V_VFUNC0]], 1
; HIRVF16-NEXT:        |   [[COMB_SHUF0:%.*]] = shufflevector [[EXTRACT_RESULT0]],  [[EXTRACT_RESULT100]],  <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
; HIRVF16-NEXT:        |   (<16 x double>*)([[DST0:%.*]])[i1] = [[COMB_SHUF0]]
; HIRVF16-NEXT:        + END LOOP
;
; HIRVF8-LABEL:  Function: test
; HIRVF8:             + DO i1 = 0, [[LOOP_UB0:%.*]], 8   <DO_LOOP>  <MAX_TC_EST = 268435455>  <LEGAL_MAX_TC = 268435455> <simd-vectorized> <nounroll> <novectorize>
; HIRVF8-NEXT:        |   [[DOTVEC40:%.*]] = (<8 x double>*)([[SRC0:%.*]])[i1]
; HIRVF8-NEXT:        |   [[SEXT0:%.*]] = sext.<8 x i1>.<8 x i64>(1)
; HIRVF8-NEXT:        |   [[TMP1:%.*]] = bitcast.<8 x i64>.<8 x double>([[SEXT0]])
; HIRVF8-NEXT:        |   [[TMP2:%.*]] = bitcast.<8 x double>.<8 x i64>([[TMP1]])
; HIRVF8-NEXT:        |   [[TRUNC0:%.*]] = trunc.<8 x i64>.<8 x i1>([[TMP2]])
; HIRVF8-NEXT:        |   [[TMP3:%.*]] = bitcast.<8 x i1>.i8([[TRUNC0]])
; HIRVF8-NEXT:        |   [[ZEXT0:%.*]] = zext.i8.i32([[TMP3]])
; HIRVF8-NEXT:        |   [[_ZGVZM8V_VFUNC0:%.*]] = @_ZGVZM8v_vfunc([[DOTVEC40]],  [[ZEXT0]])
; HIRVF8-NEXT:        |   (<8 x double>*)([[DST0:%.*]])[i1] = [[_ZGVZM8V_VFUNC0]]
; HIRVF8-NEXT:        + END LOOP
;
entry:
  %i.linear.iv = alloca i32, align 4
  %0 = load i32, ptr @ARRAY_SIZE, align 4
  %cmp = icmp sgt i32 %0, 0
  br i1 %cmp, label %DIR.OMP.SIMD.1, label %omp.precond.end

DIR.OMP.SIMD.1:                                   ; preds = %entry
  %1 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr null, i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr null, i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.OMP.SIMD.117

DIR.OMP.SIMD.117:                                 ; preds = %DIR.OMP.SIMD.1
  %wide.trip.count = zext i32 %0 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %DIR.OMP.SIMD.117
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.117 ], [ %indvars.iv.next, %omp.inner.for.body ]
  %arrayidx = getelementptr inbounds double, ptr %src, i64 %indvars.iv
  %2 = load double, ptr %arrayidx, align 8
  %callret = call fast nofpclass(nan inf) double @vfunc(double noundef nofpclass(nan inf) %2) #0
  %arrayidx6 = getelementptr inbounds double, ptr %dst, i64 %indvars.iv
  store double %callret, ptr %arrayidx6, align 8
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge: ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %1) [ "DIR.OMP.END.SIMD"() ]
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.inner.for.cond.DIR.OMP.END.SIMD.3.loopexit_crit_edge, %entry
  ret void
}

attributes #0 = { nounwind }
attributes #1 = { "vector-variants"="_ZGVZM8v_vfunc,_ZGVZM16v_vfunc" }
attributes #2 = { "target-cpu"="skylake-avx512" }
