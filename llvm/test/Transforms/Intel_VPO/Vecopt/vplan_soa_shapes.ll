; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py

; RUN: opt -vplan-print-terminator-inst=false -VPlanDriver -vplan-enable-soa -vplan-dump-plan-da -disable-vplan-codegen \
; RUN: -disable-output -vplan-force-vf=2 %s 2>&1 | FileCheck %s

; REQUIRES:asserts

@arr = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
; Function Attrs: nounwind uwtable
define dso_local void @test_soa_shapes(i32 %n) {
;
; Divergence info before SOA-shape analysis is run.
;
; CHECK:       Printing Divergence info for
; CHECK-NEXT:  Basic Block: [[BB0:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB1:BB[0-9]+]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] [1024 x i32]* [[VP_ARR_PRIV:%.*]] = allocate-priv [1024 x i32]*, OrigAlign = 4
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VF:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop for.body
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2:BB[0-9]+]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_RND_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 [[VP_IV1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_G_GEP1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32 [[VP_NON_PRIV_UNI_LD:%.*]] = load i32* [[VP_UNI_G_GEP2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IDXPROM1:%.*]] = sext i32 [[VP_NON_PRIV_UNI_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i32* [[VP_STR_G_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_G_GEP2:%.*]] = getelementptr inbounds i32* [[VP_STR_G_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_NON_PRIV_STR_LD:%.*]] = load i32* [[VP_STR_G_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_IDXPROM2:%.*]] = sext i32 [[VP_NON_PRIV_STR_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 0
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_PRIV_LD_1:%.*]] = load i32* [[VP_UNI_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_INDIRECT_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_INDIRECT_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_GEP2:%.*]] = getelementptr inbounds i32* [[VP_STR_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_INDIRECT_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_INDIRECT_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_INDIRECT_GEP1]] i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ne i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]

; Divergence info after SOA-shape analysis is run.

; CHECK:       Printing Divergence info for
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] [1024 x i32]* [[VP_ARR_PRIV]] = allocate-priv [1024 x i32]*, OrigAlign = 4
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_IND_INIT]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IV1_IND_INIT_STEP]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VF]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_ORIG_TRIP_COUNT]] = orig-trip-count for original loop for.body
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_TRIP_COUNT]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_RND_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 [[VP_IV1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP1]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP2]] = getelementptr inbounds i32* [[VP_UNI_G_GEP1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32 [[VP_NON_PRIV_UNI_LD]] = load i32* [[VP_UNI_G_GEP2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IDXPROM1]] = sext i32 [[VP_NON_PRIV_UNI_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i32* [[VP_STR_G_GEP1]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_G_GEP2]] = getelementptr inbounds i32* [[VP_STR_G_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_NON_PRIV_STR_LD]] = load i32* [[VP_STR_G_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_IDXPROM2]] = sext i32 [[VP_NON_PRIV_STR_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_GEP2]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 0
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_PRIV_LD_1]] = load i32* [[VP_UNI_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_INDIRECT_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_INDIRECT_GEP2]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Strided, Stride: VF x i64 4] i32* [[VP_STR_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Strided, Stride: ?] i32* [[VP_STR_GEP2]] = getelementptr inbounds i32* [[VP_STR_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_STR_INDIRECT_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_STR_INDIRECT_GEP2]] = getelementptr inbounds i32* [[VP_UNI_INDIRECT_GEP1]] i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND]] = icmp ne i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]
;

entry:
  %arr.priv = alloca [1024 x i32], align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i32]* %arr.priv)]
  br label %for.preheader

for.preheader:
  br label %for.body

for.body:
  %iv1 = phi i64 [ 0, %for.preheader ], [ %iv1.next, %for.body ]

  ; Strided first index, should result in a random GEP.
  %rnd.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 %iv1, i64 0

  ; Uniform GEP-chain on non-private array followed by a load.
  %uni.g.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* @arr, i64 0, i64 0
  %uni.g.gep2 = getelementptr inbounds i32, i32* %uni.g.gep1, i64 0
  %non.priv.uni.ld = load i32, i32* %uni.g.gep2, align 4
  %idxprom1 = sext i32 %non.priv.uni.ld to i64

  ; Strided GEP-chain on non-private array followed by a load.
  %str.g.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* @arr, i64 0, i64 %iv1
  %str.g.gep2 = getelementptr inbounds i32, i32* %str.g.gep1, i64 %iv1
  %non.priv.str.ld = load i32, i32* %str.g.gep2, align 4
  %idxprom2 = sext i32 %non.priv.str.ld to i64

  ; Uniform GEPs on private array.
  ; Sequence of unit-stride GEPs which result in Uniform mem-access
  %uni.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 0
  %uni.gep2 = getelementptr inbounds i32, i32* %uni.gep1, i64 0
  %priv.ld.1 = load i32, i32* %uni.gep2, align 4

  ; GEP using the loaded index from non-private array, something similar to a[b[i]].
  %uni.indirect.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %idxprom1
  %uni.indirect.gep2 = getelementptr inbounds i32, i32* %uni.gep1, i64 %idxprom1

  ; Sequence of unit-stride GEPs which result in Strided-memory access.
  %str.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %iv1
  %str.gep2 = getelementptr inbounds i32, i32* %str.gep1, i64 %iv1

  ; GEP using the loaded index from non-private array, something similar to a[b[i]].
  %str.indirect.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %idxprom2
  %str.indirect.gep2 = getelementptr inbounds i32, i32* %uni.indirect.gep1, i64 %idxprom2

  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %for.body, label %for.end
for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void

}


define dso_local void @test_memref_transform(i32 %n) {
;
; CHECK:       Printing Divergence info for
; CHECK-NEXT:  Basic Block: [[BB0:BB[0-9]+]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB1:BB[0-9]+]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] [1024 x i32]* [[VP_ARR_PRIV:%.*]] = allocate-priv [1024 x i32]*, OrigAlign = 4
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_IND_INIT:%.*]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IV1_IND_INIT_STEP:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VF:%.*]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_ORIG_TRIP_COUNT:%.*]] = orig-trip-count for original loop for.body
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_TRIP_COUNT:%.*]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2:BB[0-9]+]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Random] store i32 10 i32* [[VP_UNI_GEP1]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp ne i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]

; Divergence info after SOA-shape analysis is run.

; CHECK:       Printing Divergence info for
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] [1024 x i32]* [[VP_ARR_PRIV]] = allocate-priv [1024 x i32]*, OrigAlign = 4
; ; This GEPs shape should be SOA Unit Stride with Stride = 4. This is not the case because when pushing 'users' of allocate-privates with SOA-shape, we exclude instructions which are outside the loop-region.
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_IND_INIT]] = induction-init{add} i64 live-in0 i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IV1_IND_INIT_STEP]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VF]] = induction-init-step{add} i64 1
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_ORIG_TRIP_COUNT]] = orig-trip-count for original loop for.body
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_TRIP_COUNT]] = vector-trip-count i64 [[VP_ORIG_TRIP_COUNT]], UF = 1
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT]], [[BB2]] ]
; CHECK-NEXT:  Divergent: [Shape: Random] store i32 10 i32* [[VP_UNI_GEP1]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND]] = icmp ne i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]

omp.inner.for.body.lr.ph:
  %arr.priv = alloca [1024 x i32], align 4
  %uni.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 0
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %omp.inner.for.body.lr.ph
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i32]* %arr.priv)]
  br label %for.preheader

for.preheader:
  br label %for.body

for.body:
  %iv1 = phi i64 [ 0, %for.preheader ], [ %iv1.next, %for.body ]
  store i32 10, i32* %uni.gep1

  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %for.body, label %for.end
for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token %0)
