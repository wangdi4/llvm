; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py

; RUN: opt -VPlanDriver -vplan-enable-soa -vplan-dump-da -disable-vplan-codegen \
; RUN: -disable-output -vplan-force-vf=2 %s 2>&1 | FileCheck %s

; REQUIRES:asserts

@arr = common dso_local local_unnamed_addr global [1024 x i32] zeroinitializer, align 16
; Function Attrs: nounwind uwtable
define dso_local void @test_soa_shapes(i32 %n) {
;
; Divergence info before SOA-shape analysis is run.
;
; CHECK:       Printing Divergence info for Loop at depth 1 containing: [[BB0:BB[0-9]+]]<header><latch><exiting>
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV:%.*]] = phi  [ i64 0, [[BB1:BB[0-9]+]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1:%.*]] = phi  [ i64 [[VP_IV1_IND_INIT:%.*]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_RND_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV:%.*]] i64 [[VP_IV1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_G_GEP1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32 [[VP_NON_PRIV_UNI_LD:%.*]] = load i32* [[VP_UNI_G_GEP2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IDXPROM1:%.*]] = sext i32 [[VP_NON_PRIV_UNI_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i32* [[VP_STR_G_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_G_GEP2:%.*]] = getelementptr inbounds i32* [[VP_STR_G_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_NON_PRIV_STR_LD:%.*]] = load i32* [[VP_STR_G_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_IDXPROM2:%.*]] = sext i32 [[VP_NON_PRIV_STR_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 0
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_PRIV_LD_1:%.*]] = load i32* [[VP_UNI_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_INDIRECT_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4096] i32* [[VP_UNI_INDIRECT_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_GEP2:%.*]] = getelementptr inbounds i32* [[VP_STR_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_INDIRECT_GEP1:%.*]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_INDIRECT_GEP2:%.*]] = getelementptr inbounds i32* [[VP_UNI_INDIRECT_GEP1]] i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF:%.*]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND:%.*]] = icmp i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT:%.*]]

; Divergence info after SOA-shape analysis is run.
;
; CHECK:       Basic Block: [[BB3:BB[0-9]+]]
; CHECK:       Printing Divergence info for Loop at depth 1 containing: [[BB0]]<header><latch><exiting>
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV]] = phi  [ i64 0, [[BB1]] ],  [ i64 [[VP_VECTOR_LOOP_IV_NEXT]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1]] = phi  [ i64 [[VP_IV1_IND_INIT]], [[BB1]] ],  [ i64 [[VP_IV1_NEXT]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_RND_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 [[VP_IV1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP1]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32* [[VP_UNI_G_GEP2]] = getelementptr inbounds i32* [[VP_UNI_G_GEP1]] i64 0
; CHECK-NEXT:  Uniform: [Shape: Uniform] i32 [[VP_NON_PRIV_UNI_LD]] = load i32* [[VP_UNI_G_GEP2]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_IDXPROM1]] = sext i32 [[VP_NON_PRIV_UNI_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 4] i32* [[VP_STR_G_GEP1]] = getelementptr inbounds [1024 x i32]* @arr i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32* [[VP_STR_G_GEP2]] = getelementptr inbounds i32* [[VP_STR_G_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_NON_PRIV_STR_LD]] = load i32* [[VP_STR_G_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_IDXPROM2]] = sext i32 [[VP_NON_PRIV_STR_LD]] to i64
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 0
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_GEP2]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 0
; CHECK-NEXT:  Divergent: [Shape: Random] i32 [[VP_PRIV_LD_1]] = load i32* [[VP_UNI_GEP2]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_INDIRECT_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Unit Stride, Stride: i64 4] i32* [[VP_UNI_INDIRECT_GEP2]] = getelementptr inbounds i32* [[VP_UNI_GEP1]] i64 [[VP_IDXPROM1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Strided, Stride: VF x i64 4] i32* [[VP_STR_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Strided, Stride: ?] i32* [[VP_STR_GEP2]] = getelementptr inbounds i32* [[VP_STR_GEP1]] i64 [[VP_IV1]]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_STR_INDIRECT_GEP1]] = getelementptr inbounds [1024 x i32]* [[VP_ARR_PRIV]] i64 0 i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: SOA Random] i32* [[VP_STR_INDIRECT_GEP2]] = getelementptr inbounds i32* [[VP_UNI_INDIRECT_GEP1]] i64 [[VP_IDXPROM2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP_IV1_NEXT]] = add i64 [[VP_IV1]] i64 [[VP_IV1_IND_INIT_STEP]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i64 [[VP_VECTOR_LOOP_IV_NEXT]] = add i64 [[VP_VECTOR_LOOP_IV]] i64 [[VP_VF]]
; CHECK-NEXT:  Uniform: [Shape: Uniform] i1 [[VP_VECTOR_LOOP_EXITCOND]] = icmp i64 [[VP_VECTOR_LOOP_IV_NEXT]] i64 [[VP_VECTOR_TRIP_COUNT]]
;

entry:
  %arr.priv = alloca [1024 x i32], align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.PRIVATE"([1024 x i32]* %arr.priv)]
  br label %for.preheader

for.preheader:
  br label %for.body

for.body:
  %iv1 = phi i64 [ 0, %for.preheader ], [ %iv1.next, %for.body ]

  ; Strided first index, should result in a random GEP.
  %rnd.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 %iv1, i64 0

  ; Uniform GEP-chain on non-private array followed by a load.
  %uni.g.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* @arr, i64 0, i64 0
  %uni.g.gep2 = getelementptr inbounds i32, i32* %uni.g.gep1, i64 0
  %non.priv.uni.ld = load i32, i32* %uni.g.gep2, align 4
  %idxprom1 = sext i32 %non.priv.uni.ld to i64

  ; Strided GEP-chain on non-private array followed by a load.
  %str.g.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* @arr, i64 0, i64 %iv1
  %str.g.gep2 = getelementptr inbounds i32, i32* %str.g.gep1, i64 %iv1
  %non.priv.str.ld = load i32, i32* %str.g.gep2, align 4
  %idxprom2 = sext i32 %non.priv.str.ld to i64

  ; Uniform GEPs on private array.
  ; Sequence of unit-stride GEPs which result in Uniform mem-access
  %uni.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 0
  %uni.gep2 = getelementptr inbounds i32, i32* %uni.gep1, i64 0
  %priv.ld.1 = load i32, i32* %uni.gep2, align 4

  ; GEP using the loaded index from non-private array, something similar to a[b[i]].
  %uni.indirect.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %idxprom1
  %uni.indirect.gep2 = getelementptr inbounds i32, i32* %uni.gep1, i64 %idxprom1

  ; Sequence of unit-stride GEPs which result in Strided-memory access.
  %str.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %iv1
  %str.gep2 = getelementptr inbounds i32, i32* %str.gep1, i64 %iv1

  ; GEP using the loaded index from non-private array, something similar to a[b[i]].
  %str.indirect.gep1 = getelementptr inbounds [1024 x i32], [1024 x i32]* %arr.priv, i64 0, i64 %idxprom2
  %str.indirect.gep2 = getelementptr inbounds i32, i32* %uni.indirect.gep1, i64 %idxprom2

  %iv1.next = add nuw nsw i64 %iv1, 1
  %cmp = icmp ult i64 %iv1.next, 1024
  br i1 %cmp, label %for.body, label %for.end
for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void

}

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token %0)
