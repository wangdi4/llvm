; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN:  opt -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -disable-output < %s 2>&1 | FileCheck %s -check-prefix=CHECK-HIR
; RUN:  opt -passes='vplan-vec' -print-after=vplan-vec -vplan-force-uf=2 -disable-output < %s 2>&1 | FileCheck %s -check-prefix=CHECK-LLVM-IR


target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

;
; Input source code is:
; int a[4];
;
; void foo() {
;  int i, x;
;
; #pragma omp simd simdlen(4)
; #pragma unroll 2
;   for(i = 0; i < 4; i++)
;     x = a[i];
; }
;
; The test verifies that VPlan gives precedence to forced vf.

@a = dso_local local_unnamed_addr global [4 x i32] zeroinitializer, align 16

; Function Attrs: nounwind uwtable
define dso_local void @foo() {
;                   *** IR Dump After VPlan HIR Vectorizer (hir-vplan-vec) ***
; CHECK-HIR:       Function: foo
; CHECK-HIR:       BEGIN REGION { modified }
; CHECK-HIR-NEXT:        %.vec = (<4 x i32>*)(@a)[0][0];
; CHECK-HIR-NEXT:        ret
; CHECK-HIR-NEXT:  END REGION
;
;
; CHECK-LLVM-IR:  define dso_local void @foo() {
; CHECK-LLVM-IR:       vector.body:
; CHECK-LLVM-IR-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP2:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-LLVM-IR-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB1]] ], [ [[TMP1:%.*]], [[VECTOR_BODY]] ]
; CHECK-LLVM-IR-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds [4 x i32], ptr @a, i64 0, i64 [[UNI_PHI]]
; CHECK-LLVM-IR-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP]], align 16
; CHECK-LLVM-IR-NEXT:    [[TMP1]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-LLVM-IR-NEXT:    [[TMP2]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK-LLVM-IR-NEXT:    [[TMP3:%.*]] = icmp uge i64 [[TMP2]], 4
; CHECK-LLVM-IR-NEXT:    br i1 true, label [[VPLANNEDBB3:%.*]], label [[VECTOR_BODY]]
;
entry:
  %i.linear.iv = alloca i32, align 4
  br label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %iv = phi i64 [ 0, %omp.inner.for.body.lr.ph ], [ %iv.next, %omp.inner.for.body ]
  %idx = getelementptr inbounds [4 x i32], ptr @a, i64 0, i64 %iv
  %1 = load i32, ptr %idx, align 4
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 4
  br i1 %exitcond, label %DIR.OMP.END.SIMD.4, label %omp.inner.for.body, !llvm.loop !8

DIR.OMP.END.SIMD.4:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

; Function Attrs: nounwind
declare token @llvm.directive.region.entry() #1

; Function Attrs: nounwind
declare void @llvm.directive.region.exit(token) #1

attributes #1 = { nounwind }
!7 = distinct !{}
!8 = distinct !{!8, !9, !10}
!9 = !{!"llvm.loop.unroll.count", i32 2}
!10 = !{!"llvm.loop.parallel_accesses", !7}
