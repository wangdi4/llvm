; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -VPlanDriver -S < %s | FileCheck %s
;
; Small test to duplicate the issue reported in CMPLRLLVM-27036. Test was
; crashing during VPlan cost modeling while trying to get the cost for the
; llvm.vector.reduce.fadd.v4f32 intrinsic. The vector containing the call
; argument types used to get TTI based cost was not being setup correctly.
; Test checks that the loop is vectorized successfully. The call in
; function foo takes uniform arguments and the call is expected to be
; treated as uniform. The call in function foo2 is expected to be serialized.
;
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@arr = dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16

define void @foo(<4 x float> %float_vec, float* %arr) {
; CHECK-LABEL: @foo(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP4:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI3:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP3:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH]] ], [ [[TMP2:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> [[FLOAT_VEC:%.*]])
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <2 x float> poison, float [[TMP0]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <2 x float> [[BROADCAST_SPLATINSERT]], <2 x float> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds float, float* [[ARR:%.*]], i64 [[UNI_PHI3]]
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast float* [[SCALAR_GEP]] to <2 x float>*
; CHECK-NEXT:    store <2 x float> [[BROADCAST_SPLAT]], <2 x float>* [[TMP1]], align 4
; CHECK-NEXT:    [[TMP2]] = add nuw nsw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP3]] = add nuw nsw i64 [[UNI_PHI3]], 2
; CHECK-NEXT:    [[TMP4]] = add i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP5:%.*]] = icmp uge i64 [[TMP4]], 100
; CHECK-NEXT:    br i1 [[TMP5]], label [[VPLANNEDBB4:%.*]], label [[VECTOR_BODY]], [[LOOP0:!llvm.loop !.*]]
;
entry:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 2) ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %entry ], [ %iv.add, %for.body ]
  %fadd = tail call fast float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> %float_vec)
  %ptridx = getelementptr inbounds float, float* %arr, i64 %iv
  store float %fadd, float* %ptridx, align 4
  %iv.add = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.add, 100
  br i1 %exitcond.not, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @foo2(<4 x double> %double_vec, double* %arr) {
; CHECK-LABEL: @foo2(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI3:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP7:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH]] ], [ [[TMP6:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = uitofp <2 x i64> [[VEC_PHI]] to <2 x double>
; CHECK-NEXT:    [[DOTEXTRACT_1_:%.*]] = extractelement <2 x double> [[TMP0]], i32 1
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <2 x double> [[TMP0]], i32 0
; CHECK-NEXT:    [[TMP1:%.*]] = tail call double @llvm.vector.reduce.fadd.v4f64(double [[DOTEXTRACT_0_]], <4 x double> [[DOUBLE_VEC:%.*]])
; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <2 x double> undef, double [[TMP1]], i32 0
; CHECK-NEXT:    [[TMP3:%.*]] = tail call double @llvm.vector.reduce.fadd.v4f64(double [[DOTEXTRACT_1_]], <4 x double> [[DOUBLE_VEC]])
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x double> [[TMP2]], double [[TMP3]], i32 1
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds double, double* [[ARR:%.*]], i64 [[UNI_PHI3]]
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[SCALAR_GEP]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP4]], <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6]] = add nuw nsw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP7]] = add nuw nsw i64 [[UNI_PHI3]], 2
; CHECK-NEXT:    [[TMP8]] = add i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP9:%.*]] = icmp uge i64 [[TMP8]], 100
; CHECK-NEXT:    br i1 [[TMP9]], label [[VPLANNEDBB4:%.*]], label [[VECTOR_BODY]], [[LOOP4:!llvm.loop !.*]]
;
entry:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %entry ], [ %iv.add, %for.body ]
  %fpcvt = uitofp i64 %iv to double
  %fadd = tail call fast double @llvm.vector.reduce.fadd.v4f64(double %fpcvt, <4 x double> %double_vec)
  %ptridx = getelementptr inbounds double, double* %arr, i64 %iv
  store double %fadd, double* %ptridx, align 4
  %iv.add = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.add, 100
  br i1 %exitcond.not, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare double @llvm.vector.reduce.fadd.v4f64(double, <4 x double>)
declare float @llvm.vector.reduce.fadd.v4f32(float, <4 x float>)
declare token @llvm.directive.region.entry() #1
declare void @llvm.directive.region.exit(token) #1
