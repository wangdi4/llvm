; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -VPlanDriver -S < %s | FileCheck %s
;
; Small test to duplicate the issue reported in CMPLRLLVM-27036. Test was
; crashing during VPlan cost modeling while trying to get the cost for the
; llvm.vector.reduce.fadd.v4f32 intrinsic. The vector containing the call
; argument types used to get TTI based cost was not being setup correctly.
; Test checks that the loop is vectorized successfully. The call in
; function foo takes uniform arguments and the call is expected to be
; treated as uniform. The call in function foo2 is expected to be serialized.
;
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@arr = dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16

define void @foo(<4 x float> %float_vec, float* %arr) {
; CHECK:  define void @foo(<4 x float> [[FLOAT_VEC0:%.*]], float* [[ARR0:%.*]]) {
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VECTOR_PH0:%.*]] ], [ [[TMP3:%.*]], [[VECTOR_BODY0:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH0]] ], [ [[TMP2:%.*]], [[VECTOR_BODY0]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = tail call float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> [[FLOAT_VEC0]])
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT0:%.*]] = insertelement <2 x float> poison, float [[TMP0]], i32 0
; CHECK-NEXT:    [[BROADCAST_SPLAT0:%.*]] = shufflevector <2 x float> [[BROADCAST_SPLATINSERT0]], <2 x float> poison, <2 x i32> zeroinitializer
; CHECK-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds float, float* [[ARR0]], i64 [[UNI_PHI0]]
; CHECK-NEXT:    [[TMP1:%.*]] = bitcast float* [[SCALAR_GEP0]] to <2 x float>*
; CHECK-NEXT:    store <2 x float> [[BROADCAST_SPLAT0]], <2 x float>* [[TMP1]], align 4
; CHECK-NEXT:    [[TMP2]] = add nuw nsw <2 x i64> [[VEC_PHI0]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP3]] = add nuw nsw i64 [[UNI_PHI0]], 2
; CHECK-NEXT:    [[TMP4:%.*]] = icmp eq i64 [[TMP3]], 100
; CHECK-NEXT:    br i1 [[TMP4]], label [[VPLANNEDBB30:%.*]], label [[VECTOR_BODY0]]
;
entry:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 2) ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %entry ], [ %iv.add, %for.body ]
  %fadd = tail call fast float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> %float_vec)
  %ptridx = getelementptr inbounds float, float* %arr, i64 %iv
  store float %fadd, float* %ptridx, align 4
  %iv.add = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.add, 100
  br i1 %exitcond.not, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

define void @foo2(<4 x double> %double_vec, double* %arr) {
; CHECK:  define void @foo2(<4 x double> [[DOUBLE_VEC0:%.*]], double* [[ARR0:%.*]]) {
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI0:%.*]] = phi i64 [ 0, [[VECTOR_PH1:%.*]] ], [ [[TMP7:%.*]], [[VECTOR_BODY1:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI0:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH1]] ], [ [[TMP6:%.*]], [[VECTOR_BODY1]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = uitofp <2 x i64> [[VEC_PHI0]] to <2 x double>
; CHECK-NEXT:    [[DOTEXTRACT_1_0:%.*]] = extractelement <2 x double> [[TMP0]], i32 1
; CHECK-NEXT:    [[DOTEXTRACT_0_0:%.*]] = extractelement <2 x double> [[TMP0]], i32 0
; CHECK-NEXT:    [[TMP1:%.*]] = tail call double @llvm.vector.reduce.fadd.v4f64(double [[DOTEXTRACT_0_0]], <4 x double> [[DOUBLE_VEC0]])
; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <2 x double> undef, double [[TMP1]], i32 0
; CHECK-NEXT:    [[TMP3:%.*]] = tail call double @llvm.vector.reduce.fadd.v4f64(double [[DOTEXTRACT_1_0]], <4 x double> [[DOUBLE_VEC0]])
; CHECK-NEXT:    [[TMP4:%.*]] = insertelement <2 x double> [[TMP2]], double [[TMP3]], i32 1
; CHECK-NEXT:    [[SCALAR_GEP0:%.*]] = getelementptr inbounds double, double* [[ARR0]], i64 [[UNI_PHI0]]
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[SCALAR_GEP0]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP4]], <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6]] = add nuw nsw <2 x i64> [[VEC_PHI0]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP7]] = add nuw nsw i64 [[UNI_PHI0]], 2
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq i64 [[TMP7]], 100
; CHECK-NEXT:    br i1 [[TMP8]], label [[VPLANNEDBB30:%.*]], label [[VECTOR_BODY1]]
;
entry:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %entry ], [ %iv.add, %for.body ]
  %fpcvt = uitofp i64 %iv to double
  %fadd = tail call fast double @llvm.vector.reduce.fadd.v4f64(double %fpcvt, <4 x double> %double_vec)
  %ptridx = getelementptr inbounds double, double* %arr, i64 %iv
  store double %fadd, double* %ptridx, align 4
  %iv.add = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.add, 100
  br i1 %exitcond.not, label %for.end, label %for.body

for.end:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare double @llvm.vector.reduce.fadd.v4f64(double, <4 x double>)
declare float @llvm.vector.reduce.fadd.v4f32(float, <4 x float>)
declare token @llvm.directive.region.entry() #1
declare void @llvm.directive.region.exit(token) #1
