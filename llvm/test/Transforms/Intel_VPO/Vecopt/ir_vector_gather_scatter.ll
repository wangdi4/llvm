; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -S -VPlanDriver -enable-vp-value-codegen < %s | FileCheck %s

target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

define void @foo(<4 x i32>* nocapture %ary) {
;  typedef int32_t v4i32 __attribute__((vector_size(16)));
;  v4i32 *ary, t0;
;  for (i = 0; i < 3072; i += 11) {
;    t0 = ary[i] + 7;
;    ary[i + 3] = t0;
;  }
;
; CHECK:         vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[TMP5:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 11, i64 22, i64 33>, [[VECTOR_PH]] ], [ [[TMP4:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[MM_VECTORGEP:%.*]] = getelementptr inbounds <4 x i32>, <4 x <4 x i32>*> [[BROADCAST_SPLAT:%.*]], <4 x i64> [[VEC_PHI]]
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast <4 x <4 x i32>*> [[MM_VECTORGEP]] to <4 x i32*>
; CHECK-NEXT:    [[VECBASEPTR_:%.*]] = shufflevector <4 x i32*> [[TMP0]], <4 x i32*> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 1, i32 1, i32 1, i32 1, i32 2, i32 2, i32 2, i32 2, i32 3, i32 3, i32 3, i32 3>
; CHECK-NEXT:    [[ELEMBASEPTR_:%.*]] = getelementptr i32, <16 x i32*> [[VECBASEPTR_]], <16 x i64> <i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3>
; CHECK-NEXT:    [[WIDE_MASKED_GATHER:%.*]] = call <16 x i32> @llvm.masked.gather.v16i32.v16p0i32(<16 x i32*> [[ELEMBASEPTR_]], i32 4, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x i32> undef)
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw <16 x i32> [[WIDE_MASKED_GATHER]], <i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7, i32 7>
; CHECK-NEXT:    [[TMP2:%.*]] = add <4 x i64> [[VEC_PHI]], <i64 3, i64 3, i64 3, i64 3>
; CHECK-NEXT:    [[MM_VECTORGEP1:%.*]] = getelementptr inbounds <4 x i32>, <4 x <4 x i32>*> [[BROADCAST_SPLAT]], <4 x i64> [[TMP2]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <4 x <4 x i32>*> [[MM_VECTORGEP1]] to <4 x i32*>
; CHECK-NEXT:    [[VECBASEPTR_2:%.*]] = shufflevector <4 x i32*> [[TMP3]], <4 x i32*> undef, <16 x i32> <i32 0, i32 0, i32 0, i32 0, i32 1, i32 1, i32 1, i32 1, i32 2, i32 2, i32 2, i32 2, i32 3, i32 3, i32 3, i32 3>
; CHECK-NEXT:    [[ELEMBASEPTR_3:%.*]] = getelementptr i32, <16 x i32*> [[VECBASEPTR_2]], <16 x i64> <i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3, i64 0, i64 1, i64 2, i64 3>
; CHECK-NEXT:    call void @llvm.masked.scatter.v16i32.v16p0i32(<16 x i32> [[TMP1]], <16 x i32*> [[ELEMBASEPTR_3]], i32 4, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>)
; CHECK-NEXT:    [[TMP4]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 44, i64 44, i64 44, i64 44>
; CHECK-NEXT:    [[TMP5]] = add nuw nsw i64 [[UNI_PHI]], 44
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ult <4 x i64> [[TMP4]], <i64 3072, i64 3072, i64 3072, i64 3072>
; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <4 x i1> [[TMP6]], i32 0
; CHECK-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq i64 [[INDEX_NEXT]], 280
; CHECK-NEXT:    br i1 [[TMP8]], label [[VPLANNEDBB:%.*]], label [[VECTOR_BODY]]

entry:
  %entry.region = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %ptr.ld = getelementptr inbounds <4 x i32>, <4 x i32>* %ary, i64 %indvars.iv
  %val = load <4 x i32>, <4 x i32>* %ptr.ld, align 4
  %add7 = add nsw <4 x i32> %val, <i32 7, i32 7, i32 7, i32 7>
  %idx.st = add i64 %indvars.iv, 3
  %ptr.st = getelementptr inbounds <4 x i32>, <4 x i32>* %ary, i64 %idx.st
  store <4 x i32> %add7, <4 x i32>* %ptr.st, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 11
  %cmp = icmp ult i64 %indvars.iv.next, 3072
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  call void @llvm.directive.region.exit(token %entry.region) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
