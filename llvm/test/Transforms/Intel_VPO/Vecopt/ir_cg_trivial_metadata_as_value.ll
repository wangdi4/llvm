; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Check that VPlan's LLVM-IR vector codegen can handle trivial MetadataAsValue operands
; represented via VPMetadataAsValue in VPlan CFG.

; RUN: opt < %s -S -vplan-vec -vplan-force-vf=2 | FileCheck %s

define void @mod_gauss_hermite_mp_derquadgausshermite_(double* %ptr, double %T_fetch, i64 %N) local_unnamed_addr {
; CHECK-LABEL: @mod_gauss_hermite_mp_derquadgausshermite_(
; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VECTOR_PH:%.*]] ], [ [[TMP8:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK-NEXT:    [[UNI_PHI1:%.*]] = phi i64 [ 1, [[VECTOR_PH]] ], [ [[TMP7:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ <i64 1, i64 2>, [[VECTOR_PH]] ], [ [[TMP6:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP2:%.*]] = add nsw <2 x i64> [[VEC_PHI]], <i64 -1, i64 -1>
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <2 x i64> [[TMP2]], i32 0
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds double, double* [[PTR:%.*]], i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast double* [[SCALAR_GEP]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP3]], align 8
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_1_:%.*]] = extractelement <2 x double> [[WIDE_LOAD]], i32 1
; CHECK-NEXT:    [[WIDE_LOAD_EXTRACT_0_:%.*]] = extractelement <2 x double> [[WIDE_LOAD]], i32 0
; CHECK-NEXT:    [[TMP4:%.*]] = call double @llvm.experimental.constrained.fmul.f64(double [[WIDE_LOAD_EXTRACT_0_]], double [[T_FETCH:%.*]], metadata !"round.dynamic", metadata !"fpexcept.strict")
; CHECK-NEXT:    [[TMP5:%.*]] = call double @llvm.experimental.constrained.fmul.f64(double [[WIDE_LOAD_EXTRACT_1_]], double [[T_FETCH]], metadata !"round.dynamic", metadata !"fpexcept.strict")
; CHECK-NEXT:    [[TMP6]] = add nuw nsw <2 x i64> [[VEC_PHI]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP7]] = add nuw nsw i64 [[UNI_PHI1]], 2
; CHECK-NEXT:    [[TMP8]] = add i64 [[UNI_PHI]], 2
; CHECK-NEXT:    [[TMP9:%.*]] = icmp uge i64 [[TMP8]], [[N_VEC:%.*]]
; CHECK-NEXT:    br i1 [[TMP9]], label [[VPLANNEDBB:%.*]], label [[VECTOR_BODY]], [[LOOP0:!llvm.loop !.*]]
;
DIR.OMP.SIMD.1:
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"() ]
  br label %bb9

bb9:
  %indvars.iv = phi i64 [ 1, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %bb9 ]
  %1 = add nsw i64 %indvars.iv, -1
  %2 = getelementptr inbounds double, double* %ptr, i64 %1
  %"mod_gauss_hermite_mp_h2d_[][]_fetch" = load double, double* %2, align 8
  %mul = call double @llvm.experimental.constrained.fmul.f64(double %"mod_gauss_hermite_mp_h2d_[][]_fetch", double %T_fetch, metadata !"round.dynamic", metadata !"fpexcept.strict") #4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %rel41 = icmp sgt i64 %indvars.iv, %N
  br i1 %rel41, label %DIR.OMP.END.SIMD.3, label %bb9

DIR.OMP.END.SIMD.3:
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.454

DIR.OMP.END.SIMD.454:
  ret void
}

; Function Attrs: inaccessiblememonly nounwind willreturn
declare double @llvm.experimental.constrained.fmul.f64(double, double, metadata, metadata) #2

; Function Attrs: nounwind
declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)

attributes #2 = { inaccessiblememonly nounwind willreturn }
attributes #3 = { nounwind }
attributes #4 = { strictfp }
