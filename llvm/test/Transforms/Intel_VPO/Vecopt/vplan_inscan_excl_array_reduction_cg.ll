; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Test to verify correctness of vectorized LLVM-IR generated
; by VPlan for a loop containing array type inscan exclusive
; reduction entity.

; int A[3][8] = { {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7},
;                 {0, 1, 2, 3, 4, 5, 6, 7} };
; int B[3][8] = {0};
;
; void foo() {
;   int x[3] = {0, 0, 0};
; #pragma omp simd reduction(inscan,+:x)
; #pragma nounroll
;   for (int i = 0; i < 8; i++) {
;     B[0][i] = x[0];
;     B[1][i] = x[1];
;     B[2][i] = x[2];
; #pragma omp scan exclusive(x)
;     x[0] += A[0][i];
;     x[1] += A[1][i];
;     x[2] += A[2][i];
;   }
; }

; RUN: opt -passes="vplan-vec" -vplan-force-vf=4 -S < %s 2>&1 | FileCheck %s

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@A = dso_local local_unnamed_addr global [3 x [8 x i32]] [[8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7], [8 x i32] [i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7]], align 16
@B = dso_local local_unnamed_addr global [3 x [8 x i32]] zeroinitializer, align 16

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @_Z3foov() local_unnamed_addr {
; CHECK-LABEL: @_Z3foov(
; CHECK-NEXT:  DIR.OMP.SIMD.1.split46.split:
; CHECK-NEXT:    [[X_RED:%.*]] = alloca [3 x i32], align 4
; CHECK-NEXT:    [[X_RED_GEP:%.*]] = getelementptr inbounds [3 x i32], ptr [[X_RED]], i64 0, i64 0
; CHECK:         [[X_RED_SOA_VEC:%.*]] = alloca [3 x <4 x i32>], align 16
; CHECK-NEXT:    br label [[DIR_OMP_SIMD_1:%.*]]

; CHECK:       VPlannedBB1:
; CHECK:         [[SOA_SCALAR_GEP:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 0
; CHECK:         br label [[VECTOR_BODY:%.*]]

; CHECK:       vector.body:
; CHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP13:%.*]], [[NEW_LATCH:%.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB1]] ], [ [[TMP12:%.*]], [[NEW_LATCH]] ]
; CHECK-NEXT:    [[UNI_PHI3:%.*]] = phi i32 [ [[TMP2:%.*]], [[VPLANNEDBB1]] ], [ [[TMP28:%.*]], [[NEW_LATCH]] ]
; CHECK-NEXT:    [[VEC_PHI4:%.*]] = phi <4 x i32> [ [[TMP6:%.*]], [[VPLANNEDBB1]] ], [ [[TMP27:%.*]], [[NEW_LATCH]] ]
; CHECK-NEXT:    store <4 x i32> [[VEC_PHI4]], ptr [[I_LINEAR_IV_VEC:%.*]], align 1
; CHECK-NEXT:    br label [[ARRAY_REDN_INIT_LOOP:%.*]]

; CHECK:       array.redn.init.loop:
; CHECK-NEXT:    [[CUR_ELEM_IDX:%.*]] = phi i64 [ 0, [[VECTOR_BODY]] ], [ [[NEXT_ELEM_IDX:%.*]], [[ARRAY_REDN_INIT_LOOP]] ]
; CHECK-NEXT:    [[CUR_ELEM_PTR:%.*]] = getelementptr i32, ptr [[X_RED_SOA_VEC]], i64 [[CUR_ELEM_IDX]]
; CHECK-NEXT:    store i32 0, ptr [[CUR_ELEM_PTR]], align 4
; CHECK-NEXT:    [[NEXT_ELEM_IDX]] = add i64 [[CUR_ELEM_IDX]], 1
; CHECK-NEXT:    [[INITLOOP_COND:%.*]] = icmp ult i64 [[NEXT_ELEM_IDX]], 12
; CHECK-NEXT:    br i1 [[INITLOOP_COND]], label [[ARRAY_REDN_INIT_LOOP]], label [[ARRAY_REDN_INIT_LOOPEXIT:%.*]]

; CHECK:       array.redn.init.loop.exit:
; CHECK-NEXT:    br label [[VPLANNEDBB5:%.*]]

; CHECK:       VPlannedBB5:
; CHECK-NEXT:    br label [[VPLANNEDBB6:%.*]]

; CHECK:       VPlannedBB6:
; CHECK-NEXT:    [[TMP7:%.*]] = trunc <4 x i64> [[VEC_PHI]] to <4 x i32>
; CHECK-NEXT:    store <4 x i32> [[TMP7]], ptr [[I_LINEAR_IV_VEC]], align 4
; CHECK-NEXT:    br label [[VPLANNEDBB7:%.*]]

; CHECK:       VPlannedBB7:
; CHECK-NEXT:    br label [[VPLANNEDBB8:%.*]]

; CHECK:       VPlannedBB8:
; CHECK-NEXT:    br label [[VPLANNEDBB9:%.*]]

; CHECK:       VPlannedBB9:
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x i32>, ptr [[I_LINEAR_IV_VEC]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = sext <4 x i32> [[WIDE_LOAD]] to <4 x i64>
; CHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <4 x i64> [[TMP8]], i32 0
; CHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds [8 x i32], ptr @A, i64 0, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    [[WIDE_LOAD10:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP]], align 4
; CHECK-NEXT:    [[SOA_SCALAR_GEP11:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 0
; CHECK-NEXT:    [[WIDE_LOAD12:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP11]], align 4
; CHECK-NEXT:    [[TMP9:%.*]] = add nsw <4 x i32> [[WIDE_LOAD12]], [[WIDE_LOAD10]]
; CHECK-NEXT:    store <4 x i32> [[TMP9]], ptr [[SOA_SCALAR_GEP11]], align 4
; CHECK-NEXT:    [[SCALAR_GEP13:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 1, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    [[WIDE_LOAD14:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP13]], align 4
; CHECK-NEXT:    [[SOA_SCALAR_GEP15:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 1
; CHECK-NEXT:    [[WIDE_LOAD16:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP15]], align 4
; CHECK-NEXT:    [[TMP10:%.*]] = add nsw <4 x i32> [[WIDE_LOAD16]], [[WIDE_LOAD14]]
; CHECK-NEXT:    store <4 x i32> [[TMP10]], ptr [[SOA_SCALAR_GEP15]], align 4
; CHECK-NEXT:    [[SCALAR_GEP17:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 2, i64 [[DOTEXTRACT_0_]]
; CHECK-NEXT:    [[WIDE_LOAD18:%.*]] = load <4 x i32>, ptr [[SCALAR_GEP17]], align 4
; CHECK-NEXT:    [[SOA_SCALAR_GEP19:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 2
; CHECK-NEXT:    [[WIDE_LOAD20:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP19]], align 4
; CHECK-NEXT:    [[TMP11:%.*]] = add nsw <4 x i32> [[WIDE_LOAD20]], [[WIDE_LOAD18]]
; CHECK-NEXT:    store <4 x i32> [[TMP11]], ptr [[SOA_SCALAR_GEP19]], align 4
; CHECK-NEXT:    br label [[VPLANNEDBB21:%.*]]

; CHECK:       VPlannedBB21:
; CHECK-NEXT:    br label [[VPLANNEDBB22:%.*]]

; CHECK:       VPlannedBB22:
; CHECK-NEXT:    [[TMP12]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK-NEXT:    [[TMP13]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK-NEXT:    br label [[VPLANNEDBB23:%.*]]

; CHECK:       VPlannedBB23:
; CHECK-NEXT:    br label [[VPLANNEDBB24:%.*]]

; CHECK:       VPlannedBB24:
; CHECK-NEXT:    [[UNI_PHI25:%.*]] = phi i64 [ 0, [[VPLANNEDBB23]] ], [ [[TMP25:%.*]], [[VPLANNEDBB24]] ]
; CHECK-NEXT:    [[SOA_SCALAR_GEP26:%.*]] = getelementptr [3 x <4 x i32>], ptr [[X_RED_SOA_VEC]], i64 0, i64 [[UNI_PHI25]]
; CHECK-NEXT:    [[WIDE_LOAD27:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP26]], align 1
; CHECK-NEXT:    [[WIDE_LOAD27_EXTRACT_3_:%.*]] = extractelement <4 x i32> [[WIDE_LOAD27]], i32 3
; CHECK-NEXT:    [[SCALAR_GEP28:%.*]] = getelementptr [3 x i32], ptr [[X_RED]], i64 0, i64 [[UNI_PHI25]]
; CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[SCALAR_GEP28]], align 1
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x i32> poison, i32 [[TMP14]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <4 x i32> [[BROADCAST_SPLATINSERT]], <4 x i32> poison, <4 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[SCALAR_GEP28]], align 1
; CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[SCALAR_GEP28]], align 1
; CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[SCALAR_GEP28]], align 1
; CHECK-NEXT:    [[TMP18:%.*]] = shufflevector <4 x i32> [[WIDE_LOAD27]], <4 x i32> zeroinitializer, <4 x i32> <i32 4, i32 0, i32 1, i32 2>
; CHECK-NEXT:    [[TMP19:%.*]] = add <4 x i32> [[WIDE_LOAD27]], [[TMP18]]
; CHECK-NEXT:    [[TMP20:%.*]] = shufflevector <4 x i32> [[TMP19]], <4 x i32> zeroinitializer, <4 x i32> <i32 4, i32 5, i32 0, i32 1>
; CHECK-NEXT:    [[TMP21:%.*]] = add <4 x i32> [[TMP19]], [[TMP20]]
; CHECK-NEXT:    [[TMP22:%.*]] = shufflevector <4 x i32> [[TMP21]], <4 x i32> zeroinitializer, <4 x i32> <i32 4, i32 0, i32 1, i32 2>
; CHECK-NEXT:    [[TMP23:%.*]] = add <4 x i32> [[TMP22]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[DOTEXTRACT_3_:%.*]] = extractelement <4 x i32> [[TMP23]], i32 3
; CHECK-NEXT:    store <4 x i32> [[TMP23]], ptr [[SOA_SCALAR_GEP26]], align 1
; CHECK-NEXT:    [[TMP24:%.*]] = add i32 [[DOTEXTRACT_3_]], [[WIDE_LOAD27_EXTRACT_3_]]
; CHECK-NEXT:    store i32 [[TMP24]], ptr [[SCALAR_GEP28]], align 1
; CHECK-NEXT:    [[TMP25]] = add i64 [[UNI_PHI25]], 1
; CHECK-NEXT:    [[TMP26:%.*]] = icmp eq i64 [[TMP25]], 3
; CHECK-NEXT:    br i1 [[TMP26]], label [[VPLANNEDBB29:%.*]], label [[VPLANNEDBB24]]

; CHECK:       VPlannedBB29:
; CHECK-NEXT:    br label [[VPLANNEDBB30:%.*]]

; CHECK:       VPlannedBB30:
; CHECK-NEXT:    [[SOA_SCALAR_GEP31:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 0
; CHECK-NEXT:    [[WIDE_LOAD32:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP31]], align 4
; CHECK-NEXT:    [[SCALAR_GEP33:%.*]] = getelementptr inbounds [8 x i32], ptr @B, i64 0, i64 [[UNI_PHI]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD32]], ptr [[SCALAR_GEP33]], align 16
; CHECK-NEXT:    [[SOA_SCALAR_GEP34:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 1
; CHECK-NEXT:    [[WIDE_LOAD35:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP34]], align 4
; CHECK-NEXT:    [[SCALAR_GEP36:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 1, i64 [[UNI_PHI]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD35]], ptr [[SCALAR_GEP36]], align 16
; CHECK-NEXT:    [[SOA_SCALAR_GEP37:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[SOA_SCALAR_GEP]], i64 0, i64 2
; CHECK-NEXT:    [[WIDE_LOAD38:%.*]] = load <4 x i32>, ptr [[SOA_SCALAR_GEP37]], align 4
; CHECK-NEXT:    [[SCALAR_GEP39:%.*]] = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 2, i64 [[UNI_PHI]]
; CHECK-NEXT:    store <4 x i32> [[WIDE_LOAD38]], ptr [[SCALAR_GEP39]], align 16
; CHECK-NEXT:    br label [[VPLANNEDBB40:%.*]]

; CHECK:       VPlannedBB40:
; CHECK-NEXT:    br label [[NEW_LATCH]]

; CHECK:       new_latch:
; CHECK-NEXT:    [[TMP27]] = add <4 x i32> [[VEC_PHI4]], <i32 4, i32 4, i32 4, i32 4>
; CHECK-NEXT:    [[TMP28]] = add i32 [[UNI_PHI3]], 4
; CHECK-NEXT:    [[TMP29:%.*]] = icmp uge i64 [[TMP13]], 8
; CHECK-NEXT:    br i1 [[TMP29]], label [[VPLANNEDBB41:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
;
DIR.OMP.SIMD.1.split46.split:
  %x.red = alloca [3 x i32], align 4
  %x.red.gep = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 0
  %i.linear.iv = alloca i32, align 4
  store i32 0, ptr %x.red.gep, align 4
  %0 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 1
  store i32 0, ptr %0, align 4
  %1 = getelementptr inbounds [3 x i32], ptr %x.red, i64 0, i64 2
  store i32 0, ptr %1, align 4
  br label %DIR.OMP.SIMD.1

DIR.OMP.SIMD.1:                                   ; preds = %DIR.OMP.SIMD.1.split46.split
  %2 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr %x.red.gep, i32 0, i64 3, i64 1), "QUAL.OMP.LINEAR:IV.TYPED"(ptr %i.linear.iv, i32 0, i32 1, i32 1) ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5

DIR.VPO.END.GUARD.MEM.MOTION.5:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100, %DIR.OMP.SIMD.1
  %indvars.iv = phi i64 [ 0, %DIR.OMP.SIMD.1 ], [ %indvars.iv.next, %DIR.VPO.END.GUARD.MEM.MOTION.5100 ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1.split

DIR.VPO.GUARD.MEM.MOTION.1.split:                 ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5
  %pre.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red.gep) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.1

DIR.VPO.GUARD.MEM.MOTION.1:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.1.split
  %3 = trunc i64 %indvars.iv to i32
  store i32 %3, ptr %i.linear.iv, align 4
  %arrayidx = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 0
  %4 = load i32, ptr %arrayidx, align 4
  %arrayidx1 = getelementptr inbounds [8 x i32], ptr @B, i64 0, i64 %indvars.iv
  store i32 %4, ptr %arrayidx1, align 4
  %arrayidx2 = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 1
  %5 = load i32, ptr %arrayidx2, align 4
  %arrayidx4 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 1, i64 %indvars.iv
  store i32 %5, ptr %arrayidx4, align 4
  %arrayidx5 = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 2
  %6 = load i32, ptr %arrayidx5, align 4
  %arrayidx7 = getelementptr inbounds [3 x [8 x i32]], ptr @B, i64 0, i64 2, i64 %indvars.iv
  store i32 %6, ptr %arrayidx7, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.3

DIR.VPO.END.GUARD.MEM.MOTION.3:                   ; preds = %DIR.VPO.GUARD.MEM.MOTION.1
  call void @llvm.directive.region.exit(token %pre.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.4

DIR.VPO.END.GUARD.MEM.MOTION.4:                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.3
  %7 = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr %x.red.gep, i64 1) ]
  br label %DIR.OMP.SCAN.2

DIR.OMP.SCAN.2:                                   ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.4
  fence acq_rel
  br label %DIR.OMP.END.SCAN.6

DIR.OMP.END.SCAN.6:                               ; preds = %DIR.OMP.SCAN.2
  call void @llvm.directive.region.exit(token %7) [ "DIR.OMP.END.SCAN"() ]
  br label %DIR.OMP.END.SCAN.3

DIR.OMP.END.SCAN.3:                               ; preds = %DIR.OMP.END.SCAN.6
  br label %DIR.VPO.GUARD.MEM.MOTION.8.split

DIR.VPO.GUARD.MEM.MOTION.8.split:                 ; preds = %DIR.OMP.END.SCAN.3
  %post.scan.guard.start = call token @llvm.directive.region.entry() [ "DIR.VPO.GUARD.MEM.MOTION"(), "QUAL.OMP.LIVEIN"(ptr %x.red.gep) ]
  br label %DIR.VPO.GUARD.MEM.MOTION.4

DIR.VPO.GUARD.MEM.MOTION.4:                       ; preds = %DIR.VPO.GUARD.MEM.MOTION.8.split
  %8 = load i32, ptr %i.linear.iv, align 4
  %idxprom8 = sext i32 %8 to i64
  %arrayidx9 = getelementptr inbounds [8 x i32], ptr @A, i64 0, i64 %idxprom8
  %9 = load i32, ptr %arrayidx9, align 4
  %arrayidx10 = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 0
  %10 = load i32, ptr %arrayidx10, align 4
  %add11 = add nsw i32 %10, %9
  store i32 %add11, ptr %arrayidx10, align 4
  %arrayidx13 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 1, i64 %idxprom8
  %11 = load i32, ptr %arrayidx13, align 4
  %arrayidx14 = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 1
  %12 = load i32, ptr %arrayidx14, align 4
  %add15 = add nsw i32 %12, %11
  store i32 %add15, ptr %arrayidx14, align 4
  %arrayidx17 = getelementptr inbounds [3 x [8 x i32]], ptr @A, i64 0, i64 2, i64 %idxprom8
  %13 = load i32, ptr %arrayidx17, align 4
  %arrayidx18 = getelementptr inbounds [3 x i32], ptr %x.red.gep, i64 0, i64 2
  %14 = load i32, ptr %arrayidx18, align 4
  %add19 = add nsw i32 %14, %13
  store i32 %add19, ptr %arrayidx18, align 4
  br label %DIR.VPO.END.GUARD.MEM.MOTION.10

DIR.VPO.END.GUARD.MEM.MOTION.10:                  ; preds = %DIR.VPO.GUARD.MEM.MOTION.4
  call void @llvm.directive.region.exit(token %post.scan.guard.start) [ "DIR.VPO.END.GUARD.MEM.MOTION"() ]
  br label %DIR.VPO.END.GUARD.MEM.MOTION.5100

DIR.VPO.END.GUARD.MEM.MOTION.5100:                ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.10
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 8
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.12, label %DIR.VPO.END.GUARD.MEM.MOTION.5, !llvm.loop !0

DIR.OMP.END.SIMD.12:                              ; preds = %DIR.VPO.END.GUARD.MEM.MOTION.5100
  call void @llvm.directive.region.exit(token %2) [ "DIR.OMP.END.SIMD"() ]
  br label %DIR.OMP.END.SIMD.6

DIR.OMP.END.SIMD.6:                               ; preds = %DIR.OMP.END.SIMD.12
  ret void
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.vectorize.enable", i1 true}
!2 = !{!"llvm.loop.vectorize.ivdep_loop", i32 0}
