; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; REQUIRES: asserts
; RUN: opt -S -hir-ssa-deconstruction -hir-temp-cleanup -hir-vec-dir-insert -hir-vplan-vec -disable-output -vplan-dump-da %s 2>&1 -vplan-enable-new-cfg-merge-hir=0 | FileCheck %s
; RUN: opt -S -hir-ssa-deconstruction -hir-temp-cleanup -hir-vec-dir-insert -hir-vplan-vec -disable-output -vplan-dump-da %s 2>&1 -vplan-enable-new-cfg-merge-hir=1 | FileCheck %s
; RUN: opt -passes="hir-ssa-deconstruction,hir-temp-cleanup,hir-vec-dir-insert,hir-vplan-vec" -S -disable-output -vplan-dump-da %s 2>&1 -vplan-enable-new-cfg-merge-hir=0 | FileCheck %s
; RUN: opt -passes="hir-ssa-deconstruction,hir-temp-cleanup,hir-vec-dir-insert,hir-vplan-vec" -S -disable-output -vplan-dump-da %s 2>&1 -vplan-enable-new-cfg-merge-hir=1 | FileCheck %s


; This test checks to see that iv range information can be used to retain
; unit-strideness of the subscripts.

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@a = dso_local local_unnamed_addr global [1024 x i64] zeroinitializer, align 64
@b = dso_local local_unnamed_addr global [1024 x i64] zeroinitializer, align 64

; Function Attrs: nofree norecurse nounwind uwtable
define dso_local void @foo(i32 %arg0) local_unnamed_addr #0 {
; CHECK:       Printing Divergence info for Loop at depth 1 containing: [[BB0:BB[0-9]+]]<header><latch><exiting>
; CHECK-EMPTY:
; CHECK-NEXT:  Basic Block: [[BB0]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP0:%.*]] = phi  [ i64 [[VP__IND_INIT:%.*]], [[BB1:BB[0-9]+]] ],  [ i64 [[VP1:%.*]], [[BB0]] ]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i32 [[VP2:%.*]] = trunc i64 [[VP0]] to i32
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i32 [[VP3:%.*]] = add i32 [[ARG00:%.*]] i32 [[VP2]]
; CHECK-NEXT:  Divergent: [Shape: Unit Stride, Stride: i64 1] i64 [[VP4:%.*]] = sext i32 [[VP3]] to i64
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 8] i64* [[VP_SUBSCRIPT:%.*]] = subscript inbounds [1024 x i64]* @a i64 0 i64 [[VP4]]
; CHECK-NEXT:  Divergent: [Shape: Random] i64 [[VP_LOAD:%.*]] = load i64* [[VP_SUBSCRIPT]]
; CHECK-NEXT:  Divergent: [Shape: Strided, Stride: i64 8] i64* [[VP_SUBSCRIPT_1:%.*]] = subscript inbounds [1024 x i64]* @b i64 0 i64 [[VP4]]
; CHECK-NEXT:  Divergent: [Shape: Random] store i64 [[VP_LOAD]] i64* [[VP_SUBSCRIPT_1]]
;
entry:
  br label %for.body

for.body:                                         ; preds = %entry, %for.body
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %indvars.iv.trunc = trunc i64 %indvars.iv to i32
  %indvars.iv.mod = add i32 %arg0, %indvars.iv.trunc
  %indvars.iv.sext = sext i32 %indvars.iv.mod to i64
  %arrayidx = getelementptr inbounds [1024 x i64], [1024 x i64]* @a, i64 0, i64 %indvars.iv.sext
  %val = load i64, i64* %arrayidx, align 16
  %arrayidx2 = getelementptr inbounds [1024 x i64], [1024 x i64]* @b, i64 0, i64 %indvars.iv.sext
  store i64 %val, i64* %arrayidx2, align 16
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %cmp = icmp ult i64 %indvars.iv, 1022
  br i1 %cmp, label %for.body, label %for.end

for.end:                                          ; preds = %for.body
  ret void
}

