; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -hir-ssa-deconstruction -hir-vec-dir-insert -VPlanDriverHIR -disable-output -vplan-print-plain-cfg -vplan-force-invariant-decomposition=0 -vplan-dump-external-defs-hir < %s 2>&1 | FileCheck %s
;
; LIT test to check that invariant canon expressions are not decomposed and
; get treated as external defs. The loop being vectorized is the following:
;   for (i1 = 0; i1 < 100; i1++) {
;      arr[n1 * n2][i1] = 111;
;      farr[n1 + n2 + i1] += farr2[n1 * n2 + i1];
;    }
;
; The canon expression [n1 * n2] in the reference to arr is invariant.
; Similarly [n1 + n2] and [n1 * n2] are invariant in the canon expressions
; that reference farr/farr2 once we ignore the loop IV. The test checks we
; generate unique external defs for [n1 * n2] and [n1 + n2]. The external defs
; corresponding to [n1 * n2] and [n1 + n2] are added to the IV appropriately
; during decomposition when generating the accesses to farr/farr2.
;
; The auto generated checks for external defs are done using CHECK-DAG
; as the order of printing of external defs is non-deterministic. Auto
; generated checks that were unnecessary were also removed.
@arr = common dso_local local_unnamed_addr global [100 x [100 x i64]] zeroinitializer, align 16
@farr2 = common dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16
@farr = common dso_local local_unnamed_addr global [100 x float] zeroinitializer, align 16

define dso_local void @foo(i64 %n1, i64 %n2) {
; CHECK-LABEL:  VPlan after importing plain CFG
; CHECK-NEXT:  External Defs Start:
; CHECK-DAG:   [[VP0:%.*]] = {(%n1 * %n2)}
; CHECK-DAG:   [[VP1:%.*]] = {%n1 + %n2}
; CHECK-DAG:   [[VP2:%.*]] = {@farr2}
; CHECK-DAG:   [[VP4:%.*]] = {@farr}
; CHECK-DAG:   [[VP5:%.*]] = {@arr}
; CHECK-NEXT:  External Defs End:
; CHECK:          i64 [[VP8:%.*]] = phi  [ i64 0, {{.*}} ],  [ i64 [[VP9:%.*]], {{.*}} ]
; CHECK-NEXT:     i64* [[VP10:%.*]] = subscript inbounds [100 x [100 x i64]]* @arr i64 0 i64 [[VP0]] i64 [[VP8]]
; CHECK-NEXT:     store i64 111 i64* [[VP10]]
; CHECK-NEXT:     i64 [[VP11:%.*]] = add i64 [[VP0]] i64 [[VP8]]
; CHECK-NEXT:     float* [[VP12:%.*]] = subscript inbounds [100 x float]* @farr2 i64 0 i64 [[VP11]]
; CHECK-NEXT:     float [[VP13:%.*]] = load float* [[VP12]]
; CHECK-NEXT:     i64 [[VP14:%.*]] = add i64 [[VP1]] i64 [[VP8]]
; CHECK-NEXT:     float* [[VP15:%.*]] = subscript inbounds [100 x float]* @farr i64 0 i64 [[VP14]]
; CHECK-NEXT:     float [[VP16:%.*]] = load float* [[VP15]]
; CHECK-NEXT:     float [[VP17:%.*]] = fadd float [[VP13]] float [[VP16]]
; CHECK-NEXT:     i64 [[VP18:%.*]] = add i64 [[VP1]] i64 [[VP8]]
; CHECK-NEXT:     float* [[VP19:%.*]] = subscript inbounds [100 x float]* @farr i64 0 i64 [[VP18]]
; CHECK-NEXT:     store float [[VP17]] float* [[VP19]]
;
entry:
  %mul = mul nsw i64 %n2, %n1
  %add = add nsw i64 %n2, %n1
  br label %for.body

for.body:                                         ; preds = %for.body, %entry
  %i1.024 = phi i64 [ 0, %entry ], [ %inc, %for.body ]
  %arrayidx1 = getelementptr inbounds [100 x [100 x i64]], [100 x [100 x i64]]* @arr, i64 0, i64 %mul, i64 %i1.024
  store i64 111, i64* %arrayidx1, align 8
  %add5 = add nsw i64 %i1.024, %mul
  %arrayidx6 = getelementptr inbounds [100 x float], [100 x float]* @farr2, i64 0, i64 %add5
  %0 = load float, float* %arrayidx6, align 4
  %add9 = add nsw i64 %add, %i1.024
  %arrayidx10 = getelementptr inbounds [100 x float], [100 x float]* @farr, i64 0, i64 %add9
  %1 = load float, float* %arrayidx10, align 4
  %add11 = fadd float %0, %1
  store float %add11, float* %arrayidx10, align 4
  %inc = add nuw nsw i64 %i1.024, 1
  %exitcond = icmp eq i64 %inc, 100
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body
  ret void
}
