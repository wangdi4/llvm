; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 2
; LIT test to verify vector code generators support for -1 stride optimization
; on input VectorType loads/store.

; RUN: opt -passes='hir-ssa-deconstruction,hir-vplan-vec,print<hir>' -disable-output < %s 2>&1 | FileCheck %s --check-prefix=HIRCHECK
; RUN: opt -passes='vplan-vec' -print-after=vplan-vec -disable-output < %s 2>&1 | FileCheck %s --check-prefix=IRCHECK

define dso_local void @foo(ptr noalias nocapture %larr) local_unnamed_addr #0 {
; IRCHECK-LABEL: define dso_local void @foo
; IRCHECK:       vector.body:
; IRCHECK-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB1:%.*]] ], [ [[TMP6:%.*]], [[VPLANNEDBB6:%.*]] ]
; IRCHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB1]] ], [ [[TMP5:%.*]], [[VPLANNEDBB6]] ]
; IRCHECK-NEXT:    [[TMP0:%.*]] = sub nsw <4 x i64> zeroinitializer, [[VEC_PHI]]
; IRCHECK-NEXT:    [[DOTEXTRACT_0_:%.*]] = extractelement <4 x i64> [[TMP0]], i32 0
; IRCHECK-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds <2 x float>, ptr [[LARR:%.*]], i64 [[DOTEXTRACT_0_]]
; IRCHECK-NEXT:    [[TMP1:%.*]] = getelementptr <2 x float>, ptr [[SCALAR_GEP]], i32 -3
; IRCHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <8 x float>, ptr [[TMP1]], align 4
; IRCHECK-NEXT:    [[REVERSE:%.*]] = shufflevector <8 x float> [[WIDE_LOAD]], <8 x float> undef, <8 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
; IRCHECK-NEXT:    [[TMP2:%.*]] = icmp sgt <4 x i64> [[VEC_PHI]], <i64 10, i64 10, i64 10, i64 10>
; IRCHECK-NEXT:    br label [[VPLANNEDBB3:%.*]]
; IRCHECK:       VPlannedBB3:
; IRCHECK-NEXT:    [[TMP3:%.*]] = fadd fast <8 x float> [[REVERSE]], <float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00>
; IRCHECK-NEXT:    [[TMP4:%.*]] = getelementptr <2 x float>, ptr [[SCALAR_GEP]], i32 -3
; IRCHECK-NEXT:    [[REVERSE4:%.*]] = shufflevector <8 x float> [[TMP3]], <8 x float> undef, <8 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
; IRCHECK-NEXT:    [[REPLICATEDMASKELTS_:%.*]] = shufflevector <4 x i1> [[TMP2]], <4 x i1> undef, <8 x i32> <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>
; IRCHECK-NEXT:    [[REVERSE5:%.*]] = shufflevector <8 x i1> [[REPLICATEDMASKELTS_]], <8 x i1> undef, <8 x i32> <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>
; IRCHECK-NEXT:    call void @llvm.masked.store.v8f32.p0(<8 x float> [[REVERSE4]], ptr [[TMP4]], i32 4, <8 x i1> [[REVERSE5]])
; IRCHECK-NEXT:    br label [[VPLANNEDBB6]]
; IRCHECK:       VPlannedBB6:
; IRCHECK-NEXT:    [[TMP5]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; IRCHECK-NEXT:    [[TMP6]] = add nuw nsw i64 [[UNI_PHI]], 4
; IRCHECK-NEXT:    [[TMP7:%.*]] = icmp uge i64 [[TMP6]], 100
; IRCHECK-NEXT:    br i1 [[TMP7]], label [[VPLANNEDBB7:%.*]], label [[VECTOR_BODY:%.*]], !llvm.loop [[LOOP0:![0-9]+]]
;
; HIRCHECK:        + DO i1 = 0, 99, 4   <DO_LOOP> <simd-vectorized> <novectorize>
; HIRCHECK-NEXT:   |   %.vec = (<8 x float>*)(%larr)[-1 * i1 + -3];
; HIRCHECK-NEXT:   |   %reverse = shufflevector %.vec,  undef,  <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>;
; HIRCHECK-NEXT:   |   %.vec2 = i1 + <i64 0, i64 1, i64 2, i64 3> > 10;
; HIRCHECK-NEXT:   |   %.vec3 = %reverse  +  <float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00, float 1.000000e+00>;
; HIRCHECK-NEXT:   |   %.replicated.elts = shufflevector %.vec2,  undef,  <i32 0, i32 0, i32 1, i32 1, i32 2, i32 2, i32 3, i32 3>;
; HIRCHECK-NEXT:   |   %reverse4 = shufflevector %.replicated.elts,  undef,  <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>;
; HIRCHECK-NEXT:   |   %reverse5 = shufflevector %.vec3,  undef,  <i32 6, i32 7, i32 4, i32 5, i32 2, i32 3, i32 0, i32 1>;
; HIRCHECK-NEXT:   |   (<8 x float>*)(%larr)[-1 * i1 + -3] = %reverse5, Mask = @{%reverse4};
; HIRCHECK-NEXT:   + END LOOP
;
entry:
  %tok = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %for.body

for.body:                                         ; preds = %if.end, %entry
  %l1.011 = phi i64 [ 0, %entry ], [ %inc, %if.end ]
  %sub = sub nsw i64 0, %l1.011
  %arrayidx = getelementptr inbounds <2 x float>, ptr %larr, i64 %sub
  %0 = load <2 x float>, ptr %arrayidx, align 4
  %cmp1 = icmp sgt i64 %l1.011, 10
  br i1 %cmp1, label %if.then, label %if.end

if.then:                                          ; preds = %for.body
  %add = fadd fast <2 x float> %0, <float 1.0, float 1.0>
  store <2 x float> %add, ptr %arrayidx, align 4
  br label %if.end

if.end:                                           ; preds = %if.then, %for.body
  %inc = add nuw nsw i64 %l1.011, 1
  %exitcond = icmp eq i64 %inc, 100
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %if.end
  call void @llvm.directive.region.exit(token %tok) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)
