; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --scrub-attributes --version 2
; RUN: opt %s -S -passes=vplan-vec | FileCheck %s --check-prefixes=CHECK3,DISABLE-UNROLL
; RUN: opt %s -S -passes=vplan-vec -vplan-force-uf=2 | FileCheck %s --check-prefixes=CHECK2,DISABLE-UNROLL
; RUN: opt %s -S -passes=vplan-vec -vplan-force-uf=1 | FileCheck %s --check-prefixes=CHECK1,ENABLE-UNROLL

; RUN: opt %s -disable-output -passes="default<O1>" -print-before=vplan-vec -print-module-scope 2>&1 | FileCheck %s --check-prefixes=ENABLE-UNROLL
; RUN: opt %s -disable-output -passes="default<O1>" -print-after=vplan-vec -print-module-scope 2>&1 | FileCheck %s --check-prefixes=CHECK3,DISABLE-UNROLL

define void @foo(ptr %lp1) {
;
; Unrolled by 3 due to #pragma unroll (llvm.loop.unroll.count metadata)
;
; CHECK3:       vector.body:
; CHECK3-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB2:%.*]] ], [ [[TMP10:%.*]], [[VPLANNEDBB6:%.*]] ]
; CHECK3-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB2]] ], [ [[TMP9:%.*]], [[VPLANNEDBB6]] ]
; CHECK3-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, ptr [[LP1:%.*]], i64 [[UNI_PHI]]
; CHECK3-NEXT:    [[TMP0:%.*]] = trunc <4 x i64> [[VEC_PHI]] to <4 x i32>
; CHECK3-NEXT:    store <4 x i32> [[TMP0]], ptr [[SCALAR_GEP]], align 4
; CHECK3-NEXT:    [[TMP1:%.*]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK3-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK3-NEXT:    [[TMP3:%.*]] = icmp uge i64 [[TMP2]], 0
; CHECK3-NEXT:    br label [[VPLANNEDBB4:%.*]]
; CHECK3:       VPlannedBB4:
; CHECK3-NEXT:    [[SCALAR_GEP5:%.*]] = getelementptr inbounds i32, ptr [[LP1]], i64 [[TMP2]]
; CHECK3-NEXT:    [[TMP4:%.*]] = trunc <4 x i64> [[TMP1]] to <4 x i32>
; CHECK3-NEXT:    store <4 x i32> [[TMP4]], ptr [[SCALAR_GEP5]], align 4
; CHECK3-NEXT:    [[TMP5:%.*]] = add nuw nsw <4 x i64> [[TMP1]], <i64 4, i64 4, i64 4, i64 4>
; CHECK3-NEXT:    [[TMP6:%.*]] = add nuw nsw i64 [[TMP2]], 4
; CHECK3-NEXT:    [[TMP7:%.*]] = icmp uge i64 [[TMP6]], 0
; CHECK3-NEXT:    br label [[VPLANNEDBB6]]
; CHECK3:       VPlannedBB6:
; CHECK3-NEXT:    [[SCALAR_GEP7:%.*]] = getelementptr inbounds i32, ptr [[LP1]], i64 [[TMP6]]
; CHECK3-NEXT:    [[TMP8:%.*]] = trunc <4 x i64> [[TMP5]] to <4 x i32>
; CHECK3-NEXT:    store <4 x i32> [[TMP8]], ptr [[SCALAR_GEP7]], align 4
; CHECK3-NEXT:    [[TMP9]] = add nuw nsw <4 x i64> [[TMP5]], <i64 4, i64 4, i64 4, i64 4>
; CHECK3-NEXT:    [[TMP10]] = add nuw nsw i64 [[TMP6]], 4
; CHECK3-NEXT:    [[TMP11:%.*]] = icmp uge i64 [[TMP10]], 0
; CHECK3-NEXT:    br i1 [[TMP11]], label [[VPLANNEDBB8:%.*]], label [[VECTOR_BODY:%.*]], !llvm.loop [[LOOP0:![0-9]+]]
;
; Unrolled by 2 due to -vplan-force-uf=2 flag
;
; CHECK2:       vector.body:
; CHECK2-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB2:%.*]] ], [ [[TMP6:%.*]], [[VPLANNEDBB4:%.*]] ]
; CHECK2-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB2]] ], [ [[TMP5:%.*]], [[VPLANNEDBB4]] ]
; CHECK2-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, ptr [[LP1:%.*]], i64 [[UNI_PHI]]
; CHECK2-NEXT:    [[TMP0:%.*]] = trunc <4 x i64> [[VEC_PHI]] to <4 x i32>
; CHECK2-NEXT:    store <4 x i32> [[TMP0]], ptr [[SCALAR_GEP]], align 4
; CHECK2-NEXT:    [[TMP1:%.*]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK2-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK2-NEXT:    [[TMP3:%.*]] = icmp uge i64 [[TMP2]], 0
; CHECK2-NEXT:    br label [[VPLANNEDBB4]]
; CHECK2:       VPlannedBB4:
; CHECK2-NEXT:    [[SCALAR_GEP5:%.*]] = getelementptr inbounds i32, ptr [[LP1]], i64 [[TMP2]]
; CHECK2-NEXT:    [[TMP4:%.*]] = trunc <4 x i64> [[TMP1]] to <4 x i32>
; CHECK2-NEXT:    store <4 x i32> [[TMP4]], ptr [[SCALAR_GEP5]], align 4
; CHECK2-NEXT:    [[TMP5]] = add nuw nsw <4 x i64> [[TMP1]], <i64 4, i64 4, i64 4, i64 4>
; CHECK2-NEXT:    [[TMP6]] = add nuw nsw i64 [[TMP2]], 4
; CHECK2-NEXT:    [[TMP7:%.*]] = icmp uge i64 [[TMP6]], 0
; CHECK2-NEXT:    br i1 [[TMP7]], label [[VPLANNEDBB6:%.*]], label [[VECTOR_BODY:%.*]], !llvm.loop [[LOOP0:![0-9]+]]
;
; Wasn't unrolled due to -vplan-force-uf=1 flag
;
; CHECK1:       vector.body:
; CHECK1-NEXT:    [[UNI_PHI:%.*]] = phi i64 [ 0, [[VPLANNEDBB2:%.*]] ], [ [[TMP2:%.*]], [[VECTOR_BODY:%.*]] ]
; CHECK1-NEXT:    [[VEC_PHI:%.*]] = phi <4 x i64> [ <i64 0, i64 1, i64 2, i64 3>, [[VPLANNEDBB2]] ], [ [[TMP1:%.*]], [[VECTOR_BODY]] ]
; CHECK1-NEXT:    [[SCALAR_GEP:%.*]] = getelementptr inbounds i32, ptr [[LP1:%.*]], i64 [[UNI_PHI]]
; CHECK1-NEXT:    [[TMP0:%.*]] = trunc <4 x i64> [[VEC_PHI]] to <4 x i32>
; CHECK1-NEXT:    store <4 x i32> [[TMP0]], ptr [[SCALAR_GEP]], align 4
; CHECK1-NEXT:    [[TMP1]] = add nuw nsw <4 x i64> [[VEC_PHI]], <i64 4, i64 4, i64 4, i64 4>
; CHECK1-NEXT:    [[TMP2]] = add nuw nsw i64 [[UNI_PHI]], 4
; CHECK1-NEXT:    [[TMP3:%.*]] = icmp uge i64 [[TMP2]], 0
; CHECK1-NEXT:    br i1 [[TMP3]], label [[VPLANNEDBB4:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
;
; Check metadata
;
; DISABLE-UNROLL-NOT: llvm.loop.unroll.count
; DISABLE-UNROLL: llvm.loop.unroll.disable
;
; ENABLE-UNROLL: llvm.loop.unroll.count
; ENABLE-UNROLL-NOT: llvm.loop.unroll.disable
;
DIR.OMP.SIMD.25:
  br label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %DIR.OMP.SIMD.25
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body, %omp.inner.for.body.lr.ph
  %indvars.iv = phi i64 [ 0, %omp.inner.for.body.lr.ph ], [ %indvars.iv.next, %omp.inner.for.body ]
  %arrayidx = getelementptr inbounds i32, ptr %lp1, i64 %indvars.iv
  %1 = trunc i64 %indvars.iv to i32
  store i32 %1, ptr %arrayidx, align 4
  %indvars.iv.next = add i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 0
  br i1 %exitcond.not, label %DIR.OMP.END.SIMD.1, label %omp.inner.for.body, !llvm.loop !0

DIR.OMP.END.SIMD.1:                               ; preds = %omp.inner.for.body
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.SIMD"() ]
  ret void
}

declare token @llvm.directive.region.entry()
declare void @llvm.directive.region.exit(token)

!0 = distinct !{!0, !1, !2, !3, !4}
!1 = !{!"llvm.loop.vectorize.enable", i1 true}
!2 = !{!"llvm.loop.unroll.count", i32 3}
!3 = !{!"llvm.loop.vectorize.ivdep_loop", i32 0}
!4 = !{!"llvm.loop.parallel_accesses", !5}
!5 = distinct !{}
