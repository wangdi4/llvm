; RUN: opt -opaque-pointers=1 -bugpoint-enable-legacy-pm -switch-to-offload -vpo-cfg-restructuring -vpo-paropt-prepare -vpo-restore-operands -vpo-cfg-restructuring -vpo-paropt -S %s | FileCheck %s
; RUN: opt -opaque-pointers=1 -switch-to-offload -passes='function(vpo-cfg-restructuring,vpo-paropt-prepare,vpo-restore-operands,vpo-cfg-restructuring),vpo-paropt' -S %s | FileCheck %s
;
; Check that we generate kmpc_barrier for target loops that are optimized away before Paropt
; IR was hand modified to add "DIR.OMP.LOOP" with null clauses for zero-trip loop construct. This Construct will be eventually generated by FE.
;
; Test src:
;
; #include <stdio.h>
; int main() {
;   int x = 0;
; #pragma omp target parallel
;  {
;    x = 0;
; #pragma omp for
;    for (int i = 0; i < 0; i++);
; #pragma omp master
;    x = 100;
;  }
;   printf("%d\n", x);
; }

; CHECK: call spir_func void @__kmpc_barrier()

target datalayout = "e-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-n8:16:32:64"
target triple = "spir64"
target device_triples = "spir64"

@.str = private unnamed_addr addrspace(1) constant [4 x i8] c"%d\0A\00", align 1

define protected i32 @main() {
entry:
  %retval = alloca i32, align 4
  %x = alloca i32, align 4
  %tmp = alloca i32, align 4
  %retval.ascast = addrspacecast i32* %retval to i32 addrspace(4)*
  %x.ascast = addrspacecast i32* %x to i32 addrspace(4)*
  %tmp.ascast = addrspacecast i32* %tmp to i32 addrspace(4)*
  store i32 0, i32 addrspace(4)* %x.ascast, align 4
  %0 = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(),
    "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0),
    "QUAL.OMP.FIRSTPRIVATE:TYPED"(i32 addrspace(4)* %x.ascast, i32 0, i32 1),
    "QUAL.OMP.PRIVATE:TYPED"(i32 addrspace(4)* %tmp.ascast, i32 0, i32 1) ]

  %1 = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(),
    "QUAL.OMP.SHARED:TYPED"(i32 addrspace(4)* %x.ascast, i32 0, i32 1),
    "QUAL.OMP.PRIVATE:TYPED"(i32 addrspace(4)* %tmp.ascast, i32 0, i32 1) ]

  %dummy = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(),
    "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr %tmp, i32 0, i32 1),
    "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr %tmp, i32 0, i32 1) ]

  call void @llvm.directive.region.exit(token %dummy) [ "DIR.OMP.END.LOOP"() ]

  store i32 0, i32 addrspace(4)* %x.ascast, align 4
  %2 = call token @llvm.directive.region.entry() [ "DIR.OMP.MASTER"() ]

  fence acquire
  store i32 100, i32 addrspace(4)* %x.ascast, align 4
  fence release
  call void @llvm.directive.region.exit(token %2) [ "DIR.OMP.END.MASTER"() ]
  call void @llvm.directive.region.exit(token %1) [ "DIR.OMP.END.PARALLEL"() ]
  call void @llvm.directive.region.exit(token %0) [ "DIR.OMP.END.TARGET"() ]
  %3 = load i32, i32 addrspace(4)* %x.ascast, align 4
  %call = call spir_func i32 (i8 addrspace(4)*, ...) @printf(i8 addrspace(4)* noundef getelementptr inbounds ([4 x i8], [4 x i8] addrspace(4)* addrspacecast ([4 x i8] addrspace(1)* @.str to [4 x i8] addrspace(4)*), i64 0, i64 0), i32 noundef %3)
  ret i32 0
}

declare token @llvm.directive.region.entry()

declare void @llvm.directive.region.exit(token)

declare spir_func i32 @printf(i8 addrspace(4)* noundef, ...)

!omp_offload.info = !{!0}

!0 = !{i32 0, i32 66312, i32 104988838, !"_Z4main", i32 4, i32 0, i32 0, i32 0}
