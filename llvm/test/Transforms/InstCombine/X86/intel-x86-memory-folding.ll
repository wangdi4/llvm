; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Check that add is not turned into or when there is opportunity for memory folding.
; RUN: opt -passes="instcombine" < %s -mtriple=x86_64-unknown-unknown -S | FileCheck %s

; %indvar.addone can be folded into %ptr.addone and should stay as "add"
define void @preserve_add(i64* %base, i64 %n) {
; CHECK-LABEL: @preserve_add(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[LOOP_BODY:%.*]]
; CHECK:       loop.body:
; CHECK-NEXT:    [[INDVAR:%.*]] = phi i64 [ [[INDVAR_NEXT:%.*]], [[LOOP_BODY]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INDVAR_ADDONE:%.*]] = add nuw nsw i64 [[INDVAR]], 1
; CHECK-NEXT:    [[PTR_ADDONE:%.*]] = getelementptr inbounds i64, i64* [[BASE:%.*]], i64 [[INDVAR_ADDONE]]
; CHECK-NEXT:    [[PTR_ADDONE_CAST:%.*]] = bitcast i64* [[PTR_ADDONE]] to <16 x i64>*
; CHECK-NEXT:    [[VEC_ADDONE:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_ADDONE_CAST]], align 8
; CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds i64, i64* [[BASE]], i64 [[INDVAR]]
; CHECK-NEXT:    [[PTR_CAST:%.*]] = bitcast i64* [[PTR]] to <16 x i64>*
; CHECK-NEXT:    [[VEC:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[VEC_RESULT:%.*]] = add <16 x i64> [[VEC]], [[VEC_ADDONE]]
; CHECK-NEXT:    store <16 x i64> [[VEC_RESULT]], <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[INDVAR_NEXT]] = add i64 [[INDVAR]], 16
; CHECK-NEXT:    [[INDVAR_COND:%.*]] = icmp slt i64 [[INDVAR_NEXT]], [[N:%.*]]
; CHECK-NEXT:    br i1 [[INDVAR_COND]], label [[LOOP_BODY]], label [[LOOP_END:%.*]]
; CHECK:       loop.end:
; CHECK-NEXT:    ret void
;
entry:
  br label %loop.body
loop.body:
  %indvar = phi i64 [ %indvar.next, %loop.body ], [ 0, %entry ]
  %indvar.addone = add i64 %indvar, 1
  %ptr.addone = getelementptr inbounds i64, i64* %base, i64 %indvar.addone
  %ptr.addone.cast = bitcast i64* %ptr.addone to <16 x i64>*
  %vec.addone = load <16 x i64>, <16 x i64>* %ptr.addone.cast, align 8
  %ptr = getelementptr inbounds i64, i64* %base, i64 %indvar
  %ptr.cast = bitcast i64* %ptr to <16 x i64>*
  %vec = load <16 x i64>, <16 x i64>* %ptr.cast, align 8
  %vec.result = add <16 x i64> %vec, %vec.addone
  store <16 x i64> %vec.result, <16 x i64>* %ptr.cast, align 8
  %indvar.next = add i64 %indvar, 16
  %indvar.cond = icmp slt i64 %indvar.next, %n
  br i1 %indvar.cond, label %loop.body, label %loop.end
loop.end:
  ret void
}

; %indvar.addone has no constant operand and can be transformed to "or"
define void @nopreserve_add_nonconstant(i64* %base, i64 %n) {
; CHECK-LABEL: @nopreserve_add_nonconstant(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[N_LSHR:%.*]] = lshr i64 [[N:%.*]], 62
; CHECK-NEXT:    br label [[LOOP_BODY:%.*]]
; CHECK:       loop.body:
; CHECK-NEXT:    [[INDVAR:%.*]] = phi i64 [ [[INDVAR_NEXT:%.*]], [[LOOP_BODY]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INDVAR_ADDONE:%.*]] = or i64 [[INDVAR]], [[N_LSHR]]
; CHECK-NEXT:    [[PTR_ADDONE:%.*]] = getelementptr inbounds i64, i64* [[BASE:%.*]], i64 [[INDVAR_ADDONE]]
; CHECK-NEXT:    [[PTR_ADDONE_CAST:%.*]] = bitcast i64* [[PTR_ADDONE]] to <16 x i64>*
; CHECK-NEXT:    [[VEC_ADDONE:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_ADDONE_CAST]], align 8
; CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds i64, i64* [[BASE]], i64 [[INDVAR]]
; CHECK-NEXT:    [[PTR_CAST:%.*]] = bitcast i64* [[PTR]] to <16 x i64>*
; CHECK-NEXT:    [[VEC:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[VEC_RESULT:%.*]] = add <16 x i64> [[VEC]], [[VEC_ADDONE]]
; CHECK-NEXT:    store <16 x i64> [[VEC_RESULT]], <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[INDVAR_NEXT]] = add i64 [[INDVAR]], 16
; CHECK-NEXT:    [[INDVAR_COND:%.*]] = icmp slt i64 [[INDVAR_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[INDVAR_COND]], label [[LOOP_BODY]], label [[LOOP_END:%.*]]
; CHECK:       loop.end:
; CHECK-NEXT:    ret void
;
entry:
  %n.lshr = lshr i64 %n, 62
  br label %loop.body
loop.body:
  %indvar = phi i64 [ %indvar.next, %loop.body ], [ 0, %entry ]
  %indvar.addone = add i64 %indvar, %n.lshr
  %ptr.addone = getelementptr inbounds i64, i64* %base, i64 %indvar.addone
  %ptr.addone.cast = bitcast i64* %ptr.addone to <16 x i64>*
  %vec.addone = load <16 x i64>, <16 x i64>* %ptr.addone.cast, align 8
  %ptr = getelementptr inbounds i64, i64* %base, i64 %indvar
  %ptr.cast = bitcast i64* %ptr to <16 x i64>*
  %vec = load <16 x i64>, <16 x i64>* %ptr.cast, align 8
  %vec.result = add <16 x i64> %vec, %vec.addone
  store <16 x i64> %vec.result, <16 x i64>* %ptr.cast, align 8
  %indvar.next = add i64 %indvar, 16
  %indvar.cond = icmp slt i64 %indvar.next, %n
  br i1 %indvar.cond, label %loop.body, label %loop.end
loop.end:
  ret void
}

; %indvar.addone can be folded into %ptr.addone and should stay as "add"
define void @preserve_add_with_sext(i64* %base, i32 %n) {
; CHECK-LABEL: @preserve_add_with_sext(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[LOOP_BODY:%.*]]
; CHECK:       loop.body:
; CHECK-NEXT:    [[INDVAR:%.*]] = phi i32 [ [[INDVAR_NEXT:%.*]], [[LOOP_BODY]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INDVAR_ADDONE:%.*]] = add nuw nsw i32 [[INDVAR]], 1
; CHECK-NEXT:    [[INDVAR_ADDONE_EXT:%.*]] = sext i32 [[INDVAR_ADDONE]] to i64
; CHECK-NEXT:    [[PTR_ADDONE:%.*]] = getelementptr inbounds i64, i64* [[BASE:%.*]], i64 [[INDVAR_ADDONE_EXT]]
; CHECK-NEXT:    [[PTR_ADDONE_CAST:%.*]] = bitcast i64* [[PTR_ADDONE]] to <16 x i64>*
; CHECK-NEXT:    [[VEC_ADDONE:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_ADDONE_CAST]], align 8
; CHECK-NEXT:    [[INDVAR_EXT:%.*]] = sext i32 [[INDVAR]] to i64
; CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds i64, i64* [[BASE]], i64 [[INDVAR_EXT]]
; CHECK-NEXT:    [[PTR_CAST:%.*]] = bitcast i64* [[PTR]] to <16 x i64>*
; CHECK-NEXT:    [[VEC:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[VEC_RESULT:%.*]] = add <16 x i64> [[VEC]], [[VEC_ADDONE]]
; CHECK-NEXT:    store <16 x i64> [[VEC_RESULT]], <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[INDVAR_NEXT]] = add i32 [[INDVAR]], 16
; CHECK-NEXT:    [[INDVAR_COND:%.*]] = icmp slt i32 [[INDVAR_NEXT]], [[N:%.*]]
; CHECK-NEXT:    br i1 [[INDVAR_COND]], label [[LOOP_BODY]], label [[LOOP_END:%.*]]
; CHECK:       loop.end:
; CHECK-NEXT:    ret void
;
entry:
  br label %loop.body
loop.body:
  %indvar = phi i32 [ %indvar.next, %loop.body ], [ 0, %entry ]
  %indvar.addone = add i32 %indvar, 1
  %indvar.addone.ext = sext i32 %indvar.addone to i64
  %ptr.addone = getelementptr inbounds i64, i64* %base, i64 %indvar.addone.ext
  %ptr.addone.cast = bitcast i64* %ptr.addone to <16 x i64>*
  %vec.addone = load <16 x i64>, <16 x i64>* %ptr.addone.cast, align 8
  %indvar.ext = sext i32 %indvar to i64
  %ptr = getelementptr inbounds i64, i64* %base, i64 %indvar.ext
  %ptr.cast = bitcast i64* %ptr to <16 x i64>*
  %vec = load <16 x i64>, <16 x i64>* %ptr.cast, align 8
  %vec.result = add <16 x i64> %vec, %vec.addone
  store <16 x i64> %vec.result, <16 x i64>* %ptr.cast, align 8
  %indvar.next = add i32 %indvar, 16
  %indvar.cond = icmp slt i32 %indvar.next, %n
  br i1 %indvar.cond, label %loop.body, label %loop.end
loop.end:
  ret void
}

; %indvar.addone has other users so it can be transformed to "or"
define void @nopreserve_add_other_use(i64* %base, i64 %n) {
; CHECK-LABEL: @nopreserve_add_other_use(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[LOOP_BODY:%.*]]
; CHECK:       loop.body:
; CHECK-NEXT:    [[INDVAR:%.*]] = phi i64 [ [[INDVAR_NEXT:%.*]], [[LOOP_BODY]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[INDVAR_ADDONE:%.*]] = or i64 [[INDVAR]], 1
; CHECK-NEXT:    [[PTR_ADDONE:%.*]] = getelementptr inbounds i64, i64* [[BASE:%.*]], i64 [[INDVAR_ADDONE]]
; CHECK-NEXT:    [[PTR_ADDONE_CAST:%.*]] = bitcast i64* [[PTR_ADDONE]] to <16 x i64>*
; CHECK-NEXT:    [[VEC_ADDONE:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_ADDONE_CAST]], align 8
; CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds i64, i64* [[BASE]], i64 [[INDVAR]]
; CHECK-NEXT:    [[PTR_CAST:%.*]] = bitcast i64* [[PTR]] to <16 x i64>*
; CHECK-NEXT:    [[VEC:%.*]] = load <16 x i64>, <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[VEC_TMP0:%.*]] = add <16 x i64> [[VEC]], [[VEC_ADDONE]]
; CHECK-NEXT:    [[INDVAR_ADDONE_VEC:%.*]] = insertelement <16 x i64> undef, i64 [[INDVAR_ADDONE]], i64 0
; CHECK-NEXT:    [[INDVAR_ADDONE_BROADCAST:%.*]] = shufflevector <16 x i64> [[INDVAR_ADDONE_VEC]], <16 x i64> undef, <16 x i32> zeroinitializer
; CHECK-NEXT:    [[VEC_RESULT:%.*]] = add <16 x i64> [[VEC_TMP0]], [[INDVAR_ADDONE_BROADCAST]]
; CHECK-NEXT:    store <16 x i64> [[VEC_RESULT]], <16 x i64>* [[PTR_CAST]], align 8
; CHECK-NEXT:    [[INDVAR_NEXT]] = add i64 [[INDVAR]], 16
; CHECK-NEXT:    [[INDVAR_COND:%.*]] = icmp slt i64 [[INDVAR_NEXT]], [[N:%.*]]
; CHECK-NEXT:    br i1 [[INDVAR_COND]], label [[LOOP_BODY]], label [[LOOP_END:%.*]]
; CHECK:       loop.end:
; CHECK-NEXT:    ret void
;
entry:
  br label %loop.body
loop.body:
  %indvar = phi i64 [ %indvar.next, %loop.body ], [ 0, %entry ]
  %indvar.addone = add i64 %indvar, 1
  %ptr.addone = getelementptr inbounds i64, i64* %base, i64 %indvar.addone
  %ptr.addone.cast = bitcast i64* %ptr.addone to <16 x i64>*
  %vec.addone = load <16 x i64>, <16 x i64>* %ptr.addone.cast, align 8
  %ptr = getelementptr inbounds i64, i64* %base, i64 %indvar
  %ptr.cast = bitcast i64* %ptr to <16 x i64>*
  %vec = load <16 x i64>, <16 x i64>* %ptr.cast, align 8
  %vec.tmp0 = add <16 x i64> %vec, %vec.addone
  %indvar.addone.vec = insertelement <16 x i64> undef, i64 %indvar.addone, i32 0
  %indvar.addone.broadcast = shufflevector<16 x i64> %indvar.addone.vec, <16 x i64> undef, <16 x i32> zeroinitializer
  %vec.result = add <16 x i64> %vec.tmp0, %indvar.addone.broadcast
  store <16 x i64> %vec.result, <16 x i64>* %ptr.cast, align 8
  %indvar.next = add i64 %indvar, 16
  %indvar.cond = icmp slt i64 %indvar.next, %n
  br i1 %indvar.cond, label %loop.body, label %loop.end
loop.end:
  ret void
}
