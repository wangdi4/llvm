
; RUN: opt -passes="hir-ssa-deconstruction,hir-temp-cleanup,hir-pre-vec-complete-unroll,hir-post-vec-complete-unroll,print<hir>" 2>&1 < %s | FileCheck %s

; Verify that the loop is not completely unrolled. Previously, the loop was unrolled because long addition terms passed to llvm.fsh* intrinsics did not
; sufficiently contribute to the cost function. For example, %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93, increased the cost only by 1. There are 6
; separate non-linear addends, thus 5 additions.

; CHECK-NOT: BEGIN REGION {modified}

;      + DO i1 = 0, 9, 1   <DO_LOOP>
;      |   %tmp96.out = %tmp96;
;      |   %tmp95.out = %tmp95;
;      |   %tmp94.out = %tmp94;
;      |   %tmp93.out = %tmp93;
;      |   %tmp101 = @llvm.fshl.i64(%tmp99,  %tmp99,  50);
;      |   %tmp102 = @llvm.fshl.i64(%tmp99,  %tmp99,  46);
;      |   %tmp103 = %tmp101  ^  %tmp102;
;      |   %tmp104 = @llvm.fshl.i64(%tmp99,  %tmp99,  23);
;      |   %tmp105 = %tmp103  ^  %tmp104;
;      |   %tmp107 = %tmp97  ^  %tmp98;
;      |   %tmp108 = %tmp107  &  %tmp99;
;      |   %tmp109 = %tmp108  ^  %tmp97;
;      |   %tmp111 = (@global)[0][8 * i1];
;      |   %tmp113 = (%tmp2)[0][8 * i1];
;      |   %tmp117 = @llvm.fshl.i64(%tmp96,  %tmp96,  36);
;      |   %tmp118 = @llvm.fshl.i64(%tmp96,  %tmp96,  30);
;      |   %tmp119 = %tmp117  ^  %tmp118;
;      |   %tmp120 = @llvm.fshl.i64(%tmp96,  %tmp96,  25);
;      |   %tmp121 = %tmp119  ^  %tmp120;
;      |   %tmp122 = %tmp95  |  %tmp96;
;      |   %tmp123 = %tmp122  &  %tmp94;
;      |   %tmp124 = %tmp95  &  %tmp96;
;      |   %tmp125 = %tmp123  |  %tmp124;
;      |   %tmp129 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  50);
;      |   %tmp130 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  46);
;      |   %tmp131 = %tmp129  ^  %tmp130;
;      |   %tmp132 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93,  23);
;      |   %tmp133 = %tmp131  ^  %tmp132;
;      |   %tmp134 = %tmp98  ^  %tmp99;
;      |   %tmp135 = %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93  &  %tmp134;
;      |   %tmp136 = %tmp135  ^  %tmp98;
;      |   %tmp139 = (@global)[0][8 * i1 + 1];
;      |   %tmp141 = (%tmp2)[0][8 * i1 + 1];
;      |   %tmp146 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  36);
;      |   %tmp147 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  30);
;      |   %tmp148 = %tmp146  ^  %tmp147;
;      |   %tmp149 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100,  25);
;      |   %tmp150 = %tmp148  ^  %tmp149;
;      |   %tmp151 = %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100  |  %tmp96;
;      |   %tmp152 = %tmp151  &  %tmp95;
;      |   %tmp153 = %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100  &  %tmp96;
;      |   %tmp154 = %tmp152  |  %tmp153;
;      |   %tmp158 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  50);
;      |   %tmp159 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  46);
;      |   %tmp160 = %tmp158  ^  %tmp159;
;      |   %tmp161 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97,  23);
;      |   %tmp162 = %tmp160  ^  %tmp161;
;      |   %tmp163 = %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93  ^  %tmp99;
;      |   %tmp164 = %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97  &  %tmp163;
;      |   %tmp165 = %tmp164  ^  %tmp99;
;      |   %tmp168 = (@global)[0][8 * i1 + 2];
;      |   %tmp170 = (%tmp2)[0][8 * i1 + 2];
;      |   %tmp175 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  36);
;      |   %tmp176 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  30);
;      |   %tmp177 = %tmp175  ^  %tmp176;
;      |   %tmp178 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97,  25);
;      |   %tmp179 = %tmp177  ^  %tmp178;
;      |   %tmp180 = %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97  |  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100;
;      |   %tmp181 = %tmp180  &  %tmp96;
;      |   %tmp182 = %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97  &  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100;
;      |   %tmp183 = %tmp181  |  %tmp182;
;      |   %tmp187 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  50);
;      |   %tmp188 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  46);
;      |   %tmp189 = %tmp187  ^  %tmp188;
;      |   %tmp190 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95,  23);
;      |   %tmp191 = %tmp189  ^  %tmp190;
;      |   %tmp192 = %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97  ^  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93;
;      |   %tmp193 = %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95  &  %tmp192;
;      |   %tmp194 = %tmp193  ^  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp100 + %tmp93;
;      |   %tmp197 = (@global)[0][8 * i1 + 3];
;      |   %tmp199 = (%tmp2)[0][8 * i1 + 3];
;      |   %tmp204 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  36);
;      |   %tmp205 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  30);
;      |   %tmp206 = %tmp204  ^  %tmp205;
;      |   %tmp207 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98,  25);
;      |   %tmp208 = %tmp206  ^  %tmp207;
;      |   %tmp209 = %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98  |  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97;
;      |   %tmp210 = %tmp209  &  %tmp111 + %tmp113 + %tmp109 + %tmp105 + %tmp125 + %tmp121 + %tmp100;
;      |   %tmp211 = %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98  &  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97;
;      |   %tmp212 = %tmp210  |  %tmp211;
;      |   %tmp216 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  50);
;      |   %tmp217 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  46);
;      |   %tmp218 = %tmp216  ^  %tmp217;
;      |   %tmp219 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99,  23);
;      |   %tmp220 = %tmp218  ^  %tmp219;
;      |   %tmp221 = %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95  ^  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97;
;      |   %tmp222 = %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99  &  %tmp221;
;      |   %tmp223 = %tmp222  ^  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp94 + %tmp97;
;      |   %tmp226 = (@global)[0][8 * i1 + 4];
;      |   %tmp228 = (%tmp2)[0][8 * i1 + 4];
;      |   %tmp233 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  36);
;      |   %tmp234 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  30);
;      |   %tmp235 = %tmp233  ^  %tmp234;
;      |   %tmp236 = @llvm.fshl.i64(%tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99,  25);
;      |   %tmp237 = %tmp235  ^  %tmp236;
;      |   %tmp238 = %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99  |  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98;
;      |   %tmp239 = %tmp238  &  %tmp139 + %tmp141 + %tmp136 + %tmp133 + %tmp154 + %tmp150 + %tmp97;
;      |   %tmp240 = %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99  &  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98;
;      |   %tmp241 = %tmp239  |  %tmp240;
;      |   (%tmp243.out)[0] = 2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93;
;      |   (%tmp244.out)[0] = %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93;
;      |   %tmp245 = @llvm.fshl.i64(2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  50);
;      |   %tmp246 = @llvm.fshl.i64(2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  46);
;      |   %tmp247 = %tmp245  ^  %tmp246;
;      |   %tmp248 = @llvm.fshl.i64(2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93,  23);
;      |   %tmp249 = %tmp247  ^  %tmp248;
;      |   %tmp250 = %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99  ^  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95;
;      |   %tmp251 = 2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93  &  %tmp250;
;      |   %tmp252 = %tmp251  ^  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp98 + %tmp95;
;      |   %tmp255 = (@global)[0][8 * i1 + 5];
;      |   %tmp257 = (%tmp2)[0][8 * i1 + 5];
;      |   %tmp262 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  36);
;      |   %tmp263 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  30);
;      |   %tmp264 = %tmp262  ^  %tmp263;
;      |   %tmp265 = @llvm.fshl.i64(%tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93,  25);
;      |   %tmp266 = %tmp264  ^  %tmp265;
;      |   %tmp267 = %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93  |  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99;
;      |   %tmp268 = %tmp267  &  %tmp168 + %tmp170 + %tmp165 + %tmp162 + %tmp183 + %tmp179 + %tmp98;
;      |   %tmp269 = %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93  &  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99;
;      |   %tmp270 = %tmp268  |  %tmp269;
;      |   (%tmp272.out)[0] = 2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97;
;      |   (%tmp273.out)[0] = %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97;
;      |   %tmp274 = @llvm.fshl.i64(2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  50);
;      |   %tmp275 = @llvm.fshl.i64(2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  46);
;      |   %tmp276 = %tmp274  ^  %tmp275;
;      |   %tmp277 = @llvm.fshl.i64(2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97,  23);
;      |   %tmp278 = %tmp276  ^  %tmp277;
;      |   %tmp279 = 2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93  ^  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99;
;      |   %tmp280 = 2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97  &  %tmp279;
;      |   %tmp281 = %tmp280  ^  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp96 + %tmp99;
;      |   %tmp284 = (@global)[0][8 * i1 + 6];
;      |   %tmp286 = (%tmp2)[0][8 * i1 + 6];
;      |   %tmp291 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  36);
;      |   %tmp292 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  30);
;      |   %tmp293 = %tmp291  ^  %tmp292;
;      |   %tmp294 = @llvm.fshl.i64(%tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97,  25);
;      |   %tmp295 = %tmp293  ^  %tmp294;
;      |   %tmp296 = %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97  |  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93;
;      |   %tmp297 = %tmp296  &  %tmp197 + %tmp199 + %tmp194 + %tmp191 + %tmp212 + %tmp208 + %tmp99;
;      |   %tmp298 = %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97  &  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93;
;      |   %tmp299 = %tmp297  |  %tmp298;
;      |   (%tmp301.out)[0] = 2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95;
;      |   (%tmp302.out)[0] = %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95;
;      |   %tmp303 = @llvm.fshl.i64(2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  50);
;      |   %tmp304 = @llvm.fshl.i64(2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  46);
;      |   %tmp305 = %tmp303  ^  %tmp304;
;      |   %tmp306 = @llvm.fshl.i64(2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95,  23);
;      |   %tmp307 = %tmp305  ^  %tmp306;
;      |   %tmp308 = 2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94 + 2 * %tmp97  ^  2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93;
;      |   %tmp309 = 2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + 2 * %tmp98 + %tmp95  &  %tmp308;
;      |   %tmp310 = %tmp309  ^  2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + 2 * %tmp100 + %tmp93;
;      |   %tmp313 = (@global)[0][8 * i1 + 7];
;      |   %tmp315 = (%tmp2)[0][8 * i1 + 7];
;      |   %tmp320 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  36);
;      |   %tmp321 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  30);
;      |   %tmp322 = %tmp320  ^  %tmp321;
;      |   %tmp323 = @llvm.fshl.i64(%tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95,  25);
;      |   %tmp324 = %tmp322  ^  %tmp323;
;      |   %tmp325 = %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95  |  %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97;
;      |   %tmp326 = %tmp325  &  %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93;
;      |   %tmp327 = %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95  &  %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97;
;      |   %tmp328 = %tmp326  |  %tmp327;
;      |   (%tmp330.out)[0] = 2 * %tmp197 + 2 * %tmp199 + %tmp313 + %tmp315 + 2 * %tmp194 + 2 * %tmp191 + %tmp212 + %tmp310 + %tmp208 + %tmp307 + %tmp96 + 2 * %tmp99;
;      |   (%tmp331.out)[0] = %tmp197 + %tmp199 + %tmp313 + %tmp315 + %tmp194 + %tmp191 + %tmp310 + %tmp307 + %tmp328 + %tmp324 + %tmp96 + %tmp99;
;      |   %tmp93 = %tmp111 + %tmp113 + %tmp226 + %tmp228 + %tmp109 + %tmp105 + %tmp223 + %tmp220 + %tmp241 + %tmp237 + %tmp100 + %tmp93;
;      |   %tmp94 = %tmp139 + %tmp141 + %tmp255 + %tmp257 + %tmp136 + %tmp133 + %tmp252 + %tmp249 + %tmp270 + %tmp266 + %tmp94 + %tmp97;
;      |   %tmp95 = %tmp168 + %tmp170 + %tmp284 + %tmp286 + %tmp165 + %tmp162 + %tmp281 + %tmp278 + %tmp299 + %tmp295 + %tmp98 + %tmp95;
;      |   %tmp96 = %tmp197 + %tmp199 + %tmp313 + %tmp315 + %tmp194 + %tmp191 + %tmp310 + %tmp307 + %tmp328 + %tmp324 + %tmp96 + %tmp99;
;      |   %tmp97 = 2 * %tmp139 + 2 * %tmp141 + %tmp255 + %tmp257 + 2 * %tmp136 + 2 * %tmp133 + %tmp154 + %tmp252 + %tmp150 + %tmp249 + %tmp94.out + 2 * %tmp97;
;      |   %tmp98 = 2 * %tmp168 + 2 * %tmp170 + %tmp284 + %tmp286 + 2 * %tmp165 + 2 * %tmp162 + %tmp183 + %tmp281 + %tmp179 + %tmp278 + %tmp95.out + 2 * %tmp98;
;      |   %tmp99 = 2 * %tmp197 + 2 * %tmp199 + %tmp313 + %tmp315 + 2 * %tmp194 + 2 * %tmp191 + %tmp212 + %tmp310 + %tmp208 + %tmp307 + %tmp96.out + 2 * %tmp99;
;      |   %tmp100 = 2 * %tmp111 + 2 * %tmp113 + %tmp226 + %tmp228 + 2 * %tmp109 + 2 * %tmp105 + %tmp125 + %tmp223 + %tmp121 + %tmp220 + %tmp93.out + 2 * %tmp100;
;      + END LOOP

; ModuleID = 'renamed.ll'
source_filename = "ld-temp.o"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@global = external hidden unnamed_addr constant [80 x i64], align 16

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i64 @llvm.fshl.i64(i64, i64, i64) #0

; Function Attrs: nofree norecurse nounwind uwtable
define dso_local void @wibble.bb91(i64 %tmp51, i64 %tmp50, i64 %tmp49, i64 %tmp48, i64 %tmp47, i64 %tmp46, i64 %tmp45, i64 %tmp44, ptr %tmp2, ptr %tmp243.out, ptr %tmp244.out, ptr %tmp272.out, ptr %tmp273.out, ptr %tmp301.out, ptr %tmp302.out, ptr %tmp330.out, ptr %tmp331.out) #1 {
newFuncRoot:
  br label %bb91

bb91:                                             ; preds = %newFuncRoot, %bb91
  %tmp92 = phi i64 [ 0, %newFuncRoot ], [ %tmp332, %bb91 ]
  %tmp93 = phi i64 [ %tmp51, %newFuncRoot ], [ %tmp244, %bb91 ]
  %tmp94 = phi i64 [ %tmp50, %newFuncRoot ], [ %tmp273, %bb91 ]
  %tmp95 = phi i64 [ %tmp49, %newFuncRoot ], [ %tmp302, %bb91 ]
  %tmp96 = phi i64 [ %tmp48, %newFuncRoot ], [ %tmp331, %bb91 ]
  %tmp97 = phi i64 [ %tmp47, %newFuncRoot ], [ %tmp272, %bb91 ]
  %tmp98 = phi i64 [ %tmp46, %newFuncRoot ], [ %tmp301, %bb91 ]
  %tmp99 = phi i64 [ %tmp45, %newFuncRoot ], [ %tmp330, %bb91 ]
  %tmp100 = phi i64 [ %tmp44, %newFuncRoot ], [ %tmp243, %bb91 ]
  %tmp101 = tail call i64 @llvm.fshl.i64(i64 %tmp99, i64 %tmp99, i64 50)
  %tmp102 = tail call i64 @llvm.fshl.i64(i64 %tmp99, i64 %tmp99, i64 46)
  %tmp103 = xor i64 %tmp101, %tmp102
  %tmp104 = tail call i64 @llvm.fshl.i64(i64 %tmp99, i64 %tmp99, i64 23)
  %tmp105 = xor i64 %tmp103, %tmp104
  %tmp106 = add i64 %tmp105, %tmp100
  %tmp107 = xor i64 %tmp97, %tmp98
  %tmp108 = and i64 %tmp107, %tmp99
  %tmp109 = xor i64 %tmp108, %tmp97
  %tmp110 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp92, !intel-tbaa !8
  %tmp111 = load i64, ptr %tmp110, align 16, !tbaa !8
  %tmp112 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp92, !intel-tbaa !8
  %tmp113 = load i64, ptr %tmp112, align 16, !tbaa !8
  %tmp114 = add i64 %tmp106, %tmp111
  %tmp115 = add i64 %tmp114, %tmp113
  %tmp116 = add i64 %tmp115, %tmp109
  %tmp117 = tail call i64 @llvm.fshl.i64(i64 %tmp96, i64 %tmp96, i64 36)
  %tmp118 = tail call i64 @llvm.fshl.i64(i64 %tmp96, i64 %tmp96, i64 30)
  %tmp119 = xor i64 %tmp117, %tmp118
  %tmp120 = tail call i64 @llvm.fshl.i64(i64 %tmp96, i64 %tmp96, i64 25)
  %tmp121 = xor i64 %tmp119, %tmp120
  %tmp122 = or i64 %tmp95, %tmp96
  %tmp123 = and i64 %tmp122, %tmp94
  %tmp124 = and i64 %tmp95, %tmp96
  %tmp125 = or i64 %tmp123, %tmp124
  %tmp126 = add i64 %tmp93, %tmp116
  %tmp127 = add i64 %tmp116, %tmp121
  %tmp128 = add i64 %tmp127, %tmp125
  %tmp129 = tail call i64 @llvm.fshl.i64(i64 %tmp126, i64 %tmp126, i64 50)
  %tmp130 = tail call i64 @llvm.fshl.i64(i64 %tmp126, i64 %tmp126, i64 46)
  %tmp131 = xor i64 %tmp129, %tmp130
  %tmp132 = tail call i64 @llvm.fshl.i64(i64 %tmp126, i64 %tmp126, i64 23)
  %tmp133 = xor i64 %tmp131, %tmp132
  %tmp134 = xor i64 %tmp98, %tmp99
  %tmp135 = and i64 %tmp126, %tmp134
  %tmp136 = xor i64 %tmp135, %tmp98
  %tmp137 = or i64 %tmp92, 1
  %tmp138 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp137, !intel-tbaa !8
  %tmp139 = load i64, ptr %tmp138, align 8, !tbaa !8
  %tmp140 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp137, !intel-tbaa !8
  %tmp141 = load i64, ptr %tmp140, align 8, !tbaa !8
  %tmp142 = add i64 %tmp136, %tmp97
  %tmp143 = add i64 %tmp142, %tmp139
  %tmp144 = add i64 %tmp143, %tmp133
  %tmp145 = add i64 %tmp144, %tmp141
  %tmp146 = tail call i64 @llvm.fshl.i64(i64 %tmp128, i64 %tmp128, i64 36)
  %tmp147 = tail call i64 @llvm.fshl.i64(i64 %tmp128, i64 %tmp128, i64 30)
  %tmp148 = xor i64 %tmp146, %tmp147
  %tmp149 = tail call i64 @llvm.fshl.i64(i64 %tmp128, i64 %tmp128, i64 25)
  %tmp150 = xor i64 %tmp148, %tmp149
  %tmp151 = or i64 %tmp128, %tmp96
  %tmp152 = and i64 %tmp151, %tmp95
  %tmp153 = and i64 %tmp128, %tmp96
  %tmp154 = or i64 %tmp152, %tmp153
  %tmp155 = add i64 %tmp150, %tmp154
  %tmp156 = add i64 %tmp145, %tmp94
  %tmp157 = add i64 %tmp155, %tmp145
  %tmp158 = tail call i64 @llvm.fshl.i64(i64 %tmp156, i64 %tmp156, i64 50)
  %tmp159 = tail call i64 @llvm.fshl.i64(i64 %tmp156, i64 %tmp156, i64 46)
  %tmp160 = xor i64 %tmp158, %tmp159
  %tmp161 = tail call i64 @llvm.fshl.i64(i64 %tmp156, i64 %tmp156, i64 23)
  %tmp162 = xor i64 %tmp160, %tmp161
  %tmp163 = xor i64 %tmp126, %tmp99
  %tmp164 = and i64 %tmp156, %tmp163
  %tmp165 = xor i64 %tmp164, %tmp99
  %tmp166 = or i64 %tmp92, 2
  %tmp167 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp166, !intel-tbaa !8
  %tmp168 = load i64, ptr %tmp167, align 16, !tbaa !8
  %tmp169 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp166, !intel-tbaa !8
  %tmp170 = load i64, ptr %tmp169, align 16, !tbaa !8
  %tmp171 = add i64 %tmp168, %tmp98
  %tmp172 = add i64 %tmp171, %tmp170
  %tmp173 = add i64 %tmp172, %tmp165
  %tmp174 = add i64 %tmp173, %tmp162
  %tmp175 = tail call i64 @llvm.fshl.i64(i64 %tmp157, i64 %tmp157, i64 36)
  %tmp176 = tail call i64 @llvm.fshl.i64(i64 %tmp157, i64 %tmp157, i64 30)
  %tmp177 = xor i64 %tmp175, %tmp176
  %tmp178 = tail call i64 @llvm.fshl.i64(i64 %tmp157, i64 %tmp157, i64 25)
  %tmp179 = xor i64 %tmp177, %tmp178
  %tmp180 = or i64 %tmp157, %tmp128
  %tmp181 = and i64 %tmp180, %tmp96
  %tmp182 = and i64 %tmp157, %tmp128
  %tmp183 = or i64 %tmp181, %tmp182
  %tmp184 = add i64 %tmp179, %tmp183
  %tmp185 = add i64 %tmp174, %tmp95
  %tmp186 = add i64 %tmp184, %tmp174
  %tmp187 = tail call i64 @llvm.fshl.i64(i64 %tmp185, i64 %tmp185, i64 50)
  %tmp188 = tail call i64 @llvm.fshl.i64(i64 %tmp185, i64 %tmp185, i64 46)
  %tmp189 = xor i64 %tmp187, %tmp188
  %tmp190 = tail call i64 @llvm.fshl.i64(i64 %tmp185, i64 %tmp185, i64 23)
  %tmp191 = xor i64 %tmp189, %tmp190
  %tmp192 = xor i64 %tmp156, %tmp126
  %tmp193 = and i64 %tmp185, %tmp192
  %tmp194 = xor i64 %tmp193, %tmp126
  %tmp195 = or i64 %tmp92, 3
  %tmp196 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp195, !intel-tbaa !8
  %tmp197 = load i64, ptr %tmp196, align 8, !tbaa !8
  %tmp198 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp195, !intel-tbaa !8
  %tmp199 = load i64, ptr %tmp198, align 8, !tbaa !8
  %tmp200 = add i64 %tmp197, %tmp99
  %tmp201 = add i64 %tmp200, %tmp199
  %tmp202 = add i64 %tmp201, %tmp194
  %tmp203 = add i64 %tmp202, %tmp191
  %tmp204 = tail call i64 @llvm.fshl.i64(i64 %tmp186, i64 %tmp186, i64 36)
  %tmp205 = tail call i64 @llvm.fshl.i64(i64 %tmp186, i64 %tmp186, i64 30)
  %tmp206 = xor i64 %tmp204, %tmp205
  %tmp207 = tail call i64 @llvm.fshl.i64(i64 %tmp186, i64 %tmp186, i64 25)
  %tmp208 = xor i64 %tmp206, %tmp207
  %tmp209 = or i64 %tmp186, %tmp157
  %tmp210 = and i64 %tmp209, %tmp128
  %tmp211 = and i64 %tmp186, %tmp157
  %tmp212 = or i64 %tmp210, %tmp211
  %tmp213 = add i64 %tmp208, %tmp212
  %tmp214 = add i64 %tmp203, %tmp96
  %tmp215 = add i64 %tmp213, %tmp203
  %tmp216 = tail call i64 @llvm.fshl.i64(i64 %tmp214, i64 %tmp214, i64 50)
  %tmp217 = tail call i64 @llvm.fshl.i64(i64 %tmp214, i64 %tmp214, i64 46)
  %tmp218 = xor i64 %tmp216, %tmp217
  %tmp219 = tail call i64 @llvm.fshl.i64(i64 %tmp214, i64 %tmp214, i64 23)
  %tmp220 = xor i64 %tmp218, %tmp219
  %tmp221 = xor i64 %tmp185, %tmp156
  %tmp222 = and i64 %tmp214, %tmp221
  %tmp223 = xor i64 %tmp222, %tmp156
  %tmp224 = or i64 %tmp92, 4
  %tmp225 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp224, !intel-tbaa !8
  %tmp226 = load i64, ptr %tmp225, align 16, !tbaa !8
  %tmp227 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp224, !intel-tbaa !8
  %tmp228 = load i64, ptr %tmp227, align 16, !tbaa !8
  %tmp229 = add i64 %tmp226, %tmp126
  %tmp230 = add i64 %tmp229, %tmp228
  %tmp231 = add i64 %tmp230, %tmp223
  %tmp232 = add i64 %tmp231, %tmp220
  %tmp233 = tail call i64 @llvm.fshl.i64(i64 %tmp215, i64 %tmp215, i64 36)
  %tmp234 = tail call i64 @llvm.fshl.i64(i64 %tmp215, i64 %tmp215, i64 30)
  %tmp235 = xor i64 %tmp233, %tmp234
  %tmp236 = tail call i64 @llvm.fshl.i64(i64 %tmp215, i64 %tmp215, i64 25)
  %tmp237 = xor i64 %tmp235, %tmp236
  %tmp238 = or i64 %tmp215, %tmp186
  %tmp239 = and i64 %tmp238, %tmp157
  %tmp240 = and i64 %tmp215, %tmp186
  %tmp241 = or i64 %tmp239, %tmp240
  %tmp242 = add i64 %tmp237, %tmp241
  %tmp243 = add i64 %tmp232, %tmp128
  store i64 %tmp243, ptr %tmp243.out, align 8
  %tmp244 = add i64 %tmp242, %tmp232
  store i64 %tmp244, ptr %tmp244.out, align 8
  %tmp245 = tail call i64 @llvm.fshl.i64(i64 %tmp243, i64 %tmp243, i64 50)
  %tmp246 = tail call i64 @llvm.fshl.i64(i64 %tmp243, i64 %tmp243, i64 46)
  %tmp247 = xor i64 %tmp245, %tmp246
  %tmp248 = tail call i64 @llvm.fshl.i64(i64 %tmp243, i64 %tmp243, i64 23)
  %tmp249 = xor i64 %tmp247, %tmp248
  %tmp250 = xor i64 %tmp214, %tmp185
  %tmp251 = and i64 %tmp243, %tmp250
  %tmp252 = xor i64 %tmp251, %tmp185
  %tmp253 = or i64 %tmp92, 5
  %tmp254 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp253, !intel-tbaa !8
  %tmp255 = load i64, ptr %tmp254, align 8, !tbaa !8
  %tmp256 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp253, !intel-tbaa !8
  %tmp257 = load i64, ptr %tmp256, align 8, !tbaa !8
  %tmp258 = add i64 %tmp255, %tmp156
  %tmp259 = add i64 %tmp258, %tmp257
  %tmp260 = add i64 %tmp259, %tmp252
  %tmp261 = add i64 %tmp260, %tmp249
  %tmp262 = tail call i64 @llvm.fshl.i64(i64 %tmp244, i64 %tmp244, i64 36)
  %tmp263 = tail call i64 @llvm.fshl.i64(i64 %tmp244, i64 %tmp244, i64 30)
  %tmp264 = xor i64 %tmp262, %tmp263
  %tmp265 = tail call i64 @llvm.fshl.i64(i64 %tmp244, i64 %tmp244, i64 25)
  %tmp266 = xor i64 %tmp264, %tmp265
  %tmp267 = or i64 %tmp244, %tmp215
  %tmp268 = and i64 %tmp267, %tmp186
  %tmp269 = and i64 %tmp244, %tmp215
  %tmp270 = or i64 %tmp268, %tmp269
  %tmp271 = add i64 %tmp266, %tmp270
  %tmp272 = add i64 %tmp261, %tmp157
  store i64 %tmp272, ptr %tmp272.out, align 8
  %tmp273 = add i64 %tmp271, %tmp261
  store i64 %tmp273, ptr %tmp273.out, align 8
  %tmp274 = tail call i64 @llvm.fshl.i64(i64 %tmp272, i64 %tmp272, i64 50)
  %tmp275 = tail call i64 @llvm.fshl.i64(i64 %tmp272, i64 %tmp272, i64 46)
  %tmp276 = xor i64 %tmp274, %tmp275
  %tmp277 = tail call i64 @llvm.fshl.i64(i64 %tmp272, i64 %tmp272, i64 23)
  %tmp278 = xor i64 %tmp276, %tmp277
  %tmp279 = xor i64 %tmp243, %tmp214
  %tmp280 = and i64 %tmp272, %tmp279
  %tmp281 = xor i64 %tmp280, %tmp214
  %tmp282 = or i64 %tmp92, 6
  %tmp283 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp282, !intel-tbaa !8
  %tmp284 = load i64, ptr %tmp283, align 16, !tbaa !8
  %tmp285 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp282, !intel-tbaa !8
  %tmp286 = load i64, ptr %tmp285, align 16, !tbaa !8
  %tmp287 = add i64 %tmp284, %tmp185
  %tmp288 = add i64 %tmp287, %tmp286
  %tmp289 = add i64 %tmp288, %tmp281
  %tmp290 = add i64 %tmp289, %tmp278
  %tmp291 = tail call i64 @llvm.fshl.i64(i64 %tmp273, i64 %tmp273, i64 36)
  %tmp292 = tail call i64 @llvm.fshl.i64(i64 %tmp273, i64 %tmp273, i64 30)
  %tmp293 = xor i64 %tmp291, %tmp292
  %tmp294 = tail call i64 @llvm.fshl.i64(i64 %tmp273, i64 %tmp273, i64 25)
  %tmp295 = xor i64 %tmp293, %tmp294
  %tmp296 = or i64 %tmp273, %tmp244
  %tmp297 = and i64 %tmp296, %tmp215
  %tmp298 = and i64 %tmp273, %tmp244
  %tmp299 = or i64 %tmp297, %tmp298
  %tmp300 = add i64 %tmp295, %tmp299
  %tmp301 = add i64 %tmp290, %tmp186
  store i64 %tmp301, ptr %tmp301.out, align 8
  %tmp302 = add i64 %tmp300, %tmp290
  store i64 %tmp302, ptr %tmp302.out, align 8
  %tmp303 = tail call i64 @llvm.fshl.i64(i64 %tmp301, i64 %tmp301, i64 50)
  %tmp304 = tail call i64 @llvm.fshl.i64(i64 %tmp301, i64 %tmp301, i64 46)
  %tmp305 = xor i64 %tmp303, %tmp304
  %tmp306 = tail call i64 @llvm.fshl.i64(i64 %tmp301, i64 %tmp301, i64 23)
  %tmp307 = xor i64 %tmp305, %tmp306
  %tmp308 = xor i64 %tmp272, %tmp243
  %tmp309 = and i64 %tmp301, %tmp308
  %tmp310 = xor i64 %tmp309, %tmp243
  %tmp311 = or i64 %tmp92, 7
  %tmp312 = getelementptr inbounds [80 x i64], ptr @global, i64 0, i64 %tmp311, !intel-tbaa !8
  %tmp313 = load i64, ptr %tmp312, align 8, !tbaa !8
  %tmp314 = getelementptr inbounds [80 x i64], ptr %tmp2, i64 0, i64 %tmp311, !intel-tbaa !8
  %tmp315 = load i64, ptr %tmp314, align 8, !tbaa !8
  %tmp316 = add i64 %tmp313, %tmp214
  %tmp317 = add i64 %tmp316, %tmp315
  %tmp318 = add i64 %tmp317, %tmp310
  %tmp319 = add i64 %tmp318, %tmp307
  %tmp320 = tail call i64 @llvm.fshl.i64(i64 %tmp302, i64 %tmp302, i64 36)
  %tmp321 = tail call i64 @llvm.fshl.i64(i64 %tmp302, i64 %tmp302, i64 30)
  %tmp322 = xor i64 %tmp320, %tmp321
  %tmp323 = tail call i64 @llvm.fshl.i64(i64 %tmp302, i64 %tmp302, i64 25)
  %tmp324 = xor i64 %tmp322, %tmp323
  %tmp325 = or i64 %tmp302, %tmp273
  %tmp326 = and i64 %tmp325, %tmp244
  %tmp327 = and i64 %tmp302, %tmp273
  %tmp328 = or i64 %tmp326, %tmp327
  %tmp329 = add i64 %tmp324, %tmp328
  %tmp330 = add i64 %tmp319, %tmp215
  store i64 %tmp330, ptr %tmp330.out, align 8
  %tmp331 = add i64 %tmp329, %tmp319
  store i64 %tmp331, ptr %tmp331.out, align 8
  %tmp332 = add nuw nsw i64 %tmp92, 8
  %tmp333 = icmp ult i64 %tmp92, 72
  br i1 %tmp333, label %bb91, label %bb82.exitStub, !llvm.loop !13

bb82.exitStub:                                    ; preds = %bb91
  ret void
}

attributes #0 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #1 = { nofree norecurse nounwind uwtable "denormal-fp-math"="preserve-sign,preserve-sign" "denormal-fp-math-f32"="ieee,ieee" "frame-pointer"="none" "loopopt-pipeline"="full" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "pre_loopopt" "stack-protector-buffer-size"="8" "target-cpu"="skylake-avx512" "target-features"="+adx,+aes,+avx,+avx2,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512vl,+bmi,+bmi2,+clflushopt,+clwb,+crc32,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+pku,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "unsafe-fp-math"="true" }

!llvm.ident = !{!0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0}
!nvvm.annotations = !{}
!llvm.module.flags = !{!1, !2, !3, !4, !5, !6, !7}

!0 = !{!"Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.x.0.YYYYMMDD)"}
!1 = !{i32 1, !"wchar_size", i32 4}
!2 = !{i32 1, !"Virtual Function Elim", i32 0}
!3 = !{i32 7, !"openmp", i32 50}
!4 = !{i32 7, !"uwtable", i32 1}
!5 = !{i32 1, !"ThinLTO", i32 0}
!6 = !{i32 1, !"EnableSplitLTOUnit", i32 1}
!7 = !{i32 1, !"LTOPostLink", i32 1}
!8 = !{!9, !10, i64 0}
!9 = !{!"array@_ZTSA80_m", !10, i64 0}
!10 = !{!"long", !11, i64 0}
!11 = !{!"omnipotent char", !12, i64 0}
!12 = !{!"Simple C/C++ TBAA"}
!13 = distinct !{!13, !14}
!14 = !{!"llvm.loop.mustprogress"}
