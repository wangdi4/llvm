; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
;Check whether return value is widened correctly.

; RUN: opt -vec-clone -S < %s | FileCheck %s

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: noinline nounwind optnone uwtable mustprogress
define dso_local i32 @_Z11shift_rows4ji(i32 %v, i32 %n) #0 {
; CHECK:  define dso_local <8 x i32> @_ZGVbN8vu__Z11shift_rows4ji(<8 x i32> [[V0:%.*]], i32 [[N0:%.*]]) #1 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ALLOCA_N0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    store i32 [[N0]], i32* [[ALLOCA_N0]], align 4
; CHECK-NEXT:    [[VEC_V0:%.*]] = alloca <8 x i32>, align 32
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <8 x i32>, align 32
; CHECK-NEXT:    [[V_ADDR0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    [[N_ADDR0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    [[I0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    [[VEC_V_CAST0:%.*]] = bitcast <8 x i32>* [[VEC_V0]] to i32*
; CHECK-NEXT:    store <8 x i32> [[V0]], <8 x i32>* [[VEC_V0]], align 32
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <8 x i32>* [[VEC_RETVAL0]] to i32*
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.begin.region:
; CHECK-NEXT:    [[ENTRY_REGION0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 8), "QUAL.OMP.UNIFORM:TYPED"(i32* [[ALLOCA_N0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(i32* [[V_ADDR0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(i32* [[N_ADDR0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(i32* [[I0]], i32 0, i32 1) ]
; CHECK-NEXT:    br label [[SIMD_LOOP_PREHEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.preheader:
; CHECK-NEXT:    [[LOAD_N0:%.*]] = load i32, i32* [[ALLOCA_N0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_HEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.header:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_LOOP_PREHEADER0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_LATCH0:%.*]] ]
; CHECK-NEXT:    [[VEC_V_CAST_GEP0:%.*]] = getelementptr i32, i32* [[VEC_V_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_V_ELEM0:%.*]] = load i32, i32* [[VEC_V_CAST_GEP0]], align 4
; CHECK-NEXT:    store i32 [[VEC_V_ELEM0]], i32* [[V_ADDR0]], align 4
; CHECK-NEXT:    store i32 0, i32* [[I0]], align 4
; CHECK-NEXT:    br label [[FOR_COND0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.cond:
; CHECK-NEXT:    [[TMP0:%.*]] = load i32, i32* [[I0]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[N_ADDR0]], align 4
; CHECK-NEXT:    [[CMP0:%.*]] = icmp slt i32 [[TMP0]], [[TMP1]]
; CHECK-NEXT:    br i1 [[CMP0]], label [[FOR_BODY0:%.*]], label [[FOR_END0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.body:
; CHECK-NEXT:    [[TMP2:%.*]] = load i32, i32* [[V_ADDR0]], align 4
; CHECK-NEXT:    [[SHR0:%.*]] = lshr i32 [[TMP2]], 8
; CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32* [[V_ADDR0]], align 4
; CHECK-NEXT:    [[SHL0:%.*]] = shl i32 [[TMP3]], 24
; CHECK-NEXT:    [[OR0:%.*]] = or i32 [[SHR0]], [[SHL0]]
; CHECK-NEXT:    store i32 [[OR0]], i32* [[V_ADDR0]], align 4
; CHECK-NEXT:    br label [[FOR_INC0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.inc:
; CHECK-NEXT:    [[TMP4:%.*]] = load i32, i32* [[I0]], align 4
; CHECK-NEXT:    [[INC0:%.*]] = add nsw i32 [[TMP4]], 1
; CHECK-NEXT:    store i32 [[INC0]], i32* [[I0]], align 4
; CHECK-NEXT:    br label [[FOR_COND0]]
; CHECK-EMPTY:
; CHECK-NEXT:  for.end:
; CHECK-NEXT:    [[TMP5:%.*]] = load i32, i32* [[V_ADDR0]], align 4
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr i32, i32* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store i32 [[TMP5]], i32* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.latch:
; CHECK-NEXT:    [[INDVAR0:%.*]] = add nuw i32 [[INDEX0]], 1
; CHECK-NEXT:    [[VL_COND0:%.*]] = icmp ult i32 [[INDVAR0]], 8
; CHECK-NEXT:    br i1 [[VL_COND0]], label [[SIMD_LOOP_HEADER0]], label [[SIMD_END_REGION0:%.*]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  simd.end.region:
; CHECK-NEXT:    call void @llvm.directive.region.exit(token [[ENTRY_REGION0]]) [ "DIR.OMP.END.SIMD"() ]
; CHECK-NEXT:    br label [[RETURN0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  return:
; CHECK-NEXT:    [[VEC_RET_CAST0:%.*]] = bitcast i32* [[RET_CAST0]] to <8 x i32>*
; CHECK-NEXT:    [[VEC_RET0:%.*]] = load <8 x i32>, <8 x i32>* [[VEC_RET_CAST0]], align 32
; CHECK-NEXT:    ret <8 x i32> [[VEC_RET0]]
; CHECK-NEXT:  }
;
entry:
  %v.addr = alloca i32, align 4
  %n.addr = alloca i32, align 4
  %i = alloca i32, align 4
  store i32 %v, i32* %v.addr, align 4
  store i32 0, i32* %i, align 4
  br label %for.cond

for.cond:                                         ; preds = %for.inc, %entry
  %0 = load i32, i32* %i, align 4
  %1 = load i32, i32* %n.addr
  %cmp = icmp slt i32 %0, %1
  br i1 %cmp, label %for.body, label %for.end

for.body:                                         ; preds = %for.cond
  %2 = load i32, i32* %v.addr
  %shr = lshr i32 %2, 8
  %3 = load i32, i32* %v.addr, align 4
  %shl = shl i32 %3, 24
  %or = or i32 %shr, %shl
  store i32 %or, i32* %v.addr, align 4
  br label %for.inc

for.inc:                                          ; preds = %for.body
  %4 = load i32, i32* %i, align 4
  %inc = add nsw i32 %4, 1
  store i32 %inc, i32* %i, align 4
  br label %for.cond

for.end:                                          ; preds = %for.cond
  %5 = load i32, i32* %v.addr, align 4
  ret i32 %5
}

attributes #0 = { noinline nounwind "vector-variants"="_ZGVbN8vu__Z11shift_rows4ji" }
