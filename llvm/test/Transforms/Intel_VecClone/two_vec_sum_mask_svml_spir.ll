; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Do a sanity check on the structure of the LLVM that VecClone produces for the masked variant for SPIR.

; RUN: opt -vec-clone -S < %s | FileCheck %s
; RUN: opt -passes="vec-clone" -S < %s | FileCheck %s

; ModuleID = 'two_vec_sum.c'
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "spir64"

; Function Attrs: nounwind uwtable
define float @__svml_device_add(float %i, float %j) #0 {
; CHECK:  define float @__svml_device_add(float [[I0:%.*]], float [[J0:%.*]]) #0 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[I_ADDR0:%.*]] = alloca float, align 4
; CHECK-NEXT:    [[J_ADDR0:%.*]] = alloca float, align 4
; CHECK-NEXT:    store float [[I0]], float* [[I_ADDR0]], align 4
; CHECK-NEXT:    store float [[J0]], float* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = load float, float* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[ADD0:%.*]] = fadd float [[TMP0]], [[TMP1]]
; CHECK-NEXT:    ret float [[ADD0]]
; CHECK-NEXT:  }
;
; CHECK:  define <4 x float> @_ZGVxM4vv___svml_device_add(<4 x float> [[I0]], <4 x float> [[J0]], <4 x i32> [[MASK0:%.*]])
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VEC_I0:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[VEC_J0:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[VEC_MASK0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <4 x float>, align 16
; CHECK-NEXT:    [[I_ADDR0]] = alloca float, align 4
; CHECK-NEXT:    [[J_ADDR0]] = alloca float, align 4
; CHECK-NEXT:    [[VEC_I_CAST0:%.*]] = bitcast <4 x float>* [[VEC_I0]] to float*
; CHECK-NEXT:    store <4 x float> [[I0]], <4 x float>* [[VEC_I0]], align 16
; CHECK-NEXT:    [[VEC_J_CAST0:%.*]] = bitcast <4 x float>* [[VEC_J0]] to float*
; CHECK-NEXT:    store <4 x float> [[J0]], <4 x float>* [[VEC_J0]], align 16
; CHECK-NEXT:    [[MASK_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_MASK0]] to i32*
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <4 x float>* [[VEC_RETVAL0]] to float*
; CHECK-NEXT:    store <4 x i32> [[MASK0]], <4 x i32>* [[VEC_MASK0]], align 16
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.begin.region:
; CHECK-NEXT:    [[ENTRY_REGION0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4), "QUAL.OMP.PRIVATE"(float* [[I_ADDR0]]), "QUAL.OMP.PRIVATE"(float* [[J_ADDR0]]) ]
; CHECK-NEXT:    br label [[SIMD_LOOP_PREHEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.preheader:
; CHECK-NEXT:    br label [[SIMD_LOOP_HEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.header:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_LOOP_PREHEADER0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_LATCH0:%.*]] ]
; CHECK-NEXT:    [[MASK_GEP0:%.*]] = getelementptr i32, i32* [[MASK_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[MASK_PARM0:%.*]] = load i32, i32* [[MASK_GEP0]], align 4
; CHECK-NEXT:    [[MASK_COND0:%.*]] = icmp ne i32 [[MASK_PARM0]], 0
; CHECK-NEXT:    br i1 [[MASK_COND0]], label [[SIMD_LOOP_THEN0:%.*]], label [[SIMD_LOOP_ELSE0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.then:
; CHECK-NEXT:    [[VEC_I_CAST_GEP0:%.*]] = getelementptr float, float* [[VEC_I_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_I_ELEM0:%.*]] = load float, float* [[VEC_I_CAST_GEP0]], align 4
; CHECK-NEXT:    store float [[VEC_I_ELEM0]], float* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[VEC_J_CAST_GEP0:%.*]] = getelementptr float, float* [[VEC_J_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_J_ELEM0:%.*]] = load float, float* [[VEC_J_CAST_GEP0]], align 4
; CHECK-NEXT:    store float [[VEC_J_ELEM0]], float* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[TMP0]] = load float, float* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[TMP1]] = load float, float* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[ADD0]] = fadd float [[TMP0]], [[TMP1]]
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr float, float* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store float [[ADD0]], float* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.else:
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.latch:
; CHECK-NEXT:    [[INDVAR0]] = add nuw i32 [[INDEX0]], 1
; CHECK-NEXT:    [[VL_COND0:%.*]] = icmp ult i32 [[INDVAR0]], 4
; CHECK-NEXT:    br i1 [[VL_COND0]], label [[SIMD_LOOP_HEADER0]], label [[SIMD_END_REGION0:%.*]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  simd.end.region:
; CHECK-NEXT:    call void @llvm.directive.region.exit(token [[ENTRY_REGION0]]) [ "DIR.OMP.END.SIMD"() ]
; CHECK-NEXT:    br label [[RETURN0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  return:
; CHECK-NEXT:    [[VEC_RET_CAST0:%.*]] = bitcast float* [[RET_CAST0]] to <4 x float>*
; CHECK-NEXT:    [[VEC_RET0:%.*]] = load <4 x float>, <4 x float>* [[VEC_RET_CAST0]], align 16
; CHECK-NEXT:    ret <4 x float> [[VEC_RET0]]
; CHECK-NEXT:  }
;
entry:
  %i.addr = alloca float, align 4
  %j.addr = alloca float, align 4
  store float %i, float* %i.addr, align 4
  store float %j, float* %j.addr, align 4
  %0 = load float, float* %i.addr, align 4
  %1 = load float, float* %j.addr, align 4
  %add = fadd float %0, %1
  ret float %add
}

attributes #0 = { nounwind uwtable "vector-variants"="_ZGVxM4vv___svml_device_add" }
