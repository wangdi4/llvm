; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -passes="vec-clone" -S < %s | FileCheck %s

; Check code generated from VecClone for val vector variant for unoptimized LLVM
; Stride should be applied to value loaded from lane 0 of the vector of pointers

; Function Attrs: mustprogress noinline nounwind optnone uwtable
define dso_local noundef i64 @_Z3fooRl(i64* noundef nonnull align 8 dereferenceable(8) %x) #0 {
;
; CHECK:  define dso_local noundef <8 x i64> @_ZGVbN8L2__Z3fooRl(<8 x i64*> noundef nonnull align 8 dereferenceable(8) [[X0:%.*]]) #1 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[X_EXT0:%.*]] = extractelement <8 x i64*> [[X0]], i64 0
; CHECK-NEXT:    [[ALLOCA_X_SCALAR0:%.*]] = alloca i64*, align 8
; CHECK-NEXT:    store i64* [[X_EXT0]], i64** [[ALLOCA_X_SCALAR0]], align 8
; CHECK-NEXT:    [[VEC_X0:%.*]] = alloca <8 x i64*>, align 64
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <8 x i64>, align 64
; CHECK-NEXT:    [[X_ADDR0:%.*]] = alloca i64*, align 8
; CHECK-NEXT:    [[VEC_X_CAST0:%.*]] = bitcast <8 x i64*>* [[VEC_X0]] to i64**
; CHECK-NEXT:    store <8 x i64*> [[X0]], <8 x i64*>* [[VEC_X0]], align 64
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <8 x i64>* [[VEC_RETVAL0]] to i64*
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.begin.region:
; CHECK-NEXT:    [[ENTRY_REGION0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 8), "QUAL.OMP.LINEAR:TYPED"(i64** [[ALLOCA_X_SCALAR0]], i64* null, i32 1, i32 2), "QUAL.OMP.PRIVATE:TYPED"(i64** [[X_ADDR0]], i64* null, i32 1) ]
; CHECK-NEXT:    br label [[SIMD_LOOP_PREHEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.preheader:
; CHECK-NEXT:    [[LOAD_ALLOCA_X_SCALAR0:%.*]] = load i64*, i64** [[ALLOCA_X_SCALAR0]], align 8
; CHECK-NEXT:    br label [[SIMD_LOOP_HEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.header:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_LOOP_PREHEADER0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_LATCH0:%.*]] ]
; CHECK-NEXT:    store i64* [[LOAD_ALLOCA_X_SCALAR0]], i64** [[X_ADDR0]], align 8
; CHECK-NEXT:    [[TMP0:%.*]] = load i64*, i64** [[X_ADDR0]], align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 8
; CHECK-NEXT:    [[PHI_CAST0:%.*]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[STRIDE_MUL0:%.*]] = mul i64 2, [[PHI_CAST0]]
; CHECK-NEXT:    [[STRIDE_ADD0:%.*]] = add i64 [[TMP1]], [[STRIDE_MUL0]]
; CHECK-NEXT:    [[ADD0:%.*]] = add nsw i64 [[STRIDE_ADD0]], 1
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr i64, i64* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store i64 [[ADD0]], i64* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.latch:
; CHECK-NEXT:    [[INDVAR0]] = add nuw i32 [[INDEX0]], 1
; CHECK-NEXT:    [[VL_COND0:%.*]] = icmp ult i32 [[INDVAR0]], 8
; CHECK-NEXT:    br i1 [[VL_COND0]], label [[SIMD_LOOP_HEADER0]], label [[SIMD_END_REGION0:%.*]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  simd.end.region:
; CHECK-NEXT:    call void @llvm.directive.region.exit(token [[ENTRY_REGION0]]) [ "DIR.OMP.END.SIMD"() ]
; CHECK-NEXT:    br label [[RETURN0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  return:
; CHECK-NEXT:    [[VEC_RET_CAST0:%.*]] = bitcast i64* [[RET_CAST0]] to <8 x i64>*
; CHECK-NEXT:    [[VEC_RET0:%.*]] = load <8 x i64>, <8 x i64>* [[VEC_RET_CAST0]], align 64
; CHECK-NEXT:    ret <8 x i64> [[VEC_RET0]]
; CHECK-NEXT:  }
;
entry:
  %x.addr = alloca i64*, align 8
  store i64* %x, i64** %x.addr, align 8
  %0 = load i64*, i64** %x.addr, align 8
  %1 = load i64, i64* %0, align 8
  %add = add nsw i64 %1, 1
  ret i64 %add
}

attributes #0 = { mustprogress noinline nounwind optnone uwtable "approx-func-fp-math"="true" "frame-pointer"="all" "loopopt-pipeline"="full" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="skylake-avx512" "target-features"="+adx,+aes,+avx,+avx2,+avx512bw,+avx512cd,+avx512dq,+avx512f,+avx512vl,+bmi,+bmi2,+clflushopt,+clwb,+crc32,+cx16,+cx8,+f16c,+fma,+fsgsbase,+fxsr,+invpcid,+lzcnt,+mmx,+movbe,+pclmul,+pku,+popcnt,+prfchw,+rdrnd,+rdseed,+sahf,+sse,+sse2,+sse3,+sse4.1,+sse4.2,+ssse3,+x87,+xsave,+xsavec,+xsaveopt,+xsaves" "unsafe-fp-math"="true" "vector-variants"="_ZGVbN8L2__Z3fooRl" }
