; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; RUN: opt -vec-clone -S < %s -mtriple=x86_64-unknown-linux-gnu | FileCheck %s

; Check VecClone widens vector parameters/returns

define <4 x i32> @foo(<4 x i32> %a, <4 x i32> %b) #0 {
; CHECK:       target triple = "x86_64-unknown-linux-gnu"
;
; CHECK:  define <4 x i32> @foo(<4 x i32> [[A0:%.*]], <4 x i32> [[B0:%.*]]) #0 {
; CHECK-NEXT:    [[ADD0:%.*]] = add nsw <4 x i32> [[A0]], [[B0]]
; CHECK-NEXT:    ret <4 x i32> [[ADD0]]
; CHECK-NEXT:  }
;
; CHECK:  define <16 x i32> @_ZGVbN4vv_foo(<16 x i32> [[A0]], <16 x i32> [[B0]]) #0 {
; CHECK-NEXT:    [[VEC_A0:%.*]] = alloca <16 x i32>, align 64
; CHECK-NEXT:    [[VEC_B0:%.*]] = alloca <16 x i32>, align 64
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <16 x i32>, align 64
; CHECK-NEXT:    [[VEC_A_CAST0:%.*]] = bitcast <16 x i32>* [[VEC_A0]] to <4 x i32>*
; CHECK-NEXT:    store <16 x i32> [[A0]], <16 x i32>* [[VEC_A0]], align 64
; CHECK-NEXT:    [[VEC_B_CAST0:%.*]] = bitcast <16 x i32>* [[VEC_B0]] to <4 x i32>*
; CHECK-NEXT:    store <16 x i32> [[B0]], <16 x i32>* [[VEC_B0]], align 64
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <16 x i32>* [[VEC_RETVAL0]] to <4 x i32>*
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.begin.region:
; CHECK-NEXT:    [[ENTRY_REGION0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4) ]
; CHECK-NEXT:    br label [[SIMD_LOOP0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_BEGIN_REGION0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_EXIT0:%.*]] ]
; CHECK-NEXT:    [[VEC_A_CAST_GEP0:%.*]] = getelementptr <4 x i32>, <4 x i32>* [[VEC_A_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_A_ELEM0:%.*]] = load <4 x i32>, <4 x i32>* [[VEC_A_CAST_GEP0]], align 16
; CHECK-NEXT:    [[VEC_B_CAST_GEP0:%.*]] = getelementptr <4 x i32>, <4 x i32>* [[VEC_B_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_B_ELEM0:%.*]] = load <4 x i32>, <4 x i32>* [[VEC_B_CAST_GEP0]], align 16
; CHECK-NEXT:    [[ADD0]] = add nsw <4 x i32> [[VEC_A_ELEM0]], [[VEC_B_ELEM0]]
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr <4 x i32>, <4 x i32>* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store <4 x i32> [[ADD0]], <4 x i32>* [[RET_CAST_GEP0]], align 16
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.exit:
; CHECK-NEXT:    [[INDVAR0]] = add nuw i32 [[INDEX0]], 1
; CHECK-NEXT:    [[VL_COND0:%.*]] = icmp ult i32 [[INDVAR0]], 4
; CHECK-NEXT:    br i1 [[VL_COND0]], label [[SIMD_LOOP0]], label [[SIMD_END_REGION0:%.*]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  simd.end.region:
; CHECK-NEXT:    call void @llvm.directive.region.exit(token [[ENTRY_REGION0]]) [ "DIR.OMP.END.SIMD"() ]
; CHECK-NEXT:    br label [[RETURN0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  return:
; CHECK-NEXT:    [[VEC_RET_CAST0:%.*]] = bitcast <4 x i32>* [[RET_CAST0]] to <16 x i32>*
; CHECK-NEXT:    [[VEC_RET0:%.*]] = load <16 x i32>, <16 x i32>* [[VEC_RET_CAST0]], align 64
; CHECK-NEXT:    ret <16 x i32> [[VEC_RET0]]
; CHECK-NEXT:  }
;
  %add = add nsw <4 x i32> %a, %b
  ret <4 x i32> %add
}

attributes #0 = { "vector-variants"="_ZGVbN4vv_" }
