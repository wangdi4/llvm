; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py

;; The tested transformation is this. The original argument:
;;     %struct.A* byval(%struct.A)
;; will be turned into:
;;     [4 x %struct.A]* byval([4 x %struct.A])
;;
;; Since the GEPs from the original parameter are plain gep from a pointer
;; with 0 as the first index (we only read one struct from the pointer),
;; having the parameter changed to array of structs, we need to
;; change this 0 index to get the indexing in this array.

; RUN: opt -vec-clone -S < %s | FileCheck %s
; RUN: opt -passes="vec-clone" -S < %s | FileCheck %s

;; struct A { float a; int b; double c; long d; };
;;
;; #pragma omp declare simd simdlen(4)
;; int foo (A a) {
;;   return static_cast<int>(a.a) +
;;          static_cast<int>(a.b) +
;;          static_cast<int>(a.c) +
;;          static_cast<int>(a.d);
;; }

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.A = type { float, i32, double, i64 }

define dso_local i32 @_Z3foo1A(%struct.A* nocapture readonly byval(%struct.A) align 8 %a) #0 {
; CHECK:  define dso_local <4 x i32> @_ZGVeN4v__Z3foo1A([4 x %struct.A]* nocapture readonly byval([4 x %struct.A]) align 8 [[A0:%.*]]) #0 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VEC_A0:%.*]] = alloca [4 x %struct.A], align 8
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_A_CAST0:%.*]] = bitcast [4 x %struct.A]* [[VEC_A0]] to %struct.A*
; CHECK-NEXT:    [[VEC_A_BYVAL_LOAD0:%.*]] = load [4 x %struct.A], [4 x %struct.A]* [[A0]], align 8
; CHECK-NEXT:    store [4 x %struct.A] [[VEC_A_BYVAL_LOAD0]], [4 x %struct.A]* [[VEC_A0]], align 8
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_RETVAL0]] to i32*
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_BEGIN_REGION0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_EXIT0:%.*]] ]
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT50:%.*]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP60:%.*]] = getelementptr [[STRUCT_A0:%.*]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT50]], i32 0
; CHECK-NEXT:    [[TMP0:%.*]] = load float, float* [[VEC_A_CAST_BYVAL_GEP60]], align 8
; CHECK-NEXT:    [[CONV0:%.*]] = fptosi float [[TMP0]] to i32
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT30:%.*]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP40:%.*]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT30]], i32 1
; CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[VEC_A_CAST_BYVAL_GEP40]], align 4
; CHECK-NEXT:    [[ADD0:%.*]] = add nsw i32 [[TMP1]], [[CONV0]]
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT10:%.*]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP20:%.*]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT10]], i32 2
; CHECK-NEXT:    [[TMP2:%.*]] = load double, double* [[VEC_A_CAST_BYVAL_GEP20]], align 8
; CHECK-NEXT:    [[CONV20:%.*]] = fptosi double [[TMP2]] to i32
; CHECK-NEXT:    [[ADD30:%.*]] = add nsw i32 [[ADD0]], [[CONV20]]
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT0:%.*]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP0:%.*]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT0]], i32 3
; CHECK-NEXT:    [[TMP3:%.*]] = load i64, i64* [[VEC_A_CAST_BYVAL_GEP0]], align 8
; CHECK-NEXT:    [[CONV40:%.*]] = trunc i64 [[TMP3]] to i32
; CHECK-NEXT:    [[ADD50:%.*]] = add nsw i32 [[ADD30]], [[CONV40]]
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr i32, i32* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store i32 [[ADD50]], i32* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT0]]
;
; CHECK:  define dso_local <4 x i32> @_ZGVeM4v__Z3foo1A([4 x %struct.A]* nocapture readonly byval([4 x %struct.A]) align 8 [[A0]], <4 x i32> [[MASK0:%.*]]) #0 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VEC_A0]] = alloca [4 x %struct.A], align 8
; CHECK-NEXT:    [[VEC_MASK0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_RETVAL0]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_A_CAST0]] = bitcast [4 x %struct.A]* [[VEC_A0]] to %struct.A*
; CHECK-NEXT:    [[VEC_A_BYVAL_LOAD0]] = load [4 x %struct.A], [4 x %struct.A]* [[A0]], align 8
; CHECK-NEXT:    store [4 x %struct.A] [[VEC_A_BYVAL_LOAD0]], [4 x %struct.A]* [[VEC_A0]], align 8
; CHECK-NEXT:    [[MASK_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_MASK0]] to i32*
; CHECK-NEXT:    [[RET_CAST0]] = bitcast <4 x i32>* [[VEC_RETVAL0]] to i32*
; CHECK-NEXT:    store <4 x i32> [[MASK0]], <4 x i32>* [[VEC_MASK0]], align 16
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0]]
; CHECK-EMPTY:
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX0]] = phi i32 [ 0, [[SIMD_BEGIN_REGION0]] ], [ [[INDVAR0]], [[SIMD_LOOP_EXIT0]] ]
; CHECK-NEXT:    [[MASK_GEP0:%.*]] = getelementptr i32, i32* [[MASK_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[MASK_PARM0:%.*]] = load i32, i32* [[MASK_GEP0]], align 4
; CHECK-NEXT:    [[MASK_COND0:%.*]] = icmp ne i32 [[MASK_PARM0]], 0
; CHECK-NEXT:    br i1 [[MASK_COND0]], label [[SIMD_LOOP_THEN0:%.*]], label [[SIMD_LOOP_ELSE0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.then:
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT50]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP60]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT50]], i32 0
; CHECK-NEXT:    [[TMP0]] = load float, float* [[VEC_A_CAST_BYVAL_GEP60]], align 8
; CHECK-NEXT:    [[CONV0]] = fptosi float [[TMP0]] to i32
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT30]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP40]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT30]], i32 1
; CHECK-NEXT:    [[TMP1]] = load i32, i32* [[VEC_A_CAST_BYVAL_GEP40]], align 4
; CHECK-NEXT:    [[ADD0]] = add nsw i32 [[TMP1]], [[CONV0]]
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT10]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP20]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT10]], i32 2
; CHECK-NEXT:    [[TMP2]] = load double, double* [[VEC_A_CAST_BYVAL_GEP20]], align 8
; CHECK-NEXT:    [[CONV20]] = fptosi double [[TMP2]] to i32
; CHECK-NEXT:    [[ADD30]] = add nsw i32 [[ADD0]], [[CONV20]]
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_ZEXT0]] = zext i32 [[INDEX0]] to i64
; CHECK-NEXT:    [[VEC_A_CAST_BYVAL_GEP0]] = getelementptr [[STRUCT_A0]], %struct.A* [[VEC_A_CAST0]], i64 [[VEC_A_CAST_BYVAL_ZEXT0]], i32 3
; CHECK-NEXT:    [[TMP3]] = load i64, i64* [[VEC_A_CAST_BYVAL_GEP0]], align 8
; CHECK-NEXT:    [[CONV40]] = trunc i64 [[TMP3]] to i32
; CHECK-NEXT:    [[ADD50]] = add nsw i32 [[ADD30]], [[CONV40]]
; CHECK-NEXT:    [[RET_CAST_GEP0]] = getelementptr i32, i32* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store i32 [[ADD50]], i32* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT0]]
;
entry:
  %a1 = getelementptr inbounds %struct.A, %struct.A* %a, i64 0, i32 0
  %0 = load float, float* %a1, align 8
  %conv = fptosi float %0 to i32
  %b = getelementptr inbounds %struct.A, %struct.A* %a, i64 0, i32 1
  %1 = load i32, i32* %b, align 4
  %add = add nsw i32 %1, %conv
  %c = getelementptr inbounds %struct.A, %struct.A* %a, i64 0, i32 2
  %2 = load double, double* %c, align 8
  %conv2 = fptosi double %2 to i32
  %add3 = add nsw i32 %add, %conv2
  %d = getelementptr inbounds %struct.A, %struct.A* %a, i64 0, i32 3
  %3 = load i64, i64* %d, align 8
  %conv4 = trunc i64 %3 to i32
  %add5 = add nsw i32 %add3, %conv4
  ret i32 %add5
}

attributes #0 = { "vector-variants"="_ZGVeN4v__Z3foo1A,_ZGVeM4v__Z3foo1A" }

