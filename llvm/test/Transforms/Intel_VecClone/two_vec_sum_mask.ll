; NOTE: Assertions have been autogenerated by utils/intel_update_vplan_checks.py
; Do a sanity check on the structure of the LLVM that VecClone produces for the masked variant.

; RUN: opt -vec-clone -S < %s | FileCheck %s
; RUN: opt -passes="vec-clone" -S < %s | FileCheck %s

; ModuleID = 'two_vec_sum.c'
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

; Function Attrs: nounwind uwtable
define i32 @vec_sum(i32 %i, i32 %j) #0 {
; CHECK:       target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
; CHECK-NEXT:  target triple = "x86_64-unknown-linux-gnu"
;
; CHECK:  define i32 @vec_sum(i32 [[I0:%.*]], i32 [[J0:%.*]]) #0 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[I_ADDR0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    [[J_ADDR0:%.*]] = alloca i32, align 4
; CHECK-NEXT:    store i32 [[I0]], i32* [[I_ADDR0]], align 4
; CHECK-NEXT:    store i32 [[J0]], i32* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = load i32, i32* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[ADD0:%.*]] = add nsw i32 [[TMP0]], [[TMP1]]
; CHECK-NEXT:    ret i32 [[ADD0]]
; CHECK-NEXT:  }
;
; CHECK:  define <4 x i32> @_ZGVbM4vv_vec_sum(<4 x i32> [[I0]], <4 x i32> [[J0]], <4 x i32> [[MASK0:%.*]]) #1 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VEC_I0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_J0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_MASK0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[VEC_RETVAL0:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:    [[I_ADDR0]] = alloca i32, align 4
; CHECK-NEXT:    [[J_ADDR0]] = alloca i32, align 4
; CHECK-NEXT:    [[VEC_I_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_I0]] to i32*
; CHECK-NEXT:    store <4 x i32> [[I0]], <4 x i32>* [[VEC_I0]], align 16
; CHECK-NEXT:    [[VEC_J_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_J0]] to i32*
; CHECK-NEXT:    store <4 x i32> [[J0]], <4 x i32>* [[VEC_J0]], align 16
; CHECK-NEXT:    [[MASK_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_MASK0]] to i32*
; CHECK-NEXT:    [[RET_CAST0:%.*]] = bitcast <4 x i32>* [[VEC_RETVAL0]] to i32*
; CHECK-NEXT:    store <4 x i32> [[MASK0]], <4 x i32>* [[VEC_MASK0]], align 16
; CHECK-NEXT:    br label [[SIMD_BEGIN_REGION0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.begin.region:
; CHECK-NEXT:    [[ENTRY_REGION0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 4), "QUAL.OMP.PRIVATE"(i32* [[I_ADDR0]]), "QUAL.OMP.PRIVATE"(i32* [[J_ADDR0]]) ]
; CHECK-NEXT:    br label [[SIMD_LOOP_PREHEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.preheader:
; CHECK-NEXT:    br label [[SIMD_LOOP_HEADER0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.header:
; CHECK-NEXT:    [[INDEX0:%.*]] = phi i32 [ 0, [[SIMD_LOOP_PREHEADER0]] ], [ [[INDVAR0:%.*]], [[SIMD_LOOP_LATCH0:%.*]] ]
; CHECK-NEXT:    [[MASK_GEP0:%.*]] = getelementptr i32, i32* [[MASK_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[MASK_PARM0:%.*]] = load i32, i32* [[MASK_GEP0]], align 4
; CHECK-NEXT:    [[MASK_COND0:%.*]] = icmp ne i32 [[MASK_PARM0]], 0
; CHECK-NEXT:    br i1 [[MASK_COND0]], label [[SIMD_LOOP_THEN0:%.*]], label [[SIMD_LOOP_ELSE0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.then:
; CHECK-NEXT:    [[VEC_I_CAST_GEP0:%.*]] = getelementptr i32, i32* [[VEC_I_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_I_ELEM0:%.*]] = load i32, i32* [[VEC_I_CAST_GEP0]], align 4
; CHECK-NEXT:    store i32 [[VEC_I_ELEM0]], i32* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[VEC_J_CAST_GEP0:%.*]] = getelementptr i32, i32* [[VEC_J_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    [[VEC_J_ELEM0:%.*]] = load i32, i32* [[VEC_J_CAST_GEP0]], align 4
; CHECK-NEXT:    store i32 [[VEC_J_ELEM0]], i32* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[TMP0]] = load i32, i32* [[I_ADDR0]], align 4
; CHECK-NEXT:    [[TMP1]] = load i32, i32* [[J_ADDR0]], align 4
; CHECK-NEXT:    [[ADD0]] = add nsw i32 [[TMP0]], [[TMP1]]
; CHECK-NEXT:    [[RET_CAST_GEP0:%.*]] = getelementptr i32, i32* [[RET_CAST0]], i32 [[INDEX0]]
; CHECK-NEXT:    store i32 [[ADD0]], i32* [[RET_CAST_GEP0]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.else:
; CHECK-NEXT:    br label [[SIMD_LOOP_LATCH0]]
; CHECK-EMPTY:
; CHECK-NEXT:  simd.loop.latch:
; CHECK-NEXT:    [[INDVAR0]] = add nuw i32 [[INDEX0]], 1
; CHECK-NEXT:    [[VL_COND0:%.*]] = icmp ult i32 [[INDVAR0]], 4
; CHECK-NEXT:    br i1 [[VL_COND0]], label [[SIMD_LOOP_HEADER0]], label [[SIMD_END_REGION0:%.*]], !llvm.loop !0
; CHECK-EMPTY:
; CHECK-NEXT:  simd.end.region:
; CHECK-NEXT:    call void @llvm.directive.region.exit(token [[ENTRY_REGION0]]) [ "DIR.OMP.END.SIMD"() ]
; CHECK-NEXT:    br label [[RETURN0:%.*]]
; CHECK-EMPTY:
; CHECK-NEXT:  return:
; CHECK-NEXT:    [[VEC_RET_CAST0:%.*]] = bitcast i32* [[RET_CAST0]] to <4 x i32>*
; CHECK-NEXT:    [[VEC_RET0:%.*]] = load <4 x i32>, <4 x i32>* [[VEC_RET_CAST0]], align 16
; CHECK-NEXT:    ret <4 x i32> [[VEC_RET0]]
; CHECK-NEXT:  }
;
entry:
  %i.addr = alloca i32, align 4
  %j.addr = alloca i32, align 4
  store i32 %i, i32* %i.addr, align 4
  store i32 %j, i32* %j.addr, align 4
  %0 = load i32, i32* %i.addr, align 4
  %1 = load i32, i32* %j.addr, align 4
  %add = add nsw i32 %0, %1
  ret i32 %add
}

attributes #0 = { nounwind uwtable "vector-variants"="_ZGVbM4vv_vec_sum" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+sse,+sse2" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind uwtable "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+sse,+sse2" "unsafe-fp-math"="false" "use-soft-float"="false" }
