; REQUIRES: asserts

; RUN: opt < %s -analyze -enable-new-pm=0 -hir-region-identification -xmain-opt-level=3 -debug-only=hir-region-identification 2>&1 | FileCheck %s
; RUN: opt < %s -passes="print<hir-region-identification>" -xmain-opt-level=3 -debug-only=hir-region-identification 2>&1 | FileCheck %s

; This is a test case from spec2006test400C.
; The loop is about 900 lines long. Even though most instructions are SCEVable,
; the SCEVs that are formed are huge and take a lot of compile time to analyze
; so we want to bail out on such loops even at O3.


; CHECK: Throttled due to presence of too many statements.


target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.MD5_CTX = type { i32, i32, i32, i32, i32, i32, i32, [128 x i8] }

define hidden fastcc void @MD5Transform(%struct.MD5_CTX* nocapture %ctx, i8* readonly %buf, i64 %blocks) {
entry:
  %A1 = getelementptr inbounds %struct.MD5_CTX, %struct.MD5_CTX* %ctx, i64 0, i32 1
  %0 = load i32, i32* %A1, align 4
  %B2 = getelementptr inbounds %struct.MD5_CTX, %struct.MD5_CTX* %ctx, i64 0, i32 2
  %1 = load i32, i32* %B2, align 4
  %C3 = getelementptr inbounds %struct.MD5_CTX, %struct.MD5_CTX* %ctx, i64 0, i32 3
  %2 = load i32, i32* %C3, align 4
  %D4 = getelementptr inbounds %struct.MD5_CTX, %struct.MD5_CTX* %ctx, i64 0, i32 4
  %3 = load i32, i32* %D4, align 4
  br label %do.body

do.body:                                          ; preds = %do.body, %entry
  %D.0 = phi i32 [ %3, %entry ], [ %add909, %do.body ]
  %C.0 = phi i32 [ %2, %entry ], [ %add908, %do.body ]
  %B.0 = phi i32 [ %1, %entry ], [ %add907, %do.body ]
  %A.0 = phi i32 [ %0, %entry ], [ %add906, %do.body ]
  %blocks.addr.0 = phi i64 [ %blocks, %entry ], [ %dec, %do.body ]
  %buf.addr.0 = phi i8* [ %buf, %entry ], [ %add.ptr387, %do.body ]
  %xor = xor i32 %C.0, %D.0
  %and = and i32 %B.0, %xor
  %xor5 = xor i32 %and, %D.0
  %4 = load i8, i8* %buf.addr.0, align 1
  %conv = zext i8 %4 to i32
  %add.ptr = getelementptr inbounds i8, i8* %buf.addr.0, i64 1
  %5 = load i8, i8* %add.ptr, align 1
  %conv6 = zext i8 %5 to i32
  %shl = shl nuw nsw i32 %conv6, 8
  %or = or i32 %shl, %conv
  %add.ptr7 = getelementptr inbounds i8, i8* %buf.addr.0, i64 2
  %6 = load i8, i8* %add.ptr7, align 1
  %conv8 = zext i8 %6 to i32
  %shl9 = shl nuw nsw i32 %conv8, 16
  %or10 = or i32 %or, %shl9
  %add.ptr11 = getelementptr inbounds i8, i8* %buf.addr.0, i64 3
  %7 = load i8, i8* %add.ptr11, align 1
  %conv12 = zext i8 %7 to i32
  %shl13 = shl nuw i32 %conv12, 24
  %or14 = or i32 %or10, %shl13
  %add.ptr15 = getelementptr inbounds i8, i8* %buf.addr.0, i64 4
  %add = add i32 %A.0, -680876936
  %add16 = add i32 %add, %xor5
  %add17 = add i32 %add16, %or14
  %shl18 = shl i32 %add17, 7
  %shr = lshr i32 %add17, 25
  %or19 = add i32 %shr, %B.0
  %add20 = add i32 %or19, %shl18
  %xor21 = xor i32 %B.0, %C.0
  %and22 = and i32 %add20, %xor21
  %xor23 = xor i32 %and22, %C.0
  %8 = load i8, i8* %add.ptr15, align 1
  %conv24 = zext i8 %8 to i32
  %add.ptr25 = getelementptr inbounds i8, i8* %buf.addr.0, i64 5
  %9 = load i8, i8* %add.ptr25, align 1
  %conv26 = zext i8 %9 to i32
  %shl27 = shl nuw nsw i32 %conv26, 8
  %or28 = or i32 %shl27, %conv24
  %add.ptr29 = getelementptr inbounds i8, i8* %buf.addr.0, i64 6
  %10 = load i8, i8* %add.ptr29, align 1
  %conv30 = zext i8 %10 to i32
  %shl31 = shl nuw nsw i32 %conv30, 16
  %or32 = or i32 %or28, %shl31
  %add.ptr33 = getelementptr inbounds i8, i8* %buf.addr.0, i64 7
  %11 = load i8, i8* %add.ptr33, align 1
  %conv34 = zext i8 %11 to i32
  %shl35 = shl nuw i32 %conv34, 24
  %or36 = or i32 %or32, %shl35
  %add.ptr37 = getelementptr inbounds i8, i8* %buf.addr.0, i64 8
  %add39 = add i32 %D.0, -389564586
  %add40 = add i32 %add39, %or36
  %add41 = add i32 %add40, %xor23
  %shl42 = shl i32 %add41, 12
  %shr43 = lshr i32 %add41, 20
  %or44 = add i32 %shr43, %add20
  %add45 = add i32 %or44, %shl42
  %xor46 = xor i32 %add20, %B.0
  %and47 = and i32 %add45, %xor46
  %xor48 = xor i32 %and47, %B.0
  %12 = load i8, i8* %add.ptr37, align 1
  %conv49 = zext i8 %12 to i32
  %add.ptr50 = getelementptr inbounds i8, i8* %buf.addr.0, i64 9
  %13 = load i8, i8* %add.ptr50, align 1
  %conv51 = zext i8 %13 to i32
  %shl52 = shl nuw nsw i32 %conv51, 8
  %or53 = or i32 %shl52, %conv49
  %add.ptr54 = getelementptr inbounds i8, i8* %buf.addr.0, i64 10
  %14 = load i8, i8* %add.ptr54, align 1
  %conv55 = zext i8 %14 to i32
  %shl56 = shl nuw nsw i32 %conv55, 16
  %or57 = or i32 %or53, %shl56
  %add.ptr58 = getelementptr inbounds i8, i8* %buf.addr.0, i64 11
  %15 = load i8, i8* %add.ptr58, align 1
  %conv59 = zext i8 %15 to i32
  %shl60 = shl nuw i32 %conv59, 24
  %or61 = or i32 %or57, %shl60
  %add.ptr62 = getelementptr inbounds i8, i8* %buf.addr.0, i64 12
  %add64 = add i32 %C.0, 606105819
  %add65 = add i32 %add64, %or61
  %add66 = add i32 %add65, %xor48
  %shl67 = shl i32 %add66, 17
  %shr68 = lshr i32 %add66, 15
  %or69 = add i32 %shr68, %add45
  %add70 = add i32 %or69, %shl67
  %xor71 = xor i32 %add45, %add20
  %and72 = and i32 %add70, %xor71
  %xor73 = xor i32 %and72, %add20
  %16 = load i8, i8* %add.ptr62, align 1
  %conv74 = zext i8 %16 to i32
  %add.ptr75 = getelementptr inbounds i8, i8* %buf.addr.0, i64 13
  %17 = load i8, i8* %add.ptr75, align 1
  %conv76 = zext i8 %17 to i32
  %shl77 = shl nuw nsw i32 %conv76, 8
  %or78 = or i32 %shl77, %conv74
  %add.ptr79 = getelementptr inbounds i8, i8* %buf.addr.0, i64 14
  %18 = load i8, i8* %add.ptr79, align 1
  %conv80 = zext i8 %18 to i32
  %shl81 = shl nuw nsw i32 %conv80, 16
  %or82 = or i32 %or78, %shl81
  %add.ptr83 = getelementptr inbounds i8, i8* %buf.addr.0, i64 15
  %19 = load i8, i8* %add.ptr83, align 1
  %conv84 = zext i8 %19 to i32
  %shl85 = shl nuw i32 %conv84, 24
  %or86 = or i32 %or82, %shl85
  %add.ptr87 = getelementptr inbounds i8, i8* %buf.addr.0, i64 16
  %add89 = add i32 %B.0, -1044525330
  %add90 = add i32 %add89, %or86
  %add91 = add i32 %add90, %xor73
  %shl92 = shl i32 %add91, 22
  %shr93 = lshr i32 %add91, 10
  %or94 = add i32 %shr93, %add70
  %add95 = add i32 %or94, %shl92
  %xor96 = xor i32 %add70, %add45
  %and97 = and i32 %add95, %xor96
  %xor98 = xor i32 %and97, %add45
  %20 = load i8, i8* %add.ptr87, align 1
  %conv99 = zext i8 %20 to i32
  %add.ptr100 = getelementptr inbounds i8, i8* %buf.addr.0, i64 17
  %21 = load i8, i8* %add.ptr100, align 1
  %conv101 = zext i8 %21 to i32
  %shl102 = shl nuw nsw i32 %conv101, 8
  %or103 = or i32 %shl102, %conv99
  %add.ptr104 = getelementptr inbounds i8, i8* %buf.addr.0, i64 18
  %22 = load i8, i8* %add.ptr104, align 1
  %conv105 = zext i8 %22 to i32
  %shl106 = shl nuw nsw i32 %conv105, 16
  %or107 = or i32 %or103, %shl106
  %add.ptr108 = getelementptr inbounds i8, i8* %buf.addr.0, i64 19
  %23 = load i8, i8* %add.ptr108, align 1
  %conv109 = zext i8 %23 to i32
  %shl110 = shl nuw i32 %conv109, 24
  %or111 = or i32 %or107, %shl110
  %add.ptr112 = getelementptr inbounds i8, i8* %buf.addr.0, i64 20
  %add114 = add i32 %add20, -176418897
  %add115 = add i32 %add114, %or111
  %add116 = add i32 %add115, %xor98
  %shl117 = shl i32 %add116, 7
  %shr118 = lshr i32 %add116, 25
  %or119 = add i32 %shr118, %add95
  %add120 = add i32 %or119, %shl117
  %xor121 = xor i32 %add95, %add70
  %and122 = and i32 %add120, %xor121
  %xor123 = xor i32 %and122, %add70
  %24 = load i8, i8* %add.ptr112, align 1
  %conv124 = zext i8 %24 to i32
  %add.ptr125 = getelementptr inbounds i8, i8* %buf.addr.0, i64 21
  %25 = load i8, i8* %add.ptr125, align 1
  %conv126 = zext i8 %25 to i32
  %shl127 = shl nuw nsw i32 %conv126, 8
  %or128 = or i32 %shl127, %conv124
  %add.ptr129 = getelementptr inbounds i8, i8* %buf.addr.0, i64 22
  %26 = load i8, i8* %add.ptr129, align 1
  %conv130 = zext i8 %26 to i32
  %shl131 = shl nuw nsw i32 %conv130, 16
  %or132 = or i32 %or128, %shl131
  %add.ptr133 = getelementptr inbounds i8, i8* %buf.addr.0, i64 23
  %27 = load i8, i8* %add.ptr133, align 1
  %conv134 = zext i8 %27 to i32
  %shl135 = shl nuw i32 %conv134, 24
  %or136 = or i32 %or132, %shl135
  %add.ptr137 = getelementptr inbounds i8, i8* %buf.addr.0, i64 24
  %add139 = add i32 %add45, 1200080426
  %add140 = add i32 %add139, %or136
  %add141 = add i32 %add140, %xor123
  %shl142 = shl i32 %add141, 12
  %shr143 = lshr i32 %add141, 20
  %or144 = add i32 %shr143, %add120
  %add145 = add i32 %or144, %shl142
  %xor146 = xor i32 %add120, %add95
  %and147 = and i32 %add145, %xor146
  %xor148 = xor i32 %and147, %add95
  %28 = load i8, i8* %add.ptr137, align 1
  %conv149 = zext i8 %28 to i32
  %add.ptr150 = getelementptr inbounds i8, i8* %buf.addr.0, i64 25
  %29 = load i8, i8* %add.ptr150, align 1
  %conv151 = zext i8 %29 to i32
  %shl152 = shl nuw nsw i32 %conv151, 8
  %or153 = or i32 %shl152, %conv149
  %add.ptr154 = getelementptr inbounds i8, i8* %buf.addr.0, i64 26
  %30 = load i8, i8* %add.ptr154, align 1
  %conv155 = zext i8 %30 to i32
  %shl156 = shl nuw nsw i32 %conv155, 16
  %or157 = or i32 %or153, %shl156
  %add.ptr158 = getelementptr inbounds i8, i8* %buf.addr.0, i64 27
  %31 = load i8, i8* %add.ptr158, align 1
  %conv159 = zext i8 %31 to i32
  %shl160 = shl nuw i32 %conv159, 24
  %or161 = or i32 %or157, %shl160
  %add.ptr162 = getelementptr inbounds i8, i8* %buf.addr.0, i64 28
  %add164 = add i32 %add70, -1473231341
  %add165 = add i32 %add164, %or161
  %add166 = add i32 %add165, %xor148
  %shl167 = shl i32 %add166, 17
  %shr168 = lshr i32 %add166, 15
  %or169 = add i32 %shr168, %add145
  %add170 = add i32 %or169, %shl167
  %xor171 = xor i32 %add145, %add120
  %and172 = and i32 %add170, %xor171
  %xor173 = xor i32 %and172, %add120
  %32 = load i8, i8* %add.ptr162, align 1
  %conv174 = zext i8 %32 to i32
  %add.ptr175 = getelementptr inbounds i8, i8* %buf.addr.0, i64 29
  %33 = load i8, i8* %add.ptr175, align 1
  %conv176 = zext i8 %33 to i32
  %shl177 = shl nuw nsw i32 %conv176, 8
  %or178 = or i32 %shl177, %conv174
  %add.ptr179 = getelementptr inbounds i8, i8* %buf.addr.0, i64 30
  %34 = load i8, i8* %add.ptr179, align 1
  %conv180 = zext i8 %34 to i32
  %shl181 = shl nuw nsw i32 %conv180, 16
  %or182 = or i32 %or178, %shl181
  %add.ptr183 = getelementptr inbounds i8, i8* %buf.addr.0, i64 31
  %35 = load i8, i8* %add.ptr183, align 1
  %conv184 = zext i8 %35 to i32
  %shl185 = shl nuw i32 %conv184, 24
  %or186 = or i32 %or182, %shl185
  %add.ptr187 = getelementptr inbounds i8, i8* %buf.addr.0, i64 32
  %add189 = add i32 %or186, -45705983
  %add190 = add i32 %add189, %add95
  %add191 = add i32 %add190, %xor173
  %shl192 = shl i32 %add191, 22
  %shr193 = lshr i32 %add191, 10
  %or194 = add i32 %shr193, %add170
  %add195 = add i32 %or194, %shl192
  %xor196 = xor i32 %add170, %add145
  %and197 = and i32 %add195, %xor196
  %xor198 = xor i32 %and197, %add145
  %36 = load i8, i8* %add.ptr187, align 1
  %conv199 = zext i8 %36 to i32
  %add.ptr200 = getelementptr inbounds i8, i8* %buf.addr.0, i64 33
  %37 = load i8, i8* %add.ptr200, align 1
  %conv201 = zext i8 %37 to i32
  %shl202 = shl nuw nsw i32 %conv201, 8
  %or203 = or i32 %shl202, %conv199
  %add.ptr204 = getelementptr inbounds i8, i8* %buf.addr.0, i64 34
  %38 = load i8, i8* %add.ptr204, align 1
  %conv205 = zext i8 %38 to i32
  %shl206 = shl nuw nsw i32 %conv205, 16
  %or207 = or i32 %or203, %shl206
  %add.ptr208 = getelementptr inbounds i8, i8* %buf.addr.0, i64 35
  %39 = load i8, i8* %add.ptr208, align 1
  %conv209 = zext i8 %39 to i32
  %shl210 = shl nuw i32 %conv209, 24
  %or211 = or i32 %or207, %shl210
  %add.ptr212 = getelementptr inbounds i8, i8* %buf.addr.0, i64 36
  %add214 = add i32 %or211, 1770035416
  %add215 = add i32 %add214, %add120
  %add216 = add i32 %add215, %xor198
  %shl217 = shl i32 %add216, 7
  %shr218 = lshr i32 %add216, 25
  %or219 = add i32 %shr218, %add195
  %add220 = add i32 %or219, %shl217
  %xor221 = xor i32 %add195, %add170
  %and222 = and i32 %add220, %xor221
  %xor223 = xor i32 %and222, %add170
  %40 = load i8, i8* %add.ptr212, align 1
  %conv224 = zext i8 %40 to i32
  %add.ptr225 = getelementptr inbounds i8, i8* %buf.addr.0, i64 37
  %41 = load i8, i8* %add.ptr225, align 1
  %conv226 = zext i8 %41 to i32
  %shl227 = shl nuw nsw i32 %conv226, 8
  %or228 = or i32 %shl227, %conv224
  %add.ptr229 = getelementptr inbounds i8, i8* %buf.addr.0, i64 38
  %42 = load i8, i8* %add.ptr229, align 1
  %conv230 = zext i8 %42 to i32
  %shl231 = shl nuw nsw i32 %conv230, 16
  %or232 = or i32 %or228, %shl231
  %add.ptr233 = getelementptr inbounds i8, i8* %buf.addr.0, i64 39
  %43 = load i8, i8* %add.ptr233, align 1
  %conv234 = zext i8 %43 to i32
  %shl235 = shl nuw i32 %conv234, 24
  %or236 = or i32 %or232, %shl235
  %add.ptr237 = getelementptr inbounds i8, i8* %buf.addr.0, i64 40
  %add239 = add i32 %or236, -1958414417
  %add240 = add i32 %add239, %add145
  %add241 = add i32 %add240, %xor223
  %shl242 = shl i32 %add241, 12
  %shr243 = lshr i32 %add241, 20
  %or244 = add i32 %shr243, %add220
  %add245 = add i32 %or244, %shl242
  %xor246 = xor i32 %add220, %add195
  %and247 = and i32 %add245, %xor246
  %xor248 = xor i32 %and247, %add195
  %44 = load i8, i8* %add.ptr237, align 1
  %conv249 = zext i8 %44 to i32
  %add.ptr250 = getelementptr inbounds i8, i8* %buf.addr.0, i64 41
  %45 = load i8, i8* %add.ptr250, align 1
  %conv251 = zext i8 %45 to i32
  %shl252 = shl nuw nsw i32 %conv251, 8
  %or253 = or i32 %shl252, %conv249
  %add.ptr254 = getelementptr inbounds i8, i8* %buf.addr.0, i64 42
  %46 = load i8, i8* %add.ptr254, align 1
  %conv255 = zext i8 %46 to i32
  %shl256 = shl nuw nsw i32 %conv255, 16
  %or257 = or i32 %or253, %shl256
  %add.ptr258 = getelementptr inbounds i8, i8* %buf.addr.0, i64 43
  %47 = load i8, i8* %add.ptr258, align 1
  %conv259 = zext i8 %47 to i32
  %shl260 = shl nuw i32 %conv259, 24
  %or261 = or i32 %or257, %shl260
  %add.ptr262 = getelementptr inbounds i8, i8* %buf.addr.0, i64 44
  %add264 = add i32 %or261, -42063
  %add265 = add i32 %add264, %add170
  %add266 = add i32 %add265, %xor248
  %shl267 = shl i32 %add266, 17
  %shr268 = lshr i32 %add266, 15
  %or269 = add i32 %shr268, %add245
  %add270 = add i32 %or269, %shl267
  %xor271 = xor i32 %add245, %add220
  %and272 = and i32 %add270, %xor271
  %xor273 = xor i32 %and272, %add220
  %48 = load i8, i8* %add.ptr262, align 1
  %conv274 = zext i8 %48 to i32
  %add.ptr275 = getelementptr inbounds i8, i8* %buf.addr.0, i64 45
  %49 = load i8, i8* %add.ptr275, align 1
  %conv276 = zext i8 %49 to i32
  %shl277 = shl nuw nsw i32 %conv276, 8
  %or278 = or i32 %shl277, %conv274
  %add.ptr279 = getelementptr inbounds i8, i8* %buf.addr.0, i64 46
  %50 = load i8, i8* %add.ptr279, align 1
  %conv280 = zext i8 %50 to i32
  %shl281 = shl nuw nsw i32 %conv280, 16
  %or282 = or i32 %or278, %shl281
  %add.ptr283 = getelementptr inbounds i8, i8* %buf.addr.0, i64 47
  %51 = load i8, i8* %add.ptr283, align 1
  %conv284 = zext i8 %51 to i32
  %shl285 = shl nuw i32 %conv284, 24
  %or286 = or i32 %or282, %shl285
  %add.ptr287 = getelementptr inbounds i8, i8* %buf.addr.0, i64 48
  %add289 = add i32 %or286, -1990404162
  %add290 = add i32 %add289, %add195
  %add291 = add i32 %add290, %xor273
  %shl292 = shl i32 %add291, 22
  %shr293 = lshr i32 %add291, 10
  %or294 = add i32 %shr293, %add270
  %add295 = add i32 %or294, %shl292
  %xor296 = xor i32 %add270, %add245
  %and297 = and i32 %add295, %xor296
  %xor298 = xor i32 %and297, %add245
  %52 = load i8, i8* %add.ptr287, align 1
  %conv299 = zext i8 %52 to i32
  %add.ptr300 = getelementptr inbounds i8, i8* %buf.addr.0, i64 49
  %53 = load i8, i8* %add.ptr300, align 1
  %conv301 = zext i8 %53 to i32
  %shl302 = shl nuw nsw i32 %conv301, 8
  %or303 = or i32 %shl302, %conv299
  %add.ptr304 = getelementptr inbounds i8, i8* %buf.addr.0, i64 50
  %54 = load i8, i8* %add.ptr304, align 1
  %conv305 = zext i8 %54 to i32
  %shl306 = shl nuw nsw i32 %conv305, 16
  %or307 = or i32 %or303, %shl306
  %add.ptr308 = getelementptr inbounds i8, i8* %buf.addr.0, i64 51
  %55 = load i8, i8* %add.ptr308, align 1
  %conv309 = zext i8 %55 to i32
  %shl310 = shl nuw i32 %conv309, 24
  %or311 = or i32 %or307, %shl310
  %add.ptr312 = getelementptr inbounds i8, i8* %buf.addr.0, i64 52
  %add314 = add i32 %or311, 1804603682
  %add315 = add i32 %add314, %add220
  %add316 = add i32 %add315, %xor298
  %shl317 = shl i32 %add316, 7
  %shr318 = lshr i32 %add316, 25
  %or319 = add i32 %shr318, %add295
  %add320 = add i32 %or319, %shl317
  %xor321 = xor i32 %add295, %add270
  %and322 = and i32 %add320, %xor321
  %xor323 = xor i32 %and322, %add270
  %56 = load i8, i8* %add.ptr312, align 1
  %conv324 = zext i8 %56 to i32
  %add.ptr325 = getelementptr inbounds i8, i8* %buf.addr.0, i64 53
  %57 = load i8, i8* %add.ptr325, align 1
  %conv326 = zext i8 %57 to i32
  %shl327 = shl nuw nsw i32 %conv326, 8
  %or328 = or i32 %shl327, %conv324
  %add.ptr329 = getelementptr inbounds i8, i8* %buf.addr.0, i64 54
  %58 = load i8, i8* %add.ptr329, align 1
  %conv330 = zext i8 %58 to i32
  %shl331 = shl nuw nsw i32 %conv330, 16
  %or332 = or i32 %or328, %shl331
  %add.ptr333 = getelementptr inbounds i8, i8* %buf.addr.0, i64 55
  %59 = load i8, i8* %add.ptr333, align 1
  %conv334 = zext i8 %59 to i32
  %shl335 = shl nuw i32 %conv334, 24
  %or336 = or i32 %or332, %shl335
  %add.ptr337 = getelementptr inbounds i8, i8* %buf.addr.0, i64 56
  %add339 = add i32 %or336, -40341101
  %add340 = add i32 %add339, %add245
  %add341 = add i32 %add340, %xor323
  %shl342 = shl i32 %add341, 12
  %shr343 = lshr i32 %add341, 20
  %or344 = add i32 %shr343, %add320
  %add345 = add i32 %or344, %shl342
  %xor346 = xor i32 %add320, %add295
  %and347 = and i32 %add345, %xor346
  %xor348 = xor i32 %and347, %add295
  %60 = load i8, i8* %add.ptr337, align 1
  %conv349 = zext i8 %60 to i32
  %add.ptr350 = getelementptr inbounds i8, i8* %buf.addr.0, i64 57
  %61 = load i8, i8* %add.ptr350, align 1
  %conv351 = zext i8 %61 to i32
  %shl352 = shl nuw nsw i32 %conv351, 8
  %or353 = or i32 %shl352, %conv349
  %add.ptr354 = getelementptr inbounds i8, i8* %buf.addr.0, i64 58
  %62 = load i8, i8* %add.ptr354, align 1
  %conv355 = zext i8 %62 to i32
  %shl356 = shl nuw nsw i32 %conv355, 16
  %or357 = or i32 %or353, %shl356
  %add.ptr358 = getelementptr inbounds i8, i8* %buf.addr.0, i64 59
  %63 = load i8, i8* %add.ptr358, align 1
  %conv359 = zext i8 %63 to i32
  %shl360 = shl nuw i32 %conv359, 24
  %or361 = or i32 %or357, %shl360
  %add.ptr362 = getelementptr inbounds i8, i8* %buf.addr.0, i64 60
  %add364 = add i32 %or361, -1502002290
  %add365 = add i32 %add364, %add270
  %add366 = add i32 %add365, %xor348
  %shl367 = shl i32 %add366, 17
  %shr368 = lshr i32 %add366, 15
  %or369 = add i32 %shr368, %add345
  %add370 = add i32 %or369, %shl367
  %xor371 = xor i32 %add345, %add320
  %and372 = and i32 %add370, %xor371
  %xor373 = xor i32 %and372, %add320
  %64 = load i8, i8* %add.ptr362, align 1
  %conv374 = zext i8 %64 to i32
  %add.ptr375 = getelementptr inbounds i8, i8* %buf.addr.0, i64 61
  %65 = load i8, i8* %add.ptr375, align 1
  %conv376 = zext i8 %65 to i32
  %shl377 = shl nuw nsw i32 %conv376, 8
  %or378 = or i32 %shl377, %conv374
  %add.ptr379 = getelementptr inbounds i8, i8* %buf.addr.0, i64 62
  %66 = load i8, i8* %add.ptr379, align 1
  %conv380 = zext i8 %66 to i32
  %shl381 = shl nuw nsw i32 %conv380, 16
  %or382 = or i32 %or378, %shl381
  %add.ptr383 = getelementptr inbounds i8, i8* %buf.addr.0, i64 63
  %67 = load i8, i8* %add.ptr383, align 1
  %conv384 = zext i8 %67 to i32
  %shl385 = shl nuw i32 %conv384, 24
  %or386 = or i32 %or382, %shl385
  %add.ptr387 = getelementptr inbounds i8, i8* %buf.addr.0, i64 64
  %add389 = add i32 %or386, 1236535329
  %add390 = add i32 %add389, %add295
  %add391 = add i32 %add390, %xor373
  %shl392 = shl i32 %add391, 22
  %shr393 = lshr i32 %add391, 10
  %or394 = add i32 %shr393, %add370
  %add395 = add i32 %or394, %shl392
  %xor396 = xor i32 %add395, %add370
  %and397 = and i32 %xor396, %add345
  %xor398 = xor i32 %and397, %add370
  %add399 = add i32 %or36, -165796510
  %add400 = add i32 %add399, %add320
  %add401 = add i32 %add400, %xor398
  %shl402 = shl i32 %add401, 5
  %shr403 = lshr i32 %add401, 27
  %or404 = add i32 %shr403, %add395
  %add405 = add i32 %or404, %shl402
  %xor406 = xor i32 %add405, %add395
  %and407 = and i32 %xor406, %add370
  %xor408 = xor i32 %and407, %add395
  %add410 = add i32 %or161, -1069501632
  %add411 = add i32 %add410, %add345
  %add412 = add i32 %add411, %xor408
  %shl413 = shl i32 %add412, 9
  %shr414 = lshr i32 %add412, 23
  %or415 = add i32 %shr414, %add405
  %add416 = add i32 %or415, %shl413
  %xor417 = xor i32 %add416, %add405
  %and418 = and i32 %xor417, %add395
  %xor419 = xor i32 %and418, %add405
  %add421 = add i32 %or286, 643717713
  %add422 = add i32 %add421, %add370
  %add423 = add i32 %add422, %xor419
  %shl424 = shl i32 %add423, 14
  %shr425 = lshr i32 %add423, 18
  %or426 = add i32 %shr425, %add416
  %add427 = add i32 %or426, %shl424
  %xor428 = xor i32 %add427, %add416
  %and429 = and i32 %xor428, %add405
  %xor430 = xor i32 %and429, %add416
  %add432 = add i32 %or14, -373897302
  %add433 = add i32 %add432, %add395
  %add434 = add i32 %add433, %xor430
  %shl435 = shl i32 %add434, 20
  %shr436 = lshr i32 %add434, 12
  %or437 = add i32 %shr436, %add427
  %add438 = add i32 %or437, %shl435
  %xor439 = xor i32 %add438, %add427
  %and440 = and i32 %xor439, %add416
  %xor441 = xor i32 %and440, %add427
  %add443 = add i32 %or136, -701558691
  %add444 = add i32 %add443, %add405
  %add445 = add i32 %add444, %xor441
  %shl446 = shl i32 %add445, 5
  %shr447 = lshr i32 %add445, 27
  %or448 = add i32 %shr447, %add438
  %add449 = add i32 %or448, %shl446
  %xor450 = xor i32 %add449, %add438
  %and451 = and i32 %xor450, %add427
  %xor452 = xor i32 %and451, %add438
  %add454 = add i32 %or261, 38016083
  %add455 = add i32 %add454, %add416
  %add456 = add i32 %add455, %xor452
  %shl457 = shl i32 %add456, 9
  %shr458 = lshr i32 %add456, 23
  %or459 = add i32 %shr458, %add449
  %add460 = add i32 %or459, %shl457
  %xor461 = xor i32 %add460, %add449
  %and462 = and i32 %xor461, %add438
  %xor463 = xor i32 %and462, %add449
  %add465 = add i32 %or386, -660478335
  %add466 = add i32 %add465, %add427
  %add467 = add i32 %add466, %xor463
  %shl468 = shl i32 %add467, 14
  %shr469 = lshr i32 %add467, 18
  %or470 = add i32 %shr469, %add460
  %add471 = add i32 %or470, %shl468
  %xor472 = xor i32 %add471, %add460
  %and473 = and i32 %xor472, %add449
  %xor474 = xor i32 %and473, %add460
  %add476 = add i32 %or111, -405537848
  %add477 = add i32 %add476, %add438
  %add478 = add i32 %add477, %xor474
  %shl479 = shl i32 %add478, 20
  %shr480 = lshr i32 %add478, 12
  %or481 = add i32 %shr480, %add471
  %add482 = add i32 %or481, %shl479
  %xor483 = xor i32 %add482, %add471
  %and484 = and i32 %xor483, %add460
  %xor485 = xor i32 %and484, %add471
  %add487 = add i32 %or236, 568446438
  %add488 = add i32 %add487, %add449
  %add489 = add i32 %add488, %xor485
  %shl490 = shl i32 %add489, 5
  %shr491 = lshr i32 %add489, 27
  %or492 = add i32 %shr491, %add482
  %add493 = add i32 %or492, %shl490
  %xor494 = xor i32 %add493, %add482
  %and495 = and i32 %xor494, %add471
  %xor496 = xor i32 %and495, %add482
  %add498 = add i32 %or361, -1019803690
  %add499 = add i32 %add498, %add460
  %add500 = add i32 %add499, %xor496
  %shl501 = shl i32 %add500, 9
  %shr502 = lshr i32 %add500, 23
  %or503 = add i32 %shr502, %add493
  %add504 = add i32 %or503, %shl501
  %xor505 = xor i32 %add504, %add493
  %and506 = and i32 %xor505, %add482
  %xor507 = xor i32 %and506, %add493
  %add509 = add i32 %or86, -187363961
  %add510 = add i32 %add509, %add471
  %add511 = add i32 %add510, %xor507
  %shl512 = shl i32 %add511, 14
  %shr513 = lshr i32 %add511, 18
  %or514 = add i32 %shr513, %add504
  %add515 = add i32 %or514, %shl512
  %xor516 = xor i32 %add515, %add504
  %and517 = and i32 %xor516, %add493
  %xor518 = xor i32 %and517, %add504
  %add520 = add i32 %or211, 1163531501
  %add521 = add i32 %add520, %add482
  %add522 = add i32 %add521, %xor518
  %shl523 = shl i32 %add522, 20
  %shr524 = lshr i32 %add522, 12
  %or525 = add i32 %shr524, %add515
  %add526 = add i32 %or525, %shl523
  %xor527 = xor i32 %add526, %add515
  %and528 = and i32 %xor527, %add504
  %xor529 = xor i32 %and528, %add515
  %add531 = add i32 %or336, -1444681467
  %add532 = add i32 %add531, %add493
  %add533 = add i32 %add532, %xor529
  %shl534 = shl i32 %add533, 5
  %shr535 = lshr i32 %add533, 27
  %or536 = add i32 %shr535, %add526
  %add537 = add i32 %or536, %shl534
  %xor538 = xor i32 %add537, %add526
  %and539 = and i32 %xor538, %add515
  %xor540 = xor i32 %and539, %add526
  %add542 = add i32 %or61, -51403784
  %add543 = add i32 %add542, %add504
  %add544 = add i32 %add543, %xor540
  %shl545 = shl i32 %add544, 9
  %shr546 = lshr i32 %add544, 23
  %or547 = add i32 %shr546, %add537
  %add548 = add i32 %or547, %shl545
  %xor549 = xor i32 %add548, %add537
  %and550 = and i32 %xor549, %add526
  %xor551 = xor i32 %and550, %add537
  %add553 = add i32 %or186, 1735328473
  %add554 = add i32 %add553, %add515
  %add555 = add i32 %add554, %xor551
  %shl556 = shl i32 %add555, 14
  %shr557 = lshr i32 %add555, 18
  %or558 = add i32 %shr557, %add548
  %add559 = add i32 %or558, %shl556
  %xor560 = xor i32 %add559, %add548
  %and561 = and i32 %xor560, %add537
  %xor562 = xor i32 %and561, %add548
  %add564 = add i32 %or311, -1926607734
  %add565 = add i32 %add564, %add526
  %add566 = add i32 %add565, %xor562
  %shl567 = shl i32 %add566, 20
  %shr568 = lshr i32 %add566, 12
  %or569 = add i32 %shr568, %add559
  %add570 = add i32 %or569, %shl567
  %xor572 = xor i32 %xor560, %add570
  %add574 = add i32 %or136, -378558
  %add575 = add i32 %add574, %add537
  %add576 = add i32 %add575, %xor572
  %shl577 = shl i32 %add576, 4
  %shr578 = lshr i32 %add576, 28
  %or579 = add i32 %shr578, %add570
  %add580 = add i32 %or579, %shl577
  %xor581 = xor i32 %add570, %add559
  %xor582 = xor i32 %xor581, %add580
  %add584 = add i32 %or211, -2022574463
  %add585 = add i32 %add584, %add548
  %add586 = add i32 %add585, %xor582
  %shl587 = shl i32 %add586, 11
  %shr588 = lshr i32 %add586, 21
  %or589 = add i32 %shr588, %add580
  %add590 = add i32 %or589, %shl587
  %xor591 = xor i32 %add580, %add570
  %xor592 = xor i32 %xor591, %add590
  %add594 = add i32 %or286, 1839030562
  %add595 = add i32 %add594, %add559
  %add596 = add i32 %add595, %xor592
  %shl597 = shl i32 %add596, 16
  %shr598 = lshr i32 %add596, 16
  %or599 = add i32 %shr598, %add590
  %add600 = add i32 %or599, %shl597
  %xor601 = xor i32 %add590, %add580
  %xor602 = xor i32 %xor601, %add600
  %add604 = add i32 %or361, -35309556
  %add605 = add i32 %add604, %add570
  %add606 = add i32 %add605, %xor602
  %shl607 = shl i32 %add606, 23
  %shr608 = lshr i32 %add606, 9
  %or609 = add i32 %shr608, %add600
  %add610 = add i32 %or609, %shl607
  %xor611 = xor i32 %add600, %add590
  %xor612 = xor i32 %xor611, %add610
  %add614 = add i32 %or36, -1530992060
  %add615 = add i32 %add614, %add580
  %add616 = add i32 %add615, %xor612
  %shl617 = shl i32 %add616, 4
  %shr618 = lshr i32 %add616, 28
  %or619 = add i32 %shr618, %add610
  %add620 = add i32 %or619, %shl617
  %xor621 = xor i32 %add610, %add600
  %xor622 = xor i32 %xor621, %add620
  %add624 = add i32 %or111, 1272893353
  %add625 = add i32 %add624, %add590
  %add626 = add i32 %add625, %xor622
  %shl627 = shl i32 %add626, 11
  %shr628 = lshr i32 %add626, 21
  %or629 = add i32 %shr628, %add620
  %add630 = add i32 %or629, %shl627
  %xor631 = xor i32 %add620, %add610
  %xor632 = xor i32 %xor631, %add630
  %add634 = add i32 %or186, -155497632
  %add635 = add i32 %add634, %add600
  %add636 = add i32 %add635, %xor632
  %shl637 = shl i32 %add636, 16
  %shr638 = lshr i32 %add636, 16
  %or639 = add i32 %shr638, %add630
  %add640 = add i32 %or639, %shl637
  %xor641 = xor i32 %add630, %add620
  %xor642 = xor i32 %xor641, %add640
  %add644 = add i32 %or261, -1094730640
  %add645 = add i32 %add644, %add610
  %add646 = add i32 %add645, %xor642
  %shl647 = shl i32 %add646, 23
  %shr648 = lshr i32 %add646, 9
  %or649 = add i32 %shr648, %add640
  %add650 = add i32 %or649, %shl647
  %xor651 = xor i32 %add640, %add630
  %xor652 = xor i32 %xor651, %add650
  %add654 = add i32 %or336, 681279174
  %add655 = add i32 %add654, %add620
  %add656 = add i32 %add655, %xor652
  %shl657 = shl i32 %add656, 4
  %shr658 = lshr i32 %add656, 28
  %or659 = add i32 %shr658, %add650
  %add660 = add i32 %or659, %shl657
  %xor661 = xor i32 %add650, %add640
  %xor662 = xor i32 %xor661, %add660
  %add664 = add i32 %or14, -358537222
  %add665 = add i32 %add664, %add630
  %add666 = add i32 %add665, %xor662
  %shl667 = shl i32 %add666, 11
  %shr668 = lshr i32 %add666, 21
  %or669 = add i32 %shr668, %add660
  %add670 = add i32 %or669, %shl667
  %xor671 = xor i32 %add660, %add650
  %xor672 = xor i32 %xor671, %add670
  %add674 = add i32 %or86, -722521979
  %add675 = add i32 %add674, %add640
  %add676 = add i32 %add675, %xor672
  %shl677 = shl i32 %add676, 16
  %shr678 = lshr i32 %add676, 16
  %or679 = add i32 %shr678, %add670
  %add680 = add i32 %or679, %shl677
  %xor681 = xor i32 %add670, %add660
  %xor682 = xor i32 %xor681, %add680
  %add684 = add i32 %or161, 76029189
  %add685 = add i32 %add684, %add650
  %add686 = add i32 %add685, %xor682
  %shl687 = shl i32 %add686, 23
  %shr688 = lshr i32 %add686, 9
  %or689 = add i32 %shr688, %add680
  %add690 = add i32 %or689, %shl687
  %xor691 = xor i32 %add680, %add670
  %xor692 = xor i32 %xor691, %add690
  %add694 = add i32 %or236, -640364487
  %add695 = add i32 %add694, %add660
  %add696 = add i32 %add695, %xor692
  %shl697 = shl i32 %add696, 4
  %shr698 = lshr i32 %add696, 28
  %or699 = add i32 %shr698, %add690
  %add700 = add i32 %or699, %shl697
  %xor701 = xor i32 %add690, %add680
  %xor702 = xor i32 %xor701, %add700
  %add704 = add i32 %or311, -421815835
  %add705 = add i32 %add704, %add670
  %add706 = add i32 %add705, %xor702
  %shl707 = shl i32 %add706, 11
  %shr708 = lshr i32 %add706, 21
  %or709 = add i32 %shr708, %add700
  %add710 = add i32 %or709, %shl707
  %xor711 = xor i32 %add700, %add690
  %xor712 = xor i32 %xor711, %add710
  %add714 = add i32 %or386, 530742520
  %add715 = add i32 %add714, %add680
  %add716 = add i32 %add715, %xor712
  %shl717 = shl i32 %add716, 16
  %shr718 = lshr i32 %add716, 16
  %or719 = add i32 %shr718, %add710
  %add720 = add i32 %or719, %shl717
  %xor721 = xor i32 %add710, %add700
  %xor722 = xor i32 %xor721, %add720
  %add724 = add i32 %or61, -995338651
  %add725 = add i32 %add724, %add690
  %add726 = add i32 %add725, %xor722
  %shl727 = shl i32 %add726, 23
  %shr728 = lshr i32 %add726, 9
  %or729 = add i32 %shr728, %add720
  %add730 = add i32 %or729, %shl727
  %neg = xor i32 %add710, -1
  %or731 = or i32 %add730, %neg
  %xor732 = xor i32 %or731, %add720
  %add734 = add i32 %or14, -198630844
  %add735 = add i32 %add734, %add700
  %add736 = add i32 %add735, %xor732
  %shl737 = shl i32 %add736, 6
  %shr738 = lshr i32 %add736, 26
  %or739 = add i32 %shr738, %add730
  %add740 = add i32 %or739, %shl737
  %neg741 = xor i32 %add720, -1
  %or742 = or i32 %add740, %neg741
  %xor743 = xor i32 %or742, %add730
  %add745 = add i32 %or186, 1126891415
  %add746 = add i32 %add745, %add710
  %add747 = add i32 %add746, %xor743
  %shl748 = shl i32 %add747, 10
  %shr749 = lshr i32 %add747, 22
  %or750 = add i32 %shr749, %add740
  %add751 = add i32 %or750, %shl748
  %neg752 = xor i32 %add730, -1
  %or753 = or i32 %add751, %neg752
  %xor754 = xor i32 %or753, %add740
  %add756 = add i32 %or361, -1416354905
  %add757 = add i32 %add756, %add720
  %add758 = add i32 %add757, %xor754
  %shl759 = shl i32 %add758, 15
  %shr760 = lshr i32 %add758, 17
  %or761 = add i32 %shr760, %add751
  %add762 = add i32 %or761, %shl759
  %neg763 = xor i32 %add740, -1
  %or764 = or i32 %add762, %neg763
  %xor765 = xor i32 %or764, %add751
  %add767 = add i32 %or136, -57434055
  %add768 = add i32 %add767, %add730
  %add769 = add i32 %add768, %xor765
  %shl770 = shl i32 %add769, 21
  %shr771 = lshr i32 %add769, 11
  %or772 = add i32 %shr771, %add762
  %add773 = add i32 %or772, %shl770
  %neg774 = xor i32 %add751, -1
  %or775 = or i32 %add773, %neg774
  %xor776 = xor i32 %or775, %add762
  %add778 = add i32 %or311, 1700485571
  %add779 = add i32 %add778, %add740
  %add780 = add i32 %add779, %xor776
  %shl781 = shl i32 %add780, 6
  %shr782 = lshr i32 %add780, 26
  %or783 = add i32 %shr782, %add773
  %add784 = add i32 %or783, %shl781
  %neg785 = xor i32 %add762, -1
  %or786 = or i32 %add784, %neg785
  %xor787 = xor i32 %or786, %add773
  %add789 = add i32 %or86, -1894986606
  %add790 = add i32 %add789, %add751
  %add791 = add i32 %add790, %xor787
  %shl792 = shl i32 %add791, 10
  %shr793 = lshr i32 %add791, 22
  %or794 = add i32 %shr793, %add784
  %add795 = add i32 %or794, %shl792
  %neg796 = xor i32 %add773, -1
  %or797 = or i32 %add795, %neg796
  %xor798 = xor i32 %or797, %add784
  %add800 = add i32 %or261, -1051523
  %add801 = add i32 %add800, %add762
  %add802 = add i32 %add801, %xor798
  %shl803 = shl i32 %add802, 15
  %shr804 = lshr i32 %add802, 17
  %or805 = add i32 %shr804, %add795
  %add806 = add i32 %or805, %shl803
  %neg807 = xor i32 %add784, -1
  %or808 = or i32 %add806, %neg807
  %xor809 = xor i32 %or808, %add795
  %add811 = add i32 %or36, -2054922799
  %add812 = add i32 %add811, %add773
  %add813 = add i32 %add812, %xor809
  %shl814 = shl i32 %add813, 21
  %shr815 = lshr i32 %add813, 11
  %or816 = add i32 %shr815, %add806
  %add817 = add i32 %or816, %shl814
  %neg818 = xor i32 %add795, -1
  %or819 = or i32 %add817, %neg818
  %xor820 = xor i32 %or819, %add806
  %add822 = add i32 %or211, 1873313359
  %add823 = add i32 %add822, %add784
  %add824 = add i32 %add823, %xor820
  %shl825 = shl i32 %add824, 6
  %shr826 = lshr i32 %add824, 26
  %or827 = add i32 %shr826, %add817
  %add828 = add i32 %or827, %shl825
  %neg829 = xor i32 %add806, -1
  %or830 = or i32 %add828, %neg829
  %xor831 = xor i32 %or830, %add817
  %add833 = add i32 %or386, -30611744
  %add834 = add i32 %add833, %add795
  %add835 = add i32 %add834, %xor831
  %shl836 = shl i32 %add835, 10
  %shr837 = lshr i32 %add835, 22
  %or838 = add i32 %shr837, %add828
  %add839 = add i32 %or838, %shl836
  %neg840 = xor i32 %add817, -1
  %or841 = or i32 %add839, %neg840
  %xor842 = xor i32 %or841, %add828
  %add844 = add i32 %or161, -1560198380
  %add845 = add i32 %add844, %add806
  %add846 = add i32 %add845, %xor842
  %shl847 = shl i32 %add846, 15
  %shr848 = lshr i32 %add846, 17
  %or849 = add i32 %shr848, %add839
  %add850 = add i32 %or849, %shl847
  %neg851 = xor i32 %add828, -1
  %or852 = or i32 %add850, %neg851
  %xor853 = xor i32 %or852, %add839
  %add855 = add i32 %or336, 1309151649
  %add856 = add i32 %add855, %add817
  %add857 = add i32 %add856, %xor853
  %shl858 = shl i32 %add857, 21
  %shr859 = lshr i32 %add857, 11
  %or860 = add i32 %shr859, %add850
  %add861 = add i32 %or860, %shl858
  %neg862 = xor i32 %add839, -1
  %or863 = or i32 %add861, %neg862
  %xor864 = xor i32 %or863, %add850
  %add866 = add i32 %or111, -145523070
  %add867 = add i32 %add866, %add828
  %add868 = add i32 %add867, %xor864
  %shl869 = shl i32 %add868, 6
  %shr870 = lshr i32 %add868, 26
  %or871 = add i32 %shr870, %add861
  %add872 = add i32 %or871, %shl869
  %neg873 = xor i32 %add850, -1
  %or874 = or i32 %add872, %neg873
  %xor875 = xor i32 %or874, %add861
  %add877 = add i32 %or286, -1120210379
  %add878 = add i32 %add877, %add839
  %add879 = add i32 %add878, %xor875
  %shl880 = shl i32 %add879, 10
  %shr881 = lshr i32 %add879, 22
  %or882 = add i32 %shr881, %add872
  %add883 = add i32 %or882, %shl880
  %neg884 = xor i32 %add861, -1
  %or885 = or i32 %add883, %neg884
  %xor886 = xor i32 %or885, %add872
  %add888 = add i32 %or61, 718787259
  %add889 = add i32 %add888, %add850
  %add890 = add i32 %add889, %xor886
  %shl891 = shl i32 %add890, 15
  %shr892 = lshr i32 %add890, 17
  %or893 = add i32 %shr892, %add883
  %add894 = add i32 %or893, %shl891
  %neg895 = xor i32 %add872, -1
  %or896 = or i32 %add894, %neg895
  %xor897 = xor i32 %or896, %add883
  %add899 = add i32 %or236, -343485551
  %add900 = add i32 %add899, %add861
  %add901 = add i32 %add900, %xor897
  %shl902 = shl i32 %add901, 21
  %shr903 = lshr i32 %add901, 11
  %add906 = add i32 %add872, %A.0
  %or904 = add i32 %add894, %B.0
  %add905 = add i32 %or904, %shr903
  %add907 = add i32 %add905, %shl902
  %add908 = add i32 %add894, %C.0
  %add909 = add i32 %add883, %D.0
  %dec = add i64 %blocks.addr.0, -1
  %tobool.not = icmp eq i64 %dec, 0
  br i1 %tobool.not, label %do.end, label %do.body

do.end:                                           ; preds = %do.body
  %add906.lcssa = phi i32 [ %add906, %do.body ]
  %add907.lcssa = phi i32 [ %add907, %do.body ]
  %add908.lcssa = phi i32 [ %add908, %do.body ]
  %add909.lcssa = phi i32 [ %add909, %do.body ]
  store i32 %add906.lcssa, i32* %A1, align 4
  store i32 %add907.lcssa, i32* %B2, align 4
  store i32 %add908.lcssa, i32* %C3, align 4
  store i32 %add909.lcssa, i32* %D4, align 4
  ret void
}

