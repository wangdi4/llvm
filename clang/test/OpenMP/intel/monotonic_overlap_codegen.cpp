// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -std=c++11 -fopenmp-late-outline \
// RUN:  -opaque-pointers -triple x86_64-unknown-linux-gnu %s | FileCheck %s

// expected-no-diagnostics

typedef int(*fptr)();
int bar();
// CHECK-LABEL: @_Z3foov(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[K:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca double, align 8
// CHECK-NEXT:    [[B:%.*]] = alloca double, align 8
// CHECK-NEXT:    [[C:%.*]] = alloca double, align 8
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[L:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP5:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV6:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB7:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I11:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[FP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP19:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV20:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB21:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I25:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP34:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV35:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB36:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I40:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 0, ptr [[K]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED"(ptr [[K]]), "QUAL.OMP.PRIVATE"(ptr [[I]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[L]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// CHECK-NEXT:    store i32 0, ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 1024
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    store i32 15, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 1), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB]]), "QUAL.OMP.LINEAR:IV"(ptr [[L]], i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP1:%.*]] = icmp sle i32 [[TMP3]], [[TMP4]]
// CHECK-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[L]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP6]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[K]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I]], i32 10) ]
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[L]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// CHECK-NEXT:    store i32 [[ADD2]], ptr [[K]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.ORDERED"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP10]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP3:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[INC4:%.*]] = add nsw i32 [[TMP11]], 1
// CHECK-NEXT:    store i32 [[INC4]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    store i32 1023, ptr [[DOTOMP_UB7]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV6]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB7]]), "QUAL.OMP.LINEAR:IV"(ptr [[I11]], i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV6]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND8:%.*]]
// CHECK:       omp.inner.for.cond8:
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB7]], align 4
// CHECK-NEXT:    [[CMP9:%.*]] = icmp sle i32 [[TMP13]], [[TMP14]]
// CHECK-NEXT:    br i1 [[CMP9]], label [[OMP_INNER_FOR_BODY10:%.*]], label [[OMP_INNER_FOR_END17:%.*]]
// CHECK:       omp.inner.for.body10:
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// CHECK-NEXT:    [[MUL12:%.*]] = mul nsw i32 [[TMP15]], 1
// CHECK-NEXT:    [[ADD13:%.*]] = add nsw i32 0, [[MUL12]]
// CHECK-NEXT:    store i32 [[ADD13]], ptr [[I11]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I11]], i32 0), "QUAL.OMP.MONOTONIC"(ptr [[K]], i32 0) ]
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP17]], 0
// CHECK-NEXT:    store i32 [[SUB]], ptr [[K]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.ORDERED"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE14:%.*]]
// CHECK:       omp.body.continue14:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC15:%.*]]
// CHECK:       omp.inner.for.inc15:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// CHECK-NEXT:    [[ADD16:%.*]] = add nsw i32 [[TMP18]], 1
// CHECK-NEXT:    store i32 [[ADD16]], ptr [[DOTOMP_IV6]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND8]], !llvm.loop [[LOOP7:![0-9]+]]
// CHECK:       omp.inner.for.end17:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT18:%.*]]
// CHECK:       omp.loop.exit18:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    store ptr @_Z3barv, ptr [[FP]], align 8
// CHECK-NEXT:    store i32 1023, ptr [[DOTOMP_UB21]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV20]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB21]]), "QUAL.OMP.LINEAR:IV"(ptr [[I25]], i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV20]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND22:%.*]]
// CHECK:       omp.inner.for.cond22:
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_UB21]], align 4
// CHECK-NEXT:    [[CMP23:%.*]] = icmp sle i32 [[TMP20]], [[TMP21]]
// CHECK-NEXT:    br i1 [[CMP23]], label [[OMP_INNER_FOR_BODY24:%.*]], label [[OMP_INNER_FOR_END32:%.*]]
// CHECK:       omp.inner.for.body24:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// CHECK-NEXT:    [[MUL26:%.*]] = mul nsw i32 [[TMP22]], 1
// CHECK-NEXT:    [[ADD27:%.*]] = add nsw i32 0, [[MUL26]]
// CHECK-NEXT:    store i32 [[ADD27]], ptr [[I25]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[FP]], align 8
// CHECK-NEXT:    [[TMP24:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(ptr [[TMP23]]) ]
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[SUB28:%.*]] = sub nsw i32 [[TMP25]], 0
// CHECK-NEXT:    store i32 [[SUB28]], ptr [[K]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP24]]) [ "DIR.OMP.END.ORDERED"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE29:%.*]]
// CHECK:       omp.body.continue29:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC30:%.*]]
// CHECK:       omp.inner.for.inc30:
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// CHECK-NEXT:    [[ADD31:%.*]] = add nsw i32 [[TMP26]], 1
// CHECK-NEXT:    store i32 [[ADD31]], ptr [[DOTOMP_IV20]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND22]], !llvm.loop [[LOOP8:![0-9]+]]
// CHECK:       omp.inner.for.end32:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT33:%.*]]
// CHECK:       omp.loop.exit33:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    store i32 1023, ptr [[DOTOMP_UB36]], align 4
// CHECK-NEXT:    [[TMP27:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV35]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB36]]), "QUAL.OMP.LINEAR:IV"(ptr [[I40]], i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV35]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND37:%.*]]
// CHECK:       omp.inner.for.cond37:
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTOMP_UB36]], align 4
// CHECK-NEXT:    [[CMP38:%.*]] = icmp sle i32 [[TMP28]], [[TMP29]]
// CHECK-NEXT:    br i1 [[CMP38]], label [[OMP_INNER_FOR_BODY39:%.*]], label [[OMP_INNER_FOR_END48:%.*]]
// CHECK:       omp.inner.for.body39:
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// CHECK-NEXT:    [[MUL41:%.*]] = mul nsw i32 [[TMP30]], 1
// CHECK-NEXT:    [[ADD42:%.*]] = add nsw i32 0, [[MUL41]]
// CHECK-NEXT:    store i32 [[ADD42]], ptr [[I40]], align 4
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[I40]], align 4
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[ADD43:%.*]] = add nsw i32 [[TMP31]], [[TMP32]]
// CHECK-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(i32 [[ADD43]]) ]
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[K]], align 4
// CHECK-NEXT:    [[SUB44:%.*]] = sub nsw i32 [[TMP34]], 0
// CHECK-NEXT:    store i32 [[SUB44]], ptr [[K]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.ORDERED"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE45:%.*]]
// CHECK:       omp.body.continue45:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC46:%.*]]
// CHECK:       omp.inner.for.inc46:
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// CHECK-NEXT:    [[ADD47:%.*]] = add nsw i32 [[TMP35]], 1
// CHECK-NEXT:    store i32 [[ADD47]], ptr [[DOTOMP_IV35]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND37]], !llvm.loop [[LOOP9:![0-9]+]]
// CHECK:       omp.inner.for.end48:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT49:%.*]]
// CHECK:       omp.loop.exit49:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP27]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    ret void
//
void foo()
{
  int k = 0;
  double a, b, c;

#pragma omp parallel
  for (int i = 0; i < 1024; i++) {
#pragma omp simd simdlen(1)
    for (int l = 0; l < 16; l++) {
      k++;
#pragma omp ordered simd ompx_monotonic(i : 10)
      {
        k += l;
      }
    }
  }
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_monotonic(i,k)
    k -= 0;
  }
  fptr fp = &bar;
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_overlap(fp)
    k -= 0;
  }
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_overlap(i+k)
    k -= 0;
  }
}
