// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --prefix-filecheck-ir-name _ --version 3
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -std=c++11 -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s -check-prefix ROT0

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -std=c++11 -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s -check-prefix ROT1

// expected-no-diagnostics

typedef int(*fptr)();
int bar();

// ROT0-LABEL: define dso_local void @_Z3foov(
// ROT0-SAME: ) #[[ATTR0:[0-9]+]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[K:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[A:%.*]] = alloca double, align 8
// ROT0-NEXT:    [[B:%.*]] = alloca double, align 8
// ROT0-NEXT:    [[C:%.*]] = alloca double, align 8
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[L:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[_TMP5:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV6:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB7:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I11:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[FP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[_TMP19:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV20:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB21:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I25:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[_TMP34:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV35:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB36:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I40:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 0, ptr [[K]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED:TYPED"(ptr [[K]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[L]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// ROT0-NEXT:    store i32 0, ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 1024
// ROT0-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    store i32 15, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[L]], i32 0, i32 1, i32 1) ]
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[CMP1:%.*]] = icmp sle i32 [[TMP3]], [[TMP4]]
// ROT0-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD]], ptr [[L]], align 4
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP6]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[K]], align 4
// ROT0-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I]], i32 10) ]
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[L]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// ROT0-NEXT:    store i32 [[ADD2]], ptr [[K]], align 4
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP10]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP3:![0-9]+]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.SIMD"() ]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[INC4:%.*]] = add nsw i32 [[TMP11]], 1
// ROT0-NEXT:    store i32 [[INC4]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// ROT0:       for.end:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT0-NEXT:    store i32 1023, ptr [[DOTOMP_UB7]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV6]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB7]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I11]], i32 0, i32 1, i32 1) ]
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_IV6]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND8:%.*]]
// ROT0:       omp.inner.for.cond8:
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// ROT0-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB7]], align 4
// ROT0-NEXT:    [[CMP9:%.*]] = icmp sle i32 [[TMP13]], [[TMP14]]
// ROT0-NEXT:    br i1 [[CMP9]], label [[OMP_INNER_FOR_BODY10:%.*]], label [[OMP_INNER_FOR_END17:%.*]]
// ROT0:       omp.inner.for.body10:
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// ROT0-NEXT:    [[MUL12:%.*]] = mul nsw i32 [[TMP15]], 1
// ROT0-NEXT:    [[ADD13:%.*]] = add nsw i32 0, [[MUL12]]
// ROT0-NEXT:    store i32 [[ADD13]], ptr [[I11]], align 4
// ROT0-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I11]], i32 0), "QUAL.OMP.MONOTONIC"(ptr [[K]], i32 0) ]
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP17]], 0
// ROT0-NEXT:    store i32 [[SUB]], ptr [[K]], align 4
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE14:%.*]]
// ROT0:       omp.body.continue14:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC15:%.*]]
// ROT0:       omp.inner.for.inc15:
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV6]], align 4
// ROT0-NEXT:    [[ADD16:%.*]] = add nsw i32 [[TMP18]], 1
// ROT0-NEXT:    store i32 [[ADD16]], ptr [[DOTOMP_IV6]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND8]], !llvm.loop [[LOOP7:![0-9]+]]
// ROT0:       omp.inner.for.end17:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT18:%.*]]
// ROT0:       omp.loop.exit18:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.SIMD"() ]
// ROT0-NEXT:    store ptr @_Z3barv, ptr [[FP]], align 8
// ROT0-NEXT:    store i32 1023, ptr [[DOTOMP_UB21]], align 4
// ROT0-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV20]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB21]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I25]], i32 0, i32 1, i32 1) ]
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_IV20]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND22:%.*]]
// ROT0:       omp.inner.for.cond22:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_UB21]], align 4
// ROT0-NEXT:    [[CMP23:%.*]] = icmp sle i32 [[TMP20]], [[TMP21]]
// ROT0-NEXT:    br i1 [[CMP23]], label [[OMP_INNER_FOR_BODY24:%.*]], label [[OMP_INNER_FOR_END32:%.*]]
// ROT0:       omp.inner.for.body24:
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT0-NEXT:    [[MUL26:%.*]] = mul nsw i32 [[TMP22]], 1
// ROT0-NEXT:    [[ADD27:%.*]] = add nsw i32 0, [[MUL26]]
// ROT0-NEXT:    store i32 [[ADD27]], ptr [[I25]], align 4
// ROT0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[FP]], align 8
// ROT0-NEXT:    [[TMP24:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(ptr [[TMP23]]) ]
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[SUB28:%.*]] = sub nsw i32 [[TMP25]], 0
// ROT0-NEXT:    store i32 [[SUB28]], ptr [[K]], align 4
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP24]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE29:%.*]]
// ROT0:       omp.body.continue29:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC30:%.*]]
// ROT0:       omp.inner.for.inc30:
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT0-NEXT:    [[ADD31:%.*]] = add nsw i32 [[TMP26]], 1
// ROT0-NEXT:    store i32 [[ADD31]], ptr [[DOTOMP_IV20]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND22]], !llvm.loop [[LOOP8:![0-9]+]]
// ROT0:       omp.inner.for.end32:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT33:%.*]]
// ROT0:       omp.loop.exit33:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.SIMD"() ]
// ROT0-NEXT:    store i32 1023, ptr [[DOTOMP_UB36]], align 4
// ROT0-NEXT:    [[TMP27:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV35]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB36]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I40]], i32 0, i32 1, i32 1) ]
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_IV35]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND37:%.*]]
// ROT0:       omp.inner.for.cond37:
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTOMP_UB36]], align 4
// ROT0-NEXT:    [[CMP38:%.*]] = icmp sle i32 [[TMP28]], [[TMP29]]
// ROT0-NEXT:    br i1 [[CMP38]], label [[OMP_INNER_FOR_BODY39:%.*]], label [[OMP_INNER_FOR_END48:%.*]]
// ROT0:       omp.inner.for.body39:
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// ROT0-NEXT:    [[MUL41:%.*]] = mul nsw i32 [[TMP30]], 1
// ROT0-NEXT:    [[ADD42:%.*]] = add nsw i32 0, [[MUL41]]
// ROT0-NEXT:    store i32 [[ADD42]], ptr [[I40]], align 4
// ROT0-NEXT:    [[TMP31:%.*]] = load i32, ptr [[I40]], align 4
// ROT0-NEXT:    [[TMP32:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[ADD43:%.*]] = add nsw i32 [[TMP31]], [[TMP32]]
// ROT0-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(i32 [[ADD43]]) ]
// ROT0-NEXT:    [[TMP34:%.*]] = load i32, ptr [[K]], align 4
// ROT0-NEXT:    [[SUB44:%.*]] = sub nsw i32 [[TMP34]], 0
// ROT0-NEXT:    store i32 [[SUB44]], ptr [[K]], align 4
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE45:%.*]]
// ROT0:       omp.body.continue45:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC46:%.*]]
// ROT0:       omp.inner.for.inc46:
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTOMP_IV35]], align 4
// ROT0-NEXT:    [[ADD47:%.*]] = add nsw i32 [[TMP35]], 1
// ROT0-NEXT:    store i32 [[ADD47]], ptr [[DOTOMP_IV35]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND37]], !llvm.loop [[LOOP9:![0-9]+]]
// ROT0:       omp.inner.for.end48:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT49:%.*]]
// ROT0:       omp.loop.exit49:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP27]]) [ "DIR.OMP.END.SIMD"() ]
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z3foov(
// ROT1-SAME: ) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[K:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca double, align 8
// ROT1-NEXT:    [[B:%.*]] = alloca double, align 8
// ROT1-NEXT:    [[C:%.*]] = alloca double, align 8
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[L:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP6:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV7:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB8:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I12:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[FP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[_TMP22:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV23:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB24:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I28:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP39:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV40:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB41:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I45:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 0, ptr [[K]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED:TYPED"(ptr [[K]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[L]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 1024
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    store i32 15, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.SIMDLEN"(i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[L]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP1:%.*]] = icmp sle i32 [[TMP3]], [[TMP4]]
// ROT1-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[L]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP6]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[K]], align 4
// ROT1-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I]], i32 10) ]
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[L]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// ROT1-NEXT:    store i32 [[ADD2]], ptr [[K]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP10]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP4:%.*]] = icmp sle i32 [[TMP11]], [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]], !llvm.loop [[LOOP3:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[INC5:%.*]] = add nsw i32 [[TMP13]], 1
// ROT1-NEXT:    store i32 [[INC5]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// ROT1:       for.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    store i32 1023, ptr [[DOTOMP_UB8]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV7]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB8]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I12]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_UB8]], align 4
// ROT1-NEXT:    [[CMP9:%.*]] = icmp sle i32 [[TMP15]], [[TMP16]]
// ROT1-NEXT:    br i1 [[CMP9]], label [[OMP_INNER_FOR_BODY_LH10:%.*]], label [[OMP_INNER_FOR_END20:%.*]]
// ROT1:       omp.inner.for.body.lh10:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY11:%.*]]
// ROT1:       omp.inner.for.body11:
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[MUL13:%.*]] = mul nsw i32 [[TMP17]], 1
// ROT1-NEXT:    [[ADD14:%.*]] = add nsw i32 0, [[MUL13]]
// ROT1-NEXT:    store i32 [[ADD14]], ptr [[I12]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.MONOTONIC"(ptr [[I12]], i32 0), "QUAL.OMP.MONOTONIC"(ptr [[K]], i32 0) ]
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP19]], 0
// ROT1-NEXT:    store i32 [[SUB]], ptr [[K]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP18]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE15:%.*]]
// ROT1:       omp.body.continue15:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC16:%.*]]
// ROT1:       omp.inner.for.inc16:
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[ADD17:%.*]] = add nsw i32 [[TMP20]], 1
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_IV7]], align 4
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTOMP_UB8]], align 4
// ROT1-NEXT:    [[CMP18:%.*]] = icmp sle i32 [[TMP21]], [[TMP22]]
// ROT1-NEXT:    br i1 [[CMP18]], label [[OMP_INNER_FOR_BODY11]], label [[OMP_INNER_FOR_END_CRIT_EDGE19:%.*]], !llvm.loop [[LOOP7:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge19:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END20]]
// ROT1:       omp.inner.for.end20:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT21:%.*]]
// ROT1:       omp.loop.exit21:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    store ptr @_Z3barv, ptr [[FP]], align 8
// ROT1-NEXT:    store i32 1023, ptr [[DOTOMP_UB24]], align 4
// ROT1-NEXT:    [[TMP23:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV23]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB24]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I28]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTOMP_UB24]], align 4
// ROT1-NEXT:    [[CMP25:%.*]] = icmp sle i32 [[TMP24]], [[TMP25]]
// ROT1-NEXT:    br i1 [[CMP25]], label [[OMP_INNER_FOR_BODY_LH26:%.*]], label [[OMP_INNER_FOR_END37:%.*]]
// ROT1:       omp.inner.for.body.lh26:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY27:%.*]]
// ROT1:       omp.inner.for.body27:
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[MUL29:%.*]] = mul nsw i32 [[TMP26]], 1
// ROT1-NEXT:    [[ADD30:%.*]] = add nsw i32 0, [[MUL29]]
// ROT1-NEXT:    store i32 [[ADD30]], ptr [[I28]], align 4
// ROT1-NEXT:    [[TMP27:%.*]] = load ptr, ptr [[FP]], align 8
// ROT1-NEXT:    [[TMP28:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(ptr [[TMP27]]) ]
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[SUB31:%.*]] = sub nsw i32 [[TMP29]], 0
// ROT1-NEXT:    store i32 [[SUB31]], ptr [[K]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP28]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE32:%.*]]
// ROT1:       omp.body.continue32:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC33:%.*]]
// ROT1:       omp.inner.for.inc33:
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[ADD34:%.*]] = add nsw i32 [[TMP30]], 1
// ROT1-NEXT:    store i32 [[ADD34]], ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTOMP_UB24]], align 4
// ROT1-NEXT:    [[CMP35:%.*]] = icmp sle i32 [[TMP31]], [[TMP32]]
// ROT1-NEXT:    br i1 [[CMP35]], label [[OMP_INNER_FOR_BODY27]], label [[OMP_INNER_FOR_END_CRIT_EDGE36:%.*]], !llvm.loop [[LOOP8:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge36:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END37]]
// ROT1:       omp.inner.for.end37:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT38:%.*]]
// ROT1:       omp.loop.exit38:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP23]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    store i32 1023, ptr [[DOTOMP_UB41]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV40]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB41]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I45]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTOMP_UB41]], align 4
// ROT1-NEXT:    [[CMP42:%.*]] = icmp sle i32 [[TMP34]], [[TMP35]]
// ROT1-NEXT:    br i1 [[CMP42]], label [[OMP_INNER_FOR_BODY_LH43:%.*]], label [[OMP_INNER_FOR_END55:%.*]]
// ROT1:       omp.inner.for.body.lh43:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY44:%.*]]
// ROT1:       omp.inner.for.body44:
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[MUL46:%.*]] = mul nsw i32 [[TMP36]], 1
// ROT1-NEXT:    [[ADD47:%.*]] = add nsw i32 0, [[MUL46]]
// ROT1-NEXT:    store i32 [[ADD47]], ptr [[I45]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[I45]], align 4
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[ADD48:%.*]] = add nsw i32 [[TMP37]], [[TMP38]]
// ROT1-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.ORDERED"(), "QUAL.OMP.ORDERED.SIMD"(), "QUAL.OMP.OVERLAP"(i32 [[ADD48]]) ]
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[K]], align 4
// ROT1-NEXT:    [[SUB49:%.*]] = sub nsw i32 [[TMP40]], 0
// ROT1-NEXT:    store i32 [[SUB49]], ptr [[K]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.ORDERED"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE50:%.*]]
// ROT1:       omp.body.continue50:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC51:%.*]]
// ROT1:       omp.inner.for.inc51:
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[ADD52:%.*]] = add nsw i32 [[TMP41]], 1
// ROT1-NEXT:    store i32 [[ADD52]], ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_IV40]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_UB41]], align 4
// ROT1-NEXT:    [[CMP53:%.*]] = icmp sle i32 [[TMP42]], [[TMP43]]
// ROT1-NEXT:    br i1 [[CMP53]], label [[OMP_INNER_FOR_BODY44]], label [[OMP_INNER_FOR_END_CRIT_EDGE54:%.*]], !llvm.loop [[LOOP9:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge54:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END55]]
// ROT1:       omp.inner.for.end55:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT56:%.*]]
// ROT1:       omp.loop.exit56:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    ret void
//
void foo()
{
  int k = 0;
  double a, b, c;

#pragma omp parallel
  for (int i = 0; i < 1024; i++) {
#pragma omp simd simdlen(1)
    for (int l = 0; l < 16; l++) {
      k++;
#pragma omp ordered simd ompx_monotonic(i : 10)
      {
        k += l;
      }
    }
  }
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_monotonic(i,k)
    k -= 0;
  }
  fptr fp = &bar;
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_overlap(fp)
    k -= 0;
  }
#pragma omp simd
  for (int i = 0; i < 1024; i++) {
#pragma omp ordered simd ompx_overlap(i+k)
    k -= 0;
  }
}
