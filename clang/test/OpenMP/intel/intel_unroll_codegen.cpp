// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 3
// RUN: %clang_cc1 -verify -triple x86_64-pc-linux-gnu -fopenmp \
// RUN: -fopenmp-late-outline -fopenmp-intel-unroll -emit-llvm \
// RUN: %s -o - | FileCheck %s

// expected-no-diagnostics

// placeholder for loop body code.
extern "C" void body(...) {}

// CHECK-LABEL: define dso_local void @foo(
// CHECK-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.UNROLL"(), "QUAL.OMP.PARTIAL"(i64 4) ]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], [[TMP3]]
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @body(i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]], i32 noundef [[TMP7]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.UNROLL"() ]
// CHECK-NEXT:    ret void
//
extern "C" void foo(int start, int end, int step) {
  #pragma omp unroll partial(4)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: define dso_local void @bar(
// CHECK-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTNEW_STEP10:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP10]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP6]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    store i32 [[TMP9]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.UNROLL"(), "QUAL.OMP.PARTIAL"(i64 4) ]
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP12]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP14]], 1
// CHECK-NEXT:    [[CMP4:%.*]] = icmp ult i32 [[TMP13]], [[ADD3]]
// CHECK-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body.lh:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP16]], [[TMP17]]
// CHECK-NEXT:    [[ADD5:%.*]] = add i32 [[TMP15]], [[MUL]]
// CHECK-NEXT:    store i32 [[ADD5]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @body(i32 noundef [[TMP18]], i32 noundef [[TMP19]], i32 noundef [[TMP20]], i32 noundef [[TMP21]]) #[[ATTR2:[0-9]+]]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD6:%.*]] = add nuw i32 [[TMP22]], 1
// CHECK-NEXT:    store i32 [[ADD6]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD7:%.*]] = add i32 [[TMP24]], 1
// CHECK-NEXT:    [[CMP8:%.*]] = icmp ult i32 [[TMP23]], [[ADD7]]
// CHECK-NEXT:    br i1 [[CMP8]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// CHECK:       omp.inner.for.end_crit_edge:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_END]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.UNROLL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
extern "C" void bar(int start, int end, int step) {
  #pragma omp for
  #pragma omp unroll partial(4)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: define dso_local void @zoo(
// CHECK-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP1:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_16:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_17:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTNEW_STEP18:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTNEW_STEP14:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_15:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_19:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_20:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP18]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[J]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP4]], ptr [[DOTCAPTURE_EXPR_12]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP5]], ptr [[DOTCAPTURE_EXPR_13]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP6]], ptr [[DOTNEW_STEP14]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_12]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTNEW_STEP14]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB2]], [[TMP9]]
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTNEW_STEP14]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP10]]
// CHECK-NEXT:    [[SUB3:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[ADD4:%.*]] = add i32 [[TMP11]], 1
// CHECK-NEXT:    store i32 [[ADD4]], ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[TMP12]], [[TMP13]]
// CHECK-NEXT:    [[SUB6:%.*]] = sub i32 [[SUB5]], 1
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTNEW_STEP18]], align 4
// CHECK-NEXT:    [[ADD7:%.*]] = add i32 [[SUB6]], [[TMP14]]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTNEW_STEP18]], align 4
// CHECK-NEXT:    [[DIV8:%.*]] = udiv i32 [[ADD7]], [[TMP15]]
// CHECK-NEXT:    [[CONV:%.*]] = zext i32 [[DIV8]] to i64
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[SUB9:%.*]] = sub i32 [[TMP16]], 0
// CHECK-NEXT:    [[SUB10:%.*]] = sub i32 [[SUB9]], 1
// CHECK-NEXT:    [[ADD11:%.*]] = add i32 [[SUB10]], 2
// CHECK-NEXT:    [[DIV12:%.*]] = udiv i32 [[ADD11]], 2
// CHECK-NEXT:    [[CONV13:%.*]] = zext i32 [[DIV12]] to i64
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], [[CONV13]]
// CHECK-NEXT:    [[SUB14:%.*]] = sub nsw i64 [[MUL]], 1
// CHECK-NEXT:    store i64 [[SUB14]], ptr [[DOTCAPTURE_EXPR_20]], align 8
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP17]], [[TMP18]]
// CHECK-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       land.lhs.true:
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[CMP15:%.*]] = icmp ult i32 0, [[TMP19]]
// CHECK-NEXT:    br i1 [[CMP15]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_20]], align 8
// CHECK-NEXT:    store i64 [[TMP20]], ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_J]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_J]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 [[TMP22]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[CMP16:%.*]] = icmp sle i64 [[TMP23]], [[TMP24]]
// CHECK-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body.lh:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[CONV17:%.*]] = sext i32 [[TMP25]] to i64
// CHECK-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[SUB18:%.*]] = sub i32 [[TMP27]], 0
// CHECK-NEXT:    [[SUB19:%.*]] = sub i32 [[SUB18]], 1
// CHECK-NEXT:    [[ADD20:%.*]] = add i32 [[SUB19]], 2
// CHECK-NEXT:    [[DIV21:%.*]] = udiv i32 [[ADD20]], 2
// CHECK-NEXT:    [[MUL22:%.*]] = mul i32 1, [[DIV21]]
// CHECK-NEXT:    [[CONV23:%.*]] = zext i32 [[MUL22]] to i64
// CHECK-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP26]], [[CONV23]]
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTNEW_STEP18]], align 4
// CHECK-NEXT:    [[CONV25:%.*]] = sext i32 [[TMP28]] to i64
// CHECK-NEXT:    [[MUL26:%.*]] = mul nsw i64 [[DIV24]], [[CONV25]]
// CHECK-NEXT:    [[ADD27:%.*]] = add nsw i64 [[CONV17]], [[MUL26]]
// CHECK-NEXT:    [[CONV28:%.*]] = trunc i64 [[ADD27]] to i32
// CHECK-NEXT:    store i32 [[CONV28]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[SUB29:%.*]] = sub i32 [[TMP31]], 0
// CHECK-NEXT:    [[SUB30:%.*]] = sub i32 [[SUB29]], 1
// CHECK-NEXT:    [[ADD31:%.*]] = add i32 [[SUB30]], 2
// CHECK-NEXT:    [[DIV32:%.*]] = udiv i32 [[ADD31]], 2
// CHECK-NEXT:    [[MUL33:%.*]] = mul i32 1, [[DIV32]]
// CHECK-NEXT:    [[CONV34:%.*]] = zext i32 [[MUL33]] to i64
// CHECK-NEXT:    [[DIV35:%.*]] = sdiv i64 [[TMP30]], [[CONV34]]
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_19]], align 4
// CHECK-NEXT:    [[SUB36:%.*]] = sub i32 [[TMP32]], 0
// CHECK-NEXT:    [[SUB37:%.*]] = sub i32 [[SUB36]], 1
// CHECK-NEXT:    [[ADD38:%.*]] = add i32 [[SUB37]], 2
// CHECK-NEXT:    [[DIV39:%.*]] = udiv i32 [[ADD38]], 2
// CHECK-NEXT:    [[MUL40:%.*]] = mul i32 1, [[DIV39]]
// CHECK-NEXT:    [[CONV41:%.*]] = zext i32 [[MUL40]] to i64
// CHECK-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV35]], [[CONV41]]
// CHECK-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP29]], [[MUL42]]
// CHECK-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 2
// CHECK-NEXT:    [[ADD45:%.*]] = add nsw i64 0, [[MUL44]]
// CHECK-NEXT:    [[CONV46:%.*]] = trunc i64 [[ADD45]] to i32
// CHECK-NEXT:    store i32 [[CONV46]], ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    store i32 [[TMP33]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    [[ADD47:%.*]] = add i32 [[TMP35]], 2
// CHECK-NEXT:    [[CMP48:%.*]] = icmp ult i32 [[TMP34]], [[ADD47]]
// CHECK-NEXT:    br i1 [[CMP48]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[ADD49:%.*]] = add i32 [[TMP37]], 1
// CHECK-NEXT:    [[CMP50:%.*]] = icmp ult i32 [[TMP36]], [[ADD49]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP38:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP50]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP38]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_12]], align 4
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTNEW_STEP14]], align 4
// CHECK-NEXT:    [[MUL51:%.*]] = mul i32 [[TMP40]], [[TMP41]]
// CHECK-NEXT:    [[ADD52:%.*]] = add i32 [[TMP39]], [[MUL51]]
// CHECK-NEXT:    store i32 [[ADD52]], ptr [[J]], align 4
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[J]], align 4
// CHECK-NEXT:    [[TMP43:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP44:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @body(i32 noundef [[TMP42]], i32 noundef [[TMP43]], i32 noundef [[TMP44]], i32 noundef [[TMP45]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP46]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP7:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP47:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[ADD53:%.*]] = add nsw i64 [[TMP47]], 1
// CHECK-NEXT:    store i64 [[ADD53]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP48:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP49:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[CMP54:%.*]] = icmp sle i64 [[TMP48]], [[TMP49]]
// CHECK-NEXT:    br i1 [[CMP54]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// CHECK:       omp.inner.for.end_crit_edge:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_END]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
extern "C" void zoo(int start, int end, int step) {
  #pragma omp for collapse(2)
  for (int i = start; i < end; i+=step) {
    #pragma omp unroll partial
    for (int j = start; j < end; j+=step)
        body(j, end, step, i);
  }
}

// CHECK-LABEL: define dso_local void @moo(
// CHECK-SAME: ) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A:%.*]] = alloca [10 x i32], align 16
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I2:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[A]], i64 40, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.UNROLL"() ]
// CHECK-NEXT:    store i32 0, ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 10
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP4]] to i64
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 [[IDXPROM]]
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[ARRAYIDX]], align 4
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP5]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP9:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.UNROLL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[A]], i64 40, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[J]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.SHARED:TYPED"(ptr [[A]], i32 0, i64 10), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[J]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.UNROLL"(), "QUAL.OMP.PARTIAL"(i64 2) ]
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP9]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP1:%.*]] = icmp sle i32 [[TMP10]], [[TMP11]]
// CHECK-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body.lh:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP12]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I2]], align 4
// CHECK-NEXT:    store i32 0, ptr [[J]], align 4
// CHECK-NEXT:    br label [[FOR_COND3:%.*]]
// CHECK:       for.cond3:
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[J]], align 4
// CHECK-NEXT:    [[CMP4:%.*]] = icmp slt i32 [[TMP13]], 10
// CHECK-NEXT:    br i1 [[CMP4]], label [[FOR_BODY5:%.*]], label [[FOR_END10:%.*]]
// CHECK:       for.body5:
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[J]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[I2]], align 4
// CHECK-NEXT:    [[IDXPROM6:%.*]] = sext i32 [[TMP15]] to i64
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 [[IDXPROM6]]
// CHECK-NEXT:    store i32 [[TMP14]], ptr [[ARRAYIDX7]], align 4
// CHECK-NEXT:    br label [[FOR_INC8:%.*]]
// CHECK:       for.inc8:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[J]], align 4
// CHECK-NEXT:    [[INC9:%.*]] = add nsw i32 [[TMP16]], 1
// CHECK-NEXT:    store i32 [[INC9]], ptr [[J]], align 4
// CHECK-NEXT:    br label [[FOR_COND3]], !llvm.loop [[LOOP10:![0-9]+]]
// CHECK:       for.end10:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP17]], 1
// CHECK-NEXT:    store i32 [[ADD11]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP12:%.*]] = icmp sle i32 [[TMP18]], [[TMP19]]
// CHECK-NEXT:    br i1 [[CMP12]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// CHECK:       omp.inner.for.end_crit_edge:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_END]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]) [ "DIR.OMP.END.UNROLL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
extern "C" void moo()
{
  int a[10];
  #pragma omp target
  #pragma omp unroll
  for (int i = 0; i < 10; ++i)
    a[i] = i;

  #pragma omp target parallel for
  #pragma omp unroll partial(2)
  for (int i = 0; i < 10; ++i)
    for (int j = 0; j < 10; ++j)
      a[i] = j;
}

// CHECK-LABEL: define dso_local void @_Z4zoo1v(
// CHECK-SAME: ) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTFLOOR_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTTILE_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 0, ptr [[I]], align 4
// CHECK-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 0, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP0]], 5
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END19:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND1:%.*]]
// CHECK:       for.cond1:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP3]], 4
// CHECK-NEXT:    [[CMP2:%.*]] = icmp slt i32 5, [[ADD]]
// CHECK-NEXT:    br i1 [[CMP2]], label [[COND_TRUE:%.*]], label [[COND_FALSE:%.*]]
// CHECK:       cond.true:
// CHECK-NEXT:    br label [[COND_END:%.*]]
// CHECK:       cond.false:
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP4]], 4
// CHECK-NEXT:    br label [[COND_END]]
// CHECK:       cond.end:
// CHECK-NEXT:    [[COND:%.*]] = phi i32 [ 5, [[COND_TRUE]] ], [ [[ADD3]], [[COND_FALSE]] ]
// CHECK-NEXT:    [[CMP4:%.*]] = icmp slt i32 [[TMP2]], [[COND]]
// CHECK-NEXT:    br i1 [[CMP4]], label [[FOR_BODY5:%.*]], label [[FOR_END16:%.*]]
// CHECK:       for.body5:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 2
// CHECK-NEXT:    [[ADD6:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD6]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP6]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND7:%.*]]
// CHECK:       for.cond7:
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD8:%.*]] = add nsw i32 [[TMP8]], 2
// CHECK-NEXT:    [[CMP9:%.*]] = icmp slt i32 [[TMP7]], [[ADD8]]
// CHECK-NEXT:    br i1 [[CMP9]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[CMP10:%.*]] = icmp slt i32 [[TMP9]], 10
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP10:%.*]] = phi i1 [ false, [[FOR_COND7]] ], [ [[CMP10]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP10]], label [[FOR_BODY11:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body11:
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[MUL12:%.*]] = mul nsw i32 [[TMP11]], 1
// CHECK-NEXT:    [[ADD13:%.*]] = add nsw i32 0, [[MUL12]]
// CHECK-NEXT:    store i32 [[ADD13]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP12]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND7]], !llvm.loop [[LOOP11:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[FOR_INC14:%.*]]
// CHECK:       for.inc14:
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[INC15:%.*]] = add nsw i32 [[TMP13]], 1
// CHECK-NEXT:    store i32 [[INC15]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND1]], !llvm.loop [[LOOP12:![0-9]+]]
// CHECK:       for.end16:
// CHECK-NEXT:    br label [[FOR_INC17:%.*]]
// CHECK:       for.inc17:
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add nsw i32 [[TMP14]], 4
// CHECK-NEXT:    store i32 [[ADD18]], ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// CHECK:       for.end19:
// CHECK-NEXT:    ret void
//
void zoo1() {
  #pragma omp tile sizes(4)
  #pragma omp unroll partial
  for (int i = 0; i < 10; ++i) {}
}
//CHECK: [[LOOP6:![0-9]+]] = !{!"llvm.loop.mustprogress"}
//CHECK-NEXT: [[LOOP7]] = distinct !{[[LOOP7]], [[LOOP6]], [[LOOP8:![0-9]+]]}
//CHECK-NEXT: [[LOOP8]] = !{!"llvm.loop.unroll.count", i32 2}
//CHECK: [[LOOP11]] = distinct !{[[LOOP11]], [[LOOP6]], [[LOOP8]]}
