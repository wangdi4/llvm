// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -opaque-pointers -emit-llvm -o - -fopenmp -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN: -triple x86_64-unknown-linux-gnu %s | FileCheck %s
//
// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN: -triple x86_64-unknown-linux-gnu -emit-pch %s -o %t
//
// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN: -triple x86_64-unknown-linux-gnu \
// RUN: -include-pch %t -emit-llvm %s -o - | FileCheck %s
// expected-no-diagnostics
//
#ifndef HEADER
#define HEADER
// CHECK-LABEL: @main(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[AAA:%.*]] = alloca float, align 4
// CHECK-NEXT:    [[CCC:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[X:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[Y:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[Z:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 1, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCOPE"() ]
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP1]], 1
// CHECK-NEXT:    store i32 [[ADD]], ptr [[CCC]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SCOPE"() ]
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCOPE"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[CCC]], i32 0, i32 1), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP3]], 2
// CHECK-NEXT:    store i32 [[ADD1]], ptr [[I]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.SCOPE"() ]
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCOPE"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[CCC]], i32 0, i32 1), "QUAL.OMP.REDUCTION.MUL:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.NOWAIT"() ]
// CHECK-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP6]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP7]], 2
// CHECK-NEXT:    store i32 [[MUL]], ptr [[I]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCOPE"() ]
// CHECK-NEXT:    store i32 1, ptr [[Y]], align 4
// CHECK-NEXT:    store i32 1, ptr [[X]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCOPE"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[X]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[Z]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[Y]], align 4
// CHECK-NEXT:    store i32 [[TMP9]], ptr [[X]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[X]], align 4
// CHECK-NEXT:    store i32 [[TMP10]], ptr [[Y]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[Y]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[X]], align 4
// CHECK-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP11]], [[TMP12]]
// CHECK-NEXT:    store i32 [[ADD2]], ptr [[Z]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED:TYPED"(ptr [[X]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[Z]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[X]], align 4
// CHECK-NEXT:    [[INC3:%.*]] = add nsw i32 [[TMP14]], 1
// CHECK-NEXT:    store i32 [[INC3]], ptr [[X]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[Z]], align 4
// CHECK-NEXT:    [[INC4:%.*]] = add nsw i32 [[TMP15]], 1
// CHECK-NEXT:    store i32 [[INC4]], ptr [[Z]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP13]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[Y]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[Y]], align 4
// CHECK-NEXT:    [[INC5:%.*]] = add nsw i32 [[TMP17]], 1
// CHECK-NEXT:    store i32 [[INC5]], ptr [[Y]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]) [ "DIR.OMP.END.SCOPE"() ]
// CHECK-NEXT:    ret i32 0
//
int main() {
  int i = 1;
  float aaa;
  int ccc;

  #pragma omp scope
  ccc = i + 1;

  #pragma omp scope private(ccc) reduction(+:i)
  { i += 2; }

  #pragma omp scope private(ccc) reduction(*:i) nowait
  {
    #pragma omp parallel
    { i++; i *= 2; }
  }

  int x, y;
  x = y = 1;
  #pragma omp scope private(x)
  {
    int z;
    x = y;
    y = x;
    z = y + x;
    #pragma omp parallel
    { x++; z++; }
    #pragma omp task
    y++;
  }

}
#endif
// end INTEL_COLLAB
