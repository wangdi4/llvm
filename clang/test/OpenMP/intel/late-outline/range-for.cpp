// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --functions "test1|test2|test3|test4" --version 3
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \

// RUN:  -fopenmp-loop-rotation-control=0 \
// RUN:  -fopenmp-version=50 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT0

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 \
// RUN:  -fopenmp-version=50 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT1

// ROT0-LABEL: define dso_local noundef i32 @_Z5test1v(
// ROT0-SAME: ) #[[ATTR0:[0-9]+]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[AVAR:%.*]] = alloca [11 x i32], align 16
// ROT0-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[AREF:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    store i32 100, ptr [[SUM]], align 4
// ROT0-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[AVAR]], ptr align 16 @__const._Z5test1v.avar, i64 44, i1 false)
// ROT0-NEXT:    store ptr [[AVAR]], ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [11 x i32], ptr [[TMP0]], i64 0, i64 0
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY]], i64 11
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [11 x i32], ptr [[TMP1]], i64 0, i64 0
// ROT0-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV]], 1
// ROT0-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT0-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[AREF]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP3:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP3]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP14]], 1
// ROT0-NEXT:    [[ADD_PTR4:%.*]] = getelementptr inbounds i32, ptr [[TMP13]], i64 [[MUL]]
// ROT0-NEXT:    store ptr [[ADD_PTR4]], ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    store ptr [[TMP15]], ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[TMP16]], align 4
// ROT0-NEXT:    [[ADD5:%.*]] = add nsw i32 [[TMP17]], 2
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    [[ADD6:%.*]] = add nsw i32 [[TMP18]], [[ADD5]]
// ROT0-NEXT:    store i32 [[ADD6]], ptr [[SUM]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD7:%.*]] = add nsw i64 [[TMP19]], 1
// ROT0-NEXT:    store i64 [[ADD7]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    ret i32 [[TMP20]]
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test1v(
// ROT1-SAME: ) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[AVAR:%.*]] = alloca [11 x i32], align 16
// ROT1-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[AREF:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store i32 100, ptr [[SUM]], align 4
// ROT1-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 16 [[AVAR]], ptr align 16 @__const._Z5test1v.avar, i64 44, i1 false)
// ROT1-NEXT:    store ptr [[AVAR]], ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [11 x i32], ptr [[TMP0]], i64 0, i64 0
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY]], i64 11
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [11 x i32], ptr [[TMP1]], i64 0, i64 0
// ROT1-NEXT:    store ptr [[ARRAYDECAY1]], ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV]], 1
// ROT1-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT1-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[AREF]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP3:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP3]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP14]], 1
// ROT1-NEXT:    [[ADD_PTR4:%.*]] = getelementptr inbounds i32, ptr [[TMP13]], i64 [[MUL]]
// ROT1-NEXT:    store ptr [[ADD_PTR4]], ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    store ptr [[TMP15]], ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[TMP16]], align 4
// ROT1-NEXT:    [[ADD5:%.*]] = add nsw i32 [[TMP17]], 2
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    [[ADD6:%.*]] = add nsw i32 [[TMP18]], [[ADD5]]
// ROT1-NEXT:    store i32 [[ADD6]], ptr [[SUM]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD7:%.*]] = add nsw i64 [[TMP19]], 1
// ROT1-NEXT:    store i64 [[ADD7]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP21:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP8:%.*]] = icmp sle i64 [[TMP20]], [[TMP21]]
// ROT1-NEXT:    br i1 [[CMP8]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    ret i32 [[TMP22]]
//
int test1() {
  int sum(100);
  int avar[] = {0,1,2,3,4,5,6,7,8,9,10};

#pragma omp parallel for reduction(+:sum)
  for (const auto &aref : avar) {
    sum += (aref + 2);
  }
  return sum;
}

struct A {
  int arr[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
  int *begin() { return &arr[0]; }
  int *end()   { return &arr[10]; }
};

template<typename _Container>
inline auto begin(const _Container& __cont) -> decltype(__cont.begin())
    { return __cont.begin(); }

template<typename _Container>
inline auto end(const _Container& __cont) -> decltype(__cont.end())
{ return __cont.end(); }

// ROT0-LABEL: define dso_local noundef i32 @_Z5test2v(
// ROT0-SAME: ) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[AVAR:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT0-NEXT:    [[AVAR2:%.*]] = alloca [[STRUCT_A]], align 4
// ROT0-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[TMP1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_5:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_6:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_7:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__BEGIN2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[V2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR]]) #[[ATTR2:[0-9]+]]
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR2]]) #[[ATTR2]]
// ROT0-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT0-NEXT:    store ptr [[AVAR]], ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT0-NEXT:    store ptr [[CALL]], ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[AVAR2]], ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT0-NEXT:    store ptr [[CALL2]], ptr [[__END2]], align 8
// ROT0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[CALL3:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP2]])
// ROT0-NEXT:    store ptr [[CALL3]], ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[TMP3]], ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[CALL4:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP4]])
// ROT0-NEXT:    store ptr [[CALL4]], ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__END2]], align 8
// ROT0-NEXT:    store ptr [[TMP5]], ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP6]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP7]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST5:%.*]] = ptrtoint ptr [[TMP8]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST6:%.*]] = ptrtoint ptr [[TMP9]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB7:%.*]] = sub i64 [[SUB_PTR_LHS_CAST5]], [[SUB_PTR_RHS_CAST6]]
// ROT0-NEXT:    [[SUB_PTR_DIV8:%.*]] = sdiv exact i64 [[SUB_PTR_SUB7]], 4
// ROT0-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[SUB_PTR_DIV8]], 1
// ROT0-NEXT:    [[ADD10:%.*]] = add nsw i64 [[SUB9]], 1
// ROT0-NEXT:    [[DIV11:%.*]] = sdiv i64 [[ADD10]], 1
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[DIV]], [[DIV11]]
// ROT0-NEXT:    [[SUB12:%.*]] = sub nsw i64 [[MUL]], 1
// ROT0-NEXT:    store i64 [[SUB12]], ptr [[DOTCAPTURE_EXPR_7]], align 8
// ROT0-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP10]], [[TMP11]]
// ROT0-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       land.lhs.true:
// ROT0-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[CMP13:%.*]] = icmp ult ptr [[TMP12]], [[TMP13]]
// ROT0-NEXT:    br i1 [[CMP13]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_7]], align 8
// ROT0-NEXT:    store i64 [[TMP14]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[V2]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.MUL:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP15]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP16]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_5]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_6]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN2]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V2]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP18]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP14:%.*]] = icmp sle i64 [[TMP19]], [[TMP20]]
// ROT0-NEXT:    br i1 [[CMP14]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST15:%.*]] = ptrtoint ptr [[TMP23]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST16:%.*]] = ptrtoint ptr [[TMP24]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB17:%.*]] = sub i64 [[SUB_PTR_LHS_CAST15]], [[SUB_PTR_RHS_CAST16]]
// ROT0-NEXT:    [[SUB_PTR_DIV18:%.*]] = sdiv exact i64 [[SUB_PTR_SUB17]], 4
// ROT0-NEXT:    [[SUB19:%.*]] = sub nsw i64 [[SUB_PTR_DIV18]], 1
// ROT0-NEXT:    [[ADD20:%.*]] = add nsw i64 [[SUB19]], 1
// ROT0-NEXT:    [[DIV21:%.*]] = sdiv i64 [[ADD20]], 1
// ROT0-NEXT:    [[MUL22:%.*]] = mul nsw i64 1, [[DIV21]]
// ROT0-NEXT:    [[DIV23:%.*]] = sdiv i64 [[TMP22]], [[MUL22]]
// ROT0-NEXT:    [[MUL24:%.*]] = mul nsw i64 [[DIV23]], 1
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP21]], i64 [[MUL24]]
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP27:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST25:%.*]] = ptrtoint ptr [[TMP28]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST26:%.*]] = ptrtoint ptr [[TMP29]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB27:%.*]] = sub i64 [[SUB_PTR_LHS_CAST25]], [[SUB_PTR_RHS_CAST26]]
// ROT0-NEXT:    [[SUB_PTR_DIV28:%.*]] = sdiv exact i64 [[SUB_PTR_SUB27]], 4
// ROT0-NEXT:    [[SUB29:%.*]] = sub nsw i64 [[SUB_PTR_DIV28]], 1
// ROT0-NEXT:    [[ADD30:%.*]] = add nsw i64 [[SUB29]], 1
// ROT0-NEXT:    [[DIV31:%.*]] = sdiv i64 [[ADD30]], 1
// ROT0-NEXT:    [[MUL32:%.*]] = mul nsw i64 1, [[DIV31]]
// ROT0-NEXT:    [[DIV33:%.*]] = sdiv i64 [[TMP27]], [[MUL32]]
// ROT0-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT0-NEXT:    [[TMP31:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST34:%.*]] = ptrtoint ptr [[TMP30]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST35:%.*]] = ptrtoint ptr [[TMP31]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB36:%.*]] = sub i64 [[SUB_PTR_LHS_CAST34]], [[SUB_PTR_RHS_CAST35]]
// ROT0-NEXT:    [[SUB_PTR_DIV37:%.*]] = sdiv exact i64 [[SUB_PTR_SUB36]], 4
// ROT0-NEXT:    [[SUB38:%.*]] = sub nsw i64 [[SUB_PTR_DIV37]], 1
// ROT0-NEXT:    [[ADD39:%.*]] = add nsw i64 [[SUB38]], 1
// ROT0-NEXT:    [[DIV40:%.*]] = sdiv i64 [[ADD39]], 1
// ROT0-NEXT:    [[MUL41:%.*]] = mul nsw i64 1, [[DIV40]]
// ROT0-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV33]], [[MUL41]]
// ROT0-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP26]], [[MUL42]]
// ROT0-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 1
// ROT0-NEXT:    [[ADD_PTR45:%.*]] = getelementptr inbounds i32, ptr [[TMP25]], i64 [[MUL44]]
// ROT0-NEXT:    store ptr [[ADD_PTR45]], ptr [[__BEGIN2]], align 8
// ROT0-NEXT:    [[TMP32:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    store ptr [[TMP32]], ptr [[V]], align 8
// ROT0-NEXT:    [[TMP33:%.*]] = load ptr, ptr [[__BEGIN2]], align 8
// ROT0-NEXT:    store ptr [[TMP33]], ptr [[V2]], align 8
// ROT0-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[TMP34]], align 4
// ROT0-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[V2]], align 8
// ROT0-NEXT:    [[TMP37:%.*]] = load i32, ptr [[TMP36]], align 4
// ROT0-NEXT:    [[ADD46:%.*]] = add nsw i32 [[TMP35]], [[TMP37]]
// ROT0-NEXT:    [[TMP38:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    [[MUL47:%.*]] = mul nsw i32 [[TMP38]], [[ADD46]]
// ROT0-NEXT:    store i32 [[MUL47]], ptr [[SUM]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP39:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD48:%.*]] = add nsw i64 [[TMP39]], 1
// ROT0-NEXT:    store i64 [[ADD48]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    [[TMP40:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    ret i32 [[TMP40]]
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test2v(
// ROT1-SAME: ) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[AVAR:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT1-NEXT:    [[AVAR2:%.*]] = alloca [[STRUCT_A]], align 4
// ROT1-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[TMP1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_5:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_6:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_7:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__BEGIN2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[V2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR]]) #[[ATTR2:[0-9]+]]
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR2]]) #[[ATTR2]]
// ROT1-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT1-NEXT:    store ptr [[AVAR]], ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT1-NEXT:    store ptr [[CALL]], ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[AVAR2]], ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT1-NEXT:    store ptr [[CALL2]], ptr [[__END2]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[CALL3:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP2]])
// ROT1-NEXT:    store ptr [[CALL3]], ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[TMP3]], ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[CALL4:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP4]])
// ROT1-NEXT:    store ptr [[CALL4]], ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__END2]], align 8
// ROT1-NEXT:    store ptr [[TMP5]], ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP6]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP7]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST5:%.*]] = ptrtoint ptr [[TMP8]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST6:%.*]] = ptrtoint ptr [[TMP9]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB7:%.*]] = sub i64 [[SUB_PTR_LHS_CAST5]], [[SUB_PTR_RHS_CAST6]]
// ROT1-NEXT:    [[SUB_PTR_DIV8:%.*]] = sdiv exact i64 [[SUB_PTR_SUB7]], 4
// ROT1-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[SUB_PTR_DIV8]], 1
// ROT1-NEXT:    [[ADD10:%.*]] = add nsw i64 [[SUB9]], 1
// ROT1-NEXT:    [[DIV11:%.*]] = sdiv i64 [[ADD10]], 1
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[DIV]], [[DIV11]]
// ROT1-NEXT:    [[SUB12:%.*]] = sub nsw i64 [[MUL]], 1
// ROT1-NEXT:    store i64 [[SUB12]], ptr [[DOTCAPTURE_EXPR_7]], align 8
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP10]], [[TMP11]]
// ROT1-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       land.lhs.true:
// ROT1-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[CMP13:%.*]] = icmp ult ptr [[TMP12]], [[TMP13]]
// ROT1-NEXT:    br i1 [[CMP13]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_7]], align 8
// ROT1-NEXT:    store i64 [[TMP14]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[V2]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.MUL:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP15]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP16]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_5]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_6]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN2]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V2]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP18]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP14:%.*]] = icmp sle i64 [[TMP19]], [[TMP20]]
// ROT1-NEXT:    br i1 [[CMP14]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST15:%.*]] = ptrtoint ptr [[TMP23]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST16:%.*]] = ptrtoint ptr [[TMP24]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB17:%.*]] = sub i64 [[SUB_PTR_LHS_CAST15]], [[SUB_PTR_RHS_CAST16]]
// ROT1-NEXT:    [[SUB_PTR_DIV18:%.*]] = sdiv exact i64 [[SUB_PTR_SUB17]], 4
// ROT1-NEXT:    [[SUB19:%.*]] = sub nsw i64 [[SUB_PTR_DIV18]], 1
// ROT1-NEXT:    [[ADD20:%.*]] = add nsw i64 [[SUB19]], 1
// ROT1-NEXT:    [[DIV21:%.*]] = sdiv i64 [[ADD20]], 1
// ROT1-NEXT:    [[MUL22:%.*]] = mul nsw i64 1, [[DIV21]]
// ROT1-NEXT:    [[DIV23:%.*]] = sdiv i64 [[TMP22]], [[MUL22]]
// ROT1-NEXT:    [[MUL24:%.*]] = mul nsw i64 [[DIV23]], 1
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP21]], i64 [[MUL24]]
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP27:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST25:%.*]] = ptrtoint ptr [[TMP28]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST26:%.*]] = ptrtoint ptr [[TMP29]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB27:%.*]] = sub i64 [[SUB_PTR_LHS_CAST25]], [[SUB_PTR_RHS_CAST26]]
// ROT1-NEXT:    [[SUB_PTR_DIV28:%.*]] = sdiv exact i64 [[SUB_PTR_SUB27]], 4
// ROT1-NEXT:    [[SUB29:%.*]] = sub nsw i64 [[SUB_PTR_DIV28]], 1
// ROT1-NEXT:    [[ADD30:%.*]] = add nsw i64 [[SUB29]], 1
// ROT1-NEXT:    [[DIV31:%.*]] = sdiv i64 [[ADD30]], 1
// ROT1-NEXT:    [[MUL32:%.*]] = mul nsw i64 1, [[DIV31]]
// ROT1-NEXT:    [[DIV33:%.*]] = sdiv i64 [[TMP27]], [[MUL32]]
// ROT1-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_6]], align 8
// ROT1-NEXT:    [[TMP31:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_5]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST34:%.*]] = ptrtoint ptr [[TMP30]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST35:%.*]] = ptrtoint ptr [[TMP31]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB36:%.*]] = sub i64 [[SUB_PTR_LHS_CAST34]], [[SUB_PTR_RHS_CAST35]]
// ROT1-NEXT:    [[SUB_PTR_DIV37:%.*]] = sdiv exact i64 [[SUB_PTR_SUB36]], 4
// ROT1-NEXT:    [[SUB38:%.*]] = sub nsw i64 [[SUB_PTR_DIV37]], 1
// ROT1-NEXT:    [[ADD39:%.*]] = add nsw i64 [[SUB38]], 1
// ROT1-NEXT:    [[DIV40:%.*]] = sdiv i64 [[ADD39]], 1
// ROT1-NEXT:    [[MUL41:%.*]] = mul nsw i64 1, [[DIV40]]
// ROT1-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV33]], [[MUL41]]
// ROT1-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP26]], [[MUL42]]
// ROT1-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 1
// ROT1-NEXT:    [[ADD_PTR45:%.*]] = getelementptr inbounds i32, ptr [[TMP25]], i64 [[MUL44]]
// ROT1-NEXT:    store ptr [[ADD_PTR45]], ptr [[__BEGIN2]], align 8
// ROT1-NEXT:    [[TMP32:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    store ptr [[TMP32]], ptr [[V]], align 8
// ROT1-NEXT:    [[TMP33:%.*]] = load ptr, ptr [[__BEGIN2]], align 8
// ROT1-NEXT:    store ptr [[TMP33]], ptr [[V2]], align 8
// ROT1-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[TMP34]], align 4
// ROT1-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[V2]], align 8
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[TMP36]], align 4
// ROT1-NEXT:    [[ADD46:%.*]] = add nsw i32 [[TMP35]], [[TMP37]]
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    [[MUL47:%.*]] = mul nsw i32 [[TMP38]], [[ADD46]]
// ROT1-NEXT:    store i32 [[MUL47]], ptr [[SUM]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP39:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD48:%.*]] = add nsw i64 [[TMP39]], 1
// ROT1-NEXT:    store i64 [[ADD48]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP40:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP41:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP49:%.*]] = icmp sle i64 [[TMP40]], [[TMP41]]
// ROT1-NEXT:    br i1 [[CMP49]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    ret i32 [[TMP42]]
//
int test2() {
  A avar;
  A avar2;
  int sum(0);

  // Nested range-based for loops.
  #pragma omp distribute parallel for reduction(*:sum) collapse(2)
  for (const auto &v : avar)
    for (auto &v2 : avar2)
      sum *= (v + v2);

  return sum;
}

// ROT0-LABEL: define dso_local noundef i32 @_Z5test3v(
// ROT0-SAME: ) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[AVAR3:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT0-NEXT:    [[AVAR4:%.*]] = alloca [[STRUCT_A]], align 4
// ROT0-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[TMP1:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_10:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR3]]) #[[ATTR2]]
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR4]]) #[[ATTR2]]
// ROT0-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT0-NEXT:    store ptr [[AVAR3]], ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT0-NEXT:    store ptr [[CALL]], ptr [[__END1]], align 8
// ROT0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT0-NEXT:    store ptr [[CALL2]], ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[DIV]], 10
// ROT0-NEXT:    [[SUB3:%.*]] = sub nsw i64 [[MUL]], 1
// ROT0-NEXT:    store i64 [[SUB3]], ptr [[DOTCAPTURE_EXPR_10]], align 8
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_10]], align 8
// ROT0-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.SUB:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.SHARED:TYPED"(ptr [[AVAR4]], [[STRUCT_A]] zeroinitializer, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_8]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT0-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[DIV5:%.*]] = sdiv i64 [[TMP14]], 10
// ROT0-NEXT:    [[MUL6:%.*]] = mul nsw i64 [[DIV5]], 1
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP13]], i64 [[MUL6]]
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP16:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[DIV7:%.*]] = sdiv i64 [[TMP16]], 10
// ROT0-NEXT:    [[MUL8:%.*]] = mul nsw i64 [[DIV7]], 10
// ROT0-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[TMP15]], [[MUL8]]
// ROT0-NEXT:    [[MUL10:%.*]] = mul nsw i64 [[SUB9]], 1
// ROT0-NEXT:    [[ADD11:%.*]] = add nsw i64 0, [[MUL10]]
// ROT0-NEXT:    [[CONV:%.*]] = trunc i64 [[ADD11]] to i32
// ROT0-NEXT:    store i32 [[CONV]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    store ptr [[TMP17]], ptr [[V]], align 8
// ROT0-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[TMP18]], align 4
// ROT0-NEXT:    [[ARR:%.*]] = getelementptr inbounds [[STRUCT_A]], ptr [[AVAR4]], i32 0, i32 0
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP20]] to i64
// ROT0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARR]], i64 0, i64 [[IDXPROM]]
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// ROT0-NEXT:    [[ADD12:%.*]] = add nsw i32 [[TMP19]], [[TMP21]]
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    [[SUB13:%.*]] = sub nsw i32 [[TMP22]], [[ADD12]]
// ROT0-NEXT:    store i32 [[SUB13]], ptr [[SUM]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD14:%.*]] = add nsw i64 [[TMP23]], 1
// ROT0-NEXT:    store i64 [[ADD14]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    [[TMP24:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    ret i32 [[TMP24]]
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test3v(
// ROT1-SAME: ) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[AVAR3:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT1-NEXT:    [[AVAR4:%.*]] = alloca [[STRUCT_A]], align 4
// ROT1-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[TMP1:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_10:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR3]]) #[[ATTR2]]
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR4]]) #[[ATTR2]]
// ROT1-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT1-NEXT:    store ptr [[AVAR3]], ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT1-NEXT:    store ptr [[CALL]], ptr [[__END1]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT1-NEXT:    store ptr [[CALL2]], ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[DIV]], 10
// ROT1-NEXT:    [[SUB3:%.*]] = sub nsw i64 [[MUL]], 1
// ROT1-NEXT:    store i64 [[SUB3]], ptr [[DOTCAPTURE_EXPR_10]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_9]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_10]], align 8
// ROT1-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.SUB:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.SHARED:TYPED"(ptr [[AVAR4]], [[STRUCT_A]] zeroinitializer, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_8]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_8]], align 8
// ROT1-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[DIV5:%.*]] = sdiv i64 [[TMP14]], 10
// ROT1-NEXT:    [[MUL6:%.*]] = mul nsw i64 [[DIV5]], 1
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP13]], i64 [[MUL6]]
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[DIV7:%.*]] = sdiv i64 [[TMP16]], 10
// ROT1-NEXT:    [[MUL8:%.*]] = mul nsw i64 [[DIV7]], 10
// ROT1-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[TMP15]], [[MUL8]]
// ROT1-NEXT:    [[MUL10:%.*]] = mul nsw i64 [[SUB9]], 1
// ROT1-NEXT:    [[ADD11:%.*]] = add nsw i64 0, [[MUL10]]
// ROT1-NEXT:    [[CONV:%.*]] = trunc i64 [[ADD11]] to i32
// ROT1-NEXT:    store i32 [[CONV]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    store ptr [[TMP17]], ptr [[V]], align 8
// ROT1-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[TMP18]], align 4
// ROT1-NEXT:    [[ARR:%.*]] = getelementptr inbounds [[STRUCT_A]], ptr [[AVAR4]], i32 0, i32 0
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP20]] to i64
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARR]], i64 0, i64 [[IDXPROM]]
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// ROT1-NEXT:    [[ADD12:%.*]] = add nsw i32 [[TMP19]], [[TMP21]]
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    [[SUB13:%.*]] = sub nsw i32 [[TMP22]], [[ADD12]]
// ROT1-NEXT:    store i32 [[SUB13]], ptr [[SUM]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD14:%.*]] = add nsw i64 [[TMP23]], 1
// ROT1-NEXT:    store i64 [[ADD14]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP25:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP15:%.*]] = icmp sle i64 [[TMP24]], [[TMP25]]
// ROT1-NEXT:    br i1 [[CMP15]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    ret i32 [[TMP26]]
//
int test3() {
  A avar3;
  A avar4;
  int sum(0);

  // Regular for loop nested inside range-based loop.
  #pragma omp distribute parallel for reduction(-:sum) collapse(2)
  for (const auto &v : avar3)
    for (int i=0; i < 10; i++)
      sum -= (v + avar4.arr[i]);

  return sum;
}

// ROT0-LABEL: define dso_local noundef i32 @_Z5test4v(
// ROT0-SAME: ) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[AVAR5:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT0-NEXT:    [[AVAR6:%.*]] = alloca [[STRUCT_A]], align 4
// ROT0-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[__BEGIN2:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR5]]) #[[ATTR2]]
// ROT0-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR6]]) #[[ATTR2]]
// ROT0-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT0-NEXT:    store ptr [[AVAR5]], ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT0-NEXT:    store ptr [[CALL]], ptr [[__END2]], align 8
// ROT0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT0-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT0-NEXT:    store ptr [[CALL2]], ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END2]], align 8
// ROT0-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 10, [[DIV]]
// ROT0-NEXT:    [[SUB3:%.*]] = sub nsw i64 [[MUL]], 1
// ROT0-NEXT:    store i64 [[SUB3]], ptr [[DOTCAPTURE_EXPR_13]], align 8
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_13]], align 8
// ROT0-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.MUL:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.SHARED:TYPED"(ptr [[AVAR6]], [[STRUCT_A]] zeroinitializer, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_11]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_12]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN2]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP13:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST5:%.*]] = ptrtoint ptr [[TMP14]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST6:%.*]] = ptrtoint ptr [[TMP15]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB7:%.*]] = sub i64 [[SUB_PTR_LHS_CAST5]], [[SUB_PTR_RHS_CAST6]]
// ROT0-NEXT:    [[SUB_PTR_DIV8:%.*]] = sdiv exact i64 [[SUB_PTR_SUB7]], 4
// ROT0-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[SUB_PTR_DIV8]], 1
// ROT0-NEXT:    [[ADD10:%.*]] = add nsw i64 [[SUB9]], 1
// ROT0-NEXT:    [[DIV11:%.*]] = sdiv i64 [[ADD10]], 1
// ROT0-NEXT:    [[MUL12:%.*]] = mul nsw i64 1, [[DIV11]]
// ROT0-NEXT:    [[DIV13:%.*]] = sdiv i64 [[TMP13]], [[MUL12]]
// ROT0-NEXT:    [[MUL14:%.*]] = mul nsw i64 [[DIV13]], 1
// ROT0-NEXT:    [[ADD15:%.*]] = add nsw i64 0, [[MUL14]]
// ROT0-NEXT:    [[CONV:%.*]] = trunc i64 [[ADD15]] to i32
// ROT0-NEXT:    store i32 [[CONV]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST16:%.*]] = ptrtoint ptr [[TMP19]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST17:%.*]] = ptrtoint ptr [[TMP20]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB18:%.*]] = sub i64 [[SUB_PTR_LHS_CAST16]], [[SUB_PTR_RHS_CAST17]]
// ROT0-NEXT:    [[SUB_PTR_DIV19:%.*]] = sdiv exact i64 [[SUB_PTR_SUB18]], 4
// ROT0-NEXT:    [[SUB20:%.*]] = sub nsw i64 [[SUB_PTR_DIV19]], 1
// ROT0-NEXT:    [[ADD21:%.*]] = add nsw i64 [[SUB20]], 1
// ROT0-NEXT:    [[DIV22:%.*]] = sdiv i64 [[ADD21]], 1
// ROT0-NEXT:    [[MUL23:%.*]] = mul nsw i64 1, [[DIV22]]
// ROT0-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP18]], [[MUL23]]
// ROT0-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST25:%.*]] = ptrtoint ptr [[TMP21]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST26:%.*]] = ptrtoint ptr [[TMP22]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB27:%.*]] = sub i64 [[SUB_PTR_LHS_CAST25]], [[SUB_PTR_RHS_CAST26]]
// ROT0-NEXT:    [[SUB_PTR_DIV28:%.*]] = sdiv exact i64 [[SUB_PTR_SUB27]], 4
// ROT0-NEXT:    [[SUB29:%.*]] = sub nsw i64 [[SUB_PTR_DIV28]], 1
// ROT0-NEXT:    [[ADD30:%.*]] = add nsw i64 [[SUB29]], 1
// ROT0-NEXT:    [[DIV31:%.*]] = sdiv i64 [[ADD30]], 1
// ROT0-NEXT:    [[MUL32:%.*]] = mul nsw i64 1, [[DIV31]]
// ROT0-NEXT:    [[MUL33:%.*]] = mul nsw i64 [[DIV24]], [[MUL32]]
// ROT0-NEXT:    [[SUB34:%.*]] = sub nsw i64 [[TMP17]], [[MUL33]]
// ROT0-NEXT:    [[MUL35:%.*]] = mul nsw i64 [[SUB34]], 1
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP16]], i64 [[MUL35]]
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN2]], align 8
// ROT0-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[__BEGIN2]], align 8
// ROT0-NEXT:    store ptr [[TMP23]], ptr [[V]], align 8
// ROT0-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[V]], align 8
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[TMP24]], align 4
// ROT0-NEXT:    [[ARR:%.*]] = getelementptr inbounds [[STRUCT_A]], ptr [[AVAR6]], i32 0, i32 0
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP26]] to i64
// ROT0-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARR]], i64 0, i64 [[IDXPROM]]
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// ROT0-NEXT:    [[ADD36:%.*]] = add nsw i32 [[TMP25]], [[TMP27]]
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    [[ADD37:%.*]] = add nsw i32 [[TMP28]], [[ADD36]]
// ROT0-NEXT:    store i32 [[ADD37]], ptr [[SUM]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD38:%.*]] = add nsw i64 [[TMP29]], 1
// ROT0-NEXT:    store i64 [[ADD38]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[SUM]], align 4
// ROT0-NEXT:    ret i32 [[TMP30]]
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test4v(
// ROT1-SAME: ) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[AVAR5:%.*]] = alloca [[STRUCT_A:%.*]], align 4
// ROT1-NEXT:    [[AVAR6:%.*]] = alloca [[STRUCT_A]], align 4
// ROT1-NEXT:    [[SUM:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[__BEGIN2:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[V:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR5]]) #[[ATTR2]]
// ROT1-NEXT:    call void @_ZN1AC1Ev(ptr noundef nonnull align 4 dereferenceable(40) [[AVAR6]]) #[[ATTR2]]
// ROT1-NEXT:    store i32 0, ptr [[SUM]], align 4
// ROT1-NEXT:    store ptr [[AVAR5]], ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef ptr @_ZN1A3endEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP0]])
// ROT1-NEXT:    store ptr [[CALL]], ptr [[__END2]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__RANGE2]], align 8
// ROT1-NEXT:    [[CALL2:%.*]] = call noundef ptr @_ZN1A5beginEv(ptr noundef nonnull align 4 dereferenceable(40) [[TMP1]])
// ROT1-NEXT:    store ptr [[CALL2]], ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__END2]], align 8
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP3]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP4]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 10, [[DIV]]
// ROT1-NEXT:    [[SUB3:%.*]] = sub nsw i64 [[MUL]], 1
// ROT1-NEXT:    store i64 [[SUB3]], ptr [[DOTCAPTURE_EXPR_13]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP5]], [[TMP6]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_13]], align 8
// ROT1-NEXT:    store i64 [[TMP7]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.MUL:TYPED"(ptr [[SUM]], i32 0, i32 1), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.SHARED:TYPED"(ptr [[AVAR6]], [[STRUCT_A]] zeroinitializer, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP8]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_11]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_12]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN2]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[V]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP13:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST5:%.*]] = ptrtoint ptr [[TMP14]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST6:%.*]] = ptrtoint ptr [[TMP15]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB7:%.*]] = sub i64 [[SUB_PTR_LHS_CAST5]], [[SUB_PTR_RHS_CAST6]]
// ROT1-NEXT:    [[SUB_PTR_DIV8:%.*]] = sdiv exact i64 [[SUB_PTR_SUB7]], 4
// ROT1-NEXT:    [[SUB9:%.*]] = sub nsw i64 [[SUB_PTR_DIV8]], 1
// ROT1-NEXT:    [[ADD10:%.*]] = add nsw i64 [[SUB9]], 1
// ROT1-NEXT:    [[DIV11:%.*]] = sdiv i64 [[ADD10]], 1
// ROT1-NEXT:    [[MUL12:%.*]] = mul nsw i64 1, [[DIV11]]
// ROT1-NEXT:    [[DIV13:%.*]] = sdiv i64 [[TMP13]], [[MUL12]]
// ROT1-NEXT:    [[MUL14:%.*]] = mul nsw i64 [[DIV13]], 1
// ROT1-NEXT:    [[ADD15:%.*]] = add nsw i64 0, [[MUL14]]
// ROT1-NEXT:    [[CONV:%.*]] = trunc i64 [[ADD15]] to i32
// ROT1-NEXT:    store i32 [[CONV]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST16:%.*]] = ptrtoint ptr [[TMP19]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST17:%.*]] = ptrtoint ptr [[TMP20]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB18:%.*]] = sub i64 [[SUB_PTR_LHS_CAST16]], [[SUB_PTR_RHS_CAST17]]
// ROT1-NEXT:    [[SUB_PTR_DIV19:%.*]] = sdiv exact i64 [[SUB_PTR_SUB18]], 4
// ROT1-NEXT:    [[SUB20:%.*]] = sub nsw i64 [[SUB_PTR_DIV19]], 1
// ROT1-NEXT:    [[ADD21:%.*]] = add nsw i64 [[SUB20]], 1
// ROT1-NEXT:    [[DIV22:%.*]] = sdiv i64 [[ADD21]], 1
// ROT1-NEXT:    [[MUL23:%.*]] = mul nsw i64 1, [[DIV22]]
// ROT1-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP18]], [[MUL23]]
// ROT1-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_11]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST25:%.*]] = ptrtoint ptr [[TMP21]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST26:%.*]] = ptrtoint ptr [[TMP22]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB27:%.*]] = sub i64 [[SUB_PTR_LHS_CAST25]], [[SUB_PTR_RHS_CAST26]]
// ROT1-NEXT:    [[SUB_PTR_DIV28:%.*]] = sdiv exact i64 [[SUB_PTR_SUB27]], 4
// ROT1-NEXT:    [[SUB29:%.*]] = sub nsw i64 [[SUB_PTR_DIV28]], 1
// ROT1-NEXT:    [[ADD30:%.*]] = add nsw i64 [[SUB29]], 1
// ROT1-NEXT:    [[DIV31:%.*]] = sdiv i64 [[ADD30]], 1
// ROT1-NEXT:    [[MUL32:%.*]] = mul nsw i64 1, [[DIV31]]
// ROT1-NEXT:    [[MUL33:%.*]] = mul nsw i64 [[DIV24]], [[MUL32]]
// ROT1-NEXT:    [[SUB34:%.*]] = sub nsw i64 [[TMP17]], [[MUL33]]
// ROT1-NEXT:    [[MUL35:%.*]] = mul nsw i64 [[SUB34]], 1
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP16]], i64 [[MUL35]]
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__BEGIN2]], align 8
// ROT1-NEXT:    [[TMP23:%.*]] = load ptr, ptr [[__BEGIN2]], align 8
// ROT1-NEXT:    store ptr [[TMP23]], ptr [[V]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[V]], align 8
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[TMP24]], align 4
// ROT1-NEXT:    [[ARR:%.*]] = getelementptr inbounds [[STRUCT_A]], ptr [[AVAR6]], i32 0, i32 0
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP26]] to i64
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARR]], i64 0, i64 [[IDXPROM]]
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[ARRAYIDX]], align 4
// ROT1-NEXT:    [[ADD36:%.*]] = add nsw i32 [[TMP25]], [[TMP27]]
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    [[ADD37:%.*]] = add nsw i32 [[TMP28]], [[ADD36]]
// ROT1-NEXT:    store i32 [[ADD37]], ptr [[SUM]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD38:%.*]] = add nsw i64 [[TMP29]], 1
// ROT1-NEXT:    store i64 [[ADD38]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP30:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP31:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP39:%.*]] = icmp sle i64 [[TMP30]], [[TMP31]]
// ROT1-NEXT:    br i1 [[CMP39]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[SUM]], align 4
// ROT1-NEXT:    ret i32 [[TMP32]]
//
int test4() {
  A avar5;
  A avar6;
  int sum(0);

  // Range-based for loop nested inside regular for loop.
  #pragma omp distribute parallel for reduction(*:sum) collapse(2)
  for (int i=0; i < 10; i++)
    for (const auto &v : avar5)
      sum += (v + avar6.arr[i]);

  return sum;
}

int main() {
  int res1 = test1();
  int res2 = test2();
  int res3 = test3();
  int res4 = test4();
  return (res1 + res2 + res3 + res4);
}
// end INTEL_COLLAB
