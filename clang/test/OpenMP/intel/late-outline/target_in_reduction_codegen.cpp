// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// NOTE: Test modified after being autogenerated by
// NOTE:   utils/update_cc_tests_checks.py
//
// NOTE: (1) In test2, explicitly check (CHECK-NOT) that 'if' clause omitted
// NOTE: from implicit task for target in_reduction with nowait clause.
// NOTE: (2) In test3, explicitly check (CHECK-NOT) that 'shared' clause
// NOTE: omitted from implicit task for target in_reduction variable.
//
// RUN: %clang_cc1 -opaque-pointers -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu %s | FileCheck %s

// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu -emit-pch %s -o %t

// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu \
// RUN: -include-pch %t -emit-llvm %s -o - | FileCheck %s
// expected-no-diagnostics
//
#ifndef HEADER
#define HEADER
// CHECK-LABEL: @_Z5test1v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.IF"(i32 0), "QUAL.OMP.TARGET.TASK"(), "QUAL.OMP.INREDUCTION.ADD"(ptr [[Y]]) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[INC:%.*]] = add i16 [[TMP2]], 1
// CHECK-NEXT:    store i16 [[INC]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    ret void
//
void test1() {
  short y;
  y = 2;

  {
    #pragma omp target in_reduction(+:y)
    { y++; }
  }
}

// Verify if clause omitted for target in_reduction nowait.
// CHECK-LABEL: @_Z5test2v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2
//
// NOTE: Autogenerated CHECK modified to ensure no 'if' clause generated in
// NOTE: implicit target task when target in_reduction specifies nowait.
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"()
// CHECK-NOT:     "QUAL.OMP.IF"
// CHECK-SAME:    "QUAL.OMP.INREDUCTION.ADD"(ptr [[Y]]) ]
// NOTE: End change to autogenerated CHECK

// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.NOWAIT"(), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[INC:%.*]] = add i16 [[TMP2]], 1
// CHECK-NEXT:    store i16 [[INC]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    ret void
//
void test2() {
  short y;
  y = 2;

  #pragma omp target in_reduction(+:y) nowait
  { y++; }
}

// Verify shared clause omitted for target in_reduction variable.
// CHECK-LABEL: @_Z5test3v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2

// NOTE: Autogenerated CHECK modified to ensure no 'shared' clause on implicit
// NOET: task for target in_reduction variable.
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"()
// CHECK-NOT:     "QUAL.OMP.SHARED"
// CHECK-SAME:    "QUAL.OMP.INREDUCTION.ADD"(ptr [[Y]]) ]
// NOTE: End change to autogenerated CHECK
//
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[INC:%.*]] = add i16 [[TMP2]], 1
// CHECK-NEXT:    store i16 [[INC]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    ret void
//
void test3() {
  short y;
  y = 2;

  #pragma omp target in_reduction(+:y)
  { y++; }
}

// CHECK-LABEL: @_Z5test4v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[X:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[Z:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[M:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2
// CHECK-NEXT:    store i32 1, ptr [[X]], align 4
// CHECK-NEXT:    store i32 3, ptr [[Z]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED"(ptr [[Y]]), "QUAL.OMP.SHARED"(ptr [[X]]), "QUAL.OMP.SHARED"(ptr [[Z]]), "QUAL.OMP.SHARED"(ptr [[M]]) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.MASTER"() ]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASKGROUP"(), "QUAL.OMP.REDUCTION.ADD"(ptr [[Y]]) ]
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.TARGET.TASK"(), "QUAL.OMP.INREDUCTION.ADD"(ptr [[X]]), "QUAL.OMP.INREDUCTION.ADD"(ptr [[Y]]), "QUAL.OMP.INREDUCTION.ADD"(ptr [[Z]]), "QUAL.OMP.SHARED"(ptr [[M]]) ]
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.NOWAIT"(), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[X]], ptr [[X]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Z]], ptr [[Z]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[M]], ptr [[M]], i64 4, i64 1, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[INC:%.*]] = add i16 [[TMP5]], 1
// CHECK-NEXT:    store i16 [[INC]], ptr [[Y]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[X]], align 4
// CHECK-NEXT:    [[INC1:%.*]] = add nsw i32 [[TMP6]], 1
// CHECK-NEXT:    store i32 [[INC1]], ptr [[X]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[Z]], align 4
// CHECK-NEXT:    [[INC2:%.*]] = add nsw i32 [[TMP7]], 1
// CHECK-NEXT:    store i32 [[INC2]], ptr [[Z]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.INREDUCTION.ADD"(ptr [[Y]]) ]
// CHECK-NEXT:    [[TMP9:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[INC3:%.*]] = add i16 [[TMP9]], 1
// CHECK-NEXT:    store i16 [[INC3]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TASKGROUP"() ]
// CHECK-NEXT:    fence release
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.MASTER"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    ret void
//
void test4() {
  short y;
  y = 2;
  int x = 1;
  int z = 3;
  int m;

  #pragma omp parallel master
  #pragma omp taskgroup task_reduction(+:y)
  {
    #pragma omp target map(to:m) in_reduction(+:x,y,z) nowait
    { y++; x++; z++; }

    #pragma omp task in_reduction(+:y)
    y++;
  }
}

// CHECK-LABEL: @_Z5test5v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.SECTIONS"(), "QUAL.OMP.REDUCTION.SUB"(ptr [[Y]]) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SECTION"() ]
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.IF"(i32 0), "QUAL.OMP.TARGET.TASK"(), "QUAL.OMP.INREDUCTION.SUB"(ptr [[Y]]) ]
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP4:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[DEC:%.*]] = add i16 [[TMP4]], -1
// CHECK-NEXT:    store i16 [[DEC]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SECTION"() ]
// CHECK-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SECTION"() ]
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[DEC1:%.*]] = add i16 [[TMP6]], -1
// CHECK-NEXT:    store i16 [[DEC1]], ptr [[Y]], align 2
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.SECTION"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL.SECTIONS"() ]
// CHECK-NEXT:    ret void
//
void test5() {
  short y;
  y = 2;

  #pragma omp parallel sections reduction(-:y)
  {
    #pragma omp section
    {
      #pragma omp target in_reduction(-:y)
      y--;
    }
    #pragma omp section
    {
      y--;
    }
  }
}

// CHECK-LABEL: @_Z5test6v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[Y:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[Z:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i16 2, ptr [[Y]], align 2
// CHECK-NEXT:    store i32 1, ptr [[Z]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.SHARED"(ptr [[Y]]), "QUAL.OMP.SHARED"(ptr [[Z]]) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.MASTER"() ]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TASK"(), "QUAL.OMP.IF"(i32 0), "QUAL.OMP.TARGET.TASK"(), "QUAL.OMP.INREDUCTION.MUL"(ptr [[Y]]), "QUAL.OMP.INREDUCTION.ADD"(ptr [[Z]]) ]
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Y]], ptr [[Y]], i64 2, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:ALWAYS"(ptr [[Z]], ptr [[Z]], i64 4, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP4:%.*]] = load i16, ptr [[Y]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = sext i16 [[TMP4]] to i32
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[CONV]], 2
// CHECK-NEXT:    [[CONV1:%.*]] = trunc i32 [[MUL]] to i16
// CHECK-NEXT:    store i16 [[CONV1]], ptr [[Y]], align 2
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[Z]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP5]], 1
// CHECK-NEXT:    store i32 [[ADD]], ptr [[Z]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TASK"() ]
// CHECK-NEXT:    fence release
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.MASTER"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    ret void
//
void test6() {
  short y;
  y = 2;
  int z = 1;

  #pragma omp parallel master
  {
    #pragma omp target in_reduction(*:y) in_reduction(+:z)
    { y *= 2; z += 1; }
  }
}
#endif
// end INTEL_COLLAB
