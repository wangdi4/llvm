// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// INTEL_COLLAB
// RUN: %clang_cc1 -no-opaque-pointers -triple x86_64-unknown-linux-gnu -fopenmp \
// RUN:  -fintel-compatibility -fopenmp-late-outline \
// RUN:  -fopenmp-targets=spir64 -emit-llvm-bc %s -o %t-host.bc

// RUN: %clang_cc1 -no-opaque-pointers -verify -triple spir64 -fopenmp \
// RUN:  -fintel-compatibility -fopenmp-late-outline \
// RUN:  -fopenmp-targets=spir64 -fopenmp-is-device \
// RUN:  -fopenmp-host-ir-file-path %t-host.bc %s -emit-llvm -o - \
// RUN:  | FileCheck %s

// expected-no-diagnostics
// CHECK: [[STRUCT_S:%struct.S]] = type { i8 }
// CHECK: [[STRUCT_A:%struct.A]] = type { i32 (...)* addrspace(4)* }
// CHECK: [[STRUCT_B:%struct.B]] = type { i32 (...)* addrspace(4)* }
// CHECK: [[STRUCT_C:%struct.C]] = type { i32 (...)* addrspace(4)* }
struct S;

// CHECK-LABEL: @_Z1fP1SMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[S_ADDR:%.*]] = alloca [[STRUCT_S]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[S_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_S]] addrspace(4)*, align 8
// CHECK-NEXT:    [[S_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)** [[S_ADDR]] to [[STRUCT_S]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast { i64, i64 }* [[P_ADDR]] to { i64, i64 } addrspace(4)*
// CHECK-NEXT:    [[S_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)** [[S_MAP_PTR_TMP]] to [[STRUCT_S]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[TMP0:%.*]], align 8
// CHECK-NEXT:    store [[STRUCT_S]] addrspace(4)* [[S:%.*]], [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[P]], { i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load [[STRUCT_S]] addrspace(4)*, [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"([[STRUCT_S]] addrspace(4)* [[TMP1]], [[STRUCT_S]] addrspace(4)* [[TMP1]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.FIRSTPRIVATE"({ i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]]), "QUAL.OMP.PRIVATE"([[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_S]] addrspace(4)* [[TMP1]], [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load [[STRUCT_S]] addrspace(4)*, [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)* [[TMP3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast i8* [[TMP6]] to [[STRUCT_S]] addrspace(4)*
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast [[STRUCT_S]] addrspace(4)* [[THIS_ADJUSTED]] to i8 addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load i8 addrspace(4)*, i8 addrspace(4)* addrspace(4)* [[TMP8]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP10:%.*]] = getelementptr i8, i8 addrspace(4)* [[VTABLE]], i64 [[TMP9]], !nosanitize !9
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i8 addrspace(4)* [[TMP10]] to void ([[STRUCT_S]] addrspace(4)*)* addrspace(4)*, !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load void ([[STRUCT_S]] addrspace(4)*)*, void ([[STRUCT_S]] addrspace(4)*)* addrspace(4)* [[TMP11]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to void ([[STRUCT_S]] addrspace(4)*)*
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP12:%.*]] = phi void ([[STRUCT_S]] addrspace(4)*)* [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP12]]([[STRUCT_S]] addrspace(4)* {{.*}} [[THIS_ADJUSTED]]) #[[ATTR4:[0-9]+]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void f(S *s, void (S::*p)()) {
   #pragma omp target
  (s->*p)();
}


struct __attribute__((visibility("default"))) A {
 #pragma omp declare target
  virtual void foo();
};

// CHECK-LABEL: @_Z6test_1P1A(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca [[STRUCT_A]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_A]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_A]] addrspace(4)** [[P_ADDR]] to [[STRUCT_A]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_A]] addrspace(4)** [[P_MAP_PTR_TMP]] to [[STRUCT_A]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    store [[STRUCT_A]] addrspace(4)* [[P:%.*]], [[STRUCT_A]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load [[STRUCT_A]] addrspace(4)*, [[STRUCT_A]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"([[STRUCT_A]] addrspace(4)* [[TMP0]], [[STRUCT_A]] addrspace(4)* [[TMP0]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.PRIVATE"([[STRUCT_A]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_A]] addrspace(4)* [[TMP0]], [[STRUCT_A]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load [[STRUCT_A]] addrspace(4)*, [[STRUCT_A]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast [[STRUCT_A]] addrspace(4)* [[TMP2]] to void ([[STRUCT_A]] addrspace(4)*)* addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load void ([[STRUCT_A]] addrspace(4)*)* addrspace(4)*, void ([[STRUCT_A]] addrspace(4)*)* addrspace(4)* addrspace(4)* [[TMP3]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds void ([[STRUCT_A]] addrspace(4)*)*, void ([[STRUCT_A]] addrspace(4)*)* addrspace(4)* [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = load void ([[STRUCT_A]] addrspace(4)*)*, void ([[STRUCT_A]] addrspace(4)*)* addrspace(4)* [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP4]]([[STRUCT_A]] addrspace(4)* {{.*}} [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_1(A *p) {
 #pragma omp target
  p->foo();
}

struct __attribute__((visibility("hidden"))) [[clang::lto_visibility_public]] B {
 #pragma omp declare target
  virtual void foo();
 #pragma omp end declare target
};

// CHECK-LABEL: @_Z6test_2P1B(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca [[STRUCT_B]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_B]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_B]] addrspace(4)** [[P_ADDR]] to [[STRUCT_B]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_B]] addrspace(4)** [[P_MAP_PTR_TMP]] to [[STRUCT_B]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    store [[STRUCT_B]] addrspace(4)* [[P:%.*]], [[STRUCT_B]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load [[STRUCT_B]] addrspace(4)*, [[STRUCT_B]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"([[STRUCT_B]] addrspace(4)* [[TMP0]], [[STRUCT_B]] addrspace(4)* [[TMP0]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.PRIVATE"([[STRUCT_B]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_B]] addrspace(4)* [[TMP0]], [[STRUCT_B]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load [[STRUCT_B]] addrspace(4)*, [[STRUCT_B]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast [[STRUCT_B]] addrspace(4)* [[TMP2]] to void ([[STRUCT_B]] addrspace(4)*)* addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load void ([[STRUCT_B]] addrspace(4)*)* addrspace(4)*, void ([[STRUCT_B]] addrspace(4)*)* addrspace(4)* addrspace(4)* [[TMP3]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds void ([[STRUCT_B]] addrspace(4)*)*, void ([[STRUCT_B]] addrspace(4)*)* addrspace(4)* [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = load void ([[STRUCT_B]] addrspace(4)*)*, void ([[STRUCT_B]] addrspace(4)*)* addrspace(4)* [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP4]]([[STRUCT_B]] addrspace(4)* {{.*}} [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_2(B *p) {
 #pragma omp target
  p->foo();
}

struct __attribute__((visibility("hidden"))) C {
 #pragma omp declare target
  virtual void foo();
  virtual void bar();
  #pragma omp end declare target
};

// CHECK-LABEL: @_Z6test_3P1C(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca [[STRUCT_C]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_C]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)** [[P_ADDR]] to [[STRUCT_C]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)** [[P_MAP_PTR_TMP]] to [[STRUCT_C]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    store [[STRUCT_C]] addrspace(4)* [[P:%.*]], [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load [[STRUCT_C]] addrspace(4)*, [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"([[STRUCT_C]] addrspace(4)* [[TMP0]], [[STRUCT_C]] addrspace(4)* [[TMP0]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.PRIVATE"([[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_C]] addrspace(4)* [[TMP0]], [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load [[STRUCT_C]] addrspace(4)*, [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast [[STRUCT_C]] addrspace(4)* [[TMP2]] to void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* addrspace(4)* [[TMP3]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds void ([[STRUCT_C]] addrspace(4)*)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = load void ([[STRUCT_C]] addrspace(4)*)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP4]]([[STRUCT_C]] addrspace(4)* {{.*}} [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_3(C *p) {
 #pragma omp target
  p->foo();
}


// CHECK-LABEL: @_Z6test_4P1C(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca [[STRUCT_C:%.*]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)** [[P_ADDR]] to [[STRUCT_C]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    store [[STRUCT_C]] addrspace(4)* [[P:%.*]], [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load [[STRUCT_C]] addrspace(4)*, [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast [[STRUCT_C]] addrspace(4)* [[TMP0]] to void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* addrspace(4)* [[TMP1]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds void ([[STRUCT_C]] addrspace(4)*)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* [[VTABLE]], i64 1
// CHECK-NEXT:    [[TMP2:%.*]] = load void ([[STRUCT_C]] addrspace(4)*)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP2]]([[STRUCT_C]] addrspace(4)* {{.*}} [[TMP0]]) #[[ATTR5:[0-9]+]]
// CHECK-NEXT:    ret void
//
void test_4(C *p) {
  p->bar();
}

// CHECK-LABEL: @_Z6test_5P1CMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca [[STRUCT_C:%.*]] addrspace(4)*, align 8
// CHECK-NEXT:    [[Q_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_C]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)** [[P_ADDR]] to [[STRUCT_C]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[Q_ADDR_ASCAST:%.*]] = addrspacecast { i64, i64 }* [[Q_ADDR]] to { i64, i64 } addrspace(4)*
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)** [[P_MAP_PTR_TMP]] to [[STRUCT_C]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[Q:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[TMP0:%.*]], align 8
// CHECK-NEXT:    store [[STRUCT_C]] addrspace(4)* [[P:%.*]], [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[Q]], { i64, i64 } addrspace(4)* [[Q_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load [[STRUCT_C]] addrspace(4)*, [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"([[STRUCT_C]] addrspace(4)* [[TMP1]], [[STRUCT_C]] addrspace(4)* [[TMP1]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.FIRSTPRIVATE"({ i64, i64 } addrspace(4)* [[Q_ADDR_ASCAST]]), "QUAL.OMP.PRIVATE"([[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_C]] addrspace(4)* [[TMP1]], [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load [[STRUCT_C]] addrspace(4)*, [[STRUCT_C]] addrspace(4)* addrspace(4)* [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[Q_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast [[STRUCT_C]] addrspace(4)* [[TMP3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast i8* [[TMP6]] to [[STRUCT_C]] addrspace(4)*
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast [[STRUCT_C]] addrspace(4)* [[THIS_ADJUSTED]] to i8 addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load i8 addrspace(4)*, i8 addrspace(4)* addrspace(4)* [[TMP8]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP10:%.*]] = getelementptr i8, i8 addrspace(4)* [[VTABLE]], i64 [[TMP9]], !nosanitize !9
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i8 addrspace(4)* [[TMP10]] to void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)*, !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load void ([[STRUCT_C]] addrspace(4)*)*, void ([[STRUCT_C]] addrspace(4)*)* addrspace(4)* [[TMP11]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to void ([[STRUCT_C]] addrspace(4)*)*
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP12:%.*]] = phi void ([[STRUCT_C]] addrspace(4)*)* [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP12]]([[STRUCT_C]] addrspace(4)* {{.*}} [[THIS_ADJUSTED]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_5(C *p, void (C::*q)(void)) {
 #pragma omp target
  (p->*q)();
}


struct B1 {};
struct B2 {};
struct B3 : B2 {};
struct S : B1, B3 {};

// CHECK-LABEL: @_Z3fooP1SMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[S_ADDR:%.*]] = alloca [[STRUCT_S]] addrspace(4)*, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[S_MAP_PTR_TMP:%.*]] = alloca [[STRUCT_S]] addrspace(4)*, align 8
// CHECK-NEXT:    [[S_ADDR_ASCAST:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)** [[S_ADDR]] to [[STRUCT_S]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast { i64, i64 }* [[P_ADDR]] to { i64, i64 } addrspace(4)*
// CHECK-NEXT:    [[S_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)** [[S_MAP_PTR_TMP]] to [[STRUCT_S]] addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[P:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[TMP0:%.*]], align 8
// CHECK-NEXT:    store [[STRUCT_S]] addrspace(4)* [[S:%.*]], [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[P]], { i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load [[STRUCT_S]] addrspace(4)*, [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"([[STRUCT_S]] addrspace(4)* [[TMP1]], [[STRUCT_S]] addrspace(4)* [[TMP1]], i64 0, i64 544, i8* null, i8* null), "QUAL.OMP.FIRSTPRIVATE"({ i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]]), "QUAL.OMP.PRIVATE"([[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]]) ]
// CHECK-NEXT:    store [[STRUCT_S]] addrspace(4)* [[TMP1]], [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load [[STRUCT_S]] addrspace(4)*, [[STRUCT_S]] addrspace(4)* addrspace(4)* [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, { i64, i64 } addrspace(4)* [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast [[STRUCT_S]] addrspace(4)* [[TMP3]] to i8*
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast i8* [[TMP6]] to [[STRUCT_S]] addrspace(4)*
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast [[STRUCT_S]] addrspace(4)* [[THIS_ADJUSTED]] to i8 addrspace(4)* addrspace(4)*
// CHECK-NEXT:    [[VTABLE:%.*]] = load i8 addrspace(4)*, i8 addrspace(4)* addrspace(4)* [[TMP8]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP10:%.*]] = getelementptr i8, i8 addrspace(4)* [[VTABLE]], i64 [[TMP9]], !nosanitize !9
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i8 addrspace(4)* [[TMP10]] to void ([[STRUCT_S]] addrspace(4)*)* addrspace(4)*, !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load void ([[STRUCT_S]] addrspace(4)*)*, void ([[STRUCT_S]] addrspace(4)*)* addrspace(4)* [[TMP11]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to void ([[STRUCT_S]] addrspace(4)*)*
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP12:%.*]] = phi void ([[STRUCT_S]] addrspace(4)*)* [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP12]]([[STRUCT_S]] addrspace(4)* {{.*}} [[THIS_ADJUSTED]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void foo(S *s, void (S::*p)()) {
 #pragma omp target
  (s->*p)();
}
 #pragma omp end declare target

// end INTEL_COLLAB
