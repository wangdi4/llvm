// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --functions "test|test1|test2" --prefix-filecheck-ir-name _ --version 3

// RUN: %clang_cc1 -verify -fopenmp -fopenmp-version=50 -x c++ \
// RUN:  -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 -triple x86_64-unknown-unknown \
// RUN:  -emit-llvm %s -o - | FileCheck %s

// RUN: %clang_cc1 -verify -fopenmp -fopenmp-version=50 -x c++ \
// RUN:  -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 -triple x86_64-unknown-unknown \
// RUN:  -emit-llvm %s -o - | FileCheck %s --check-prefix ROT1

// expected-no-diagnostics
void foo();
void bar();

class C {
public:
  int a = 0;
};

void my_comb(C &out, C &in) { out.a += in.a; };
void my_init(C &priv, C &orig) { priv.a = orig.a; }

#pragma omp declare reduction(my_add:C                                         \
  : my_comb(omp_out, omp_in))                      \
  initializer(my_init(omp_priv, omp_orig))

C p[10];
C q[10];
int y;
// CHECK-LABEL: define dso_local void @_Z4testRA10_1C(
// CHECK-SAME: ptr noundef nonnull align 4 dereferenceable(40) [[X:%.*]]) #[[ATTR1:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[X_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store ptr [[X]], ptr [[X_ADDR]], align 8
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.UDR:BYREF.ARRSECT.INSCAN.TYPED"(ptr [[X_ADDR]], [[CLASS_C:%.*]] zeroinitializer, i64 5, i64 2, ptr null, ptr null, ptr @.omp_combiner., ptr @.omp_initializer., i64 1), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr @y, i32 0, i32 1, i64 2), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP2]], [[TMP3]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP4]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x %class.C], ptr [[TMP5]], i64 0, i64 5
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP6]] to i64
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x %class.C], ptr @p, i64 0, i64 [[IDXPROM]]
// CHECK-NEXT:    call void @_Z7my_combR1CS0_(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX]], ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX1]]) #[[ATTR2:[0-9]+]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM2:%.*]] = sext i32 [[TMP7]] to i64
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x %class.C], ptr @p, i64 0, i64 [[IDXPROM2]]
// CHECK-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_C]], ptr [[ARRAYIDX3]], i32 0, i32 0
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr @y, align 4
// CHECK-NEXT:    [[ADD4:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// CHECK-NEXT:    store i32 [[ADD4]], ptr @y, align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr @y, i64 2), "QUAL.OMP.INCLUSIVE"(ptr [[X_ADDR]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM5:%.*]] = sext i32 [[TMP12]] to i64
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x %class.C], ptr @q, i64 0, i64 [[IDXPROM5]]
// CHECK-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x %class.C], ptr [[TMP13]], i64 0, i64 5
// CHECK-NEXT:    call void @_Z7my_initR1CS0_(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX6]], ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX7]]) #[[ATTR2]]
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr @y, align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM8:%.*]] = sext i32 [[TMP15]] to i64
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x %class.C], ptr @q, i64 0, i64 [[IDXPROM8]]
// CHECK-NEXT:    [[A10:%.*]] = getelementptr inbounds [[CLASS_C]], ptr [[ARRAYIDX9]], i32 0, i32 0
// CHECK-NEXT:    store i32 [[TMP14]], ptr [[A10]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP16]], 1
// CHECK-NEXT:    store i32 [[ADD11]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP3:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z4testRA10_1C(
// ROT1-SAME: ptr noundef nonnull align 4 dereferenceable(40) [[X:%.*]]) #[[ATTR1:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[X_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store ptr [[X]], ptr [[X_ADDR]], align 8
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.UDR:BYREF.ARRSECT.INSCAN.TYPED"(ptr [[X_ADDR]], [[CLASS_C:%.*]] zeroinitializer, i64 5, i64 2, ptr null, ptr null, ptr @.omp_combiner., ptr @.omp_initializer., i64 1), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr @y, i32 0, i32 1, i64 2), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP2]], [[TMP3]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP4]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x %class.C], ptr [[TMP5]], i64 0, i64 5
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP6]] to i64
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x %class.C], ptr @p, i64 0, i64 [[IDXPROM]]
// ROT1-NEXT:    call void @_Z7my_combR1CS0_(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX]], ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX1]]) #[[ATTR2:[0-9]+]]
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM2:%.*]] = sext i32 [[TMP7]] to i64
// ROT1-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x %class.C], ptr @p, i64 0, i64 [[IDXPROM2]]
// ROT1-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_C]], ptr [[ARRAYIDX3]], i32 0, i32 0
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[A]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr @y, align 4
// ROT1-NEXT:    [[ADD4:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// ROT1-NEXT:    store i32 [[ADD4]], ptr @y, align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr @y, i64 2), "QUAL.OMP.INCLUSIVE"(ptr [[X_ADDR]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM5:%.*]] = sext i32 [[TMP12]] to i64
// ROT1-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x %class.C], ptr @q, i64 0, i64 [[IDXPROM5]]
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[X_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x %class.C], ptr [[TMP13]], i64 0, i64 5
// ROT1-NEXT:    call void @_Z7my_initR1CS0_(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX6]], ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX7]]) #[[ATTR2]]
// ROT1-NEXT:    [[TMP14:%.*]] = load i32, ptr @y, align 4
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM8:%.*]] = sext i32 [[TMP15]] to i64
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x %class.C], ptr @q, i64 0, i64 [[IDXPROM8]]
// ROT1-NEXT:    [[A10:%.*]] = getelementptr inbounds [[CLASS_C]], ptr [[ARRAYIDX9]], i32 0, i32 0
// ROT1-NEXT:    store i32 [[TMP14]], ptr [[A10]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP16]], 1
// ROT1-NEXT:    store i32 [[ADD11]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP12:%.*]] = icmp sle i32 [[TMP17]], [[TMP18]]
// ROT1-NEXT:    br i1 [[CMP12]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]], !llvm.loop [[LOOP3:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    ret void
//
void test(C (&x)[10]) {
  #pragma omp simd reduction(inscan, my_add : x[2:5]) reduction(inscan, +: y)
  for (int i = 0; i < 10; i++) {
  my_comb(x[5], p[i]);
  y += p[i].a;
  #pragma omp scan inclusive(y, x[2:5])
  my_init(q[i], x[5]);
  q[i].a = y;
  }
}

struct S {
  int a;
  S() {}
  ~S() {}
  S& operator+(const S&);
  S& operator=(const S&);
};

// CHECK-LABEL: define dso_local void @_Z5test1v(
// CHECK-SAME: ) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[S:%.*]] = alloca [2 x %struct.S], align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[STRUCT_S:%.*]], ptr [[ARRAY_BEGIN]], i64 2
// CHECK-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// CHECK:       arrayctor.loop:
// CHECK-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// CHECK-NEXT:    call void @_ZN1SC1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// CHECK-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAYCTOR_CUR]], i64 1
// CHECK-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// CHECK-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// CHECK:       arrayctor.cont:
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ARRAY_BEGIN1:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.UDR:INSCAN.TYPED"(ptr [[S]], [[STRUCT_S]] zeroinitializer, i64 2, ptr @_ZTS1S.omp.def_constr, ptr @_ZTS1S.omp.destr, ptr @.omp_combiner..1, ptr null, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP3]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    call void @_Z3foov() #[[ATTR2]]
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[S]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    call void @_Z3barv() #[[ATTR2]]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP5]], 1
// CHECK-NEXT:    store i32 [[ADD2]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    [[ARRAY_BEGIN3:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAY_BEGIN3]], i64 2
// CHECK-NEXT:    br label [[ARRAYDESTROY_BODY:%.*]]
// CHECK:       arraydestroy.body:
// CHECK-NEXT:    [[ARRAYDESTROY_ELEMENTPAST:%.*]] = phi ptr [ [[TMP6]], [[OMP_LOOP_EXIT]] ], [ [[ARRAYDESTROY_ELEMENT:%.*]], [[ARRAYDESTROY_BODY]] ]
// CHECK-NEXT:    [[ARRAYDESTROY_ELEMENT]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAYDESTROY_ELEMENTPAST]], i64 -1
// CHECK-NEXT:    call void @_ZN1SD1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYDESTROY_ELEMENT]]) #[[ATTR2]]
// CHECK-NEXT:    [[ARRAYDESTROY_DONE:%.*]] = icmp eq ptr [[ARRAYDESTROY_ELEMENT]], [[ARRAY_BEGIN3]]
// CHECK-NEXT:    br i1 [[ARRAYDESTROY_DONE]], label [[ARRAYDESTROY_DONE4:%.*]], label [[ARRAYDESTROY_BODY]]
// CHECK:       arraydestroy.done4:
// CHECK-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z5test1v(
// ROT1-SAME: ) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[S:%.*]] = alloca [2 x %struct.S], align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// ROT1-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[STRUCT_S:%.*]], ptr [[ARRAY_BEGIN]], i64 2
// ROT1-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// ROT1:       arrayctor.loop:
// ROT1-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// ROT1-NEXT:    call void @_ZN1SC1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// ROT1-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAYCTOR_CUR]], i64 1
// ROT1-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// ROT1-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// ROT1:       arrayctor.cont:
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ARRAY_BEGIN1:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.UDR:INSCAN.TYPED"(ptr [[S]], [[STRUCT_S]] zeroinitializer, i64 2, ptr @_ZTS1S.omp.def_constr, ptr @_ZTS1S.omp.destr, ptr @.omp_combiner..1, ptr null, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP1]], [[TMP2]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP3]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    call void @_Z3foov() #[[ATTR2]]
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[S]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    call void @_Z3barv() #[[ATTR2]]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD2:%.*]] = add nsw i32 [[TMP5]], 1
// ROT1-NEXT:    store i32 [[ADD2]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP3:%.*]] = icmp sle i32 [[TMP6]], [[TMP7]]
// ROT1-NEXT:    br i1 [[CMP3]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]], !llvm.loop [[LOOP5:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    [[ARRAY_BEGIN4:%.*]] = getelementptr inbounds [2 x %struct.S], ptr [[S]], i32 0, i32 0
// ROT1-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAY_BEGIN4]], i64 2
// ROT1-NEXT:    br label [[ARRAYDESTROY_BODY:%.*]]
// ROT1:       arraydestroy.body:
// ROT1-NEXT:    [[ARRAYDESTROY_ELEMENTPAST:%.*]] = phi ptr [ [[TMP8]], [[OMP_LOOP_EXIT]] ], [ [[ARRAYDESTROY_ELEMENT:%.*]], [[ARRAYDESTROY_BODY]] ]
// ROT1-NEXT:    [[ARRAYDESTROY_ELEMENT]] = getelementptr inbounds [[STRUCT_S]], ptr [[ARRAYDESTROY_ELEMENTPAST]], i64 -1
// ROT1-NEXT:    call void @_ZN1SD1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYDESTROY_ELEMENT]]) #[[ATTR2]]
// ROT1-NEXT:    [[ARRAYDESTROY_DONE:%.*]] = icmp eq ptr [[ARRAYDESTROY_ELEMENT]], [[ARRAY_BEGIN4]]
// ROT1-NEXT:    br i1 [[ARRAYDESTROY_DONE]], label [[ARRAYDESTROY_DONE5:%.*]], label [[ARRAYDESTROY_BODY]]
// ROT1:       arraydestroy.done5:
// ROT1-NEXT:    ret void
//
void test1() {
  S s[2];
  #pragma omp simd reduction(inscan, + : s)
  for (int i = 0; i < 10; ++i) {
  foo();
  #pragma omp scan exclusive(s)
  bar();
  }
}

// CHECK-LABEL: define dso_local void @_Z5test2RA10_iRi(
// CHECK-SAME: ptr noundef nonnull align 4 dereferenceable(40) [[ARR_REF:%.*]], ptr noundef nonnull align 4 dereferenceable(4) [[IREF:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ARR_REF_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[IREF_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[ARR:%.*]] = alloca [10 x i32], align 16
// CHECK-NEXT:    [[IFOO:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[_TMP2:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV3:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB4:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I8:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[_TMP16:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV17:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB18:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I22:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[_TMP30:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV31:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB32:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I36:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store ptr [[ARR_REF]], ptr [[ARR_REF_ADDR]], align 8
// CHECK-NEXT:    store ptr [[IREF]], ptr [[IREF_ADDR]], align 8
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.INSCAN.TYPED"(ptr [[ARR]], i32 0, i64 5, i64 2, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP3]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr [[ARR]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP5]], 1
// CHECK-NEXT:    store i32 [[ADD1]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP6:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB4]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[ARR_REF_ADDR]], align 8
// CHECK-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:BYREF.ARRSECT.INSCAN.TYPED"(ptr [[ARR_REF_ADDR]], i32 0, i64 5, i64 2, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV3]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB4]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I8]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV3]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND5:%.*]]
// CHECK:       omp.inner.for.cond5:
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTOMP_IV3]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTOMP_UB4]], align 4
// CHECK-NEXT:    [[CMP6:%.*]] = icmp sle i32 [[TMP8]], [[TMP9]]
// CHECK-NEXT:    br i1 [[CMP6]], label [[OMP_INNER_FOR_BODY7:%.*]], label [[OMP_INNER_FOR_END14:%.*]]
// CHECK:       omp.inner.for.body7:
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV3]], align 4
// CHECK-NEXT:    [[MUL9:%.*]] = mul nsw i32 [[TMP10]], 1
// CHECK-NEXT:    [[ADD10:%.*]] = add nsw i32 0, [[MUL9]]
// CHECK-NEXT:    store i32 [[ADD10]], ptr [[I8]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[ARR_REF_ADDR]], align 8
// CHECK-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr [[ARR_REF_ADDR]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE11:%.*]]
// CHECK:       omp.body.continue11:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC12:%.*]]
// CHECK:       omp.inner.for.inc12:
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV3]], align 4
// CHECK-NEXT:    [[ADD13:%.*]] = add nsw i32 [[TMP13]], 1
// CHECK-NEXT:    store i32 [[ADD13]], ptr [[DOTOMP_IV3]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND5]], !llvm.loop [[LOOP7:![0-9]+]]
// CHECK:       omp.inner.for.end14:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT15:%.*]]
// CHECK:       omp.loop.exit15:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB18]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr [[IFOO]], i32 0, i32 1, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV17]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB18]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I22]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV17]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND19:%.*]]
// CHECK:       omp.inner.for.cond19:
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV17]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_UB18]], align 4
// CHECK-NEXT:    [[CMP20:%.*]] = icmp sle i32 [[TMP15]], [[TMP16]]
// CHECK-NEXT:    br i1 [[CMP20]], label [[OMP_INNER_FOR_BODY21:%.*]], label [[OMP_INNER_FOR_END28:%.*]]
// CHECK:       omp.inner.for.body21:
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_IV17]], align 4
// CHECK-NEXT:    [[MUL23:%.*]] = mul nsw i32 [[TMP17]], 1
// CHECK-NEXT:    [[ADD24:%.*]] = add nsw i32 0, [[MUL23]]
// CHECK-NEXT:    store i32 [[ADD24]], ptr [[I22]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[IFOO]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP18]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE25:%.*]]
// CHECK:       omp.body.continue25:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC26:%.*]]
// CHECK:       omp.inner.for.inc26:
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTOMP_IV17]], align 4
// CHECK-NEXT:    [[ADD27:%.*]] = add nsw i32 [[TMP19]], 1
// CHECK-NEXT:    store i32 [[ADD27]], ptr [[DOTOMP_IV17]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND19]], !llvm.loop [[LOOP8:![0-9]+]]
// CHECK:       omp.inner.for.end28:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT29:%.*]]
// CHECK:       omp.loop.exit29:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    store i32 9, ptr [[DOTOMP_UB32]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[IREF_ADDR]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:BYREF.INSCAN.TYPED"(ptr [[IREF_ADDR]], i32 0, i32 1, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV31]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB32]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I36]], i32 0, i32 1, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_IV31]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND33:%.*]]
// CHECK:       omp.inner.for.cond33:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTOMP_IV31]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTOMP_UB32]], align 4
// CHECK-NEXT:    [[CMP34:%.*]] = icmp sle i32 [[TMP22]], [[TMP23]]
// CHECK-NEXT:    br i1 [[CMP34]], label [[OMP_INNER_FOR_BODY35:%.*]], label [[OMP_INNER_FOR_END42:%.*]]
// CHECK:       omp.inner.for.body35:
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_IV31]], align 4
// CHECK-NEXT:    [[MUL37:%.*]] = mul nsw i32 [[TMP24]], 1
// CHECK-NEXT:    [[ADD38:%.*]] = add nsw i32 0, [[MUL37]]
// CHECK-NEXT:    store i32 [[ADD38]], ptr [[I36]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[IREF_ADDR]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[IREF_ADDR]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP26]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE39:%.*]]
// CHECK:       omp.body.continue39:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC40:%.*]]
// CHECK:       omp.inner.for.inc40:
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTOMP_IV31]], align 4
// CHECK-NEXT:    [[ADD41:%.*]] = add nsw i32 [[TMP27]], 1
// CHECK-NEXT:    store i32 [[ADD41]], ptr [[DOTOMP_IV31]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND33]], !llvm.loop [[LOOP9:![0-9]+]]
// CHECK:       omp.inner.for.end42:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT43:%.*]]
// CHECK:       omp.loop.exit43:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z5test2RA10_iRi(
// ROT1-SAME: ptr noundef nonnull align 4 dereferenceable(40) [[ARR_REF:%.*]], ptr noundef nonnull align 4 dereferenceable(4) [[IREF:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[ARR_REF_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[IREF_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[ARR:%.*]] = alloca [10 x i32], align 16
// ROT1-NEXT:    [[IFOO:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP3:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV4:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB5:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I9:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP19:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV20:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB21:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I25:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP35:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV36:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB37:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I41:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store ptr [[ARR_REF]], ptr [[ARR_REF_ADDR]], align 8
// ROT1-NEXT:    store ptr [[IREF]], ptr [[IREF_ADDR]], align 8
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.INSCAN.TYPED"(ptr [[ARR]], i32 0, i64 5, i64 2, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP1]], [[TMP2]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP3]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr [[ARR]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP5]], 1
// ROT1-NEXT:    store i32 [[ADD1]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP2:%.*]] = icmp sle i32 [[TMP6]], [[TMP7]]
// ROT1-NEXT:    br i1 [[CMP2]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]], !llvm.loop [[LOOP6:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB5]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[ARR_REF_ADDR]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:BYREF.ARRSECT.INSCAN.TYPED"(ptr [[ARR_REF_ADDR]], i32 0, i64 5, i64 2, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV4]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB5]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I9]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTOMP_UB5]], align 4
// ROT1-NEXT:    [[CMP6:%.*]] = icmp sle i32 [[TMP10]], [[TMP11]]
// ROT1-NEXT:    br i1 [[CMP6]], label [[OMP_INNER_FOR_BODY_LH7:%.*]], label [[OMP_INNER_FOR_END17:%.*]]
// ROT1:       omp.inner.for.body.lh7:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY8:%.*]]
// ROT1:       omp.inner.for.body8:
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[MUL10:%.*]] = mul nsw i32 [[TMP12]], 1
// ROT1-NEXT:    [[ADD11:%.*]] = add nsw i32 0, [[MUL10]]
// ROT1-NEXT:    store i32 [[ADD11]], ptr [[I9]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[ARR_REF_ADDR]], align 8
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(ptr [[ARR_REF_ADDR]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE12:%.*]]
// ROT1:       omp.body.continue12:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC13:%.*]]
// ROT1:       omp.inner.for.inc13:
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[ADD14:%.*]] = add nsw i32 [[TMP15]], 1
// ROT1-NEXT:    store i32 [[ADD14]], ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV4]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB5]], align 4
// ROT1-NEXT:    [[CMP15:%.*]] = icmp sle i32 [[TMP16]], [[TMP17]]
// ROT1-NEXT:    br i1 [[CMP15]], label [[OMP_INNER_FOR_BODY8]], label [[OMP_INNER_FOR_END_CRIT_EDGE16:%.*]], !llvm.loop [[LOOP7:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge16:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END17]]
// ROT1:       omp.inner.for.end17:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT18:%.*]]
// ROT1:       omp.loop.exit18:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB21]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:INSCAN.TYPED"(ptr [[IFOO]], i32 0, i32 1, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV20]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB21]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I25]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_UB21]], align 4
// ROT1-NEXT:    [[CMP22:%.*]] = icmp sle i32 [[TMP19]], [[TMP20]]
// ROT1-NEXT:    br i1 [[CMP22]], label [[OMP_INNER_FOR_BODY_LH23:%.*]], label [[OMP_INNER_FOR_END33:%.*]]
// ROT1:       omp.inner.for.body.lh23:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY24:%.*]]
// ROT1:       omp.inner.for.body24:
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[MUL26:%.*]] = mul nsw i32 [[TMP21]], 1
// ROT1-NEXT:    [[ADD27:%.*]] = add nsw i32 0, [[MUL26]]
// ROT1-NEXT:    store i32 [[ADD27]], ptr [[I25]], align 4
// ROT1-NEXT:    [[TMP22:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[IFOO]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP22]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE28:%.*]]
// ROT1:       omp.body.continue28:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC29:%.*]]
// ROT1:       omp.inner.for.inc29:
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[ADD30:%.*]] = add nsw i32 [[TMP23]], 1
// ROT1-NEXT:    store i32 [[ADD30]], ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_IV20]], align 4
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTOMP_UB21]], align 4
// ROT1-NEXT:    [[CMP31:%.*]] = icmp sle i32 [[TMP24]], [[TMP25]]
// ROT1-NEXT:    br i1 [[CMP31]], label [[OMP_INNER_FOR_BODY24]], label [[OMP_INNER_FOR_END_CRIT_EDGE32:%.*]], !llvm.loop [[LOOP8:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge32:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END33]]
// ROT1:       omp.inner.for.end33:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT34:%.*]]
// ROT1:       omp.loop.exit34:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP18]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    store i32 9, ptr [[DOTOMP_UB37]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[IREF_ADDR]], align 8
// ROT1-NEXT:    [[TMP27:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.ADD:BYREF.INSCAN.TYPED"(ptr [[IREF_ADDR]], i32 0, i32 1, i64 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV36]], i32 0), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB37]], i32 0), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I41]], i32 0, i32 1, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTOMP_UB37]], align 4
// ROT1-NEXT:    [[CMP38:%.*]] = icmp sle i32 [[TMP28]], [[TMP29]]
// ROT1-NEXT:    br i1 [[CMP38]], label [[OMP_INNER_FOR_BODY_LH39:%.*]], label [[OMP_INNER_FOR_END49:%.*]]
// ROT1:       omp.inner.for.body.lh39:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY40:%.*]]
// ROT1:       omp.inner.for.body40:
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[MUL42:%.*]] = mul nsw i32 [[TMP30]], 1
// ROT1-NEXT:    [[ADD43:%.*]] = add nsw i32 0, [[MUL42]]
// ROT1-NEXT:    store i32 [[ADD43]], ptr [[I41]], align 4
// ROT1-NEXT:    [[TMP31:%.*]] = load ptr, ptr [[IREF_ADDR]], align 8
// ROT1-NEXT:    [[TMP32:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"(ptr [[IREF_ADDR]], i64 1) ]
// ROT1-NEXT:    fence acq_rel
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP32]]) [ "DIR.OMP.END.SCAN"() ]
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE44:%.*]]
// ROT1:       omp.body.continue44:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC45:%.*]]
// ROT1:       omp.inner.for.inc45:
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[ADD46:%.*]] = add nsw i32 [[TMP33]], 1
// ROT1-NEXT:    store i32 [[ADD46]], ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTOMP_IV36]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTOMP_UB37]], align 4
// ROT1-NEXT:    [[CMP47:%.*]] = icmp sle i32 [[TMP34]], [[TMP35]]
// ROT1-NEXT:    br i1 [[CMP47]], label [[OMP_INNER_FOR_BODY40]], label [[OMP_INNER_FOR_END_CRIT_EDGE48:%.*]], !llvm.loop [[LOOP9:![0-9]+]]
// ROT1:       omp.inner.for.end_crit_edge48:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END49]]
// ROT1:       omp.inner.for.end49:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT50:%.*]]
// ROT1:       omp.loop.exit50:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP27]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-NEXT:    ret void
//
void test2(int (&arr_ref)[10], int &iref) {
int arr[10];
int ifoo;
  #pragma omp simd reduction(inscan, +:arr[2:5])
  for (int i = 0; i < 10; i++) {
    #pragma omp scan inclusive(arr[2:5])
  }
  #pragma omp simd reduction(inscan, +:arr_ref[2:5])
  for (int i = 0; i < 10; i++) {
    #pragma omp scan inclusive(arr_ref[2:5])
  }
  #pragma omp simd reduction(inscan, +:ifoo)
  for (int i = 0; i < 10; i++) {
    #pragma omp scan exclusive(ifoo)
  }
  #pragma omp simd reduction(inscan, +:iref)
  for (int i = 0; i < 10; i++) {
    #pragma omp scan exclusive(iref)
  }
}
// end INTEL_COLLAB
