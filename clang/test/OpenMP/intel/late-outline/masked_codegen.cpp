// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// INTEL_COLLAB
// RUN: %clang_cc1 -opaque-pointers -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu %s | FileCheck %s
//
// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu -emit-pch %s -o %t
//
// RUN: %clang_cc1 -opaque-pointers -fopenmp -fopenmp-late-outline \
// RUN: -triple x86_64-unknown-linux-gnu \
// RUN: -include-pch %t -emit-llvm %s -o - | FileCheck %s
//
// expected-no-diagnostics
//
#ifndef HEADER
#define HEADER
// CHECK-LABEL: @_Z3foov(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @_Z8mayThrowv()
// CHECK-NEXT:    ret void
//
void foo() { extern void mayThrow(); mayThrow(); }

// CHECK-LABEL: @main(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[X:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.MASKED"() ]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    store i8 2, ptr [[A]], align 1
// CHECK-NEXT:    fence release
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.MASKED"() ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.MASKED"(), "QUAL.OMP.FILTER"(i32 2) ]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    call void @_Z3foov()
// CHECK-NEXT:    fence release
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.MASKED"() ]
// CHECK-NEXT:    store i32 9, ptr [[X]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[X]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.MASKED"(), "QUAL.OMP.FILTER"(i32 [[TMP2]]) ]
// CHECK-NEXT:    fence acquire
// CHECK-NEXT:    call void @_Z3foov()
// CHECK-NEXT:    fence release
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.MASKED"() ]
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP4]] to i32
// CHECK-NEXT:    ret i32 [[CONV]]
//
int main() {
  char a;

  #pragma omp masked
  a = 2;
  #pragma omp masked filter(2)
  foo();
  int x = 9;
  #pragma omp masked filter(x)
  foo();
  return a;
}

// CHECK-LABEL: @_Z13lambda_maskedii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[L:%.*]] = alloca [[CLASS_ANON:%.*]], align 4
// CHECK-NEXT:    [[L1:%.*]] = alloca [[CLASS_ANON_0:%.*]], align 4
// CHECK-NEXT:    [[Y:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[L2:%.*]] = alloca [[CLASS_ANON_1:%.*]], align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = getelementptr inbounds [[CLASS_ANON]], ptr [[L]], i32 0, i32 0
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[TMP0]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [[CLASS_ANON]], ptr [[L]], i32 0, i32 1
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[TMP2]], align 4
// CHECK-NEXT:    call void @_ZZ13lambda_maskediiENKUlvE_clEv(ptr noundef {{.*}} [[L]])
// CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [[CLASS_ANON_0]], ptr [[L1]], i32 0, i32 0
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP5]], ptr [[TMP4]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[CLASS_ANON_0]], ptr [[L1]], i32 0, i32 1
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP7]], ptr [[TMP6]], align 4
// CHECK-NEXT:    call void @_ZZ13lambda_maskediiENKUlvE0_clEv(ptr noundef {{.*}} [[L1]])
// CHECK-NEXT:    store i32 1, ptr [[Y]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds [[CLASS_ANON_1]], ptr [[L2]], i32 0, i32 0
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP9]], ptr [[TMP8]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [[CLASS_ANON_1]], ptr [[L2]], i32 0, i32 1
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP11]], ptr [[TMP10]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[Y]], align 4
// CHECK-NEXT:    call void @_ZZ13lambda_maskediiENKUliE_clEi(ptr noundef nonnull align 4 dereferenceable(8) [[L2]], i32 noundef [[TMP12]])
// CHECK-NEXT:    ret void
//
void lambda_masked(int a, int b) {
  auto l = [=]() {
  #pragma omp masked
    {
      int c = a + b;
    }
  };

  l();

  auto l1 = [=]() {
#pragma omp parallel
#pragma omp masked filter(1)
    {
      int c = a + b;
    }
  };

  l1();

  int y = 1;
  auto l2 = [=](int yy) {
#pragma omp parallel
#pragma omp masked filter(yy)
    {
      int c = a + b;
    }
  };

  l2(y);
}
#endif
// end INTEL_COLLAB
