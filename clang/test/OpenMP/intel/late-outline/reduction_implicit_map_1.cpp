// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs --replace-value-regex "__omp_offloading_[0-9a-z]+_[0-9a-z]+" "reduction_size[.].+[.]" "pl_cond[.].+[.|,]" --prefix-filecheck-ir-name _

// RUN: %clang_cc1 -verify -fopenmp -x c++ \
// RUN:   -fno-openmp-use-host-usm-for-implicit-reduction-map  \
// RUN:   -fopenmp-late-outline -emit-llvm \
// RUN:   -fopenmp-loop-rotation-control=0 -triple x86_64-pc-linux-gnu \
// RUN:   %s -o - | FileCheck %s

// RUN: %clang_cc1 -verify -fopenmp -x c++ \
// RUN:   -fno-openmp-use-host-usm-for-implicit-reduction-map  \
// RUN:   -fopenmp-late-outline -emit-llvm \
// RUN:   -fopenmp-loop-rotation-control=0 -triple x86_64-pc-linux-gnu \
// RUN:   %s -o - | FileCheck %s --check-prefix ROT1

// Test map type with OMP_TGT_MAPTYPE_HOST_MEM 0x8000

int foo(int n) {
  double *e;
  double sum;
  #pragma omp target parallel reduction(+: e[:1])
    *e=10;
  #pragma omp target parallel map(e[:1]) reduction(+: e[:1])
    *e=10;
  #pragma omp target parallel reduction(+: sum)
    sum += e[0];
  return 0;
}
class S2 {
  mutable int a;
public:
  S2():a(0) { }
  S2(S2 &s2):a(s2.a) { }
  S2 &operator +(S2 &s);
  void moo() {
    #pragma omp target parallel reduction(+:a)
    a += 10;
  }
};
int bar() {
 S2 o[5];
#pragma omp target parallel reduction(+:o[0]) // expected-warning {{Type 'S2' is not trivially copyable and not guaranteed to be mapped correctly}}
  for (int i = 0; i < 10; i++);
  double b[10][10][10];
#pragma omp target parallel for reduction(task, +: b[0:2][2:4][1])
  for (long long i = 0; i < 10; ++i);
  o[0].moo();
  return 0;
}
void sum(int* input, int size, int* output)
{
#pragma omp target teams distribute parallel for reduction(+: output[0]) \
                                                 map(to: input [0:size])
  for (int i = 0; i < size; i++)
    output[0] += input[i];
#pragma omp target teams distribute parallel for reduction(+: output[:3])  \
                                                 map(to: input [0:size])
  for (int i = 0; i < size; i++)
    output[0] += input[i];
  int a[10];
#pragma omp target parallel reduction(+: a[:2])
  for (int i = 0; i < size; i++)
    ;
#pragma omp target parallel reduction(+: a[3])
  for (int i = 0; i < size; i++)
    ;
}
int main()
{
  int a = foo(10);
  a = bar();
  const int size = 100;
  int *array = new int[size];
  int result = 0;
  sum(array, size, &result);
  return 0;
}

// CHECK-LABEL: define {{[^@]+}}@_Z3fooi
// CHECK-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[E:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SUM:%.*]] = alloca double, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP6:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP8:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds double, ptr [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP2]], ptr [[ARRAYIDX1]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP]], ptr null, i32 1) ]
// CHECK-NEXT:    store ptr [[TMP2]], ptr [[E_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds double, ptr [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[E_MAP_PTR_TMP]], double 0.000000e+00, i64 1, i64 0) ]
// CHECK-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    store double 1.000000e+01, ptr [[TMP7]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds double, ptr [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds double, ptr [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds double, ptr [[TMP13]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP10]], ptr [[ARRAYIDX4]], i64 8, i64 35, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:CHAIN"(ptr [[TMP12]], ptr [[ARRAYIDX5]], i64 8, i64 515, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP6]], ptr null, i32 1) ]
// CHECK-NEXT:    store ptr [[TMP10]], ptr [[E_MAP_PTR_TMP6]], align 8
// CHECK-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP6]], align 8
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds double, ptr [[TMP15]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[E_MAP_PTR_TMP6]], double 0.000000e+00, i64 1, i64 0) ]
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP6]], align 8
// CHECK-NEXT:    store double 1.000000e+01, ptr [[TMP17]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"(ptr [[SUM]], ptr [[SUM]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP18]], ptr [[TMP18]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP8]], ptr null, i32 1) ]
// CHECK-NEXT:    store ptr [[TMP18]], ptr [[E_MAP_PTR_TMP8]], align 8
// CHECK-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[SUM]], double 0.000000e+00, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[E_MAP_PTR_TMP8]], ptr null, i32 1) ]
// CHECK-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP8]], align 8
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds double, ptr [[TMP21]], i64 0
// CHECK-NEXT:    [[TMP22:%.*]] = load double, ptr [[ARRAYIDX9]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = load double, ptr [[SUM]], align 8
// CHECK-NEXT:    [[ADD:%.*]] = fadd double [[TMP23]], [[TMP22]]
// CHECK-NEXT:    store double [[ADD]], ptr [[SUM]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret i32 0
//
//
// CHECK-LABEL: define {{[^@]+}}@_Z3barv
// CHECK-SAME: () #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[O:%.*]] = alloca [5 x %class.S2], align 16
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B:%.*]] = alloca [10 x [10 x [10 x double]]], align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[I11:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[ARRAY_BEGIN]], i64 5
// CHECK-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// CHECK:       arrayctor.loop:
// CHECK-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// CHECK-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// CHECK-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[CLASS_S2]], ptr [[ARRAYCTOR_CUR]], i64 1
// CHECK-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// CHECK-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// CHECK:       arrayctor.cont:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"(ptr [[O]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.UDR:ARRSECT.TYPED"(ptr [[O]], [[CLASS_S2]] zeroinitializer, i64 1, i64 0, ptr @_ZTS2S2.omp.def_constr, ptr @_ZTS2S2.omp.destr, ptr @.omp_combiner..1, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 10
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP3]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x [10 x [10 x double]]], ptr [[B]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[ARRAYIDX1]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYDECAY]], i64 2
// CHECK-NEXT:    [[ARRAYDECAY3:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX2]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY3]], i64 1
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"(ptr [[B]], ptr [[B]], i64 8000, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I11]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i64 0, i32 1) ]
// CHECK-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 9, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x [10 x double]]], ptr [[B]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYDECAY6:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[ARRAYIDX5]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYDECAY6]], i64 2
// CHECK-NEXT:    [[ARRAYDECAY8:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX7]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY8]], i64 1
// CHECK-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TASK.TYPED"(ptr [[B]], double 0.000000e+00, i64 80, i64 21), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I11]], i64 0, i32 1) ]
// CHECK-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[CMP10:%.*]] = icmp sle i64 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP9:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP9]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 0, [[MUL]]
// CHECK-NEXT:    store i64 [[ADD]], ptr [[I11]], align 8
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[ADD12:%.*]] = add nsw i64 [[TMP10]], 1
// CHECK-NEXT:    store i64 [[ADD12]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// CHECK-NEXT:    call void @_ZN2S23mooEv(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX13]])
// CHECK-NEXT:    ret i32 0
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S2C1Ev
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2:[0-9]+]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    call void @_ZN2S2C2Ev(ptr noundef nonnull align 4 dereferenceable(4) [[THIS1]])
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@.omp_combiner.
// CHECK-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZTS2S2.omp.def_constr
// CHECK-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[TMP1]])
// CHECK-NEXT:    ret ptr [[TMP1]]
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZTS2S2.omp.destr
// CHECK-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@.omp_combiner..1
// CHECK-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S23mooEv
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) #[[ATTR0]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[A2:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[A2]], ptr [[A]], align 8
// CHECK-NEXT:    [[A3:%.*]] = getelementptr inbounds [[CLASS_S2]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"(ptr [[A3]], ptr [[A3]], i64 4, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[A3]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A3]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 10
// CHECK-NEXT:    store i32 [[ADD]], ptr [[A3]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S2C2Ev
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    store i32 0, ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_Z3sumPiiS_
// CHECK-SAME: (ptr noundef [[INPUT:%.*]], i32 noundef [[SIZE:%.*]], ptr noundef [[OUTPUT:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[INPUT_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[OUTPUT_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[OUTPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[INPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[OUTPUT_MAP_PTR_TMP17:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[INPUT_MAP_PTR_TMP18:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[_TMP20:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV28:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB29:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB30:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I35:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca [10 x i32], align 16
// CHECK-NEXT:    [[I49:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I52:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store ptr [[INPUT]], ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store ptr [[OUTPUT]], ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i32, ptr [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = sext i32 [[TMP7]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[CONV]], 4
// CHECK-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 6), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP3]], ptr [[ARRAYIDX1]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP5]], ptr [[ARRAYIDX2]], i64 [[TMP8]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_1]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// CHECK-NEXT:    store ptr [[TMP3]], ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    store ptr [[TMP5]], ptr [[INPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds i32, ptr [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], i32 0, i64 1, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_1]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP12]], ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP13]], 0
// CHECK-NEXT:    [[SUB4:%.*]] = sub nsw i32 [[SUB]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[SUB4]], 1
// CHECK-NEXT:    [[DIV:%.*]] = sdiv i32 [[ADD]], 1
// CHECK-NEXT:    [[SUB5:%.*]] = sub nsw i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB5]], ptr [[DOTCAPTURE_EXPR_1]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 0, [[TMP14]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_1]], align 4
// CHECK-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds i32, ptr [[TMP16]], i64 0
// CHECK-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], i32 0, i64 1, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP18]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP7:%.*]] = icmp sle i32 [[TMP19]], [[TMP20]]
// CHECK-NEXT:    br i1 [[CMP7]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP21]], 1
// CHECK-NEXT:    [[ADD8:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD8]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP23]] to i64
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds i32, ptr [[TMP22]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[ARRAYIDX9]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds i32, ptr [[TMP25]], i64 0
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[ARRAYIDX10]], align 4
// CHECK-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP26]], [[TMP24]]
// CHECK-NEXT:    store i32 [[ADD11]], ptr [[ARRAYIDX10]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD12:%.*]] = add nsw i32 [[TMP27]], 1
// CHECK-NEXT:    store i32 [[ADD12]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.TEAMS"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds i32, ptr [[TMP28]], i64 0
// CHECK-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP32:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds i32, ptr [[TMP32]], i64 0
// CHECK-NEXT:    [[TMP33:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX15:%.*]] = getelementptr inbounds i32, ptr [[TMP34]], i64 0
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CONV16:%.*]] = sext i32 [[TMP35]] to i64
// CHECK-NEXT:    [[TMP36:%.*]] = mul nuw i64 [[CONV16]], 4
// CHECK-NEXT:    [[TMP37:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 7), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP31]], ptr [[ARRAYIDX14]], i64 12, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP33]], ptr [[ARRAYIDX15]], i64 [[TMP36]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV28]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB30]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[_TMP20]], i32 0, i32 1) ]
// CHECK-NEXT:    store ptr [[TMP31]], ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// CHECK-NEXT:    store ptr [[TMP33]], ptr [[INPUT_MAP_PTR_TMP18]], align 8
// CHECK-NEXT:    [[TMP38:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// CHECK-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds i32, ptr [[TMP38]], i64 0
// CHECK-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], i32 0, i64 3, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV28]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB30]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[_TMP20]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP40]], ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[SUB21:%.*]] = sub nsw i32 [[TMP41]], 0
// CHECK-NEXT:    [[SUB22:%.*]] = sub nsw i32 [[SUB21]], 1
// CHECK-NEXT:    [[ADD23:%.*]] = add nsw i32 [[SUB22]], 1
// CHECK-NEXT:    [[DIV24:%.*]] = sdiv i32 [[ADD23]], 1
// CHECK-NEXT:    [[SUB25:%.*]] = sub nsw i32 [[DIV24]], 1
// CHECK-NEXT:    store i32 [[SUB25]], ptr [[DOTCAPTURE_EXPR_3]], align 4
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[CMP26:%.*]] = icmp slt i32 0, [[TMP42]]
// CHECK-NEXT:    br i1 [[CMP26]], label [[OMP_PRECOND_THEN27:%.*]], label [[OMP_PRECOND_END47:%.*]]
// CHECK:       omp.precond.then27:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB29]], align 4
// CHECK-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_3]], align 4
// CHECK-NEXT:    store i32 [[TMP43]], ptr [[DOTOMP_UB30]], align 4
// CHECK-NEXT:    [[TMP44:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// CHECK-NEXT:    [[ARRAYIDX31:%.*]] = getelementptr inbounds i32, ptr [[TMP44]], i64 0
// CHECK-NEXT:    [[TMP45:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], i32 0, i64 3, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV28]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB30]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[DOTOMP_LB29]], align 4
// CHECK-NEXT:    store i32 [[TMP46]], ptr [[DOTOMP_IV28]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND32:%.*]]
// CHECK:       omp.inner.for.cond32:
// CHECK-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// CHECK-NEXT:    [[TMP48:%.*]] = load i32, ptr [[DOTOMP_UB30]], align 4
// CHECK-NEXT:    [[CMP33:%.*]] = icmp sle i32 [[TMP47]], [[TMP48]]
// CHECK-NEXT:    br i1 [[CMP33]], label [[OMP_INNER_FOR_BODY34:%.*]], label [[OMP_INNER_FOR_END45:%.*]]
// CHECK:       omp.inner.for.body34:
// CHECK-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// CHECK-NEXT:    [[MUL36:%.*]] = mul nsw i32 [[TMP49]], 1
// CHECK-NEXT:    [[ADD37:%.*]] = add nsw i32 0, [[MUL36]]
// CHECK-NEXT:    store i32 [[ADD37]], ptr [[I35]], align 4
// CHECK-NEXT:    [[TMP50:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP18]], align 8
// CHECK-NEXT:    [[TMP51:%.*]] = load i32, ptr [[I35]], align 4
// CHECK-NEXT:    [[IDXPROM38:%.*]] = sext i32 [[TMP51]] to i64
// CHECK-NEXT:    [[ARRAYIDX39:%.*]] = getelementptr inbounds i32, ptr [[TMP50]], i64 [[IDXPROM38]]
// CHECK-NEXT:    [[TMP52:%.*]] = load i32, ptr [[ARRAYIDX39]], align 4
// CHECK-NEXT:    [[TMP53:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// CHECK-NEXT:    [[ARRAYIDX40:%.*]] = getelementptr inbounds i32, ptr [[TMP53]], i64 0
// CHECK-NEXT:    [[TMP54:%.*]] = load i32, ptr [[ARRAYIDX40]], align 4
// CHECK-NEXT:    [[ADD41:%.*]] = add nsw i32 [[TMP54]], [[TMP52]]
// CHECK-NEXT:    store i32 [[ADD41]], ptr [[ARRAYIDX40]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE42:%.*]]
// CHECK:       omp.body.continue42:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC43:%.*]]
// CHECK:       omp.inner.for.inc43:
// CHECK-NEXT:    [[TMP55:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// CHECK-NEXT:    [[ADD44:%.*]] = add nsw i32 [[TMP55]], 1
// CHECK-NEXT:    store i32 [[ADD44]], ptr [[DOTOMP_IV28]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND32]]
// CHECK:       omp.inner.for.end45:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT46:%.*]]
// CHECK:       omp.loop.exit46:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP45]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END47]]
// CHECK:       omp.precond.end47:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.TEAMS"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP37]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX48:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 0
// CHECK-NEXT:    [[TMP56:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 8), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX48]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I49]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP57:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TYPED"(ptr [[A]], i32 0, i64 2, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I49]], i32 0, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[I49]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP58:%.*]] = load i32, ptr [[I49]], align 4
// CHECK-NEXT:    [[TMP59:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CMP50:%.*]] = icmp slt i32 [[TMP58]], [[TMP59]]
// CHECK-NEXT:    br i1 [[CMP50]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP60:%.*]] = load i32, ptr [[I49]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP60]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I49]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP15:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP57]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP56]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX51:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 3
// CHECK-NEXT:    [[TMP61:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 9), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX51]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I52]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP62:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TYPED"(ptr [[A]], i32 0, i64 1, i64 3), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I52]], i32 0, i32 1) ]
// CHECK-NEXT:    store i32 0, ptr [[I52]], align 4
// CHECK-NEXT:    br label [[FOR_COND53:%.*]]
// CHECK:       for.cond53:
// CHECK-NEXT:    [[TMP63:%.*]] = load i32, ptr [[I52]], align 4
// CHECK-NEXT:    [[TMP64:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CMP54:%.*]] = icmp slt i32 [[TMP63]], [[TMP64]]
// CHECK-NEXT:    br i1 [[CMP54]], label [[FOR_BODY55:%.*]], label [[FOR_END58:%.*]]
// CHECK:       for.body55:
// CHECK-NEXT:    br label [[FOR_INC56:%.*]]
// CHECK:       for.inc56:
// CHECK-NEXT:    [[TMP65:%.*]] = load i32, ptr [[I52]], align 4
// CHECK-NEXT:    [[INC57:%.*]] = add nsw i32 [[TMP65]], 1
// CHECK-NEXT:    store i32 [[INC57]], ptr [[I52]], align 4
// CHECK-NEXT:    br label [[FOR_COND53]], !llvm.loop [[LOOP16:![0-9]+]]
// CHECK:       for.end58:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP62]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP61]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@main
// CHECK-SAME: () #[[ATTR6:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ARRAY:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[RESULT:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// CHECK-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooi(i32 noundef 10)
// CHECK-NEXT:    store i32 [[CALL]], ptr [[A]], align 4
// CHECK-NEXT:    [[CALL1:%.*]] = call noundef i32 @_Z3barv()
// CHECK-NEXT:    store i32 [[CALL1]], ptr [[A]], align 4
// CHECK-NEXT:    store i32 100, ptr [[SIZE]], align 4
// CHECK-NEXT:    [[CALL2:%.*]] = call noalias noundef nonnull ptr @_Znam(i64 noundef 400) #[[ATTR8:[0-9]+]]
// CHECK-NEXT:    store ptr [[CALL2]], ptr [[ARRAY]], align 8
// CHECK-NEXT:    store i32 0, ptr [[RESULT]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARRAY]], align 8
// CHECK-NEXT:    call void @_Z3sumPiiS_(ptr noundef [[TMP0]], i32 noundef 100, ptr noundef [[RESULT]])
// CHECK-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3fooi
// ROT1-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[E:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[SUM:%.*]] = alloca double, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP6:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP8:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr [[TMP0]], i64 0
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds double, ptr [[TMP3]], i64 0
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP2]], ptr [[ARRAYIDX1]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP]], ptr null, i32 1) ]
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[E_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds double, ptr [[TMP5]], i64 0
// ROT1-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[E_MAP_PTR_TMP]], double 0.000000e+00, i64 1, i64 0) ]
// ROT1-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    store double 1.000000e+01, ptr [[TMP7]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds double, ptr [[TMP8]], i64 0
// ROT1-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds double, ptr [[TMP11]], i64 0
// ROT1-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds double, ptr [[TMP13]], i64 0
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP10]], ptr [[ARRAYIDX4]], i64 8, i64 35, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:CHAIN"(ptr [[TMP12]], ptr [[ARRAYIDX5]], i64 8, i64 515, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP6]], ptr null, i32 1) ]
// ROT1-NEXT:    store ptr [[TMP10]], ptr [[E_MAP_PTR_TMP6]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP6]], align 8
// ROT1-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds double, ptr [[TMP15]], i64 0
// ROT1-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[E_MAP_PTR_TMP6]], double 0.000000e+00, i64 1, i64 0) ]
// ROT1-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP6]], align 8
// ROT1-NEXT:    store double 1.000000e+01, ptr [[TMP17]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"(ptr [[SUM]], ptr [[SUM]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP18]], ptr [[TMP18]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[E_MAP_PTR_TMP8]], ptr null, i32 1) ]
// ROT1-NEXT:    store ptr [[TMP18]], ptr [[E_MAP_PTR_TMP8]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[SUM]], double 0.000000e+00, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[E_MAP_PTR_TMP8]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP8]], align 8
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds double, ptr [[TMP21]], i64 0
// ROT1-NEXT:    [[TMP22:%.*]] = load double, ptr [[ARRAYIDX9]], align 8
// ROT1-NEXT:    [[TMP23:%.*]] = load double, ptr [[SUM]], align 8
// ROT1-NEXT:    [[ADD:%.*]] = fadd double [[TMP23]], [[TMP22]]
// ROT1-NEXT:    store double [[ADD]], ptr [[SUM]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3barv
// ROT1-SAME: () #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[O:%.*]] = alloca [5 x %class.S2], align 16
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[B:%.*]] = alloca [10 x [10 x [10 x double]]], align 16
// ROT1-NEXT:    [[TMP:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[I11:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i32 0, i32 0
// ROT1-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[ARRAY_BEGIN]], i64 5
// ROT1-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// ROT1:       arrayctor.loop:
// ROT1-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// ROT1-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// ROT1-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[CLASS_S2]], ptr [[ARRAYCTOR_CUR]], i64 1
// ROT1-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// ROT1-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// ROT1:       arrayctor.cont:
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"(ptr [[O]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.UDR:ARRSECT.TYPED"(ptr [[O]], [[CLASS_S2]] zeroinitializer, i64 1, i64 0, ptr @_ZTS2S2.omp.def_constr, ptr @_ZTS2S2.omp.destr, ptr @.omp_combiner..1, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 10
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP3]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// ROT1:       for.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x [10 x [10 x double]]], ptr [[B]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[ARRAYIDX1]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYDECAY]], i64 2
// ROT1-NEXT:    [[ARRAYDECAY3:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX2]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY3]], i64 1
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"(ptr [[B]], ptr [[B]], i64 8000, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I11]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i64 0, i32 1) ]
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 9, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x [10 x double]]], ptr [[B]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYDECAY6:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[ARRAYIDX5]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYDECAY6]], i64 2
// ROT1-NEXT:    [[ARRAYDECAY8:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX7]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY8]], i64 1
// ROT1-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TASK.TYPED"(ptr [[B]], double 0.000000e+00, i64 80, i64 21), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I11]], i64 0, i32 1) ]
// ROT1-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT1:       omp.inner.for.cond:
// ROT1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP10:%.*]] = icmp sle i64 [[TMP7]], [[TMP8]]
// ROT1-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP9:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP9]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 0, [[MUL]]
// ROT1-NEXT:    store i64 [[ADD]], ptr [[I11]], align 8
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD12:%.*]] = add nsw i64 [[TMP10]], 1
// ROT1-NEXT:    store i64 [[ADD12]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// ROT1-NEXT:    call void @_ZN2S23mooEv(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX13]])
// ROT1-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S2C1Ev
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2:[0-9]+]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    call void @_ZN2S2C2Ev(ptr noundef nonnull align 4 dereferenceable(4) [[THIS1]])
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@.omp_combiner.
// ROT1-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// ROT1-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZTS2S2.omp.def_constr
// ROT1-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[TMP1]])
// ROT1-NEXT:    ret ptr [[TMP1]]
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZTS2S2.omp.destr
// ROT1-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@.omp_combiner..1
// ROT1-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// ROT1-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S23mooEv
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) #[[ATTR0]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[A:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[A2:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    store ptr [[A2]], ptr [[A]], align 8
// ROT1-NEXT:    [[A3:%.*]] = getelementptr inbounds [[CLASS_S2]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"(ptr [[A3]], ptr [[A3]], i64 4, i64 547, ptr null, ptr null) ]
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:TYPED"(ptr [[A3]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A3]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 10
// ROT1-NEXT:    store i32 [[ADD]], ptr [[A3]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S2C2Ev
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    store i32 0, ptr [[A]], align 4
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3sumPiiS_
// ROT1-SAME: (ptr noundef [[INPUT:%.*]], i32 noundef [[SIZE:%.*]], ptr noundef [[OUTPUT:%.*]]) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[INPUT_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[OUTPUT_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[OUTPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[INPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[OUTPUT_MAP_PTR_TMP17:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[INPUT_MAP_PTR_TMP18:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[_TMP20:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV28:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB29:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB30:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I35:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca [10 x i32], align 16
// ROT1-NEXT:    [[I49:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I52:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store ptr [[INPUT]], ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store ptr [[OUTPUT]], ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[TMP0]], i64 0
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP4]], i64 0
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i32, ptr [[TMP6]], i64 0
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CONV:%.*]] = sext i32 [[TMP7]] to i64
// ROT1-NEXT:    [[TMP8:%.*]] = mul nuw i64 [[CONV]], 4
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 6), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP3]], ptr [[ARRAYIDX1]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP5]], ptr [[ARRAYIDX2]], i64 [[TMP8]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_1]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// ROT1-NEXT:    store ptr [[TMP3]], ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    store ptr [[TMP5]], ptr [[INPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds i32, ptr [[TMP10]], i64 0
// ROT1-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], i32 0, i64 1, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_1]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP12]], ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP13]], 0
// ROT1-NEXT:    [[SUB4:%.*]] = sub nsw i32 [[SUB]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[SUB4]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i32 [[ADD]], 1
// ROT1-NEXT:    [[SUB5:%.*]] = sub nsw i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB5]], ptr [[DOTCAPTURE_EXPR_1]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 0, [[TMP14]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_1]], align 4
// ROT1-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds i32, ptr [[TMP16]], i64 0
// ROT1-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP]], i32 0, i64 1, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP18]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT1:       omp.inner.for.cond:
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP7:%.*]] = icmp sle i32 [[TMP19]], [[TMP20]]
// ROT1-NEXT:    br i1 [[CMP7]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP21]], 1
// ROT1-NEXT:    [[ADD8:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD8]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP23]] to i64
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds i32, ptr [[TMP22]], i64 [[IDXPROM]]
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[ARRAYIDX9]], align 4
// ROT1-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds i32, ptr [[TMP25]], i64 0
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[ARRAYIDX10]], align 4
// ROT1-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP26]], [[TMP24]]
// ROT1-NEXT:    store i32 [[ADD11]], ptr [[ARRAYIDX10]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD12:%.*]] = add nsw i32 [[TMP27]], 1
// ROT1-NEXT:    store i32 [[ADD12]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.TEAMS"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds i32, ptr [[TMP28]], i64 0
// ROT1-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP31:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP32:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds i32, ptr [[TMP32]], i64 0
// ROT1-NEXT:    [[TMP33:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX15:%.*]] = getelementptr inbounds i32, ptr [[TMP34]], i64 0
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CONV16:%.*]] = sext i32 [[TMP35]] to i64
// ROT1-NEXT:    [[TMP36:%.*]] = mul nuw i64 [[CONV16]], 4
// ROT1-NEXT:    [[TMP37:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 7), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP31]], ptr [[ARRAYIDX14]], i64 12, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP33]], ptr [[ARRAYIDX15]], i64 [[TMP36]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV28]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB30]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[_TMP20]], i32 0, i32 1) ]
// ROT1-NEXT:    store ptr [[TMP31]], ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// ROT1-NEXT:    store ptr [[TMP33]], ptr [[INPUT_MAP_PTR_TMP18]], align 8
// ROT1-NEXT:    [[TMP38:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// ROT1-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds i32, ptr [[TMP38]], i64 0
// ROT1-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], i32 0, i64 3, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_3]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_IV28]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTOMP_UB30]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_2]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[_TMP20]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP40]], ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[SUB21:%.*]] = sub nsw i32 [[TMP41]], 0
// ROT1-NEXT:    [[SUB22:%.*]] = sub nsw i32 [[SUB21]], 1
// ROT1-NEXT:    [[ADD23:%.*]] = add nsw i32 [[SUB22]], 1
// ROT1-NEXT:    [[DIV24:%.*]] = sdiv i32 [[ADD23]], 1
// ROT1-NEXT:    [[SUB25:%.*]] = sub nsw i32 [[DIV24]], 1
// ROT1-NEXT:    store i32 [[SUB25]], ptr [[DOTCAPTURE_EXPR_3]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[CMP26:%.*]] = icmp slt i32 0, [[TMP42]]
// ROT1-NEXT:    br i1 [[CMP26]], label [[OMP_PRECOND_THEN27:%.*]], label [[OMP_PRECOND_END47:%.*]]
// ROT1:       omp.precond.then27:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB29]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_3]], align 4
// ROT1-NEXT:    store i32 [[TMP43]], ptr [[DOTOMP_UB30]], align 4
// ROT1-NEXT:    [[TMP44:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// ROT1-NEXT:    [[ARRAYIDX31:%.*]] = getelementptr inbounds i32, ptr [[TMP44]], i64 0
// ROT1-NEXT:    [[TMP45:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.PTR_TO_PTR.TYPED"(ptr [[OUTPUT_MAP_PTR_TMP17]], i32 0, i64 3, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[INPUT_MAP_PTR_TMP18]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV28]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB29]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB30]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I35]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP46:%.*]] = load i32, ptr [[DOTOMP_LB29]], align 4
// ROT1-NEXT:    store i32 [[TMP46]], ptr [[DOTOMP_IV28]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND32:%.*]]
// ROT1:       omp.inner.for.cond32:
// ROT1-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// ROT1-NEXT:    [[TMP48:%.*]] = load i32, ptr [[DOTOMP_UB30]], align 4
// ROT1-NEXT:    [[CMP33:%.*]] = icmp sle i32 [[TMP47]], [[TMP48]]
// ROT1-NEXT:    br i1 [[CMP33]], label [[OMP_INNER_FOR_BODY34:%.*]], label [[OMP_INNER_FOR_END45:%.*]]
// ROT1:       omp.inner.for.body34:
// ROT1-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// ROT1-NEXT:    [[MUL36:%.*]] = mul nsw i32 [[TMP49]], 1
// ROT1-NEXT:    [[ADD37:%.*]] = add nsw i32 0, [[MUL36]]
// ROT1-NEXT:    store i32 [[ADD37]], ptr [[I35]], align 4
// ROT1-NEXT:    [[TMP50:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP18]], align 8
// ROT1-NEXT:    [[TMP51:%.*]] = load i32, ptr [[I35]], align 4
// ROT1-NEXT:    [[IDXPROM38:%.*]] = sext i32 [[TMP51]] to i64
// ROT1-NEXT:    [[ARRAYIDX39:%.*]] = getelementptr inbounds i32, ptr [[TMP50]], i64 [[IDXPROM38]]
// ROT1-NEXT:    [[TMP52:%.*]] = load i32, ptr [[ARRAYIDX39]], align 4
// ROT1-NEXT:    [[TMP53:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP17]], align 8
// ROT1-NEXT:    [[ARRAYIDX40:%.*]] = getelementptr inbounds i32, ptr [[TMP53]], i64 0
// ROT1-NEXT:    [[TMP54:%.*]] = load i32, ptr [[ARRAYIDX40]], align 4
// ROT1-NEXT:    [[ADD41:%.*]] = add nsw i32 [[TMP54]], [[TMP52]]
// ROT1-NEXT:    store i32 [[ADD41]], ptr [[ARRAYIDX40]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE42:%.*]]
// ROT1:       omp.body.continue42:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC43:%.*]]
// ROT1:       omp.inner.for.inc43:
// ROT1-NEXT:    [[TMP55:%.*]] = load i32, ptr [[DOTOMP_IV28]], align 4
// ROT1-NEXT:    [[ADD44:%.*]] = add nsw i32 [[TMP55]], 1
// ROT1-NEXT:    store i32 [[ADD44]], ptr [[DOTOMP_IV28]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND32]]
// ROT1:       omp.inner.for.end45:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT46:%.*]]
// ROT1:       omp.loop.exit46:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP45]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END47]]
// ROT1:       omp.precond.end47:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.TEAMS"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP37]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX48:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 0
// ROT1-NEXT:    [[TMP56:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 8), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX48]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I49]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP57:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TYPED"(ptr [[A]], i32 0, i64 2, i64 0), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I49]], i32 0, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[I49]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP58:%.*]] = load i32, ptr [[I49]], align 4
// ROT1-NEXT:    [[TMP59:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CMP50:%.*]] = icmp slt i32 [[TMP58]], [[TMP59]]
// ROT1-NEXT:    br i1 [[CMP50]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP60:%.*]] = load i32, ptr [[I49]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP60]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[I49]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP15:![0-9]+]]
// ROT1:       for.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP57]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP56]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX51:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 3
// ROT1-NEXT:    [[TMP61:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 9), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX51]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I52]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP62:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TYPED"(ptr [[A]], i32 0, i64 1, i64 3), "QUAL.OMP.SHARED:TYPED"(ptr [[SIZE_ADDR]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I52]], i32 0, i32 1) ]
// ROT1-NEXT:    store i32 0, ptr [[I52]], align 4
// ROT1-NEXT:    br label [[FOR_COND53:%.*]]
// ROT1:       for.cond53:
// ROT1-NEXT:    [[TMP63:%.*]] = load i32, ptr [[I52]], align 4
// ROT1-NEXT:    [[TMP64:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CMP54:%.*]] = icmp slt i32 [[TMP63]], [[TMP64]]
// ROT1-NEXT:    br i1 [[CMP54]], label [[FOR_BODY55:%.*]], label [[FOR_END58:%.*]]
// ROT1:       for.body55:
// ROT1-NEXT:    br label [[FOR_INC56:%.*]]
// ROT1:       for.inc56:
// ROT1-NEXT:    [[TMP65:%.*]] = load i32, ptr [[I52]], align 4
// ROT1-NEXT:    [[INC57:%.*]] = add nsw i32 [[TMP65]], 1
// ROT1-NEXT:    store i32 [[INC57]], ptr [[I52]], align 4
// ROT1-NEXT:    br label [[FOR_COND53]], !llvm.loop [[LOOP16:![0-9]+]]
// ROT1:       for.end58:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP62]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP61]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@main
// ROT1-SAME: () #[[ATTR6:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[ARRAY:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[RESULT:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// ROT1-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooi(i32 noundef 10)
// ROT1-NEXT:    store i32 [[CALL]], ptr [[A]], align 4
// ROT1-NEXT:    [[CALL1:%.*]] = call noundef i32 @_Z3barv()
// ROT1-NEXT:    store i32 [[CALL1]], ptr [[A]], align 4
// ROT1-NEXT:    store i32 100, ptr [[SIZE]], align 4
// ROT1-NEXT:    [[CALL2:%.*]] = call noalias noundef nonnull ptr @_Znam(i64 noundef 400) #[[ATTR8:[0-9]+]]
// ROT1-NEXT:    store ptr [[CALL2]], ptr [[ARRAY]], align 8
// ROT1-NEXT:    store i32 0, ptr [[RESULT]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARRAY]], align 8
// ROT1-NEXT:    call void @_Z3sumPiiS_(ptr noundef [[TMP0]], i32 noundef 100, ptr noundef [[RESULT]])
// ROT1-NEXT:    ret i32 0
//
// end  INTEL_COLLAB
