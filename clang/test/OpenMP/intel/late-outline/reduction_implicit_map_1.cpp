// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --function-signature --include-generated-funcs --replace-value-regex "__omp_offloading_[0-9a-z]+_[0-9a-z]+" "reduction_size[.].+[.]" "pl_cond[.].+[.|,]" --prefix-filecheck-ir-name _

// RUN: %clang_cc1 -verify -fopenmp -x c++ \
// RUN:   -fno-openmp-use-host-usm-for-implicit-reduction-map  \
// RUN:   -fopenmp-late-outline -emit-llvm \
// RUN:   -fopenmp-loop-rotation-control=0 -triple x86_64-pc-linux-gnu \
// RUN:   %s -o - | FileCheck %s

// RUN: %clang_cc1 -verify -fopenmp -x c++ \
// RUN:   -fno-openmp-use-host-usm-for-implicit-reduction-map  \
// RUN:   -fopenmp-late-outline -emit-llvm \
// RUN:   -fopenmp-loop-rotation-control=0 -triple x86_64-pc-linux-gnu \
// RUN:   %s -o - | FileCheck %s --check-prefix ROT1

// Test map type with OMP_TGT_MAPTYPE_HOST_MEM 0x8000

int foo(int n) {
  double *e;
  double sum;
  #pragma omp target parallel reduction(+: e[:1])
    *e=10;
  #pragma omp target parallel map(e[:1]) reduction(+: e[:1])
    *e=10;
  #pragma omp target parallel reduction(+: sum)
    sum += e[0];
  return 0;
}
class S2 {
  mutable int a;
public:
  S2():a(0) { }
  S2(S2 &s2):a(s2.a) { }
  S2 &operator +(S2 &s);
  void moo() {
    #pragma omp target parallel reduction(+:a)
    a += 10;
  }
};
int bar() {
 S2 o[5];
#pragma omp target parallel reduction(+:o[0]) // expected-warning {{Type 'S2' is not trivially copyable and not guaranteed to be mapped correctly}}
  for (int i = 0; i < 10; i++);
  double b[10][10][10];
#pragma omp target parallel for reduction(task, +: b[0:2][2:4][1])
  for (long long i = 0; i < 10; ++i);
  o[0].moo();
  return 0;
}
void sum(int* input, int size, int* output)
{
#pragma omp target teams distribute parallel for reduction(+: output[0]) \
                                                 map(to: input [0:size])
  for (int i = 0; i < size; i++)
    output[0] += input[i];
#pragma omp target teams distribute parallel for reduction(+: output[:3])  \
                                                 map(to: input [0:size])
  for (int i = 0; i < size; i++)
    output[0] += input[i];
  int a[10];
#pragma omp target parallel reduction(+: a[:2])
  for (int i = 0; i < size; i++)
    ;
#pragma omp target parallel reduction(+: a[3])
  for (int i = 0; i < size; i++)
    ;
}
int main()
{
  int a = foo(10);
  a = bar();
  const int size = 100;
  int *array = new int[size];
  int result = 0;
  sum(array, size, &result);
  return 0;
}

// CHECK-LABEL: define {{[^@]+}}@_Z3fooi
// CHECK-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[E:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SUM:%.*]] = alloca double, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP3:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[E_MAP_PTR_TMP4:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP1]], ptr [[ARRAYIDX]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP]]) ]
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[E_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[E_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1) ]
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    store double 1.000000e+01, ptr [[TMP5]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds double, ptr [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds double, ptr [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP7]], ptr [[ARRAYIDX1]], i64 8, i64 35, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:CHAIN"(ptr [[TMP9]], ptr [[ARRAYIDX2]], i64 8, i64 515, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP3]]) ]
// CHECK-NEXT:    store ptr [[TMP7]], ptr [[E_MAP_PTR_TMP3]], align 8
// CHECK-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[E_MAP_PTR_TMP3]], i64 1, i64 0, i64 1, i64 1) ]
// CHECK-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP3]], align 8
// CHECK-NEXT:    store double 1.000000e+01, ptr [[TMP13]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[E]], align 8
// CHECK-NEXT:    [[TMP15:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"(ptr [[SUM]], ptr [[SUM]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP14]], ptr [[TMP14]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP4]]) ]
// CHECK-NEXT:    store ptr [[TMP14]], ptr [[E_MAP_PTR_TMP4]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD"(ptr [[SUM]]), "QUAL.OMP.SHARED"(ptr [[E_MAP_PTR_TMP4]]) ]
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP4]], align 8
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds double, ptr [[TMP17]], i64 0
// CHECK-NEXT:    [[TMP18:%.*]] = load double, ptr [[ARRAYIDX5]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = load double, ptr [[SUM]], align 8
// CHECK-NEXT:    [[ADD:%.*]] = fadd double [[TMP19]], [[TMP18]]
// CHECK-NEXT:    store double [[ADD]], ptr [[SUM]], align 8
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP15]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret i32 0
//
//
// CHECK-LABEL: define {{[^@]+}}@_Z3barv
// CHECK-SAME: () #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[O:%.*]] = alloca [5 x %class.S2], align 16
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B:%.*]] = alloca [10 x [10 x [10 x double]]], align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[I2:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[ARRAY_BEGIN]], i64 5
// CHECK-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// CHECK:       arrayctor.loop:
// CHECK-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// CHECK-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// CHECK-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[CLASS_S2]], ptr [[ARRAYCTOR_CUR]], i64 1
// CHECK-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// CHECK-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// CHECK:       arrayctor.cont:
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"(ptr [[O]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.UDR:ARRSECT"(ptr [[O]], i64 1, i64 0, i64 1, i64 1, ptr @_ZTS2S2.omp.def_constr, ptr @_ZTS2S2.omp.destr, ptr @.omp_combiner..1, ptr null), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// CHECK-NEXT:    store i32 0, ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 10
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP3]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"(ptr [[B]], ptr [[B]], i64 8000, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I2]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// CHECK-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 9, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TASK"(ptr [[B]], i64 3, i64 0, i64 2, i64 1, i64 2, i64 4, i64 1, i64 1, i64 9, i64 1), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I2]]) ]
// CHECK-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[CMP1:%.*]] = icmp sle i64 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP9:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP9]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 0, [[MUL]]
// CHECK-NEXT:    store i64 [[ADD]], ptr [[I2]], align 8
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[ADD3:%.*]] = add nsw i64 [[TMP10]], 1
// CHECK-NEXT:    store i64 [[ADD3]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// CHECK-NEXT:    call void @_ZN2S23mooEv(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX4]])
// CHECK-NEXT:    ret i32 0
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S2C1Ev
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2:[0-9]+]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    call void @_ZN2S2C2Ev(ptr noundef nonnull align 4 dereferenceable(4) [[THIS1]])
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@.omp_combiner.
// CHECK-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZTS2S2.omp.def_constr
// CHECK-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[TMP1]])
// CHECK-NEXT:    ret ptr [[TMP1]]
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZTS2S2.omp.destr
// CHECK-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@.omp_combiner..1
// CHECK-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// CHECK-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S23mooEv
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) #[[ATTR0]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[A2:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[A2]], ptr [[A]], align 8
// CHECK-NEXT:    [[A3:%.*]] = getelementptr inbounds [[CLASS_S2]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"(ptr [[A3]], ptr [[A3]], i64 4, i64 547, ptr null, ptr null) ]
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD"(ptr [[A3]]) ]
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A3]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 10
// CHECK-NEXT:    store i32 [[ADD]], ptr [[A3]], align 4
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_ZN2S2C2Ev
// CHECK-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2]] comdat align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// CHECK-NEXT:    store i32 0, ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@_Z3sumPiiS_
// CHECK-SAME: (ptr noundef [[INPUT:%.*]], i32 noundef [[SIZE:%.*]], ptr noundef [[OUTPUT:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[INPUT_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[OUTPUT_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[OUTPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[INPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[OUTPUT_MAP_PTR_TMP13:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[INPUT_MAP_PTR_TMP14:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[_TMP15:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV23:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB24:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB25:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I29:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca [10 x i32], align 16
// CHECK-NEXT:    [[I43:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I46:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store ptr [[INPUT]], ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store ptr [[OUTPUT]], ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = sext i32 [[TMP6]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = mul nuw i64 [[CONV]], 4
// CHECK-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 6), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP2]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP4]], ptr [[ARRAYIDX1]], i64 [[TMP7]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_1]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_0]]), "QUAL.OMP.PRIVATE"(ptr [[OUTPUT_MAP_PTR_TMP]]), "QUAL.OMP.PRIVATE"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// CHECK-NEXT:    store ptr [[TMP2]], ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    store ptr [[TMP4]], ptr [[INPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_1]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_0]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP10]], ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP11]], 0
// CHECK-NEXT:    [[SUB2:%.*]] = sub nsw i32 [[SUB]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[SUB2]], 1
// CHECK-NEXT:    [[DIV:%.*]] = sdiv i32 [[ADD]], 1
// CHECK-NEXT:    [[SUB3:%.*]] = sub nsw i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_1]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 0, [[TMP12]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_1]], align 4
// CHECK-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP4:%.*]] = icmp sle i32 [[TMP16]], [[TMP17]]
// CHECK-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP18]], 1
// CHECK-NEXT:    [[ADD5:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD5]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP20]] to i64
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds i32, ptr [[TMP19]], i64 [[IDXPROM]]
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[ARRAYIDX6]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds i32, ptr [[TMP22]], i64 0
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[ARRAYIDX7]], align 4
// CHECK-NEXT:    [[ADD8:%.*]] = add nsw i32 [[TMP23]], [[TMP21]]
// CHECK-NEXT:    store i32 [[ADD8]], ptr [[ARRAYIDX7]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add nsw i32 [[TMP24]], 1
// CHECK-NEXT:    store i32 [[ADD9]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.TEAMS"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds i32, ptr [[TMP28]], i64 0
// CHECK-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds i32, ptr [[TMP30]], i64 0
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CONV12:%.*]] = sext i32 [[TMP31]] to i64
// CHECK-NEXT:    [[TMP32:%.*]] = mul nuw i64 [[CONV12]], 4
// CHECK-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 7), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP27]], ptr [[ARRAYIDX10]], i64 12, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP29]], ptr [[ARRAYIDX11]], i64 [[TMP32]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_3]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_2]]), "QUAL.OMP.PRIVATE"(ptr [[OUTPUT_MAP_PTR_TMP13]]), "QUAL.OMP.PRIVATE"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.PRIVATE"(ptr [[_TMP15]]) ]
// CHECK-NEXT:    store ptr [[TMP27]], ptr [[OUTPUT_MAP_PTR_TMP13]], align 8
// CHECK-NEXT:    store ptr [[TMP29]], ptr [[INPUT_MAP_PTR_TMP14]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP13]], i64 1, i64 0, i64 3, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_3]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_2]]), "QUAL.OMP.PRIVATE"(ptr [[_TMP15]]) ]
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP35]], ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[SUB16:%.*]] = sub nsw i32 [[TMP36]], 0
// CHECK-NEXT:    [[SUB17:%.*]] = sub nsw i32 [[SUB16]], 1
// CHECK-NEXT:    [[ADD18:%.*]] = add nsw i32 [[SUB17]], 1
// CHECK-NEXT:    [[DIV19:%.*]] = sdiv i32 [[ADD18]], 1
// CHECK-NEXT:    [[SUB20:%.*]] = sub nsw i32 [[DIV19]], 1
// CHECK-NEXT:    store i32 [[SUB20]], ptr [[DOTCAPTURE_EXPR_3]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// CHECK-NEXT:    [[CMP21:%.*]] = icmp slt i32 0, [[TMP37]]
// CHECK-NEXT:    br i1 [[CMP21]], label [[OMP_PRECOND_THEN22:%.*]], label [[OMP_PRECOND_END41:%.*]]
// CHECK:       omp.precond.then22:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB24]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_3]], align 4
// CHECK-NEXT:    store i32 [[TMP38]], ptr [[DOTOMP_UB25]], align 4
// CHECK-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP13]], i64 1, i64 0, i64 3, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]) ]
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTOMP_LB24]], align 4
// CHECK-NEXT:    store i32 [[TMP40]], ptr [[DOTOMP_IV23]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND26:%.*]]
// CHECK:       omp.inner.for.cond26:
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_UB25]], align 4
// CHECK-NEXT:    [[CMP27:%.*]] = icmp sle i32 [[TMP41]], [[TMP42]]
// CHECK-NEXT:    br i1 [[CMP27]], label [[OMP_INNER_FOR_BODY28:%.*]], label [[OMP_INNER_FOR_END39:%.*]]
// CHECK:       omp.inner.for.body28:
// CHECK-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// CHECK-NEXT:    [[MUL30:%.*]] = mul nsw i32 [[TMP43]], 1
// CHECK-NEXT:    [[ADD31:%.*]] = add nsw i32 0, [[MUL30]]
// CHECK-NEXT:    store i32 [[ADD31]], ptr [[I29]], align 4
// CHECK-NEXT:    [[TMP44:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP14]], align 8
// CHECK-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I29]], align 4
// CHECK-NEXT:    [[IDXPROM32:%.*]] = sext i32 [[TMP45]] to i64
// CHECK-NEXT:    [[ARRAYIDX33:%.*]] = getelementptr inbounds i32, ptr [[TMP44]], i64 [[IDXPROM32]]
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[ARRAYIDX33]], align 4
// CHECK-NEXT:    [[TMP47:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP13]], align 8
// CHECK-NEXT:    [[ARRAYIDX34:%.*]] = getelementptr inbounds i32, ptr [[TMP47]], i64 0
// CHECK-NEXT:    [[TMP48:%.*]] = load i32, ptr [[ARRAYIDX34]], align 4
// CHECK-NEXT:    [[ADD35:%.*]] = add nsw i32 [[TMP48]], [[TMP46]]
// CHECK-NEXT:    store i32 [[ADD35]], ptr [[ARRAYIDX34]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE36:%.*]]
// CHECK:       omp.body.continue36:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC37:%.*]]
// CHECK:       omp.inner.for.inc37:
// CHECK-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// CHECK-NEXT:    [[ADD38:%.*]] = add nsw i32 [[TMP49]], 1
// CHECK-NEXT:    store i32 [[ADD38]], ptr [[DOTOMP_IV23]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND26]]
// CHECK:       omp.inner.for.end39:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT40:%.*]]
// CHECK:       omp.loop.exit40:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END41]]
// CHECK:       omp.precond.end41:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP34]]) [ "DIR.OMP.END.TEAMS"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX42:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 0
// CHECK-NEXT:    [[TMP50:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 8), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX42]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I43]]) ]
// CHECK-NEXT:    [[TMP51:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[A]], i64 1, i64 0, i64 2, i64 1), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I43]]) ]
// CHECK-NEXT:    store i32 0, ptr [[I43]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP52:%.*]] = load i32, ptr [[I43]], align 4
// CHECK-NEXT:    [[TMP53:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CMP44:%.*]] = icmp slt i32 [[TMP52]], [[TMP53]]
// CHECK-NEXT:    br i1 [[CMP44]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP54:%.*]] = load i32, ptr [[I43]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP54]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[I43]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP15:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP51]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP50]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    [[ARRAYIDX45:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 3
// CHECK-NEXT:    [[TMP55:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 9), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX45]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I46]]) ]
// CHECK-NEXT:    [[TMP56:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[A]], i64 1, i64 3, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I46]]) ]
// CHECK-NEXT:    store i32 0, ptr [[I46]], align 4
// CHECK-NEXT:    br label [[FOR_COND47:%.*]]
// CHECK:       for.cond47:
// CHECK-NEXT:    [[TMP57:%.*]] = load i32, ptr [[I46]], align 4
// CHECK-NEXT:    [[TMP58:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// CHECK-NEXT:    [[CMP48:%.*]] = icmp slt i32 [[TMP57]], [[TMP58]]
// CHECK-NEXT:    br i1 [[CMP48]], label [[FOR_BODY49:%.*]], label [[FOR_END52:%.*]]
// CHECK:       for.body49:
// CHECK-NEXT:    br label [[FOR_INC50:%.*]]
// CHECK:       for.inc50:
// CHECK-NEXT:    [[TMP59:%.*]] = load i32, ptr [[I46]], align 4
// CHECK-NEXT:    [[INC51:%.*]] = add nsw i32 [[TMP59]], 1
// CHECK-NEXT:    store i32 [[INC51]], ptr [[I46]], align 4
// CHECK-NEXT:    br label [[FOR_COND47]], !llvm.loop [[LOOP16:![0-9]+]]
// CHECK:       for.end52:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP56]]) [ "DIR.OMP.END.PARALLEL"() ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP55]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
//
// CHECK-LABEL: define {{[^@]+}}@main
// CHECK-SAME: () #[[ATTR6:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ARRAY:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[RESULT:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// CHECK-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooi(i32 noundef 10)
// CHECK-NEXT:    store i32 [[CALL]], ptr [[A]], align 4
// CHECK-NEXT:    [[CALL1:%.*]] = call noundef i32 @_Z3barv()
// CHECK-NEXT:    store i32 [[CALL1]], ptr [[A]], align 4
// CHECK-NEXT:    store i32 100, ptr [[SIZE]], align 4
// CHECK-NEXT:    [[CALL2:%.*]] = call noalias noundef nonnull ptr @_Znam(i64 noundef 400) #[[ATTR8:[0-9]+]]
// CHECK-NEXT:    store ptr [[CALL2]], ptr [[ARRAY]], align 8
// CHECK-NEXT:    store i32 0, ptr [[RESULT]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARRAY]], align 8
// CHECK-NEXT:    call void @_Z3sumPiiS_(ptr noundef [[TMP0]], i32 noundef 100, ptr noundef [[RESULT]])
// CHECK-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3fooi
// ROT1-SAME: (i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[E:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[SUM:%.*]] = alloca double, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP3:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[E_MAP_PTR_TMP4:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr [[TMP2]], i64 0
// ROT1-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP1]], ptr [[ARRAYIDX]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP]]) ]
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[E_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[E_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1) ]
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    store double 1.000000e+01, ptr [[TMP5]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds double, ptr [[TMP8]], i64 0
// ROT1-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds double, ptr [[TMP10]], i64 0
// ROT1-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP7]], ptr [[ARRAYIDX1]], i64 8, i64 35, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM:CHAIN"(ptr [[TMP9]], ptr [[ARRAYIDX2]], i64 8, i64 515, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP3]]) ]
// ROT1-NEXT:    store ptr [[TMP7]], ptr [[E_MAP_PTR_TMP3]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[E_MAP_PTR_TMP3]], i64 1, i64 0, i64 1, i64 1) ]
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP3]], align 8
// ROT1-NEXT:    store double 1.000000e+01, ptr [[TMP13]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP14:%.*]] = load ptr, ptr [[E]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"(ptr [[SUM]], ptr [[SUM]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP14]], ptr [[TMP14]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[E_MAP_PTR_TMP4]]) ]
// ROT1-NEXT:    store ptr [[TMP14]], ptr [[E_MAP_PTR_TMP4]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD"(ptr [[SUM]]), "QUAL.OMP.SHARED"(ptr [[E_MAP_PTR_TMP4]]) ]
// ROT1-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[E_MAP_PTR_TMP4]], align 8
// ROT1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds double, ptr [[TMP17]], i64 0
// ROT1-NEXT:    [[TMP18:%.*]] = load double, ptr [[ARRAYIDX5]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load double, ptr [[SUM]], align 8
// ROT1-NEXT:    [[ADD:%.*]] = fadd double [[TMP19]], [[TMP18]]
// ROT1-NEXT:    store double [[ADD]], ptr [[SUM]], align 8
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP16]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP15]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3barv
// ROT1-SAME: () #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[O:%.*]] = alloca [5 x %class.S2], align 16
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[B:%.*]] = alloca [10 x [10 x [10 x double]]], align 16
// ROT1-NEXT:    [[TMP:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[I2:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i32 0, i32 0
// ROT1-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[ARRAY_BEGIN]], i64 5
// ROT1-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// ROT1:       arrayctor.loop:
// ROT1-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi ptr [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// ROT1-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYCTOR_CUR]])
// ROT1-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[CLASS_S2]], ptr [[ARRAYCTOR_CUR]], i64 1
// ROT1-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq ptr [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// ROT1-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// ROT1:       arrayctor.cont:
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"(ptr [[O]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.UDR:ARRSECT"(ptr [[O]], i64 1, i64 0, i64 1, i64 1, ptr @_ZTS2S2.omp.def_constr, ptr @_ZTS2S2.omp.destr, ptr @.omp_combiner..1, ptr null), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// ROT1-NEXT:    store i32 0, ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 10
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP3]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// ROT1:       for.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"(ptr [[B]], ptr [[B]], i64 8000, i64 547, ptr null, ptr null), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I2]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 9, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT.TASK"(ptr [[B]], i64 3, i64 0, i64 2, i64 1, i64 2, i64 4, i64 1, i64 1, i64 9, i64 1), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I2]]) ]
// ROT1-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT1:       omp.inner.for.cond:
// ROT1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP1:%.*]] = icmp sle i64 [[TMP7]], [[TMP8]]
// ROT1-NEXT:    br i1 [[CMP1]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP9:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP9]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 0, [[MUL]]
// ROT1-NEXT:    store i64 [[ADD]], ptr [[I2]], align 8
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD3:%.*]] = add nsw i64 [[TMP10]], 1
// ROT1-NEXT:    store i64 [[ADD3]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP5]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [5 x %class.S2], ptr [[O]], i64 0, i64 0
// ROT1-NEXT:    call void @_ZN2S23mooEv(ptr noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX4]])
// ROT1-NEXT:    ret i32 0
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S2C1Ev
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2:[0-9]+]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    call void @_ZN2S2C2Ev(ptr noundef nonnull align 4 dereferenceable(4) [[THIS1]])
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@.omp_combiner.
// ROT1-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// ROT1-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZTS2S2.omp.def_constr
// ROT1-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    call void @_ZN2S2C1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[TMP1]])
// ROT1-NEXT:    ret ptr [[TMP1]]
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZTS2S2.omp.destr
// ROT1-SAME: (ptr noundef [[TMP0:%.*]]) #[[ATTR3]] section ".text.startup" {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@.omp_combiner..1
// ROT1-SAME: (ptr noalias noundef [[TMP0:%.*]], ptr noalias noundef [[TMP1:%.*]]) #[[ATTR3]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[DOTADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTADDR1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[TMP0]], ptr [[DOTADDR]], align 8
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTADDR1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[DOTADDR]], align 8
// ROT1-NEXT:    [[CALL:%.*]] = call noundef nonnull align 4 dereferenceable(4) ptr @_ZN2S2plERS_(ptr noundef nonnull align 4 dereferenceable(4) [[TMP3]], ptr noundef nonnull align 4 dereferenceable(4) [[TMP2]])
// ROT1-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 4 [[TMP3]], ptr align 4 [[CALL]], i64 4, i1 false)
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S23mooEv
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) #[[ATTR0]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[A:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[A2:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    store ptr [[A2]], ptr [[A]], align 8
// ROT1-NEXT:    [[A3:%.*]] = getelementptr inbounds [[CLASS_S2]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"(ptr [[A3]], ptr [[A3]], i64 4, i64 547, ptr null, ptr null) ]
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD"(ptr [[A3]]) ]
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A3]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 10
// ROT1-NEXT:    store i32 [[ADD]], ptr [[A3]], align 4
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_ZN2S2C2Ev
// ROT1-SAME: (ptr noundef nonnull align 4 dereferenceable(4) [[THIS:%.*]]) unnamed_addr #[[ATTR2]] comdat align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_S2:%.*]], ptr [[THIS1]], i32 0, i32 0
// ROT1-NEXT:    store i32 0, ptr [[A]], align 4
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@_Z3sumPiiS_
// ROT1-SAME: (ptr noundef [[INPUT:%.*]], i32 noundef [[SIZE:%.*]], ptr noundef [[OUTPUT:%.*]]) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[INPUT_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[SIZE_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[OUTPUT_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[OUTPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[INPUT_MAP_PTR_TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[OUTPUT_MAP_PTR_TMP13:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[INPUT_MAP_PTR_TMP14:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[_TMP15:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV23:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB24:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB25:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I29:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca [10 x i32], align 16
// ROT1-NEXT:    [[I43:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I46:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store ptr [[INPUT]], ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    store i32 [[SIZE]], ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store ptr [[OUTPUT]], ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 0
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i32, ptr [[TMP5]], i64 0
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CONV:%.*]] = sext i32 [[TMP6]] to i64
// ROT1-NEXT:    [[TMP7:%.*]] = mul nuw i64 [[CONV]], 4
// ROT1-NEXT:    [[TMP8:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 6), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP2]], ptr [[ARRAYIDX]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP4]], ptr [[ARRAYIDX1]], i64 [[TMP7]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_1]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_0]]), "QUAL.OMP.PRIVATE"(ptr [[OUTPUT_MAP_PTR_TMP]]), "QUAL.OMP.PRIVATE"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    store ptr [[TMP4]], ptr [[INPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_1]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_0]]), "QUAL.OMP.PRIVATE"(ptr [[TMP]]) ]
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP10]], ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i32 [[TMP11]], 0
// ROT1-NEXT:    [[SUB2:%.*]] = sub nsw i32 [[SUB]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[SUB2]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i32 [[ADD]], 1
// ROT1-NEXT:    [[SUB3:%.*]] = sub nsw i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_1]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_0]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 0, [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_1]], align 4
// ROT1-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP]], i64 1, i64 0, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP]]), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB]]), "QUAL.OMP.PRIVATE"(ptr [[I]]) ]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT1:       omp.inner.for.cond:
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[CMP4:%.*]] = icmp sle i32 [[TMP16]], [[TMP17]]
// ROT1-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP18]], 1
// ROT1-NEXT:    [[ADD5:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD5]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP20]] to i64
// ROT1-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds i32, ptr [[TMP19]], i64 [[IDXPROM]]
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[ARRAYIDX6]], align 4
// ROT1-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP]], align 8
// ROT1-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds i32, ptr [[TMP22]], i64 0
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[ARRAYIDX7]], align 4
// ROT1-NEXT:    [[ADD8:%.*]] = add nsw i32 [[TMP23]], [[TMP21]]
// ROT1-NEXT:    store i32 [[ADD8]], ptr [[ARRAYIDX7]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add nsw i32 [[TMP24]], 1
// ROT1-NEXT:    store i32 [[ADD9]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.TEAMS"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP8]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[TMP25:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP27:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP28:%.*]] = load ptr, ptr [[OUTPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds i32, ptr [[TMP28]], i64 0
// ROT1-NEXT:    [[TMP29:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[TMP30:%.*]] = load ptr, ptr [[INPUT_ADDR]], align 8
// ROT1-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds i32, ptr [[TMP30]], i64 0
// ROT1-NEXT:    [[TMP31:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CONV12:%.*]] = sext i32 [[TMP31]] to i64
// ROT1-NEXT:    [[TMP32:%.*]] = mul nuw i64 [[CONV12]], 4
// ROT1-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 7), "QUAL.OMP.MAP.TOFROM"(ptr [[TMP27]], ptr [[ARRAYIDX10]], i64 12, i64 547, ptr null, ptr null), "QUAL.OMP.MAP.TO"(ptr [[TMP29]], ptr [[ARRAYIDX11]], i64 [[TMP32]], i64 33, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_3]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_2]]), "QUAL.OMP.PRIVATE"(ptr [[OUTPUT_MAP_PTR_TMP13]]), "QUAL.OMP.PRIVATE"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.PRIVATE"(ptr [[_TMP15]]) ]
// ROT1-NEXT:    store ptr [[TMP27]], ptr [[OUTPUT_MAP_PTR_TMP13]], align 8
// ROT1-NEXT:    store ptr [[TMP29]], ptr [[INPUT_MAP_PTR_TMP14]], align 8
// ROT1-NEXT:    [[TMP34:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TEAMS"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP13]], i64 1, i64 0, i64 3, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_3]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.PRIVATE"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]), "QUAL.OMP.PRIVATE"(ptr [[DOTCAPTURE_EXPR_2]]), "QUAL.OMP.PRIVATE"(ptr [[_TMP15]]) ]
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP35]], ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[SUB16:%.*]] = sub nsw i32 [[TMP36]], 0
// ROT1-NEXT:    [[SUB17:%.*]] = sub nsw i32 [[SUB16]], 1
// ROT1-NEXT:    [[ADD18:%.*]] = add nsw i32 [[SUB17]], 1
// ROT1-NEXT:    [[DIV19:%.*]] = sdiv i32 [[ADD18]], 1
// ROT1-NEXT:    [[SUB20:%.*]] = sub nsw i32 [[DIV19]], 1
// ROT1-NEXT:    store i32 [[SUB20]], ptr [[DOTCAPTURE_EXPR_3]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_2]], align 4
// ROT1-NEXT:    [[CMP21:%.*]] = icmp slt i32 0, [[TMP37]]
// ROT1-NEXT:    br i1 [[CMP21]], label [[OMP_PRECOND_THEN22:%.*]], label [[OMP_PRECOND_END41:%.*]]
// ROT1:       omp.precond.then22:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB24]], align 4
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_3]], align 4
// ROT1-NEXT:    store i32 [[TMP38]], ptr [[DOTOMP_UB25]], align 4
// ROT1-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[OUTPUT_MAP_PTR_TMP13]], i64 1, i64 0, i64 3, i64 1), "QUAL.OMP.SHARED"(ptr [[INPUT_MAP_PTR_TMP14]]), "QUAL.OMP.NORMALIZED.IV"(ptr [[DOTOMP_IV23]]), "QUAL.OMP.FIRSTPRIVATE"(ptr [[DOTOMP_LB24]]), "QUAL.OMP.NORMALIZED.UB"(ptr [[DOTOMP_UB25]]), "QUAL.OMP.PRIVATE"(ptr [[I29]]) ]
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTOMP_LB24]], align 4
// ROT1-NEXT:    store i32 [[TMP40]], ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND26:%.*]]
// ROT1:       omp.inner.for.cond26:
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_UB25]], align 4
// ROT1-NEXT:    [[CMP27:%.*]] = icmp sle i32 [[TMP41]], [[TMP42]]
// ROT1-NEXT:    br i1 [[CMP27]], label [[OMP_INNER_FOR_BODY28:%.*]], label [[OMP_INNER_FOR_END39:%.*]]
// ROT1:       omp.inner.for.body28:
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[MUL30:%.*]] = mul nsw i32 [[TMP43]], 1
// ROT1-NEXT:    [[ADD31:%.*]] = add nsw i32 0, [[MUL30]]
// ROT1-NEXT:    store i32 [[ADD31]], ptr [[I29]], align 4
// ROT1-NEXT:    [[TMP44:%.*]] = load ptr, ptr [[INPUT_MAP_PTR_TMP14]], align 8
// ROT1-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I29]], align 4
// ROT1-NEXT:    [[IDXPROM32:%.*]] = sext i32 [[TMP45]] to i64
// ROT1-NEXT:    [[ARRAYIDX33:%.*]] = getelementptr inbounds i32, ptr [[TMP44]], i64 [[IDXPROM32]]
// ROT1-NEXT:    [[TMP46:%.*]] = load i32, ptr [[ARRAYIDX33]], align 4
// ROT1-NEXT:    [[TMP47:%.*]] = load ptr, ptr [[OUTPUT_MAP_PTR_TMP13]], align 8
// ROT1-NEXT:    [[ARRAYIDX34:%.*]] = getelementptr inbounds i32, ptr [[TMP47]], i64 0
// ROT1-NEXT:    [[TMP48:%.*]] = load i32, ptr [[ARRAYIDX34]], align 4
// ROT1-NEXT:    [[ADD35:%.*]] = add nsw i32 [[TMP48]], [[TMP46]]
// ROT1-NEXT:    store i32 [[ADD35]], ptr [[ARRAYIDX34]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE36:%.*]]
// ROT1:       omp.body.continue36:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC37:%.*]]
// ROT1:       omp.inner.for.inc37:
// ROT1-NEXT:    [[TMP49:%.*]] = load i32, ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    [[ADD38:%.*]] = add nsw i32 [[TMP49]], 1
// ROT1-NEXT:    store i32 [[ADD38]], ptr [[DOTOMP_IV23]], align 4
// ROT1-NEXT:    br label [[OMP_INNER_FOR_COND26]]
// ROT1:       omp.inner.for.end39:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT40:%.*]]
// ROT1:       omp.loop.exit40:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END41]]
// ROT1:       omp.precond.end41:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP34]]) [ "DIR.OMP.END.TEAMS"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX42:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 0
// ROT1-NEXT:    [[TMP50:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 8), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX42]], i64 8, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I43]]) ]
// ROT1-NEXT:    [[TMP51:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[A]], i64 1, i64 0, i64 2, i64 1), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I43]]) ]
// ROT1-NEXT:    store i32 0, ptr [[I43]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP52:%.*]] = load i32, ptr [[I43]], align 4
// ROT1-NEXT:    [[TMP53:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CMP44:%.*]] = icmp slt i32 [[TMP52]], [[TMP53]]
// ROT1-NEXT:    br i1 [[CMP44]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP54:%.*]] = load i32, ptr [[I43]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP54]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[I43]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP15:![0-9]+]]
// ROT1:       for.end:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP51]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP50]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    [[ARRAYIDX45:%.*]] = getelementptr inbounds [10 x i32], ptr [[A]], i64 0, i64 3
// ROT1-NEXT:    [[TMP55:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 9), "QUAL.OMP.MAP.TOFROM"(ptr [[A]], ptr [[ARRAYIDX45]], i64 4, i64 547, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I46]]) ]
// ROT1-NEXT:    [[TMP56:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL"(), "QUAL.OMP.REDUCTION.ADD:ARRSECT"(ptr [[A]], i64 1, i64 3, i64 1, i64 1), "QUAL.OMP.SHARED"(ptr [[SIZE_ADDR]]), "QUAL.OMP.PRIVATE"(ptr [[I46]]) ]
// ROT1-NEXT:    store i32 0, ptr [[I46]], align 4
// ROT1-NEXT:    br label [[FOR_COND47:%.*]]
// ROT1:       for.cond47:
// ROT1-NEXT:    [[TMP57:%.*]] = load i32, ptr [[I46]], align 4
// ROT1-NEXT:    [[TMP58:%.*]] = load i32, ptr [[SIZE_ADDR]], align 4
// ROT1-NEXT:    [[CMP48:%.*]] = icmp slt i32 [[TMP57]], [[TMP58]]
// ROT1-NEXT:    br i1 [[CMP48]], label [[FOR_BODY49:%.*]], label [[FOR_END52:%.*]]
// ROT1:       for.body49:
// ROT1-NEXT:    br label [[FOR_INC50:%.*]]
// ROT1:       for.inc50:
// ROT1-NEXT:    [[TMP59:%.*]] = load i32, ptr [[I46]], align 4
// ROT1-NEXT:    [[INC51:%.*]] = add nsw i32 [[TMP59]], 1
// ROT1-NEXT:    store i32 [[INC51]], ptr [[I46]], align 4
// ROT1-NEXT:    br label [[FOR_COND47]], !llvm.loop [[LOOP16:![0-9]+]]
// ROT1:       for.end52:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP56]]) [ "DIR.OMP.END.PARALLEL"() ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP55]]) [ "DIR.OMP.END.TARGET"() ]
// ROT1-NEXT:    ret void
//
//
// ROT1-LABEL: define {{[^@]+}}@main
// ROT1-SAME: () #[[ATTR6:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[RETVAL:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[SIZE:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[ARRAY:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[RESULT:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 0, ptr [[RETVAL]], align 4
// ROT1-NEXT:    [[CALL:%.*]] = call noundef i32 @_Z3fooi(i32 noundef 10)
// ROT1-NEXT:    store i32 [[CALL]], ptr [[A]], align 4
// ROT1-NEXT:    [[CALL1:%.*]] = call noundef i32 @_Z3barv()
// ROT1-NEXT:    store i32 [[CALL1]], ptr [[A]], align 4
// ROT1-NEXT:    store i32 100, ptr [[SIZE]], align 4
// ROT1-NEXT:    [[CALL2:%.*]] = call noalias noundef nonnull ptr @_Znam(i64 noundef 400) #[[ATTR8:[0-9]+]]
// ROT1-NEXT:    store ptr [[CALL2]], ptr [[ARRAY]], align 8
// ROT1-NEXT:    store i32 0, ptr [[RESULT]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[ARRAY]], align 8
// ROT1-NEXT:    call void @_Z3sumPiiS_(ptr noundef [[TMP0]], i32 noundef 100, ptr noundef [[RESULT]])
// ROT1-NEXT:    ret i32 0
//
// end  INTEL_COLLAB
