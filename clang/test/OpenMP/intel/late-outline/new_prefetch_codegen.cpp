// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --prefix-filecheck-ir-name _ --version 3
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 -triple i386-unknown-unknown %s \
// RUN:  | FileCheck %s --check-prefix=CHECK-32
//
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-loop-rotation-control=0 \
// RUN:  -fopenmp-late-outline \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s
//
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-loop-rotation-control=0 -fopenmp-late-outline \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-loop-rotation-control=0 -fopenmp-late-outline -DMULTI_DATA \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s
//
// RUN: %clang_cc1 -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 \
// RUN:  -triple x86_64-unknown-linux-gnu -emit-pch %s -o %t

// RUN: %clang_cc1 -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 \
// RUN:  -triple x86_64-unknown-linux-gnu -include-pch %t \
// RUN:  -emit-llvm %s -o - | FileCheck %s --check-prefix ROT1
//
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 -triple i386-unknown-unknown %s \
// RUN:  | FileCheck %s --check-prefix=ROT1-32
//
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-loop-rotation-control=1 \
// RUN:  -fopenmp-late-outline \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s --check-prefix ROT1
//
// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-loop-rotation-control=1 -fopenmp-late-outline \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s --check-prefix ROT1

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp \
// RUN:  -fopenmp-loop-rotation-control=1 -fopenmp-late-outline -DMULTI_DATA \
// RUN:  -triple x86_64-unknown-linux-gnu %s | FileCheck %s --check-prefix ROT1
//
// RUN: %clang_cc1 -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 \
// RUN:  -triple x86_64-unknown-linux-gnu -include-pch %t \
// RUN:  -emit-llvm %s -o - | FileCheck %s --check-prefix ROT1
//
// expected-no-diagnostics
#ifndef HEADER
#define HEADER

struct foobar {
  int i;
  double d[10];
  double d2[10][10];

  void check();
};

// CHECK-32-LABEL: define dso_local void @_ZN6foobar5checkEv(
// CHECK-32-SAME: ptr noundef nonnull align 4 dereferenceable(884) [[THIS:%.*]]) #[[ATTR0:[0-9]+]] align 2 {
// CHECK-32-NEXT:  entry:
// CHECK-32-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 4
// CHECK-32-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 4
// CHECK-32-NEXT:    [[D:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR:%.*]], ptr [[THIS1]], i32 0, i32 1
// CHECK-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x double], ptr [[D]], i32 0, i32 1
// CHECK-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], double 0.000000e+00, i32 2, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[D2:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 1
// CHECK-32-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x double], ptr [[D2]], i32 0, i32 5
// CHECK-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], double 0.000000e+00, i32 1, i32 3) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[D24:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// CHECK-32-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D24]], i32 0, i32 1
// CHECK-32-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX5]], i32 0, i32 0
// CHECK-32-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY]], i32 0
// CHECK-32-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], double 0.000000e+00, i32 20, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[D27:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// CHECK-32-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D27]], i32 0, i32 6
// CHECK-32-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX8]], i32 0, i32 8
// CHECK-32-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], double 0.000000e+00, i32 1, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    ret void
//
// CHECK-LABEL: define dso_local void @_ZN6foobar5checkEv(
// CHECK-SAME: ptr noundef nonnull align 8 dereferenceable(888) [[THIS:%.*]]) #[[ATTR0:[0-9]+]] align 2 {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// CHECK-NEXT:    [[D:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR:%.*]], ptr [[THIS1]], i32 0, i32 1
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x double], ptr [[D]], i64 0, i64 1
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], double 0.000000e+00, i64 2, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[D2:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 1
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x double], ptr [[D2]], i64 0, i64 5
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], double 0.000000e+00, i64 1, i32 3) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[D24:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D24]], i64 0, i64 1
// CHECK-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX5]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY]], i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], double 0.000000e+00, i64 20, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[D27:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// CHECK-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D27]], i64 0, i64 6
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX8]], i64 0, i64 8
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], double 0.000000e+00, i64 1, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_ZN6foobar5checkEv(
// ROT1-SAME: ptr noundef nonnull align 8 dereferenceable(888) [[THIS:%.*]]) #[[ATTR0:[0-9]+]] align 2 {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
// ROT1-NEXT:    [[D:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR:%.*]], ptr [[THIS1]], i32 0, i32 1
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x double], ptr [[D]], i64 0, i64 1
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], double 0.000000e+00, i64 2, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[D2:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 1
// ROT1-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x double], ptr [[D2]], i64 0, i64 5
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], double 0.000000e+00, i64 1, i32 3) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[D24:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// ROT1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D24]], i64 0, i64 1
// ROT1-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX5]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY]], i64 0
// ROT1-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], double 0.000000e+00, i64 20, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[D27:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// ROT1-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D27]], i64 0, i64 6
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX8]], i64 0, i64 8
// ROT1-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], double 0.000000e+00, i64 1, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    ret void
//
// ROT1-32-LABEL: define dso_local void @_ZN6foobar5checkEv(
// ROT1-32-SAME: ptr noundef nonnull align 4 dereferenceable(884) [[THIS:%.*]]) #[[ATTR0:[0-9]+]] align 2 {
// ROT1-32-NEXT:  entry:
// ROT1-32-NEXT:    [[THIS_ADDR:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    store ptr [[THIS]], ptr [[THIS_ADDR]], align 4
// ROT1-32-NEXT:    [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 4
// ROT1-32-NEXT:    [[D:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR:%.*]], ptr [[THIS1]], i32 0, i32 1
// ROT1-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x double], ptr [[D]], i32 0, i32 1
// ROT1-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], double 0.000000e+00, i32 2, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[D2:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 1
// ROT1-32-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x double], ptr [[D2]], i32 0, i32 5
// ROT1-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], double 0.000000e+00, i32 1, i32 3) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[D24:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// ROT1-32-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D24]], i32 0, i32 1
// ROT1-32-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX5]], i32 0, i32 0
// ROT1-32-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds double, ptr [[ARRAYDECAY]], i32 0
// ROT1-32-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], double 0.000000e+00, i32 20, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[D27:%.*]] = getelementptr inbounds [[STRUCT_FOOBAR]], ptr [[THIS1]], i32 0, i32 2
// ROT1-32-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x [10 x double]], ptr [[D27]], i32 0, i32 6
// ROT1-32-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x double], ptr [[ARRAYIDX8]], i32 0, i32 8
// ROT1-32-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], double 0.000000e+00, i32 1, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    ret void
//
void foobar::check() {
    //fields
    #pragma ompx prefetch data(4:d[1:2])
    #pragma ompx prefetch data(3:d[5])
    #pragma ompx prefetch data(4:d2[1:2][:])
    #pragma ompx prefetch data(4:d2[6][8])
}

foobar bar;

// CHECK-32-LABEL: define dso_local void @_Z13test_prefetchm(
// CHECK-32-SAME: i32 noundef [[N:%.*]]) #[[ATTR0]] {
// CHECK-32-NEXT:  entry:
// CHECK-32-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[A:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[B:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[FOO1:%.*]] = alloca [10 x i32], align 4
// CHECK-32-NEXT:    [[FOO2:%.*]] = alloca [10 x [10 x i32]], align 4
// CHECK-32-NEXT:    [[P:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[PTR_BASE:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    [[PBREF:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// CHECK-32-NEXT:    store i32 1, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    store ptr [[ARRAYDECAY]], ptr [[P]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], i32 0, i32 1, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// CHECK-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX1]], i32 0, i32 1, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX2]], i32 0, i32 2, i32 1) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], i32 0, i32 9, i32 2) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 4
// CHECK-32-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX4]], i32 0, i32 8, i32 3) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 3
// CHECK-32-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX5]], i32 0, i32 [[TMP5]], i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], i32 0, i32 10, i32 5) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[TMP9:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 [[TMP9]]
// CHECK-32-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX7]], i32 0, i32 5, i32 6) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// CHECK-32-NEXT:    [[TMP11:%.*]] = load i32, ptr [[A]], align 4
// CHECK-32-NEXT:    [[TMP12:%.*]] = load i32, ptr [[B]], align 4
// CHECK-32-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP11]], [[TMP12]]
// CHECK-32-NEXT:    [[TMP13:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX8]], i32 0, i32 2, i32 1), "QUAL.OMP.IF"(i1 [[CMP]]) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP13]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP14:%.*]] = load i32, ptr [[A]], align 4
// CHECK-32-NEXT:    [[CMP9:%.*]] = icmp eq i32 [[TMP14]], 0
// CHECK-32-NEXT:    [[TMP15:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[TMP16:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 [[TMP16]]
// CHECK-32-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.IF"(i1 [[CMP9]]), "QUAL.OMP.DATA"(ptr [[ARRAYIDX10]], i32 0, i32 3, i32 1) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// CHECK-32-NEXT:    [[ARRAYIDX12:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i32 0, i32 0
// CHECK-32-NEXT:    [[ARRAYDECAY13:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX12]], i32 0, i32 0
// CHECK-32-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY13]], i32 0
// CHECK-32-NEXT:    [[TMP18:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX11]], i32 0, i32 1, i32 1), "QUAL.OMP.DATA"(ptr [[ARRAYIDX14]], i32 0, i32 50, i32 1) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP18]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX15:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// CHECK-32-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX15]], i32 0, i32 1, i32 3) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i32 0, i32 2
// CHECK-32-NEXT:    [[ARRAYIDX17:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX16]], i32 0, i32 3
// CHECK-32-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX17]], i32 0, i32 1, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP21:%.*]] = load i32, ptr [[N_ADDR]], align 4
// CHECK-32-NEXT:    [[TMP22:%.*]] = call ptr @llvm.stacksave.p0()
// CHECK-32-NEXT:    store ptr [[TMP22]], ptr [[SAVED_STACK]], align 4
// CHECK-32-NEXT:    [[TMP23:%.*]] = mul nuw i32 1000, [[TMP21]]
// CHECK-32-NEXT:    [[VLA:%.*]] = alloca i16, i32 [[TMP23]], align 2
// CHECK-32-NEXT:    store i32 [[TMP21]], ptr [[__VLA_EXPR0]], align 4
// CHECK-32-NEXT:    [[TMP24:%.*]] = mul nuw i32 10, [[TMP21]]
// CHECK-32-NEXT:    [[TMP25:%.*]] = mul nsw i32 50, [[TMP24]]
// CHECK-32-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i32 [[TMP25]]
// CHECK-32-NEXT:    [[TMP26:%.*]] = mul nsw i32 5, [[TMP21]]
// CHECK-32-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX18]], i32 [[TMP26]]
// CHECK-32-NEXT:    [[ARRAYIDX20:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX19]], i32 0
// CHECK-32-NEXT:    [[SEC_LOWER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX20]] to i32
// CHECK-32-NEXT:    [[TMP27:%.*]] = load i32, ptr [[N_ADDR]], align 4
// CHECK-32-NEXT:    [[LB_ADD_LEN:%.*]] = add nsw i32 -1, [[TMP27]]
// CHECK-32-NEXT:    [[TMP28:%.*]] = mul nuw i32 10, [[TMP21]]
// CHECK-32-NEXT:    [[TMP29:%.*]] = mul nsw i32 50, [[TMP28]]
// CHECK-32-NEXT:    [[ARRAYIDX21:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i32 [[TMP29]]
// CHECK-32-NEXT:    [[TMP30:%.*]] = mul nsw i32 7, [[TMP21]]
// CHECK-32-NEXT:    [[ARRAYIDX22:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX21]], i32 [[TMP30]]
// CHECK-32-NEXT:    [[ARRAYIDX23:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX22]], i32 [[LB_ADD_LEN]]
// CHECK-32-NEXT:    [[SEC_UPPER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX23]] to i32
// CHECK-32-NEXT:    [[TMP31:%.*]] = sub i32 [[SEC_UPPER_CAST]], [[SEC_LOWER_CAST]]
// CHECK-32-NEXT:    [[TMP32:%.*]] = sdiv exact i32 [[TMP31]], 2
// CHECK-32-NEXT:    [[SEC_NUMBER_OF_ELEMENTS:%.*]] = add i32 [[TMP32]], 1
// CHECK-32-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX20]], i16 0, i32 [[SEC_NUMBER_OF_ELEMENTS]], i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    store ptr [[PTR_BASE]], ptr [[PBREF]], align 4
// CHECK-32-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[PTR_BASE]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX24:%.*]] = getelementptr inbounds i32, ptr [[TMP34]], i32 3
// CHECK-32-NEXT:    [[TMP35:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX24]], i32 0, i32 10, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP35]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[PBREF]], align 4
// CHECK-32-NEXT:    [[TMP37:%.*]] = load ptr, ptr [[TMP36]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX25:%.*]] = getelementptr inbounds i32, ptr [[TMP37]], i32 3
// CHECK-32-NEXT:    [[TMP38:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX25]], i32 0, i32 10, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP38]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    call void @_ZN6foobar5checkEv(ptr noundef nonnull align 4 dereferenceable(884) @bar)
// CHECK-32-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR:%.*]], ptr @bar, i32 0, i32 1, i32 1), double 0.000000e+00, i32 2, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP40:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 1, i32 5), double 0.000000e+00, i32 1, i32 3) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP40]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP41:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i32 1), double 0.000000e+00, i32 20, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP41]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP42:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i32 6, i32 8), double 0.000000e+00, i32 1, i32 4) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP42]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP43:%.*]] = load ptr, ptr [[SAVED_STACK]], align 4
// CHECK-32-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP43]])
// CHECK-32-NEXT:    ret void
//
// CHECK-LABEL: define dso_local void @_Z13test_prefetchm(
// CHECK-SAME: i64 noundef [[N:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[N_ADDR:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[FOO1:%.*]] = alloca [10 x i32], align 16
// CHECK-NEXT:    [[FOO2:%.*]] = alloca [10 x [10 x i32]], align 16
// CHECK-NEXT:    [[P:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[PTR_BASE:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[PBREF:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store i64 [[N]], ptr [[N_ADDR]], align 8
// CHECK-NEXT:    store i32 1, ptr [[I]], align 4
// CHECK-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    store ptr [[ARRAYDECAY]], ptr [[P]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], i32 0, i64 1, i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX1]], i32 0, i64 1, i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX2]], i32 0, i64 2, i32 1) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], i32 0, i64 9, i32 2) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 4
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX4]], i32 0, i64 8, i32 3) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = sext i32 [[TMP5]] to i64
// CHECK-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 3
// CHECK-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX5]], i32 0, i64 [[CONV]], i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], i32 0, i64 10, i32 5) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CONV7:%.*]] = sext i32 [[TMP8]] to i64
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = sext i32 [[TMP9]] to i64
// CHECK-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 [[TMP10]]
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX8]], i32 0, i64 5, i32 6) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[B]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP12]], [[TMP13]]
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], i32 0, i64 2, i32 1), "QUAL.OMP.IF"(i1 [[CMP]]) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[CMP10:%.*]] = icmp eq i32 [[TMP15]], 0
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CONV11:%.*]] = sext i32 [[TMP16]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = sext i32 [[TMP17]] to i64
// CHECK-NEXT:    [[ARRAYIDX12:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 [[TMP18]]
// CHECK-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.IF"(i1 [[CMP10]]), "QUAL.OMP.DATA"(ptr [[ARRAYIDX12]], i32 0, i64 3, i32 1) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYDECAY15:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX14]], i64 0, i64 0
// CHECK-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY15]], i64 0
// CHECK-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX13]], i32 0, i64 1, i32 1), "QUAL.OMP.DATA"(ptr [[ARRAYIDX16]], i32 0, i64 50, i32 1) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX17:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX17]], i32 0, i64 1, i32 3) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i64 0, i64 2
// CHECK-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX18]], i64 0, i64 3
// CHECK-NEXT:    [[TMP22:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX19]], i32 0, i64 1, i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP22]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP23:%.*]] = load i64, ptr [[N_ADDR]], align 8
// CHECK-NEXT:    [[TMP24:%.*]] = call ptr @llvm.stacksave.p0()
// CHECK-NEXT:    store ptr [[TMP24]], ptr [[SAVED_STACK]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = mul nuw i64 1000, [[TMP23]]
// CHECK-NEXT:    [[VLA:%.*]] = alloca i16, i64 [[TMP25]], align 16
// CHECK-NEXT:    store i64 [[TMP23]], ptr [[__VLA_EXPR0]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = mul nuw i64 10, [[TMP23]]
// CHECK-NEXT:    [[TMP27:%.*]] = mul nsw i64 50, [[TMP26]]
// CHECK-NEXT:    [[ARRAYIDX20:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i64 [[TMP27]]
// CHECK-NEXT:    [[TMP28:%.*]] = mul nsw i64 5, [[TMP23]]
// CHECK-NEXT:    [[ARRAYIDX21:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX20]], i64 [[TMP28]]
// CHECK-NEXT:    [[ARRAYIDX22:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX21]], i64 0
// CHECK-NEXT:    [[SEC_LOWER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX22]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = load i64, ptr [[N_ADDR]], align 8
// CHECK-NEXT:    [[LB_ADD_LEN:%.*]] = add nsw i64 -1, [[TMP29]]
// CHECK-NEXT:    [[TMP30:%.*]] = mul nuw i64 10, [[TMP23]]
// CHECK-NEXT:    [[TMP31:%.*]] = mul nsw i64 50, [[TMP30]]
// CHECK-NEXT:    [[ARRAYIDX23:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i64 [[TMP31]]
// CHECK-NEXT:    [[TMP32:%.*]] = mul nsw i64 7, [[TMP23]]
// CHECK-NEXT:    [[ARRAYIDX24:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX23]], i64 [[TMP32]]
// CHECK-NEXT:    [[ARRAYIDX25:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX24]], i64 [[LB_ADD_LEN]]
// CHECK-NEXT:    [[SEC_UPPER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX25]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = sub i64 [[SEC_UPPER_CAST]], [[SEC_LOWER_CAST]]
// CHECK-NEXT:    [[TMP34:%.*]] = sdiv exact i64 [[TMP33]], 2
// CHECK-NEXT:    [[SEC_NUMBER_OF_ELEMENTS:%.*]] = add i64 [[TMP34]], 1
// CHECK-NEXT:    [[TMP35:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX22]], i16 0, i64 [[SEC_NUMBER_OF_ELEMENTS]], i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP35]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    store ptr [[PTR_BASE]], ptr [[PBREF]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[PTR_BASE]], align 8
// CHECK-NEXT:    [[ARRAYIDX26:%.*]] = getelementptr inbounds i32, ptr [[TMP36]], i64 3
// CHECK-NEXT:    [[TMP37:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX26]], i32 0, i64 10, i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP37]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP38:%.*]] = load ptr, ptr [[PBREF]], align 8
// CHECK-NEXT:    [[TMP39:%.*]] = load ptr, ptr [[TMP38]], align 8
// CHECK-NEXT:    [[ARRAYIDX27:%.*]] = getelementptr inbounds i32, ptr [[TMP39]], i64 3
// CHECK-NEXT:    [[TMP40:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX27]], i32 0, i64 10, i32 0) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP40]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    call void @_ZN6foobar5checkEv(ptr noundef nonnull align 8 dereferenceable(888) @bar)
// CHECK-NEXT:    [[TMP41:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR:%.*]], ptr @bar, i32 0, i32 1, i64 1), double 0.000000e+00, i64 2, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP41]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP42:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 1, i64 5), double 0.000000e+00, i64 1, i32 3) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP42]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP43:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i64 1), double 0.000000e+00, i64 20, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP43]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP44:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i64 6, i64 8), double 0.000000e+00, i64 1, i32 4) ]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP44]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-NEXT:    [[TMP45:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8
// CHECK-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP45]])
// CHECK-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z13test_prefetchm(
// ROT1-SAME: i64 noundef [[N:%.*]]) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[N_ADDR:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[A:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[B:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[FOO1:%.*]] = alloca [10 x i32], align 16
// ROT1-NEXT:    [[FOO2:%.*]] = alloca [10 x [10 x i32]], align 16
// ROT1-NEXT:    [[P:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[PTR_BASE:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[PBREF:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store i64 [[N]], ptr [[N_ADDR]], align 8
// ROT1-NEXT:    store i32 1, ptr [[I]], align 4
// ROT1-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    store ptr [[ARRAYDECAY]], ptr [[P]], align 8
// ROT1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], i32 0, i64 1, i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// ROT1-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX1]], i32 0, i64 1, i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX2]], i32 0, i64 2, i32 1) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], i32 0, i64 9, i32 2) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 4
// ROT1-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX4]], i32 0, i64 8, i32 3) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CONV:%.*]] = sext i32 [[TMP5]] to i64
// ROT1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 3
// ROT1-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX5]], i32 0, i64 [[CONV]], i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], i32 0, i64 10, i32 5) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CONV7:%.*]] = sext i32 [[TMP8]] to i64
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = sext i32 [[TMP9]] to i64
// ROT1-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 [[TMP10]]
// ROT1-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX8]], i32 0, i64 5, i32 6) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[A]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[B]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP12]], [[TMP13]]
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX9]], i32 0, i64 2, i32 1), "QUAL.OMP.IF"(i1 [[CMP]]) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[A]], align 4
// ROT1-NEXT:    [[CMP10:%.*]] = icmp eq i32 [[TMP15]], 0
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CONV11:%.*]] = sext i32 [[TMP16]] to i64
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = sext i32 [[TMP17]] to i64
// ROT1-NEXT:    [[ARRAYIDX12:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 [[TMP18]]
// ROT1-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.IF"(i1 [[CMP10]]), "QUAL.OMP.DATA"(ptr [[ARRAYIDX12]], i32 0, i64 3, i32 1) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX13:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYDECAY15:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX14]], i64 0, i64 0
// ROT1-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY15]], i64 0
// ROT1-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX13]], i32 0, i64 1, i32 1), "QUAL.OMP.DATA"(ptr [[ARRAYIDX16]], i32 0, i64 50, i32 1) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX17:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i64 0, i64 1
// ROT1-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX17]], i32 0, i64 1, i32 3) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i64 0, i64 2
// ROT1-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX18]], i64 0, i64 3
// ROT1-NEXT:    [[TMP22:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX19]], i32 0, i64 1, i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP22]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP23:%.*]] = load i64, ptr [[N_ADDR]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = call ptr @llvm.stacksave.p0()
// ROT1-NEXT:    store ptr [[TMP24]], ptr [[SAVED_STACK]], align 8
// ROT1-NEXT:    [[TMP25:%.*]] = mul nuw i64 1000, [[TMP23]]
// ROT1-NEXT:    [[VLA:%.*]] = alloca i16, i64 [[TMP25]], align 16
// ROT1-NEXT:    store i64 [[TMP23]], ptr [[__VLA_EXPR0]], align 8
// ROT1-NEXT:    [[TMP26:%.*]] = mul nuw i64 10, [[TMP23]]
// ROT1-NEXT:    [[TMP27:%.*]] = mul nsw i64 50, [[TMP26]]
// ROT1-NEXT:    [[ARRAYIDX20:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i64 [[TMP27]]
// ROT1-NEXT:    [[TMP28:%.*]] = mul nsw i64 5, [[TMP23]]
// ROT1-NEXT:    [[ARRAYIDX21:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX20]], i64 [[TMP28]]
// ROT1-NEXT:    [[ARRAYIDX22:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX21]], i64 0
// ROT1-NEXT:    [[SEC_LOWER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX22]] to i64
// ROT1-NEXT:    [[TMP29:%.*]] = load i64, ptr [[N_ADDR]], align 8
// ROT1-NEXT:    [[LB_ADD_LEN:%.*]] = add nsw i64 -1, [[TMP29]]
// ROT1-NEXT:    [[TMP30:%.*]] = mul nuw i64 10, [[TMP23]]
// ROT1-NEXT:    [[TMP31:%.*]] = mul nsw i64 50, [[TMP30]]
// ROT1-NEXT:    [[ARRAYIDX23:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i64 [[TMP31]]
// ROT1-NEXT:    [[TMP32:%.*]] = mul nsw i64 7, [[TMP23]]
// ROT1-NEXT:    [[ARRAYIDX24:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX23]], i64 [[TMP32]]
// ROT1-NEXT:    [[ARRAYIDX25:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX24]], i64 [[LB_ADD_LEN]]
// ROT1-NEXT:    [[SEC_UPPER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX25]] to i64
// ROT1-NEXT:    [[TMP33:%.*]] = sub i64 [[SEC_UPPER_CAST]], [[SEC_LOWER_CAST]]
// ROT1-NEXT:    [[TMP34:%.*]] = sdiv exact i64 [[TMP33]], 2
// ROT1-NEXT:    [[SEC_NUMBER_OF_ELEMENTS:%.*]] = add i64 [[TMP34]], 1
// ROT1-NEXT:    [[TMP35:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX22]], i16 0, i64 [[SEC_NUMBER_OF_ELEMENTS]], i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP35]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    store ptr [[PTR_BASE]], ptr [[PBREF]], align 8
// ROT1-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[PTR_BASE]], align 8
// ROT1-NEXT:    [[ARRAYIDX26:%.*]] = getelementptr inbounds i32, ptr [[TMP36]], i64 3
// ROT1-NEXT:    [[TMP37:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX26]], i32 0, i64 10, i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP37]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP38:%.*]] = load ptr, ptr [[PBREF]], align 8
// ROT1-NEXT:    [[TMP39:%.*]] = load ptr, ptr [[TMP38]], align 8
// ROT1-NEXT:    [[ARRAYIDX27:%.*]] = getelementptr inbounds i32, ptr [[TMP39]], i64 3
// ROT1-NEXT:    [[TMP40:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX27]], i32 0, i64 10, i32 0) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP40]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    call void @_ZN6foobar5checkEv(ptr noundef nonnull align 8 dereferenceable(888) @bar)
// ROT1-NEXT:    [[TMP41:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR:%.*]], ptr @bar, i32 0, i32 1, i64 1), double 0.000000e+00, i64 2, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP41]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP42:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 1, i64 5), double 0.000000e+00, i64 1, i32 3) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP42]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP43:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i64 1), double 0.000000e+00, i64 20, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP43]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP44:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i64 6, i64 8), double 0.000000e+00, i64 1, i32 4) ]
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP44]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-NEXT:    [[TMP45:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8
// ROT1-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP45]])
// ROT1-NEXT:    ret void
//
// ROT1-32-LABEL: define dso_local void @_Z13test_prefetchm(
// ROT1-32-SAME: i32 noundef [[N:%.*]]) #[[ATTR0]] {
// ROT1-32-NEXT:  entry:
// ROT1-32-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[A:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[B:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[FOO1:%.*]] = alloca [10 x i32], align 4
// ROT1-32-NEXT:    [[FOO2:%.*]] = alloca [10 x [10 x i32]], align 4
// ROT1-32-NEXT:    [[P:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[PTR_BASE:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    [[PBREF:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// ROT1-32-NEXT:    store i32 1, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    store ptr [[ARRAYDECAY]], ptr [[P]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], i32 0, i32 1, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// ROT1-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX1]], i32 0, i32 1, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX2]], i32 0, i32 2, i32 1) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    [[TMP3:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX3]], i32 0, i32 9, i32 2) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP3]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 4
// ROT1-32-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX4]], i32 0, i32 8, i32 3) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP5:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 3
// ROT1-32-NEXT:    [[TMP6:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX5]], i32 0, i32 [[TMP5]], i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP6]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    [[TMP7:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX6]], i32 0, i32 10, i32 5) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP7]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[TMP9:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 [[TMP9]]
// ROT1-32-NEXT:    [[TMP10:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX7]], i32 0, i32 5, i32 6) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP10]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX8:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// ROT1-32-NEXT:    [[TMP11:%.*]] = load i32, ptr [[A]], align 4
// ROT1-32-NEXT:    [[TMP12:%.*]] = load i32, ptr [[B]], align 4
// ROT1-32-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP11]], [[TMP12]]
// ROT1-32-NEXT:    [[TMP13:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX8]], i32 0, i32 2, i32 1), "QUAL.OMP.IF"(i1 [[CMP]]) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP13]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP14:%.*]] = load i32, ptr [[A]], align 4
// ROT1-32-NEXT:    [[CMP9:%.*]] = icmp eq i32 [[TMP14]], 0
// ROT1-32-NEXT:    [[TMP15:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[TMP16:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX10:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 [[TMP16]]
// ROT1-32-NEXT:    [[TMP17:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.IF"(i1 [[CMP9]]), "QUAL.OMP.DATA"(ptr [[ARRAYIDX10]], i32 0, i32 3, i32 1) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP17]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX11:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 0
// ROT1-32-NEXT:    [[ARRAYIDX12:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i32 0, i32 0
// ROT1-32-NEXT:    [[ARRAYDECAY13:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX12]], i32 0, i32 0
// ROT1-32-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY13]], i32 0
// ROT1-32-NEXT:    [[TMP18:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX11]], i32 0, i32 1, i32 1), "QUAL.OMP.DATA"(ptr [[ARRAYIDX14]], i32 0, i32 50, i32 1) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP18]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX15:%.*]] = getelementptr inbounds [10 x i32], ptr [[FOO1]], i32 0, i32 1
// ROT1-32-NEXT:    [[TMP19:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX15]], i32 0, i32 1, i32 3) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP19]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds [10 x [10 x i32]], ptr [[FOO2]], i32 0, i32 2
// ROT1-32-NEXT:    [[ARRAYIDX17:%.*]] = getelementptr inbounds [10 x i32], ptr [[ARRAYIDX16]], i32 0, i32 3
// ROT1-32-NEXT:    [[TMP20:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX17]], i32 0, i32 1, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP20]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP21:%.*]] = load i32, ptr [[N_ADDR]], align 4
// ROT1-32-NEXT:    [[TMP22:%.*]] = call ptr @llvm.stacksave.p0()
// ROT1-32-NEXT:    store ptr [[TMP22]], ptr [[SAVED_STACK]], align 4
// ROT1-32-NEXT:    [[TMP23:%.*]] = mul nuw i32 1000, [[TMP21]]
// ROT1-32-NEXT:    [[VLA:%.*]] = alloca i16, i32 [[TMP23]], align 2
// ROT1-32-NEXT:    store i32 [[TMP21]], ptr [[__VLA_EXPR0]], align 4
// ROT1-32-NEXT:    [[TMP24:%.*]] = mul nuw i32 10, [[TMP21]]
// ROT1-32-NEXT:    [[TMP25:%.*]] = mul nsw i32 50, [[TMP24]]
// ROT1-32-NEXT:    [[ARRAYIDX18:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i32 [[TMP25]]
// ROT1-32-NEXT:    [[TMP26:%.*]] = mul nsw i32 5, [[TMP21]]
// ROT1-32-NEXT:    [[ARRAYIDX19:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX18]], i32 [[TMP26]]
// ROT1-32-NEXT:    [[ARRAYIDX20:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX19]], i32 0
// ROT1-32-NEXT:    [[SEC_LOWER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX20]] to i32
// ROT1-32-NEXT:    [[TMP27:%.*]] = load i32, ptr [[N_ADDR]], align 4
// ROT1-32-NEXT:    [[LB_ADD_LEN:%.*]] = add nsw i32 -1, [[TMP27]]
// ROT1-32-NEXT:    [[TMP28:%.*]] = mul nuw i32 10, [[TMP21]]
// ROT1-32-NEXT:    [[TMP29:%.*]] = mul nsw i32 50, [[TMP28]]
// ROT1-32-NEXT:    [[ARRAYIDX21:%.*]] = getelementptr inbounds i16, ptr [[VLA]], i32 [[TMP29]]
// ROT1-32-NEXT:    [[TMP30:%.*]] = mul nsw i32 7, [[TMP21]]
// ROT1-32-NEXT:    [[ARRAYIDX22:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX21]], i32 [[TMP30]]
// ROT1-32-NEXT:    [[ARRAYIDX23:%.*]] = getelementptr inbounds i16, ptr [[ARRAYIDX22]], i32 [[LB_ADD_LEN]]
// ROT1-32-NEXT:    [[SEC_UPPER_CAST:%.*]] = ptrtoint ptr [[ARRAYIDX23]] to i32
// ROT1-32-NEXT:    [[TMP31:%.*]] = sub i32 [[SEC_UPPER_CAST]], [[SEC_LOWER_CAST]]
// ROT1-32-NEXT:    [[TMP32:%.*]] = sdiv exact i32 [[TMP31]], 2
// ROT1-32-NEXT:    [[SEC_NUMBER_OF_ELEMENTS:%.*]] = add i32 [[TMP32]], 1
// ROT1-32-NEXT:    [[TMP33:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX20]], i16 0, i32 [[SEC_NUMBER_OF_ELEMENTS]], i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP33]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    store ptr [[PTR_BASE]], ptr [[PBREF]], align 4
// ROT1-32-NEXT:    [[TMP34:%.*]] = load ptr, ptr [[PTR_BASE]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX24:%.*]] = getelementptr inbounds i32, ptr [[TMP34]], i32 3
// ROT1-32-NEXT:    [[TMP35:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX24]], i32 0, i32 10, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP35]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP36:%.*]] = load ptr, ptr [[PBREF]], align 4
// ROT1-32-NEXT:    [[TMP37:%.*]] = load ptr, ptr [[TMP36]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX25:%.*]] = getelementptr inbounds i32, ptr [[TMP37]], i32 3
// ROT1-32-NEXT:    [[TMP38:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX25]], i32 0, i32 10, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP38]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    call void @_ZN6foobar5checkEv(ptr noundef nonnull align 4 dereferenceable(884) @bar)
// ROT1-32-NEXT:    [[TMP39:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR:%.*]], ptr @bar, i32 0, i32 1, i32 1), double 0.000000e+00, i32 2, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP39]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP40:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 1, i32 5), double 0.000000e+00, i32 1, i32 3) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP40]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP41:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i32 1), double 0.000000e+00, i32 20, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP41]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP42:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr getelementptr inbounds ([[STRUCT_FOOBAR]], ptr @bar, i32 0, i32 2, i32 6, i32 8), double 0.000000e+00, i32 1, i32 4) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP42]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP43:%.*]] = load ptr, ptr [[SAVED_STACK]], align 4
// ROT1-32-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP43]])
// ROT1-32-NEXT:    ret void
//
void test_prefetch(unsigned long n) {
  int i = 1;
  int a;
  int b;
  int foo1[10];
  int foo2[10][10];
  int *p = foo1;

  //sections
  #pragma ompx prefetch data(foo1[:1])
  #pragma ompx prefetch data(0:foo1[1:1])
  #pragma ompx prefetch data(1:foo1[:2])
  #pragma ompx prefetch data(2:foo1[:9])
  #pragma ompx prefetch data(3:foo1[4:8])
  #pragma ompx prefetch data(4:foo1[3:i])
  #pragma ompx prefetch data(5:foo1[:])
  #pragma ompx prefetch data(6:foo1[i:5])

  // if clause
  #pragma ompx prefetch data(1:foo1[1:2]) if (a < b)
  #pragma ompx prefetch if (a == 0) data(1:foo1[i:3])

  // One data clause with two prefetch expressions should generate same IR
  // as prefetch with two data clauses and same expressions.
#ifndef MULTI_DATA
  #pragma ompx prefetch data(1:foo1[:1], foo2[:5][:])
#else
  #pragma ompx prefetch data(1:foo1[:1]) data(1:foo2[:5][:])
#endif

  //subscripts
  #pragma ompx prefetch data(3:foo1[1])
  #pragma ompx prefetch data(foo2[2][3])

  // vlas
  short vla[100][10][n];
  #pragma ompx prefetch data(vla[50][5:3][0:n])

  // pointers
  int *ptr_base;
  int *(&pbref) = ptr_base;
  #pragma ompx prefetch data(ptr_base[3:10])
  #pragma ompx prefetch data(pbref[3:10])

  // fields
  bar.check();
  #pragma ompx prefetch data(4:bar.d[1:2])
  #pragma ompx prefetch data(3:bar.d[5])
  #pragma ompx prefetch data(4:bar.d2[1:2][:])
  #pragma ompx prefetch data(4:bar.d2[6][8])
}

float mathfunc(float);
// CHECK-32-LABEL: define dso_local void @_Z14test_with_simdPfS_(
// CHECK-32-SAME: ptr noundef [[PTR:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-32-NEXT:  entry:
// CHECK-32-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// CHECK-32-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-32-NEXT:    store ptr [[PTR]], ptr [[PTR_ADDR]], align 4
// CHECK-32-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// CHECK-32-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-32-NEXT:    store i32 31, ptr [[DOTOMP_UB]], align 4
// CHECK-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.SHARED:TYPED"(ptr [[PTR_ADDR]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[A_ADDR]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// CHECK-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// CHECK-32-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-32-NEXT:    store i32 [[TMP2]], ptr [[DOTOMP_IV]], align 4
// CHECK-32-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK-32:       omp.inner.for.cond:
// CHECK-32-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-32-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-32-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP3]], [[TMP4]]
// CHECK-32-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK-32:       omp.inner.for.body:
// CHECK-32-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-32-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 1
// CHECK-32-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-32-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-32-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds float, ptr [[TMP8]], i32 [[TMP7]]
// CHECK-32-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], float 0.000000e+00, i32 32, i32 0) ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PREFETCH"() ]
// CHECK-32-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// CHECK-32-NEXT:    [[TMP11:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds float, ptr [[TMP10]], i32 [[TMP11]]
// CHECK-32-NEXT:    [[TMP12:%.*]] = load float, ptr [[ARRAYIDX1]], align 4
// CHECK-32-NEXT:    [[CALL:%.*]] = call noundef float @_Z8mathfuncf(float noundef [[TMP12]]) #[[ATTR1:[0-9]+]]
// CHECK-32-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[PTR_ADDR]], align 4
// CHECK-32-NEXT:    [[TMP14:%.*]] = load i32, ptr [[I]], align 4
// CHECK-32-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds float, ptr [[TMP13]], i32 [[TMP14]]
// CHECK-32-NEXT:    store float [[CALL]], ptr [[ARRAYIDX2]], align 4
// CHECK-32-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK-32:       omp.body.continue:
// CHECK-32-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK-32:       omp.inner.for.inc:
// CHECK-32-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-32-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP15]], 1
// CHECK-32-NEXT:    store i32 [[ADD3]], ptr [[DOTOMP_IV]], align 4
// CHECK-32-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP4:![0-9]+]]
// CHECK-32:       omp.inner.for.end:
// CHECK-32-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK-32:       omp.loop.exit:
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// CHECK-32-NEXT:    ret void
//
// ROT1-32-LABEL: define dso_local void @_Z14test_with_simdPfS_(
// ROT1-32-SAME: ptr noundef [[PTR:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// ROT1-32-NEXT:  entry:
// ROT1-32-NEXT:    [[PTR_ADDR:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// ROT1-32-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-32-NEXT:    store ptr [[PTR]], ptr [[PTR_ADDR]], align 4
// ROT1-32-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// ROT1-32-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-32-NEXT:    store i32 31, ptr [[DOTOMP_UB]], align 4
// ROT1-32-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.SHARED:TYPED"(ptr [[PTR_ADDR]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[A_ADDR]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1) ]
// ROT1-32-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.LINEAR:IV.TYPED"(ptr [[I]], i32 0, i32 1, i32 1) ]
// ROT1-32-NEXT:    [[TMP2:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-32-NEXT:    store i32 [[TMP2]], ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-32-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP3]], [[TMP4]]
// ROT1-32-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1-32:       omp.inner.for.body.lh:
// ROT1-32-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1-32:       omp.inner.for.body:
// ROT1-32-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP5]], 1
// ROT1-32-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// ROT1-32-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-32-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[TMP7:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds float, ptr [[TMP8]], i32 [[TMP7]]
// ROT1-32-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PREFETCH"(), "QUAL.OMP.DATA"(ptr [[ARRAYIDX]], float 0.000000e+00, i32 32, i32 0) ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PREFETCH"() ]
// ROT1-32-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// ROT1-32-NEXT:    [[TMP11:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds float, ptr [[TMP10]], i32 [[TMP11]]
// ROT1-32-NEXT:    [[TMP12:%.*]] = load float, ptr [[ARRAYIDX1]], align 4
// ROT1-32-NEXT:    [[CALL:%.*]] = call noundef float @_Z8mathfuncf(float noundef [[TMP12]]) #[[ATTR1:[0-9]+]]
// ROT1-32-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[PTR_ADDR]], align 4
// ROT1-32-NEXT:    [[TMP14:%.*]] = load i32, ptr [[I]], align 4
// ROT1-32-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds float, ptr [[TMP13]], i32 [[TMP14]]
// ROT1-32-NEXT:    store float [[CALL]], ptr [[ARRAYIDX2]], align 4
// ROT1-32-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1-32:       omp.body.continue:
// ROT1-32-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1-32:       omp.inner.for.inc:
// ROT1-32-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[ADD3:%.*]] = add nsw i32 [[TMP15]], 1
// ROT1-32-NEXT:    store i32 [[ADD3]], ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-32-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-32-NEXT:    [[CMP4:%.*]] = icmp sle i32 [[TMP16]], [[TMP17]]
// ROT1-32-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]], !llvm.loop [[LOOP4:![0-9]+]]
// ROT1-32:       omp.inner.for.end_crit_edge:
// ROT1-32-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1-32:       omp.inner.for.end:
// ROT1-32-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1-32:       omp.loop.exit:
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SIMD"() ]
// ROT1-32-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-32-NEXT:    ret void
//
void test_with_simd(float *ptr, float a[]) {
  #pragma omp parallel for simd
  for (int i = 0; i < 32; ++i) {
    #pragma ompx prefetch data(a[i:32])
    ptr[i] = mathfunc(a[i]);
  }
}
#endif
// end INTEL_COLLAB
