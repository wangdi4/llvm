// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --functions "test1|test2" --version 3

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 -fopenmp-typed-clauses \
// RUN:  -fopenmp-version=50 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT0

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 -fopenmp-typed-clauses \
// RUN:  -fopenmp-version=50 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT1

// ROT0-LABEL: define dso_local noundef i32 @_Z5test1i(
// ROT0-SAME: i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[OMP_VLA_TMP:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[AREF:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
// ROT0-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave.p0()
// ROT0-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8
// ROT0-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 16
// ROT0-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8
// ROT0-NEXT:    store ptr [[VLA]], ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = mul nuw i64 4, [[TMP1]]
// ROT0-NEXT:    [[DIV:%.*]] = udiv i64 [[TMP4]], 4
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 [[DIV]]
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    store ptr [[TMP5]], ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[TMP6]], ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP7]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP8]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV1:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV1]], 1
// ROT0-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT0-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP9]], [[TMP10]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT0-NEXT:    store i64 [[TMP11]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    store i64 [[TMP1]], ptr [[OMP_VLA_TMP]], align 8
// ROT0-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP13:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP12]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[OMP_VLA_TMP]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[AREF]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP14:%.*]] = load i64, ptr [[OMP_VLA_TMP]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP15]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP16:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP3:%.*]] = icmp sle i64 [[TMP16]], [[TMP17]]
// ROT0-NEXT:    br i1 [[CMP3]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT0-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP19]], 1
// ROT0-NEXT:    [[ADD_PTR4:%.*]] = getelementptr inbounds i32, ptr [[TMP18]], i64 [[MUL]]
// ROT0-NEXT:    store ptr [[ADD_PTR4]], ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    store ptr [[TMP20]], ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[TMP21]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP22]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[TMP21]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD5:%.*]] = add nsw i64 [[TMP23]], 1
// ROT0-NEXT:    store i64 [[ADD5]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP13]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    [[TMP24:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8
// ROT0-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP24]])
// ROT0-NEXT:    ret i32 0
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test1i(
// ROT1-SAME: i32 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[N_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[SAVED_STACK:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__VLA_EXPR0:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_0:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_2:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[OMP_VLA_TMP:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[AREF:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store i32 [[N]], ptr [[N_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[N_ADDR]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
// ROT1-NEXT:    [[TMP2:%.*]] = call ptr @llvm.stacksave.p0()
// ROT1-NEXT:    store ptr [[TMP2]], ptr [[SAVED_STACK]], align 8
// ROT1-NEXT:    [[VLA:%.*]] = alloca i32, i64 [[TMP1]], align 16
// ROT1-NEXT:    store i64 [[TMP1]], ptr [[__VLA_EXPR0]], align 8
// ROT1-NEXT:    store ptr [[VLA]], ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = mul nuw i64 4, [[TMP1]]
// ROT1-NEXT:    [[DIV:%.*]] = udiv i64 [[TMP4]], 4
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[TMP3]], i64 [[DIV]]
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    store ptr [[TMP5]], ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[TMP6]], ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP7]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[TMP8]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV1:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV1]], 1
// ROT1-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT1-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP10:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_1]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[TMP9]], [[TMP10]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_2]], align 8
// ROT1-NEXT:    store i64 [[TMP11]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    store i64 [[TMP1]], ptr [[OMP_VLA_TMP]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP13:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.DISTRIBUTE.PARLOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP12]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_0]], ptr null, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[OMP_VLA_TMP]], i64 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[AREF]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP14:%.*]] = load i64, ptr [[OMP_VLA_TMP]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP15]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP3:%.*]] = icmp sle i64 [[TMP16]], [[TMP17]]
// ROT1-NEXT:    br i1 [[CMP3]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP18:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_0]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP19]], 1
// ROT1-NEXT:    [[ADD_PTR4:%.*]] = getelementptr inbounds i32, ptr [[TMP18]], i64 [[MUL]]
// ROT1-NEXT:    store ptr [[ADD_PTR4]], ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    store ptr [[TMP20]], ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP21:%.*]] = load ptr, ptr [[AREF]], align 8
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[TMP21]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP22]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[TMP21]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD5:%.*]] = add nsw i64 [[TMP23]], 1
// ROT1-NEXT:    store i64 [[ADD5]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP25:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP6:%.*]] = icmp sle i64 [[TMP24]], [[TMP25]]
// ROT1-NEXT:    br i1 [[CMP6]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP13]]) [ "DIR.OMP.END.DISTRIBUTE.PARLOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    [[TMP26:%.*]] = load ptr, ptr [[SAVED_STACK]], align 8
// ROT1-NEXT:    call void @llvm.stackrestore.p0(ptr [[TMP26]])
// ROT1-NEXT:    ret i32 0
//
int test1(int n) {
  int avar[n];

#pragma omp distribute parallel for
  for (auto &aref : avar) {
    aref++;
  }
  return 0;
}

class B {
public:
  B() { }
  int c;
};

class A : virtual B {
public:
  A() { }
  ~A() { }
  A operator=(const A&foo) { return foo; }
  int x[10] = {1,2,3,4,5,6,7,8,9,10};
};

A foobar;

// ROT0-LABEL: define dso_local noundef i32 @_Z5test2v(
// ROT0-SAME: ) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    [[A:%.*]] = alloca ptr, align 8
// ROT0-NEXT:    store ptr getelementptr (i8, ptr @foobar, i64 8), ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP0]], i64 0, i64 0
// ROT0-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY]], i64 10
// ROT0-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT0-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT0-NEXT:    store ptr [[TMP1]], ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP3]], i64 0, i64 0
// ROT0-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP2]] to i64
// ROT0-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[ARRAYDECAY1]] to i64
// ROT0-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT0-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT0-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT0-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT0-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV]], 1
// ROT0-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT0-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY3:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP4]], i64 0, i64 0
// ROT0-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult ptr [[ARRAYDECAY3]], [[TMP5]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT0-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[A]], align 8
// ROT0-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP8]], i32 0, i32 0
// ROT0-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.FIRSTPRIVATE:NONPOD.TYPED"(ptr @foobar, [[CLASS_A:%.*]] zeroinitializer, i32 1, ptr @_ZTS1A.omp.copy_constr, ptr @_ZTS1A.omp.destr), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP7]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.SHARED:BYREF.TYPED"(ptr [[__RANGE1]], i32 0, i64 10), "QUAL.OMP.PRIVATE:TYPED"(ptr [[A]], ptr null, i32 1) ]
// ROT0-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT0-NEXT:    [[ARRAYDECAY5:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP13]], i64 0, i64 0
// ROT0-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP14]], 1
// ROT0-NEXT:    [[ADD_PTR6:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY5]], i64 [[MUL]]
// ROT0-NEXT:    store ptr [[ADD_PTR6]], ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT0-NEXT:    store ptr [[TMP15]], ptr [[A]], align 8
// ROT0-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[A]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[TMP16]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP17]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[TMP16]], align 4
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD7:%.*]] = add nsw i64 [[TMP18]], 1
// ROT0-NEXT:    store i64 [[ADD7]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret i32 0
//
// ROT1-LABEL: define dso_local noundef i32 @_Z5test2v(
// ROT1-SAME: ) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[TMP:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__RANGE1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[__END1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_3:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[__BEGIN1:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    [[A:%.*]] = alloca ptr, align 8
// ROT1-NEXT:    store ptr getelementptr (i8, ptr @foobar, i64 8), ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP0]], i64 0, i64 0
// ROT1-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY]], i64 10
// ROT1-NEXT:    store ptr [[ADD_PTR]], ptr [[__END1]], align 8
// ROT1-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__END1]], align 8
// ROT1-NEXT:    store ptr [[TMP1]], ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY1:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP3]], i64 0, i64 0
// ROT1-NEXT:    [[SUB_PTR_LHS_CAST:%.*]] = ptrtoint ptr [[TMP2]] to i64
// ROT1-NEXT:    [[SUB_PTR_RHS_CAST:%.*]] = ptrtoint ptr [[ARRAYDECAY1]] to i64
// ROT1-NEXT:    [[SUB_PTR_SUB:%.*]] = sub i64 [[SUB_PTR_LHS_CAST]], [[SUB_PTR_RHS_CAST]]
// ROT1-NEXT:    [[SUB_PTR_DIV:%.*]] = sdiv exact i64 [[SUB_PTR_SUB]], 4
// ROT1-NEXT:    [[SUB:%.*]] = sub nsw i64 [[SUB_PTR_DIV]], 1
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i64 [[SUB]], 1
// ROT1-NEXT:    [[DIV:%.*]] = sdiv i64 [[ADD]], 1
// ROT1-NEXT:    [[SUB2:%.*]] = sub nsw i64 [[DIV]], 1
// ROT1-NEXT:    store i64 [[SUB2]], ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT1-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY3:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP4]], i64 0, i64 0
// ROT1-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[DOTCAPTURE_EXPR_3]], align 8
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult ptr [[ARRAYDECAY3]], [[TMP5]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP6:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_4]], align 8
// ROT1-NEXT:    store i64 [[TMP6]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[A]], align 8
// ROT1-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP8]], i32 0, i32 0
// ROT1-NEXT:    [[TMP9:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.FIRSTPRIVATE:NONPOD.TYPED"(ptr @foobar, [[CLASS_A:%.*]] zeroinitializer, i32 1, ptr @_ZTS1A.omp.copy_constr, ptr @_ZTS1A.omp.destr), "QUAL.OMP.PRIVATE:TYPED"(ptr [[TMP7]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[__BEGIN1]], ptr null, i32 1), "QUAL.OMP.SHARED:BYREF.TYPED"(ptr [[__RANGE1]], i32 0, i64 10), "QUAL.OMP.PRIVATE:TYPED"(ptr [[A]], ptr null, i32 1) ]
// ROT1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP10]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP12:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP4:%.*]] = icmp sle i64 [[TMP11]], [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP4]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[__RANGE1]], align 8
// ROT1-NEXT:    [[ARRAYDECAY5:%.*]] = getelementptr inbounds [10 x i32], ptr [[TMP13]], i64 0, i64 0
// ROT1-NEXT:    [[TMP14:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[TMP14]], 1
// ROT1-NEXT:    [[ADD_PTR6:%.*]] = getelementptr inbounds i32, ptr [[ARRAYDECAY5]], i64 [[MUL]]
// ROT1-NEXT:    store ptr [[ADD_PTR6]], ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    [[TMP15:%.*]] = load ptr, ptr [[__BEGIN1]], align 8
// ROT1-NEXT:    store ptr [[TMP15]], ptr [[A]], align 8
// ROT1-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[A]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[TMP16]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add nsw i32 [[TMP17]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[TMP16]], align 4
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP18:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD7:%.*]] = add nsw i64 [[TMP18]], 1
// ROT1-NEXT:    store i64 [[ADD7]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP19:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP8:%.*]] = icmp sle i64 [[TMP19]], [[TMP20]]
// ROT1-NEXT:    br i1 [[CMP8]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP9]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret i32 0
//
int test2() {

#pragma omp parallel for firstprivate(foobar)
  for (auto &a : foobar.x)
    a++;
  return 0;
}

int main() {
  int res1 = test1(100);
  int res2 = test2();
  return 0;
}
// end INTEL_COLLAB
