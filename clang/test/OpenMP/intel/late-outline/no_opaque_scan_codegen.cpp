// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// INTEL_COLLAB
// RUN: %clang_cc1 -no-opaque-pointers -verify -fopenmp -fopenmp-version=50 \
// RUN: -fopenmp-late-outline -x c++  \
// RUN: -triple x86_64-unknown-unknown -emit-llvm %s -o - | FileCheck %s
// expected-no-diagnostics
void foo();
void bar();

class C {
public:
  int a = 0;
};

void my_comb(C &out, C &in) { out.a += in.a; };
void my_init(C &priv, C &orig) { priv.a = orig.a; }

#pragma omp declare reduction(my_add:C                                         \
  : my_comb(omp_out, omp_in))                      \
  initializer(my_init(omp_priv, omp_orig))

C p[10];
C q[10];
int y;
// CHECK-LABEL: @_Z4testRA10_1C(
// CHECK-NEXT:  entry:
// CHECK-NO:  %array.begin =
// CHECK-NO:  omp.arrayinit.body
// CHECK-NO:  br label %omp.inscan.dispatch
// CHECK-NO:  omp.before.scan.bb
// CHECK-NEXT:    [[X_ADDR:%.*]] = alloca [10 x %class.C]*, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NOT:     [[X3:%.*]] = alloca [5 x %class.C], align 16
// CHECK-NOT:     [[TMP5:%.*]] = alloca [10 x %class.C]*, align 8
// CHECK-NEXT:    store [10 x %class.C]* [[X:%.*]], [10 x %class.C]** [[X_ADDR]], align 8
// CHECK-NEXT:    store i32 9, i32* [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load [10 x %class.C]*, [10 x %class.C]** [[X_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry()
// CHECK-SAME: "DIR.OMP.SIMD"()
// CHECK-SAME: "QUAL.OMP.REDUCTION.UDR:BYREF.ARRSECT.INSCAN"([10 x %class.C]** [[X_ADDR]], i64 1, i64 2, i64 5, i64 1, i8* null, i8* null, void (%class.C*, %class.C*)* @.omp_combiner., void (%class.C*, %class.C*)* @.omp_initializer., i64 1)
// CHECK-SAME "QUAL.OMP.REDUCTION.ADD:INSCAN"(i32* @y, i64 2)
// CHECK-NEXT:    store i32 0, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32* [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP2]], [[TMP3]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP4]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], i32* [[I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load [10 x %class.C]*, [10 x %class.C]** [[X_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* [[TMP5]], i64 0, i64 5
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, i32* [[I]], align 4
// CHECK-NEXT:    [[IDXPROM:%.*]] = sext i32 [[TMP6]] to i64
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* @p, i64 0, i64 [[IDXPROM]]
// CHECK-NEXT:    call void @_Z7my_combR1CS0_(%class.C* {{.*}} [[ARRAYIDX]], %class.C* noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX1]]) #[[ATTR2:[0-9]+]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, i32* [[I]], align 4
// CHECK-NEXT:    [[IDXPROM2:%.*]] = sext i32 [[TMP7]] to i64
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* @p, i64 0, i64 [[IDXPROM2]]
// CHECK-NEXT:    [[A:%.*]] = getelementptr inbounds [[CLASS_C:%.*]], %class.C* [[ARRAYIDX3]], i32 0, i32 0
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, i32* [[A]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, i32* @y, align 4
// CHECK-NEXT:    [[ADD4:%.*]] = add nsw i32 [[TMP9]], [[TMP8]]
// CHECK-NEXT:    store i32 [[ADD4]], i32* @y, align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load [10 x %class.C]*, [10 x %class.C]** [[X_ADDR]], align 8
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.INCLUSIVE"(i32* @y, i64 2), "QUAL.OMP.INCLUSIVE"([10 x %class.C]* [[TMP10]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, i32* [[I]], align 4
// CHECK-NEXT:    [[IDXPROM5:%.*]] = sext i32 [[TMP12]] to i64
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* @q, i64 0, i64 [[IDXPROM5]]
// CHECK-NEXT:    [[TMP13:%.*]] = load [10 x %class.C]*, [10 x %class.C]** [[X_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX7:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* [[TMP13]], i64 0, i64 5
// CHECK-NEXT:    call void @_Z7my_initR1CS0_(%class.C* {{.*}} [[ARRAYIDX6]], %class.C* noundef nonnull align 4 dereferenceable(4) [[ARRAYIDX7]]) #[[ATTR2]]
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, i32* @y, align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, i32* [[I]], align 4
// CHECK-NEXT:    [[IDXPROM8:%.*]] = sext i32 [[TMP15]] to i64
// CHECK-NEXT:    [[ARRAYIDX9:%.*]] = getelementptr inbounds [10 x %class.C], [10 x %class.C]* @q, i64 0, i64 [[IDXPROM8]]
// CHECK-NEXT:    [[A10:%.*]] = getelementptr inbounds [[CLASS_C]], %class.C* [[ARRAYIDX9]], i32 0, i32 0
// CHECK-NEXT:    store i32 [[TMP14]], i32* [[A10]], align 4
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD11:%.*]] = add nsw i32 [[TMP16]], 1
// CHECK-NEXT:    store i32 [[ADD11]], i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP3:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    ret void
//
void test(C (&x)[10]) {
  #pragma omp simd reduction(inscan, my_add : x[2:5]) reduction(inscan, +: y)
  for (int i = 0; i < 10; i++) {
  my_comb(x[5], p[i]);
  y += p[i].a;
  #pragma omp scan inclusive(y, x[2:5])
  my_init(q[i], x[5]);
  q[i].a = y;
  }
}

struct S {
  int a;
  S() {}
  ~S() {}
  S& operator+(const S&);
  S& operator=(const S&);
};

// CHECK-LABEL: @_Z5test1v(
// CHECK-NEXT:  entry:
// CHECK-NO:    omp.arrayinit.done:
// CHECK-NO:    br label %omp.inscan.dispatch
// CHECK-NO:    omp.before.scan.bb:
// CHECK-NEXT:    [[S:%.*]] = alloca [2 x %struct.S], align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NO:      [[S1:%.*]] = alloca [2 x %struct.S], align 4
// CHECK-NO:      [[S3:%.*]] = alloca [2 x %struct.S], align 4
// CHECK-NEXT:    [[ARRAY_BEGIN:%.*]] = getelementptr inbounds [2 x %struct.S], [2 x %struct.S]* [[S]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYCTOR_END:%.*]] = getelementptr inbounds [[STRUCT_S:%.*]], %struct.S* [[ARRAY_BEGIN]], i64 2
// CHECK-NEXT:    br label [[ARRAYCTOR_LOOP:%.*]]
// CHECK:       arrayctor.loop:
// CHECK-NEXT:    [[ARRAYCTOR_CUR:%.*]] = phi %struct.S* [ [[ARRAY_BEGIN]], [[ENTRY:%.*]] ], [ [[ARRAYCTOR_NEXT:%.*]], [[ARRAYCTOR_LOOP]] ]
// CHECK-NEXT:    call void @_ZN1SC1Ev(%struct.S* {{.*}} [[ARRAYCTOR_CUR]])
// CHECK-NEXT:    [[ARRAYCTOR_NEXT]] = getelementptr inbounds [[STRUCT_S]], %struct.S* [[ARRAYCTOR_CUR]], i64 1
// CHECK-NEXT:    [[ARRAYCTOR_DONE:%.*]] = icmp eq %struct.S* [[ARRAYCTOR_NEXT]], [[ARRAYCTOR_END]]
// CHECK-NEXT:    br i1 [[ARRAYCTOR_DONE]], label [[ARRAYCTOR_CONT:%.*]], label [[ARRAYCTOR_LOOP]]
// CHECK:       arrayctor.cont:
// CHECK-NEXT:    store i32 9, i32* [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SIMD"(), "QUAL.OMP.REDUCTION.UDR:INSCAN"([2 x %struct.S]* [[S]], %struct.S* (%struct.S*)* @_ZTS1S.omp.def_constr, void (%struct.S*)* @_ZTS1S.omp.destr, void (%struct.S*, %struct.S*)* @.omp_combiner..1, i8* null, i64 1), "QUAL.OMP.NORMALIZED.IV"(i32* [[DOTOMP_IV]]), "QUAL.OMP.NORMALIZED.UB"(i32* [[DOTOMP_UB]]), "QUAL.OMP.LINEAR:IV"(i32* [[I]], i32 1) ]
// CHECK-NEXT:    store i32 0, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, i32* [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp sle i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i32 [[TMP3]], 1
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD]], i32* [[I]], align 4
// CHECK-NEXT:    call void @_Z3foov() #[[ATTR2]]
// CHECK-NEXT:    [[TMP4:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.SCAN"(), "QUAL.OMP.EXCLUSIVE"([2 x %struct.S]* [[S]], i64 1) ]
// CHECK-NEXT:    fence acq_rel
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP4]]) [ "DIR.OMP.END.SCAN"() ]
// CHECK-NEXT:    call void @_Z3barv() #[[ATTR2]]
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[TMP5]], 1
// CHECK-NEXT:    store i32 [[ADD1]], i32* [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]], !llvm.loop [[LOOP5:![0-9]+]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP0]]) [ "DIR.OMP.END.SIMD"() ]
// CHECK-NEXT:    [[ARRAY_BEGIN2:%.*]] = getelementptr inbounds [2 x %struct.S], [2 x %struct.S]* [[S]], i32 0, i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds [[STRUCT_S]], %struct.S* [[ARRAY_BEGIN2]], i64 2
// CHECK-NEXT:    br label [[ARRAYDESTROY_BODY:%.*]]
// CHECK:       arraydestroy.body:
// CHECK-NEXT:    [[ARRAYDESTROY_ELEMENTPAST:%.*]] = phi %struct.S* [ [[TMP6]], [[OMP_LOOP_EXIT]] ], [ [[ARRAYDESTROY_ELEMENT:%.*]], [[ARRAYDESTROY_BODY]] ]
// CHECK-NEXT:    [[ARRAYDESTROY_ELEMENT]] = getelementptr inbounds [[STRUCT_S]], %struct.S* [[ARRAYDESTROY_ELEMENTPAST]], i64 -1
// CHECK-NEXT:    call void @_ZN1SD1Ev(%struct.S* {{.*}} [[ARRAYDESTROY_ELEMENT]]) #[[ATTR2]]
// CHECK-NEXT:    [[ARRAYDESTROY_DONE:%.*]] = icmp eq %struct.S* [[ARRAYDESTROY_ELEMENT]], [[ARRAY_BEGIN2]]
// CHECK-NEXT:    br i1 [[ARRAYDESTROY_DONE]], label [[ARRAYDESTROY_DONE3:%.*]], label [[ARRAYDESTROY_BODY]]
// CHECK:       arraydestroy.done3:
// CHECK-NEXT:    ret void
//
void test1() {
  S s[2];
  #pragma omp simd reduction(inscan, + : s)
  for (int i = 0; i < 10; ++i) {
  foo();
  #pragma omp scan exclusive(s)
  bar();
  }
}
// end INTEL_COLLAB
