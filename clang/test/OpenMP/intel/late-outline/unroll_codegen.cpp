// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --check-globals --prefix-filecheck-ir-name _
// NOTE: Some editing added to match the loop metadata
// RUN: %clang_cc1 -opaque-pointers -emit-llvm -o - -fopenmp -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN:  -fopenmp-version=51 -triple x86_64-unknown-linux-gnu %s | FileCheck %s

// FE outlining is used for this directive. This test is to ensure that it
// works with the late-outlining option.

// CHECK-LABEL: @_Z4bodyz(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret void
//
void body(...) {}

// CHECK-LABEL: @_Z11func_factoriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP3:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    ret void
//
void func_factor(int start, int end, int step) {
  #pragma omp unroll partial(4)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z23func_for_collapse_outeriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[_TMP1:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_10:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_5:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_6:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_7:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_J:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_10]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[J]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP4]], ptr [[DOTCAPTURE_EXPR_4]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP5]], ptr [[DOTCAPTURE_EXPR_5]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP6]], ptr [[DOTCAPTURE_EXPR_6]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_5]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_6]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB2]], [[TMP9]]
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_6]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP10]]
// CHECK-NEXT:    [[SUB3:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_7]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// CHECK-NEXT:    [[ADD4:%.*]] = add i32 [[TMP11]], 1
// CHECK-NEXT:    store i32 [[ADD4]], ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[TMP12]], [[TMP13]]
// CHECK-NEXT:    [[SUB6:%.*]] = sub i32 [[SUB5]], 1
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_10]], align 4
// CHECK-NEXT:    [[ADD7:%.*]] = add i32 [[SUB6]], [[TMP14]]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_10]], align 4
// CHECK-NEXT:    [[DIV8:%.*]] = udiv i32 [[ADD7]], [[TMP15]]
// CHECK-NEXT:    [[CONV:%.*]] = zext i32 [[DIV8]] to i64
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[SUB9:%.*]] = sub i32 [[TMP16]], 0
// CHECK-NEXT:    [[SUB10:%.*]] = sub i32 [[SUB9]], 1
// CHECK-NEXT:    [[ADD11:%.*]] = add i32 [[SUB10]], 2
// CHECK-NEXT:    [[DIV12:%.*]] = udiv i32 [[ADD11]], 2
// CHECK-NEXT:    [[CONV13:%.*]] = zext i32 [[DIV12]] to i64
// CHECK-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], [[CONV13]]
// CHECK-NEXT:    [[SUB14:%.*]] = sub nsw i64 [[MUL]], 1
// CHECK-NEXT:    store i64 [[SUB14]], ptr [[DOTCAPTURE_EXPR_12]], align 8
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP17]], [[TMP18]]
// CHECK-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       land.lhs.true:
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[CMP15:%.*]] = icmp ult i32 0, [[TMP19]]
// CHECK-NEXT:    br i1 [[CMP15]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_12]], align 8
// CHECK-NEXT:    store i64 [[TMP20]], ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_J]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_J]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// CHECK-NEXT:    store i64 [[TMP22]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// CHECK-NEXT:    [[CMP16:%.*]] = icmp sle i64 [[TMP23]], [[TMP24]]
// CHECK-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// CHECK-NEXT:    [[CONV17:%.*]] = sext i32 [[TMP25]] to i64
// CHECK-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[SUB18:%.*]] = sub i32 [[TMP27]], 0
// CHECK-NEXT:    [[SUB19:%.*]] = sub i32 [[SUB18]], 1
// CHECK-NEXT:    [[ADD20:%.*]] = add i32 [[SUB19]], 2
// CHECK-NEXT:    [[DIV21:%.*]] = udiv i32 [[ADD20]], 2
// CHECK-NEXT:    [[MUL22:%.*]] = mul i32 1, [[DIV21]]
// CHECK-NEXT:    [[CONV23:%.*]] = zext i32 [[MUL22]] to i64
// CHECK-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP26]], [[CONV23]]
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_10]], align 4
// CHECK-NEXT:    [[CONV25:%.*]] = sext i32 [[TMP28]] to i64
// CHECK-NEXT:    [[MUL26:%.*]] = mul nsw i64 [[DIV24]], [[CONV25]]
// CHECK-NEXT:    [[ADD27:%.*]] = add nsw i64 [[CONV17]], [[MUL26]]
// CHECK-NEXT:    [[CONV28:%.*]] = trunc i64 [[ADD27]] to i32
// CHECK-NEXT:    store i32 [[CONV28]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[SUB29:%.*]] = sub i32 [[TMP31]], 0
// CHECK-NEXT:    [[SUB30:%.*]] = sub i32 [[SUB29]], 1
// CHECK-NEXT:    [[ADD31:%.*]] = add i32 [[SUB30]], 2
// CHECK-NEXT:    [[DIV32:%.*]] = udiv i32 [[ADD31]], 2
// CHECK-NEXT:    [[MUL33:%.*]] = mul i32 1, [[DIV32]]
// CHECK-NEXT:    [[CONV34:%.*]] = zext i32 [[MUL33]] to i64
// CHECK-NEXT:    [[DIV35:%.*]] = sdiv i64 [[TMP30]], [[CONV34]]
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// CHECK-NEXT:    [[SUB36:%.*]] = sub i32 [[TMP32]], 0
// CHECK-NEXT:    [[SUB37:%.*]] = sub i32 [[SUB36]], 1
// CHECK-NEXT:    [[ADD38:%.*]] = add i32 [[SUB37]], 2
// CHECK-NEXT:    [[DIV39:%.*]] = udiv i32 [[ADD38]], 2
// CHECK-NEXT:    [[MUL40:%.*]] = mul i32 1, [[DIV39]]
// CHECK-NEXT:    [[CONV41:%.*]] = zext i32 [[MUL40]] to i64
// CHECK-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV35]], [[CONV41]]
// CHECK-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP29]], [[MUL42]]
// CHECK-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 2
// CHECK-NEXT:    [[ADD45:%.*]] = add nsw i64 0, [[MUL44]]
// CHECK-NEXT:    [[CONV46:%.*]] = trunc i64 [[ADD45]] to i32
// CHECK-NEXT:    store i32 [[CONV46]], ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    store i32 [[TMP33]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// CHECK-NEXT:    [[ADD47:%.*]] = add i32 [[TMP35]], 2
// CHECK-NEXT:    [[CMP48:%.*]] = icmp ule i32 [[TMP34]], [[ADD47]]
// CHECK-NEXT:    br i1 [[CMP48]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// CHECK-NEXT:    [[ADD49:%.*]] = add i32 [[TMP37]], 1
// CHECK-NEXT:    [[CMP50:%.*]] = icmp ule i32 [[TMP36]], [[ADD49]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP38:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP50]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP38]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_6]], align 4
// CHECK-NEXT:    [[MUL51:%.*]] = mul i32 [[TMP40]], [[TMP41]]
// CHECK-NEXT:    [[ADD52:%.*]] = add i32 [[TMP39]], [[MUL51]]
// CHECK-NEXT:    store i32 [[ADD52]], ptr [[J]], align 4
// CHECK-NEXT:    [[TMP42:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP43:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP44:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP46:%.*]] = load i32, ptr [[J]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP42]], i32 noundef [[TMP43]], i32 noundef [[TMP44]], i32 noundef [[TMP45]], i32 noundef [[TMP46]]) #[[ATTR2:[0-9]+]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP47]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP7:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP48:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    [[ADD53:%.*]] = add nsw i64 [[TMP48]], 1
// CHECK-NEXT:    store i64 [[ADD53]], ptr [[DOTOMP_IV]], align 8
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_for_collapse_outer(int start, int end, int step) {
  #pragma omp for collapse(2)
  for (int i = start; i < end; i+=step) {
    #pragma omp unroll partial
    for (int j = start; j < end; j+=step)
        body(start, end, step, i, j);
  }
}

// CHECK-LABEL: @_Z16func_for_partialiii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_14:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_15:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_16:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_17:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_18:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_13]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_14]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_14]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP6]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP7]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP8]], 0
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// CHECK-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// CHECK-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// CHECK-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// CHECK-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_18]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP9]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_18]], align 4
// CHECK-NEXT:    store i32 [[TMP10]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP12]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add i32 [[TMP14]], 1
// CHECK-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP13]], [[ADD9]]
// CHECK-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP15]], 2
// CHECK-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP16]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD12:%.*]] = add i32 [[TMP18]], 2
// CHECK-NEXT:    [[CMP13:%.*]] = icmp ule i32 [[TMP17]], [[ADD12]]
// CHECK-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// CHECK-NEXT:    [[ADD14:%.*]] = add i32 [[TMP20]], 1
// CHECK-NEXT:    [[CMP15:%.*]] = icmp ule i32 [[TMP19]], [[ADD14]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP21:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP21]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_15]], align 4
// CHECK-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP23]], [[TMP24]]
// CHECK-NEXT:    [[ADD17:%.*]] = add i32 [[TMP22]], [[MUL16]]
// CHECK-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP25]], i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP29]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP9:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP30]], 1
// CHECK-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_for_partial(int start, int end, int step) {
  int i;
  #pragma omp for
  #pragma omp unroll partial
  for (i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z9func_fullv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 7, ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP0]], 17
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP1]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 3
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP10:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    ret void
//
void func_full() {
  #pragma omp unroll full
  for (int i = 7; i < 17; i += 3)
    body(i);
}

// CHECK-LABEL: @_Z14func_heuristiciii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP12:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    ret void
//
void func_heuristic(int start, int end, int step) {
  #pragma omp unroll
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z24func_parallel_for_factoriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_23:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_24:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_25:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_26:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_27:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_28:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_23]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_24]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[DOTCAPTURE_EXPR_25]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_24]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_25]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_25]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_26]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_27]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// CHECK-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// CHECK-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 7
// CHECK-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 7
// CHECK-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// CHECK-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_28]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP10]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_28]], align 4
// CHECK-NEXT:    store i32 [[TMP11]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[START_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[END_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[STEP_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_26]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_23]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_25]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add i32 [[TMP15]], 1
// CHECK-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP14]], [[ADD9]]
// CHECK-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP16]], 7
// CHECK-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP17]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD12:%.*]] = add i32 [[TMP19]], 7
// CHECK-NEXT:    [[CMP13:%.*]] = icmp ule i32 [[TMP18]], [[ADD12]]
// CHECK-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// CHECK-NEXT:    [[ADD14:%.*]] = add i32 [[TMP21]], 1
// CHECK-NEXT:    [[CMP15:%.*]] = icmp ule i32 [[TMP20]], [[ADD14]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP22:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP22]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_25]], align 4
// CHECK-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP24]], [[TMP25]]
// CHECK-NEXT:    [[ADD17:%.*]] = add i32 [[TMP23]], [[MUL16]]
// CHECK-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]], i32 noundef [[TMP29]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP30]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP13:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP31]], 1
// CHECK-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_parallel_for_factor(int start, int end, int step) {
  #pragma omp parallel for
  #pragma omp unroll partial(7)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z12func_partialiii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// CHECK-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP15:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    ret void
//
void func_partial(int start, int end, int step) {
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z13func_tile_foriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_33:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_34:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_35:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_36:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_37:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_38:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_39:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_40:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTFLOOR_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTTILE_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_33]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_34]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[DOTCAPTURE_EXPR_35]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_34]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_35]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_35]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_36]], align 4
// CHECK-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_37]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_37]], align 4
// CHECK-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// CHECK-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// CHECK-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// CHECK-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// CHECK-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_38]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// CHECK-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_39]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// CHECK-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// CHECK-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// CHECK-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 4
// CHECK-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 4
// CHECK-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// CHECK-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_40]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_40]], align 4
// CHECK-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_36]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_38]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_33]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_35]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// CHECK-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// CHECK-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 4
// CHECK-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD17]], ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP19]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 1
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD19:%.*]] = add i32 [[TMP22]], 4
// CHECK-NEXT:    [[CMP20:%.*]] = icmp ult i32 [[ADD18]], [[ADD19]]
// CHECK-NEXT:    br i1 [[CMP20]], label [[COND_TRUE:%.*]], label [[COND_FALSE:%.*]]
// CHECK:       cond.true:
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// CHECK-NEXT:    [[ADD21:%.*]] = add i32 [[TMP23]], 1
// CHECK-NEXT:    br label [[COND_END:%.*]]
// CHECK:       cond.false:
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD22:%.*]] = add i32 [[TMP24]], 4
// CHECK-NEXT:    br label [[COND_END]]
// CHECK:       cond.end:
// CHECK-NEXT:    [[COND:%.*]] = phi i32 [ [[ADD21]], [[COND_TRUE]] ], [ [[ADD22]], [[COND_FALSE]] ]
// CHECK-NEXT:    [[CMP23:%.*]] = icmp ult i32 [[TMP20]], [[COND]]
// CHECK-NEXT:    br i1 [[CMP23]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[MUL24:%.*]] = mul i32 [[TMP25]], 2
// CHECK-NEXT:    [[ADD25:%.*]] = add i32 0, [[MUL24]]
// CHECK-NEXT:    store i32 [[ADD25]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND26:%.*]]
// CHECK:       for.cond26:
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD27:%.*]] = add i32 [[TMP28]], 2
// CHECK-NEXT:    [[CMP28:%.*]] = icmp ule i32 [[TMP27]], [[ADD27]]
// CHECK-NEXT:    br i1 [[CMP28]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// CHECK-NEXT:    [[ADD29:%.*]] = add i32 [[TMP30]], 1
// CHECK-NEXT:    [[CMP30:%.*]] = icmp ule i32 [[TMP29]], [[ADD29]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND26]] ], [ [[CMP30]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body31:
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_35]], align 4
// CHECK-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// CHECK-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// CHECK-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND26]], !llvm.loop [[LOOP16:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[FOR_INC34:%.*]]
// CHECK:       for.inc34:
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// CHECK-NEXT:    store i32 [[INC35]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP17:![0-9]+]]
// CHECK:       for.end36:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// CHECK-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_tile_for(int start, int end, int step) {
  #pragma omp for
  #pragma omp tile sizes(4)
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z20func_unroll_for_attriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_41:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_42:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_43:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_44:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_45:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_46:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_47:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_48:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_41]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_42]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[DOTCAPTURE_EXPR_43]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_42]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_43]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_43]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_44]], align 4
// CHECK-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_45]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_45]], align 4
// CHECK-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// CHECK-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// CHECK-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// CHECK-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// CHECK-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_46]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// CHECK-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_47]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// CHECK-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// CHECK-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// CHECK-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// CHECK-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// CHECK-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// CHECK-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_48]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_48]], align 4
// CHECK-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// CHECK-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// CHECK-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// CHECK-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// CHECK-NEXT:    [[CMP19:%.*]] = icmp ule i32 [[TMP20]], [[ADD18]]
// CHECK-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// CHECK-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// CHECK-NEXT:    [[CMP21:%.*]] = icmp ule i32 [[TMP22]], [[ADD20]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// CHECK-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// CHECK-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND24:%.*]]
// CHECK:       for.cond24:
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// CHECK-NEXT:    [[CMP26:%.*]] = icmp ule i32 [[TMP27]], [[ADD25]]
// CHECK-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// CHECK:       land.rhs27:
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// CHECK-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// CHECK-NEXT:    [[CMP29:%.*]] = icmp ule i32 [[TMP29]], [[ADD28]]
// CHECK-NEXT:    br label [[LAND_END30]]
// CHECK:       land.end30:
// CHECK-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// CHECK-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body31:
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_43]], align 4
// CHECK-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// CHECK-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// CHECK-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND24]], !llvm.loop [[LOOP18:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[FOR_INC34:%.*]]
// CHECK:       for.inc34:
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// CHECK-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP19:![0-9]+]]
// CHECK:       for.end36:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// CHECK-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_unroll_for_attr(int start, int end, int step) {
  [[omp::sequence(directive(for), directive(unroll partial), directive(unroll partial))]]
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK-LABEL: @_Z15func_unroll_foriii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_49:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_50:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_51:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_52:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_53:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_54:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_55:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCAPTURE_EXPR_56:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[START:%.*]], ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[END:%.*]], ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[STEP:%.*]], ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_49]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_50]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[DOTCAPTURE_EXPR_51]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_50]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// CHECK-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// CHECK-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_51]], align 4
// CHECK-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_51]], align 4
// CHECK-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// CHECK-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// CHECK-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_52]], align 4
// CHECK-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// CHECK-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// CHECK-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_53]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_53]], align 4
// CHECK-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// CHECK-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// CHECK-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// CHECK-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// CHECK-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// CHECK-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_54]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// CHECK-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// CHECK-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_55]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// CHECK-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// CHECK-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// CHECK-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// CHECK-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// CHECK-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// CHECK-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_56]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// CHECK-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// CHECK:       omp.precond.then:
// CHECK-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_56]], align 4
// CHECK-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// CHECK-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// CHECK:       omp.inner.for.cond:
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// CHECK-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// CHECK-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// CHECK-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// CHECK:       omp.inner.for.body:
// CHECK-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// CHECK-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// CHECK-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND:%.*]]
// CHECK:       for.cond:
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// CHECK-NEXT:    [[CMP19:%.*]] = icmp ule i32 [[TMP20]], [[ADD18]]
// CHECK-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// CHECK:       land.rhs:
// CHECK-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// CHECK-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// CHECK-NEXT:    [[CMP21:%.*]] = icmp ule i32 [[TMP22]], [[ADD20]]
// CHECK-NEXT:    br label [[LAND_END]]
// CHECK:       land.end:
// CHECK-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// CHECK-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// CHECK:       for.body:
// CHECK-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// CHECK-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// CHECK-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND24:%.*]]
// CHECK:       for.cond24:
// CHECK-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// CHECK-NEXT:    [[CMP26:%.*]] = icmp ule i32 [[TMP27]], [[ADD25]]
// CHECK-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// CHECK:       land.rhs27:
// CHECK-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// CHECK-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// CHECK-NEXT:    [[CMP29:%.*]] = icmp ule i32 [[TMP29]], [[ADD28]]
// CHECK-NEXT:    br label [[LAND_END30]]
// CHECK:       land.end30:
// CHECK-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// CHECK-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// CHECK:       for.body31:
// CHECK-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// CHECK-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_51]], align 4
// CHECK-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// CHECK-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// CHECK-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// CHECK-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// CHECK-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// CHECK-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// CHECK-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// CHECK-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// CHECK-NEXT:    br label [[FOR_INC:%.*]]
// CHECK:       for.inc:
// CHECK-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// CHECK-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND24]], !llvm.loop [[LOOP20:![0-9]+]]
// CHECK:       for.end:
// CHECK-NEXT:    br label [[FOR_INC34:%.*]]
// CHECK:       for.inc34:
// CHECK-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// CHECK-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// CHECK-NEXT:    br label [[FOR_COND]], !llvm.loop [[LOOP21:![0-9]+]]
// CHECK:       for.end36:
// CHECK-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// CHECK:       omp.body.continue:
// CHECK-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// CHECK:       omp.inner.for.inc:
// CHECK-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// CHECK-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// CHECK-NEXT:    br label [[OMP_INNER_FOR_COND]]
// CHECK:       omp.inner.for.end:
// CHECK-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// CHECK:       omp.loop.exit:
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// CHECK-NEXT:    br label [[OMP_PRECOND_END]]
// CHECK:       omp.precond.end:
// CHECK-NEXT:    ret void
//
void func_unroll_for(int start, int end, int step) {
  #pragma omp for
  #pragma omp unroll partial
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[MUSTPROGRESS:![0-9]+]], [[UC4:![0-9]+]], [[EN:![0-9]+]]}
// CHECK: [[MUSTPROGRESS]] = !{!"llvm.loop.mustprogress"}
// CHECK: [[UC4]] = !{!"llvm.loop.unroll.count", i32 4}
// CHECK: [[EN]] = !{!"llvm.loop.unroll.enable"}

// CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[MUSTPROGRESS]], [[UC2:![0-9]+]]}
// CHECK: [[UC2]] = !{!"llvm.loop.unroll.count", i32 2}

// CHECK: [[LOOP9]] = distinct !{[[LOOP9]], [[MUSTPROGRESS]], [[UC2]]}

// CHECK: [[LOOP10]] = distinct !{[[LOOP10]], [[MUSTPROGRESS]], [[FULL:![0-9]+]]}
// CHECK: [[FULL]] = !{!"llvm.loop.unroll.full"}

// CHECK: [[LOOP12]] = distinct !{[[LOOP12]], [[MUSTPROGRESS]], [[EN]]}

// CHECK: [[LOOP13]] = distinct !{[[LOOP13]], [[MUSTPROGRESS]], [[UC7:![0-9]+]]}
// CHECK: [[UC7]] = !{!"llvm.loop.unroll.count", i32 7}

// CHECK: [[LOOP15]] = distinct !{[[LOOP15]], [[MUSTPROGRESS]], [[EN]]}
// CHECK: [[LOOP16]] = distinct !{[[LOOP16]], [[MUSTPROGRESS]], [[UC2]]}
// CHECK: [[LOOP17]] = distinct !{[[LOOP17]], [[MUSTPROGRESS]]}
// CHECK: [[LOOP18]] = distinct !{[[LOOP18]], [[MUSTPROGRESS]], [[UC2]]}
// CHECK: [[LOOP19]] = distinct !{[[LOOP19]], [[MUSTPROGRESS]], [[UC2]]}
// CHECK: [[LOOP20]] = distinct !{[[LOOP20]], [[MUSTPROGRESS]], [[UC2]]}
// CHECK: [[LOOP21]] = distinct !{[[LOOP21]], [[MUSTPROGRESS]], [[UC2]]}
// end INTEL_COLLAB
