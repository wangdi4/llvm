// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --functions "func_*" --check-globals --prefix-filecheck-ir-name _ --version 3

// NOTE: Some editing added to match the loop metadata and remove version line.

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=0 \
// RUN:  -fopenmp-version=51 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT0

// RUN: %clang_cc1 -emit-llvm -o - -fopenmp -fopenmp-late-outline \
// RUN:  -fopenmp-loop-rotation-control=1 \
// RUN:  -fopenmp-version=51 -triple x86_64-unknown-linux-gnu %s \
// RUN:  | FileCheck %s --check-prefix ROT1

// FE outlining is used for this directive. This test is to ensure that it
// works with the late-outlining option.

void body(...) {}

// ROT0-LABEL: define dso_local void @_Z11func_factoriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT0-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT0-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !3
// ROT0:       for.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z11func_factoriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !3
// ROT1:       for.end:
// ROT1-NEXT:    ret void
//
void func_factor(int start, int end, int step) {
  #pragma omp unroll partial(4)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z23func_for_collapse_outeriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1:[0-9]+]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[_TMP1:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP10:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[J:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_5:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP6:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_7:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_J:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_J:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP10]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP3]], ptr [[J]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP4]], ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP5]], ptr [[DOTCAPTURE_EXPR_5]], align 4
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP6]], ptr [[DOTNEW_STEP6]], align 4
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_5]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP7]], [[TMP8]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB2]], [[TMP9]]
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP10]]
// ROT0-NEXT:    [[SUB3:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT0-NEXT:    [[ADD4:%.*]] = add i32 [[TMP11]], 1
// ROT0-NEXT:    store i32 [[ADD4]], ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[TMP12]], [[TMP13]]
// ROT0-NEXT:    [[SUB6:%.*]] = sub i32 [[SUB5]], 1
// ROT0-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT0-NEXT:    [[ADD7:%.*]] = add i32 [[SUB6]], [[TMP14]]
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT0-NEXT:    [[DIV8:%.*]] = udiv i32 [[ADD7]], [[TMP15]]
// ROT0-NEXT:    [[CONV:%.*]] = zext i32 [[DIV8]] to i64
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[SUB9:%.*]] = sub i32 [[TMP16]], 0
// ROT0-NEXT:    [[SUB10:%.*]] = sub i32 [[SUB9]], 1
// ROT0-NEXT:    [[ADD11:%.*]] = add i32 [[SUB10]], 2
// ROT0-NEXT:    [[DIV12:%.*]] = udiv i32 [[ADD11]], 2
// ROT0-NEXT:    [[CONV13:%.*]] = zext i32 [[DIV12]] to i64
// ROT0-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], [[CONV13]]
// ROT0-NEXT:    [[SUB14:%.*]] = sub nsw i64 [[MUL]], 1
// ROT0-NEXT:    store i64 [[SUB14]], ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP17]], [[TMP18]]
// ROT0-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       land.lhs.true:
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[CMP15:%.*]] = icmp ult i32 0, [[TMP19]]
// ROT0-NEXT:    br i1 [[CMP15]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT0-NEXT:    store i64 [[TMP20]], ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_J]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_J]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT0-NEXT:    store i64 [[TMP22]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT0-NEXT:    [[CMP16:%.*]] = icmp sle i64 [[TMP23]], [[TMP24]]
// ROT0-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT0-NEXT:    [[CONV17:%.*]] = sext i32 [[TMP25]] to i64
// ROT0-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[SUB18:%.*]] = sub i32 [[TMP27]], 0
// ROT0-NEXT:    [[SUB19:%.*]] = sub i32 [[SUB18]], 1
// ROT0-NEXT:    [[ADD20:%.*]] = add i32 [[SUB19]], 2
// ROT0-NEXT:    [[DIV21:%.*]] = udiv i32 [[ADD20]], 2
// ROT0-NEXT:    [[MUL22:%.*]] = mul i32 1, [[DIV21]]
// ROT0-NEXT:    [[CONV23:%.*]] = zext i32 [[MUL22]] to i64
// ROT0-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP26]], [[CONV23]]
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT0-NEXT:    [[CONV25:%.*]] = sext i32 [[TMP28]] to i64
// ROT0-NEXT:    [[MUL26:%.*]] = mul nsw i64 [[DIV24]], [[CONV25]]
// ROT0-NEXT:    [[ADD27:%.*]] = add nsw i64 [[CONV17]], [[MUL26]]
// ROT0-NEXT:    [[CONV28:%.*]] = trunc i64 [[ADD27]] to i32
// ROT0-NEXT:    store i32 [[CONV28]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP30:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[SUB29:%.*]] = sub i32 [[TMP31]], 0
// ROT0-NEXT:    [[SUB30:%.*]] = sub i32 [[SUB29]], 1
// ROT0-NEXT:    [[ADD31:%.*]] = add i32 [[SUB30]], 2
// ROT0-NEXT:    [[DIV32:%.*]] = udiv i32 [[ADD31]], 2
// ROT0-NEXT:    [[MUL33:%.*]] = mul i32 1, [[DIV32]]
// ROT0-NEXT:    [[CONV34:%.*]] = zext i32 [[MUL33]] to i64
// ROT0-NEXT:    [[DIV35:%.*]] = sdiv i64 [[TMP30]], [[CONV34]]
// ROT0-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT0-NEXT:    [[SUB36:%.*]] = sub i32 [[TMP32]], 0
// ROT0-NEXT:    [[SUB37:%.*]] = sub i32 [[SUB36]], 1
// ROT0-NEXT:    [[ADD38:%.*]] = add i32 [[SUB37]], 2
// ROT0-NEXT:    [[DIV39:%.*]] = udiv i32 [[ADD38]], 2
// ROT0-NEXT:    [[MUL40:%.*]] = mul i32 1, [[DIV39]]
// ROT0-NEXT:    [[CONV41:%.*]] = zext i32 [[MUL40]] to i64
// ROT0-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV35]], [[CONV41]]
// ROT0-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP29]], [[MUL42]]
// ROT0-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 2
// ROT0-NEXT:    [[ADD45:%.*]] = add nsw i64 0, [[MUL44]]
// ROT0-NEXT:    [[CONV46:%.*]] = trunc i64 [[ADD45]] to i32
// ROT0-NEXT:    store i32 [[CONV46]], ptr [[DOTUNROLLED_IV_J]], align 4
// ROT0-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// ROT0-NEXT:    store i32 [[TMP33]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// ROT0-NEXT:    [[ADD47:%.*]] = add i32 [[TMP35]], 2
// ROT0-NEXT:    [[CMP48:%.*]] = icmp ult i32 [[TMP34]], [[ADD47]]
// ROT0-NEXT:    br i1 [[CMP48]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT0-NEXT:    [[ADD49:%.*]] = add i32 [[TMP37]], 1
// ROT0-NEXT:    [[CMP50:%.*]] = icmp ult i32 [[TMP36]], [[ADD49]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP38:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP50]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP38]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT0-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT0-NEXT:    [[MUL51:%.*]] = mul i32 [[TMP40]], [[TMP41]]
// ROT0-NEXT:    [[ADD52:%.*]] = add i32 [[TMP39]], [[MUL51]]
// ROT0-NEXT:    store i32 [[ADD52]], ptr [[J]], align 4
// ROT0-NEXT:    [[TMP42:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP43:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP44:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[TMP46:%.*]] = load i32, ptr [[J]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP42]], i32 noundef [[TMP43]], i32 noundef [[TMP44]], i32 noundef [[TMP45]], i32 noundef [[TMP46]]) #[[ATTR2:[0-9]+]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP47]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !7
// ROT0:       for.end:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP48:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    [[ADD53:%.*]] = add nsw i64 [[TMP48]], 1
// ROT0-NEXT:    store i64 [[ADD53]], ptr [[DOTOMP_IV]], align 8
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z23func_for_collapse_outeriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1:[0-9]+]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[_TMP1:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_8:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_9:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP10:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[J:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_4:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_5:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP6:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_7:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_11:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_12:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i64, align 8
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_J:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_J:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP10]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP3]], ptr [[J]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP4]], ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP5]], ptr [[DOTCAPTURE_EXPR_5]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP6]], ptr [[DOTNEW_STEP6]], align 4
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_5]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP7]], [[TMP8]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB2]], [[TMP9]]
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP10]]
// ROT1-NEXT:    [[SUB3:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB3]], ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT1-NEXT:    [[ADD4:%.*]] = add i32 [[TMP11]], 1
// ROT1-NEXT:    store i32 [[ADD4]], ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[TMP12]], [[TMP13]]
// ROT1-NEXT:    [[SUB6:%.*]] = sub i32 [[SUB5]], 1
// ROT1-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT1-NEXT:    [[ADD7:%.*]] = add i32 [[SUB6]], [[TMP14]]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT1-NEXT:    [[DIV8:%.*]] = udiv i32 [[ADD7]], [[TMP15]]
// ROT1-NEXT:    [[CONV:%.*]] = zext i32 [[DIV8]] to i64
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[SUB9:%.*]] = sub i32 [[TMP16]], 0
// ROT1-NEXT:    [[SUB10:%.*]] = sub i32 [[SUB9]], 1
// ROT1-NEXT:    [[ADD11:%.*]] = add i32 [[SUB10]], 2
// ROT1-NEXT:    [[DIV12:%.*]] = udiv i32 [[ADD11]], 2
// ROT1-NEXT:    [[CONV13:%.*]] = zext i32 [[DIV12]] to i64
// ROT1-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], [[CONV13]]
// ROT1-NEXT:    [[SUB14:%.*]] = sub nsw i64 [[MUL]], 1
// ROT1-NEXT:    store i64 [[SUB14]], ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_9]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP17]], [[TMP18]]
// ROT1-NEXT:    br i1 [[CMP]], label [[LAND_LHS_TRUE:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       land.lhs.true:
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[CMP15:%.*]] = icmp ult i32 0, [[TMP19]]
// ROT1-NEXT:    br i1 [[CMP15]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i64 0, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    [[TMP20:%.*]] = load i64, ptr [[DOTCAPTURE_EXPR_12]], align 8
// ROT1-NEXT:    store i64 [[TMP20]], ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[TMP21:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.COLLAPSE"(i32 2), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_J]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i64 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i64 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i64 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_J]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP22:%.*]] = load i64, ptr [[DOTOMP_LB]], align 8
// ROT1-NEXT:    store i64 [[TMP22]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP23:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP24:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP16:%.*]] = icmp sle i64 [[TMP23]], [[TMP24]]
// ROT1-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_8]], align 4
// ROT1-NEXT:    [[CONV17:%.*]] = sext i32 [[TMP25]] to i64
// ROT1-NEXT:    [[TMP26:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[SUB18:%.*]] = sub i32 [[TMP27]], 0
// ROT1-NEXT:    [[SUB19:%.*]] = sub i32 [[SUB18]], 1
// ROT1-NEXT:    [[ADD20:%.*]] = add i32 [[SUB19]], 2
// ROT1-NEXT:    [[DIV21:%.*]] = udiv i32 [[ADD20]], 2
// ROT1-NEXT:    [[MUL22:%.*]] = mul i32 1, [[DIV21]]
// ROT1-NEXT:    [[CONV23:%.*]] = zext i32 [[MUL22]] to i64
// ROT1-NEXT:    [[DIV24:%.*]] = sdiv i64 [[TMP26]], [[CONV23]]
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTNEW_STEP10]], align 4
// ROT1-NEXT:    [[CONV25:%.*]] = sext i32 [[TMP28]] to i64
// ROT1-NEXT:    [[MUL26:%.*]] = mul nsw i64 [[DIV24]], [[CONV25]]
// ROT1-NEXT:    [[ADD27:%.*]] = add nsw i64 [[CONV17]], [[MUL26]]
// ROT1-NEXT:    [[CONV28:%.*]] = trunc i64 [[ADD27]] to i32
// ROT1-NEXT:    store i32 [[CONV28]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP29:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP30:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[SUB29:%.*]] = sub i32 [[TMP31]], 0
// ROT1-NEXT:    [[SUB30:%.*]] = sub i32 [[SUB29]], 1
// ROT1-NEXT:    [[ADD31:%.*]] = add i32 [[SUB30]], 2
// ROT1-NEXT:    [[DIV32:%.*]] = udiv i32 [[ADD31]], 2
// ROT1-NEXT:    [[MUL33:%.*]] = mul i32 1, [[DIV32]]
// ROT1-NEXT:    [[CONV34:%.*]] = zext i32 [[MUL33]] to i64
// ROT1-NEXT:    [[DIV35:%.*]] = sdiv i64 [[TMP30]], [[CONV34]]
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_11]], align 4
// ROT1-NEXT:    [[SUB36:%.*]] = sub i32 [[TMP32]], 0
// ROT1-NEXT:    [[SUB37:%.*]] = sub i32 [[SUB36]], 1
// ROT1-NEXT:    [[ADD38:%.*]] = add i32 [[SUB37]], 2
// ROT1-NEXT:    [[DIV39:%.*]] = udiv i32 [[ADD38]], 2
// ROT1-NEXT:    [[MUL40:%.*]] = mul i32 1, [[DIV39]]
// ROT1-NEXT:    [[CONV41:%.*]] = zext i32 [[MUL40]] to i64
// ROT1-NEXT:    [[MUL42:%.*]] = mul nsw i64 [[DIV35]], [[CONV41]]
// ROT1-NEXT:    [[SUB43:%.*]] = sub nsw i64 [[TMP29]], [[MUL42]]
// ROT1-NEXT:    [[MUL44:%.*]] = mul nsw i64 [[SUB43]], 2
// ROT1-NEXT:    [[ADD45:%.*]] = add nsw i64 0, [[MUL44]]
// ROT1-NEXT:    [[CONV46:%.*]] = trunc i64 [[ADD45]] to i32
// ROT1-NEXT:    store i32 [[CONV46]], ptr [[DOTUNROLLED_IV_J]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// ROT1-NEXT:    store i32 [[TMP33]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[DOTUNROLLED_IV_J]], align 4
// ROT1-NEXT:    [[ADD47:%.*]] = add i32 [[TMP35]], 2
// ROT1-NEXT:    [[CMP48:%.*]] = icmp ult i32 [[TMP34]], [[ADD47]]
// ROT1-NEXT:    br i1 [[CMP48]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_7]], align 4
// ROT1-NEXT:    [[ADD49:%.*]] = add i32 [[TMP37]], 1
// ROT1-NEXT:    [[CMP50:%.*]] = icmp ult i32 [[TMP36]], [[ADD49]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP38:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP50]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP38]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_4]], align 4
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTNEW_STEP6]], align 4
// ROT1-NEXT:    [[MUL51:%.*]] = mul i32 [[TMP40]], [[TMP41]]
// ROT1-NEXT:    [[ADD52:%.*]] = add i32 [[TMP39]], [[MUL51]]
// ROT1-NEXT:    store i32 [[ADD52]], ptr [[J]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP44:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP45:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP46:%.*]] = load i32, ptr [[J]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP42]], i32 noundef [[TMP43]], i32 noundef [[TMP44]], i32 noundef [[TMP45]], i32 noundef [[TMP46]]) #[[ATTR2:[0-9]+]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP47:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP47]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_J]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !7
// ROT1:       for.end:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP48:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[ADD53:%.*]] = add nsw i64 [[TMP48]], 1
// ROT1-NEXT:    store i64 [[ADD53]], ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP49:%.*]] = load i64, ptr [[DOTOMP_IV]], align 8
// ROT1-NEXT:    [[TMP50:%.*]] = load i64, ptr [[DOTOMP_UB]], align 8
// ROT1-NEXT:    [[CMP54:%.*]] = icmp sle i64 [[TMP49]], [[TMP50]]
// ROT1-NEXT:    br i1 [[CMP54]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP21]]) [ "DIR.OMP.END.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_for_collapse_outer(int start, int end, int step) {
  #pragma omp for collapse(2)
  for (int i = start; i < end; i+=step) {
    #pragma omp unroll partial
    for (int j = start; j < end; j+=step)
        body(start, end, step, i, j);
  }
}

// ROT0-LABEL: define dso_local void @_Z16func_for_partialiii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_14:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP15:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_16:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_17:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_18:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_14]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP15]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_14]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP3]], [[TMP4]]
// ROT0-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP5]]
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP6]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add i32 [[TMP7]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT0-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP8]], 0
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT0-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT0-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT0-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT0-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_18]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP9]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_18]], align 4
// ROT0-NEXT:    store i32 [[TMP10]], ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    store i32 [[TMP12]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[ADD9:%.*]] = add i32 [[TMP14]], 1
// ROT0-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP13]], [[ADD9]]
// ROT0-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul i32 [[TMP15]], 2
// ROT0-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP16]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD12:%.*]] = add i32 [[TMP18]], 2
// ROT0-NEXT:    [[CMP13:%.*]] = icmp ult i32 [[TMP17]], [[ADD12]]
// ROT0-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT0-NEXT:    [[ADD14:%.*]] = add i32 [[TMP20]], 1
// ROT0-NEXT:    [[CMP15:%.*]] = icmp ult i32 [[TMP19]], [[ADD14]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP21:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP21]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT0-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT0-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP23]], [[TMP24]]
// ROT0-NEXT:    [[ADD17:%.*]] = add i32 [[TMP22]], [[MUL16]]
// ROT0-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP25]], i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]]) #[[ATTR2]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP29]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !9
// ROT0:       for.end:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP30]], 1
// ROT0-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z16func_for_partialiii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_13:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_14:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP15:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_16:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_17:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_18:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_14]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTNEW_STEP15]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_14]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP3]], [[TMP4]]
// ROT1-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP5]]
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP6]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add i32 [[TMP7]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT1-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP8]], 0
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT1-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT1-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT1-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT1-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_18]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_17]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP9]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_18]], align 4
// ROT1-NEXT:    store i32 [[TMP10]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP12]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add i32 [[TMP14]], 1
// ROT1-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP13]], [[ADD9]]
// ROT1-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul i32 [[TMP15]], 2
// ROT1-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP16]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD12:%.*]] = add i32 [[TMP18]], 2
// ROT1-NEXT:    [[CMP13:%.*]] = icmp ult i32 [[TMP17]], [[ADD12]]
// ROT1-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_16]], align 4
// ROT1-NEXT:    [[ADD14:%.*]] = add i32 [[TMP20]], 1
// ROT1-NEXT:    [[CMP15:%.*]] = icmp ult i32 [[TMP19]], [[ADD14]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP21:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP21]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_13]], align 4
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTNEW_STEP15]], align 4
// ROT1-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP23]], [[TMP24]]
// ROT1-NEXT:    [[ADD17:%.*]] = add i32 [[TMP22]], [[MUL16]]
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP25]], i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]]) #[[ATTR2]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP29]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !9
// ROT1:       for.end:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP30]], 1
// ROT1-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD19:%.*]] = add i32 [[TMP32]], 1
// ROT1-NEXT:    [[CMP20:%.*]] = icmp ult i32 [[TMP31]], [[ADD19]]
// ROT1-NEXT:    br i1 [[CMP20]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP11]]) [ "DIR.OMP.END.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_for_partial(int start, int end, int step) {
  int i;
  #pragma omp for
  #pragma omp unroll partial
  for (i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z9func_fullv(
// ROT0-SAME: ) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 7, ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP0]], 17
// ROT0-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP1]])
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 3
// ROT0-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !10
// ROT0:       for.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z9func_fullv(
// ROT1-SAME: ) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 7, ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP0]], 17
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP1]])
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP2]], 3
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !10
// ROT1:       for.end:
// ROT1-NEXT:    ret void
//
void func_full() {
  #pragma omp unroll full
  for (int i = 7; i < 17; i += 3)
    body(i);
}

// ROT0-LABEL: define dso_local void @_Z14func_heuristiciii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT0-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT0-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !12
// ROT0:       for.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z14func_heuristiciii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !12
// ROT1:       for.end:
// ROT1-NEXT:    ret void
//
void func_heuristic(int start, int end, int step) {
  #pragma omp unroll
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z24func_parallel_for_factoriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_23:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_24:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP25:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_26:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_27:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_28:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_24]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP25]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_24]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT0-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT0-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT0-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 7
// ROT0-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 7
// ROT0-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT0-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_28]], align 4
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP10]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_28]], align 4
// ROT0-NEXT:    store i32 [[TMP11]], ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[START_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[END_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[STEP_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_26]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_23]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTNEW_STEP25]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[ADD9:%.*]] = add i32 [[TMP15]], 1
// ROT0-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP14]], [[ADD9]]
// ROT0-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul i32 [[TMP16]], 7
// ROT0-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP17]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD12:%.*]] = add i32 [[TMP19]], 7
// ROT0-NEXT:    [[CMP13:%.*]] = icmp ult i32 [[TMP18]], [[ADD12]]
// ROT0-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT0-NEXT:    [[ADD14:%.*]] = add i32 [[TMP21]], 1
// ROT0-NEXT:    [[CMP15:%.*]] = icmp ult i32 [[TMP20]], [[ADD14]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP22:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP22]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT0-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT0-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP24]], [[TMP25]]
// ROT0-NEXT:    [[ADD17:%.*]] = add i32 [[TMP23]], [[MUL16]]
// ROT0-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]], i32 noundef [[TMP29]]) #[[ATTR2]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP30]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !13
// ROT0:       for.end:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP31]], 1
// ROT0-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z24func_parallel_for_factoriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_23:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_24:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP25:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_26:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_27:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_28:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_24]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP25]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_24]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT1-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT1-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT1-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 7
// ROT1-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 7
// ROT1-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT1-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_28]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_27]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP10]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_28]], align 4
// ROT1-NEXT:    store i32 [[TMP11]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.PARALLEL.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[START_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[END_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[STEP_ADDR]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_26]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTCAPTURE_EXPR_23]], i32 0, i32 1), "QUAL.OMP.SHARED:TYPED"(ptr [[DOTNEW_STEP25]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add i32 [[TMP15]], 1
// ROT1-NEXT:    [[CMP10:%.*]] = icmp ult i32 [[TMP14]], [[ADD9]]
// ROT1-NEXT:    br i1 [[CMP10]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul i32 [[TMP16]], 7
// ROT1-NEXT:    [[ADD11:%.*]] = add i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD11]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP17]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD12:%.*]] = add i32 [[TMP19]], 7
// ROT1-NEXT:    [[CMP13:%.*]] = icmp ult i32 [[TMP18]], [[ADD12]]
// ROT1-NEXT:    br i1 [[CMP13]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_26]], align 4
// ROT1-NEXT:    [[ADD14:%.*]] = add i32 [[TMP21]], 1
// ROT1-NEXT:    [[CMP15:%.*]] = icmp ult i32 [[TMP20]], [[ADD14]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP22:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP15]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP22]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_23]], align 4
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTNEW_STEP25]], align 4
// ROT1-NEXT:    [[MUL16:%.*]] = mul i32 [[TMP24]], [[TMP25]]
// ROT1-NEXT:    [[ADD17:%.*]] = add i32 [[TMP23]], [[MUL16]]
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP26]], i32 noundef [[TMP27]], i32 noundef [[TMP28]], i32 noundef [[TMP29]]) #[[ATTR2]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP30]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !13
// ROT1:       for.end:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP31:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD18:%.*]] = add nuw i32 [[TMP31]], 1
// ROT1-NEXT:    store i32 [[ADD18]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD19:%.*]] = add i32 [[TMP33]], 1
// ROT1-NEXT:    [[CMP20:%.*]] = icmp ult i32 [[TMP32]], [[ADD19]]
// ROT1-NEXT:    br i1 [[CMP20]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP12]]) [ "DIR.OMP.END.PARALLEL.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_parallel_for_factor(int start, int end, int step) {
  #pragma omp parallel for
  #pragma omp unroll partial(7)
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z12func_partialiii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT0-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT0-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !15
// ROT0:       for.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z12func_partialiii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR0]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], [[TMP2]]
// ROT1-NEXT:    br i1 [[CMP]], label [[FOR_BODY:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP3]], i32 noundef [[TMP4]], i32 noundef [[TMP5]], i32 noundef [[TMP6]])
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add nsw i32 [[TMP8]], [[TMP7]]
// ROT1-NEXT:    store i32 [[ADD]], ptr [[I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !15
// ROT1:       for.end:
// ROT1-NEXT:    ret void
//
void func_partial(int start, int end, int step) {
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z13func_tile_foriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_33:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_34:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP35:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_36:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_37:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_38:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_39:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_40:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTFLOOR_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTTILE_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_34]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP35]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_34]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT0-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT0-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_37]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_37]], align 4
// ROT0-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT0-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT0-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT0-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT0-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT0-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT0-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT0-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT0-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT0-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 4
// ROT0-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 4
// ROT0-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT0-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_40]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_40]], align 4
// ROT0-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_36]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_38]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_33]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTNEW_STEP35]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT0-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT0-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 4
// ROT0-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD17]], ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP19]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT0-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 1
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD19:%.*]] = add i32 [[TMP22]], 4
// ROT0-NEXT:    [[CMP20:%.*]] = icmp ult i32 [[ADD18]], [[ADD19]]
// ROT0-NEXT:    br i1 [[CMP20]], label [[COND_TRUE:%.*]], label [[COND_FALSE:%.*]]
// ROT0:       cond.true:
// ROT0-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT0-NEXT:    [[ADD21:%.*]] = add i32 [[TMP23]], 1
// ROT0-NEXT:    br label [[COND_END:%.*]]
// ROT0:       cond.false:
// ROT0-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD22:%.*]] = add i32 [[TMP24]], 4
// ROT0-NEXT:    br label [[COND_END]]
// ROT0:       cond.end:
// ROT0-NEXT:    [[COND:%.*]] = phi i32 [ [[ADD21]], [[COND_TRUE]] ], [ [[ADD22]], [[COND_FALSE]] ]
// ROT0-NEXT:    [[CMP23:%.*]] = icmp ult i32 [[TMP20]], [[COND]]
// ROT0-NEXT:    br i1 [[CMP23]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[MUL24:%.*]] = mul i32 [[TMP25]], 2
// ROT0-NEXT:    [[ADD25:%.*]] = add i32 0, [[MUL24]]
// ROT0-NEXT:    store i32 [[ADD25]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND26:%.*]]
// ROT0:       for.cond26:
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD27:%.*]] = add i32 [[TMP28]], 2
// ROT0-NEXT:    [[CMP28:%.*]] = icmp ult i32 [[TMP27]], [[ADD27]]
// ROT0-NEXT:    br i1 [[CMP28]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT0-NEXT:    [[ADD29:%.*]] = add i32 [[TMP30]], 1
// ROT0-NEXT:    [[CMP30:%.*]] = icmp ult i32 [[TMP29]], [[ADD29]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND26]] ], [ [[CMP30]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body31:
// ROT0-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT0-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT0-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT0-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT0-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND26]], !llvm.loop !16
// ROT0:       for.end:
// ROT0-NEXT:    br label [[FOR_INC34:%.*]]
// ROT0:       for.inc34:
// ROT0-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT0-NEXT:    store i32 [[INC35]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !17
// ROT0:       for.end36:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT0-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z13func_tile_foriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_33:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_34:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP35:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_36:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_37:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_38:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_39:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_40:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTFLOOR_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTTILE_0_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_34]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP35]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_34]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT1-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT1-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_37]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_37]], align 4
// ROT1-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT1-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT1-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT1-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT1-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT1-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT1-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT1-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT1-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 4
// ROT1-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 4
// ROT1-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT1-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_40]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_39]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_40]], align 4
// ROT1-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_36]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_38]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTCAPTURE_EXPR_33]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTNEW_STEP35]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT1-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT1-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 4
// ROT1-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP19]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT1-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 1
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD19:%.*]] = add i32 [[TMP22]], 4
// ROT1-NEXT:    [[CMP20:%.*]] = icmp ult i32 [[ADD18]], [[ADD19]]
// ROT1-NEXT:    br i1 [[CMP20]], label [[COND_TRUE:%.*]], label [[COND_FALSE:%.*]]
// ROT1:       cond.true:
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_38]], align 4
// ROT1-NEXT:    [[ADD21:%.*]] = add i32 [[TMP23]], 1
// ROT1-NEXT:    br label [[COND_END:%.*]]
// ROT1:       cond.false:
// ROT1-NEXT:    [[TMP24:%.*]] = load i32, ptr [[DOTFLOOR_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD22:%.*]] = add i32 [[TMP24]], 4
// ROT1-NEXT:    br label [[COND_END]]
// ROT1:       cond.end:
// ROT1-NEXT:    [[COND:%.*]] = phi i32 [ [[ADD21]], [[COND_TRUE]] ], [ [[ADD22]], [[COND_FALSE]] ]
// ROT1-NEXT:    [[CMP23:%.*]] = icmp ult i32 [[TMP20]], [[COND]]
// ROT1-NEXT:    br i1 [[CMP23]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[MUL24:%.*]] = mul i32 [[TMP25]], 2
// ROT1-NEXT:    [[ADD25:%.*]] = add i32 0, [[MUL24]]
// ROT1-NEXT:    store i32 [[ADD25]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND26:%.*]]
// ROT1:       for.cond26:
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD27:%.*]] = add i32 [[TMP28]], 2
// ROT1-NEXT:    [[CMP28:%.*]] = icmp ult i32 [[TMP27]], [[ADD27]]
// ROT1-NEXT:    br i1 [[CMP28]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_36]], align 4
// ROT1-NEXT:    [[ADD29:%.*]] = add i32 [[TMP30]], 1
// ROT1-NEXT:    [[CMP30:%.*]] = icmp ult i32 [[TMP29]], [[ADD29]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND26]] ], [ [[CMP30]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body31:
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_33]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP35]], align 4
// ROT1-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT1-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT1-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND26]], !llvm.loop !16
// ROT1:       for.end:
// ROT1-NEXT:    br label [[FOR_INC34:%.*]]
// ROT1:       for.inc34:
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT1-NEXT:    store i32 [[INC35]], ptr [[DOTTILE_0_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !17
// ROT1:       for.end36:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT1-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD38:%.*]] = add i32 [[TMP43]], 1
// ROT1-NEXT:    [[CMP39:%.*]] = icmp ult i32 [[TMP42]], [[ADD38]]
// ROT1-NEXT:    br i1 [[CMP39]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_tile_for(int start, int end, int step) {
  #pragma omp for
  #pragma omp tile sizes(4)
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z20func_unroll_for_attriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_41:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_42:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP43:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_44:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_45:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_46:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_47:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_48:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_42]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP43]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_42]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT0-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT0-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_45]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_45]], align 4
// ROT0-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT0-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT0-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT0-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT0-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT0-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT0-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT0-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT0-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT0-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// ROT0-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// ROT0-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT0-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_48]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_48]], align 4
// ROT0-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT0-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT0-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// ROT0-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// ROT0-NEXT:    [[CMP19:%.*]] = icmp ult i32 [[TMP20]], [[ADD18]]
// ROT0-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT0-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// ROT0-NEXT:    [[CMP21:%.*]] = icmp ult i32 [[TMP22]], [[ADD20]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// ROT0-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// ROT0-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND24:%.*]]
// ROT0:       for.cond24:
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// ROT0-NEXT:    [[CMP26:%.*]] = icmp ult i32 [[TMP27]], [[ADD25]]
// ROT0-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// ROT0:       land.rhs27:
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT0-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// ROT0-NEXT:    [[CMP29:%.*]] = icmp ult i32 [[TMP29]], [[ADD28]]
// ROT0-NEXT:    br label [[LAND_END30]]
// ROT0:       land.end30:
// ROT0-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// ROT0-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body31:
// ROT0-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT0-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT0-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT0-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT0-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND24]], !llvm.loop !18
// ROT0:       for.end:
// ROT0-NEXT:    br label [[FOR_INC34:%.*]]
// ROT0:       for.inc34:
// ROT0-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT0-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !19
// ROT0:       for.end36:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT0-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z20func_unroll_for_attriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_41:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_42:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP43:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_44:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_45:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_46:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_47:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_48:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_42]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP43]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_42]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT1-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT1-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_45]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_45]], align 4
// ROT1-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT1-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT1-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT1-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT1-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT1-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT1-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT1-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT1-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// ROT1-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// ROT1-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT1-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_48]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_47]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_48]], align 4
// ROT1-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT1-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT1-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// ROT1-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// ROT1-NEXT:    [[CMP19:%.*]] = icmp ult i32 [[TMP20]], [[ADD18]]
// ROT1-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_46]], align 4
// ROT1-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// ROT1-NEXT:    [[CMP21:%.*]] = icmp ult i32 [[TMP22]], [[ADD20]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// ROT1-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// ROT1-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND24:%.*]]
// ROT1:       for.cond24:
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// ROT1-NEXT:    [[CMP26:%.*]] = icmp ult i32 [[TMP27]], [[ADD25]]
// ROT1-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// ROT1:       land.rhs27:
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_44]], align 4
// ROT1-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// ROT1-NEXT:    [[CMP29:%.*]] = icmp ult i32 [[TMP29]], [[ADD28]]
// ROT1-NEXT:    br label [[LAND_END30]]
// ROT1:       land.end30:
// ROT1-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// ROT1-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body31:
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_41]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP43]], align 4
// ROT1-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT1-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT1-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND24]], !llvm.loop !18
// ROT1:       for.end:
// ROT1-NEXT:    br label [[FOR_INC34:%.*]]
// ROT1:       for.inc34:
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT1-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !19
// ROT1:       for.end36:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT1-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD38:%.*]] = add i32 [[TMP43]], 1
// ROT1-NEXT:    [[CMP39:%.*]] = icmp ult i32 [[TMP42]], [[ADD38]]
// ROT1-NEXT:    br i1 [[CMP39]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_unroll_for_attr(int start, int end, int step) {
  [[omp::sequence(directive(for), directive(unroll partial), directive(unroll partial))]]
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

// ROT0-LABEL: define dso_local void @_Z15func_unroll_foriii(
// ROT0-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT0-NEXT:  entry:
// ROT0-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_49:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_50:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTNEW_STEP51:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_52:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_53:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_54:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_55:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTCAPTURE_EXPR_56:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT0-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT0-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_50]], align 4
// ROT0-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP51]], align 4
// ROT0-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_50]], align 4
// ROT0-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT0-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT0-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT0-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT0-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT0-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT0-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT0-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT0-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT0-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT0-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT0-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_53]], align 4
// ROT0-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_53]], align 4
// ROT0-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT0-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT0-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT0-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT0-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT0-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT0-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT0-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT0-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT0-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT0-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT0-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT0-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// ROT0-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// ROT0-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT0-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_56]], align 4
// ROT0-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT0-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT0-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT0:       omp.precond.then:
// ROT0-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_56]], align 4
// ROT0-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT0-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT0-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND:%.*]]
// ROT0:       omp.inner.for.cond:
// ROT0-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT0-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT0-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT0-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT0:       omp.inner.for.body:
// ROT0-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// ROT0-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT0-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND:%.*]]
// ROT0:       for.cond:
// ROT0-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// ROT0-NEXT:    [[CMP19:%.*]] = icmp ult i32 [[TMP20]], [[ADD18]]
// ROT0-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT0:       land.rhs:
// ROT0-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT0-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// ROT0-NEXT:    [[CMP21:%.*]] = icmp ult i32 [[TMP22]], [[ADD20]]
// ROT0-NEXT:    br label [[LAND_END]]
// ROT0:       land.end:
// ROT0-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// ROT0-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT0:       for.body:
// ROT0-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// ROT0-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// ROT0-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND24:%.*]]
// ROT0:       for.cond24:
// ROT0-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// ROT0-NEXT:    [[CMP26:%.*]] = icmp ult i32 [[TMP27]], [[ADD25]]
// ROT0-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// ROT0:       land.rhs27:
// ROT0-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT0-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// ROT0-NEXT:    [[CMP29:%.*]] = icmp ult i32 [[TMP29]], [[ADD28]]
// ROT0-NEXT:    br label [[LAND_END30]]
// ROT0:       land.end30:
// ROT0-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// ROT0-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT0:       for.body31:
// ROT0-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT0-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT0-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT0-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT0-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT0-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT0-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT0-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT0-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT0-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT0-NEXT:    br label [[FOR_INC:%.*]]
// ROT0:       for.inc:
// ROT0-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT0-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND24]], !llvm.loop !20
// ROT0:       for.end:
// ROT0-NEXT:    br label [[FOR_INC34:%.*]]
// ROT0:       for.inc34:
// ROT0-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT0-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT0-NEXT:    br label [[FOR_COND]], !llvm.loop !21
// ROT0:       for.end36:
// ROT0-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT0:       omp.body.continue:
// ROT0-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT0:       omp.inner.for.inc:
// ROT0-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT0-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT0-NEXT:    br label [[OMP_INNER_FOR_COND]]
// ROT0:       omp.inner.for.end:
// ROT0-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT0:       omp.loop.exit:
// ROT0-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT0-NEXT:    br label [[OMP_PRECOND_END]]
// ROT0:       omp.precond.end:
// ROT0-NEXT:    ret void
//
// ROT1-LABEL: define dso_local void @_Z15func_unroll_foriii(
// ROT1-SAME: i32 noundef [[START:%.*]], i32 noundef [[END:%.*]], i32 noundef [[STEP:%.*]]) #[[ATTR1]] {
// ROT1-NEXT:  entry:
// ROT1-NEXT:    [[START_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[END_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[STEP_ADDR:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[TMP:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_49:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_50:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTNEW_STEP51:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_52:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_53:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_54:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_55:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTCAPTURE_EXPR_56:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_IV:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_LB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTOMP_UB:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLLED_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV__UNROLLED_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    [[DOTUNROLL_INNER_IV_I:%.*]] = alloca i32, align 4
// ROT1-NEXT:    store i32 [[START]], ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[END]], ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[STEP]], ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP0:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP0]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP1]], ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP2]], ptr [[DOTCAPTURE_EXPR_50]], align 4
// ROT1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    store i32 [[TMP3]], ptr [[DOTNEW_STEP51]], align 4
// ROT1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_50]], align 4
// ROT1-NEXT:    [[TMP5:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT1-NEXT:    [[SUB:%.*]] = sub i32 [[TMP4]], [[TMP5]]
// ROT1-NEXT:    [[SUB1:%.*]] = sub i32 [[SUB]], 1
// ROT1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT1-NEXT:    [[ADD:%.*]] = add i32 [[SUB1]], [[TMP6]]
// ROT1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT1-NEXT:    [[DIV:%.*]] = udiv i32 [[ADD]], [[TMP7]]
// ROT1-NEXT:    [[SUB2:%.*]] = sub i32 [[DIV]], 1
// ROT1-NEXT:    store i32 [[SUB2]], ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT1-NEXT:    store i32 0, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP8:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT1-NEXT:    [[ADD3:%.*]] = add i32 [[TMP8]], 1
// ROT1-NEXT:    store i32 [[ADD3]], ptr [[DOTCAPTURE_EXPR_53]], align 4
// ROT1-NEXT:    [[TMP9:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_53]], align 4
// ROT1-NEXT:    [[SUB4:%.*]] = sub i32 [[TMP9]], 0
// ROT1-NEXT:    [[SUB5:%.*]] = sub i32 [[SUB4]], 1
// ROT1-NEXT:    [[ADD6:%.*]] = add i32 [[SUB5]], 2
// ROT1-NEXT:    [[DIV7:%.*]] = udiv i32 [[ADD6]], 2
// ROT1-NEXT:    [[SUB8:%.*]] = sub i32 [[DIV7]], 1
// ROT1-NEXT:    store i32 [[SUB8]], ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT1-NEXT:    [[ADD9:%.*]] = add i32 [[TMP10]], 1
// ROT1-NEXT:    store i32 [[ADD9]], ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT1-NEXT:    [[SUB10:%.*]] = sub i32 [[TMP11]], 0
// ROT1-NEXT:    [[SUB11:%.*]] = sub i32 [[SUB10]], 1
// ROT1-NEXT:    [[ADD12:%.*]] = add i32 [[SUB11]], 2
// ROT1-NEXT:    [[DIV13:%.*]] = udiv i32 [[ADD12]], 2
// ROT1-NEXT:    [[SUB14:%.*]] = sub i32 [[DIV13]], 1
// ROT1-NEXT:    store i32 [[SUB14]], ptr [[DOTCAPTURE_EXPR_56]], align 4
// ROT1-NEXT:    [[TMP12:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_55]], align 4
// ROT1-NEXT:    [[CMP:%.*]] = icmp ult i32 0, [[TMP12]]
// ROT1-NEXT:    br i1 [[CMP]], label [[OMP_PRECOND_THEN:%.*]], label [[OMP_PRECOND_END:%.*]]
// ROT1:       omp.precond.then:
// ROT1-NEXT:    store i32 0, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    [[TMP13:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_56]], align 4
// ROT1-NEXT:    store i32 [[TMP13]], ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[TMP14:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.LOOP"(), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.IV:TYPED"(ptr [[DOTOMP_IV]], i32 0), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr [[DOTOMP_LB]], i32 0, i32 1), "QUAL.OMP.NORMALIZED.UB:TYPED"(ptr [[DOTOMP_UB]], i32 0), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], i32 0, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr [[DOTUNROLL_INNER_IV_I]], i32 0, i32 1) ]
// ROT1-NEXT:    [[TMP15:%.*]] = load i32, ptr [[DOTOMP_LB]], align 4
// ROT1-NEXT:    store i32 [[TMP15]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP16:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP17:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD15:%.*]] = add i32 [[TMP17]], 1
// ROT1-NEXT:    [[CMP16:%.*]] = icmp ult i32 [[TMP16]], [[ADD15]]
// ROT1-NEXT:    br i1 [[CMP16]], label [[OMP_INNER_FOR_BODY_LH:%.*]], label [[OMP_INNER_FOR_END:%.*]]
// ROT1:       omp.inner.for.body.lh:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_BODY:%.*]]
// ROT1:       omp.inner.for.body:
// ROT1-NEXT:    [[TMP18:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[MUL:%.*]] = mul i32 [[TMP18]], 2
// ROT1-NEXT:    [[ADD17:%.*]] = add i32 0, [[MUL]]
// ROT1-NEXT:    store i32 [[ADD17]], ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP19:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP19]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND:%.*]]
// ROT1:       for.cond:
// ROT1-NEXT:    [[TMP20:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP21:%.*]] = load i32, ptr [[DOTUNROLLED_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD18:%.*]] = add i32 [[TMP21]], 2
// ROT1-NEXT:    [[CMP19:%.*]] = icmp ult i32 [[TMP20]], [[ADD18]]
// ROT1-NEXT:    br i1 [[CMP19]], label [[LAND_RHS:%.*]], label [[LAND_END:%.*]]
// ROT1:       land.rhs:
// ROT1-NEXT:    [[TMP22:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP23:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_54]], align 4
// ROT1-NEXT:    [[ADD20:%.*]] = add i32 [[TMP23]], 1
// ROT1-NEXT:    [[CMP21:%.*]] = icmp ult i32 [[TMP22]], [[ADD20]]
// ROT1-NEXT:    br label [[LAND_END]]
// ROT1:       land.end:
// ROT1-NEXT:    [[TMP24:%.*]] = phi i1 [ false, [[FOR_COND]] ], [ [[CMP21]], [[LAND_RHS]] ]
// ROT1-NEXT:    br i1 [[TMP24]], label [[FOR_BODY:%.*]], label [[FOR_END36:%.*]]
// ROT1:       for.body:
// ROT1-NEXT:    [[TMP25:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[MUL22:%.*]] = mul i32 [[TMP25]], 2
// ROT1-NEXT:    [[ADD23:%.*]] = add i32 0, [[MUL22]]
// ROT1-NEXT:    store i32 [[ADD23]], ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[TMP26:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    store i32 [[TMP26]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND24:%.*]]
// ROT1:       for.cond24:
// ROT1-NEXT:    [[TMP27:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP28:%.*]] = load i32, ptr [[DOTUNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[ADD25:%.*]] = add i32 [[TMP28]], 2
// ROT1-NEXT:    [[CMP26:%.*]] = icmp ult i32 [[TMP27]], [[ADD25]]
// ROT1-NEXT:    br i1 [[CMP26]], label [[LAND_RHS27:%.*]], label [[LAND_END30:%.*]]
// ROT1:       land.rhs27:
// ROT1-NEXT:    [[TMP29:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP30:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_52]], align 4
// ROT1-NEXT:    [[ADD28:%.*]] = add i32 [[TMP30]], 1
// ROT1-NEXT:    [[CMP29:%.*]] = icmp ult i32 [[TMP29]], [[ADD28]]
// ROT1-NEXT:    br label [[LAND_END30]]
// ROT1:       land.end30:
// ROT1-NEXT:    [[TMP31:%.*]] = phi i1 [ false, [[FOR_COND24]] ], [ [[CMP29]], [[LAND_RHS27]] ]
// ROT1-NEXT:    br i1 [[TMP31]], label [[FOR_BODY31:%.*]], label [[FOR_END:%.*]]
// ROT1:       for.body31:
// ROT1-NEXT:    [[TMP32:%.*]] = load i32, ptr [[DOTCAPTURE_EXPR_49]], align 4
// ROT1-NEXT:    [[TMP33:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[TMP34:%.*]] = load i32, ptr [[DOTNEW_STEP51]], align 4
// ROT1-NEXT:    [[MUL32:%.*]] = mul i32 [[TMP33]], [[TMP34]]
// ROT1-NEXT:    [[ADD33:%.*]] = add i32 [[TMP32]], [[MUL32]]
// ROT1-NEXT:    store i32 [[ADD33]], ptr [[I]], align 4
// ROT1-NEXT:    [[TMP35:%.*]] = load i32, ptr [[START_ADDR]], align 4
// ROT1-NEXT:    [[TMP36:%.*]] = load i32, ptr [[END_ADDR]], align 4
// ROT1-NEXT:    [[TMP37:%.*]] = load i32, ptr [[STEP_ADDR]], align 4
// ROT1-NEXT:    [[TMP38:%.*]] = load i32, ptr [[I]], align 4
// ROT1-NEXT:    call void (...) @_Z4bodyz(i32 noundef [[TMP35]], i32 noundef [[TMP36]], i32 noundef [[TMP37]], i32 noundef [[TMP38]]) #[[ATTR2]]
// ROT1-NEXT:    br label [[FOR_INC:%.*]]
// ROT1:       for.inc:
// ROT1-NEXT:    [[TMP39:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    [[INC:%.*]] = add i32 [[TMP39]], 1
// ROT1-NEXT:    store i32 [[INC]], ptr [[DOTUNROLL_INNER_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND24]], !llvm.loop !20
// ROT1:       for.end:
// ROT1-NEXT:    br label [[FOR_INC34:%.*]]
// ROT1:       for.inc34:
// ROT1-NEXT:    [[TMP40:%.*]] = load i32, ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    [[INC35:%.*]] = add i32 [[TMP40]], 1
// ROT1-NEXT:    store i32 [[INC35]], ptr [[DOTUNROLL_INNER_IV__UNROLLED_IV_I]], align 4
// ROT1-NEXT:    br label [[FOR_COND]], !llvm.loop !21
// ROT1:       for.end36:
// ROT1-NEXT:    br label [[OMP_BODY_CONTINUE:%.*]]
// ROT1:       omp.body.continue:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_INC:%.*]]
// ROT1:       omp.inner.for.inc:
// ROT1-NEXT:    [[TMP41:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[ADD37:%.*]] = add nuw i32 [[TMP41]], 1
// ROT1-NEXT:    store i32 [[ADD37]], ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP42:%.*]] = load i32, ptr [[DOTOMP_IV]], align 4
// ROT1-NEXT:    [[TMP43:%.*]] = load i32, ptr [[DOTOMP_UB]], align 4
// ROT1-NEXT:    [[ADD38:%.*]] = add i32 [[TMP43]], 1
// ROT1-NEXT:    [[CMP39:%.*]] = icmp ult i32 [[TMP42]], [[ADD38]]
// ROT1-NEXT:    br i1 [[CMP39]], label [[OMP_INNER_FOR_BODY]], label [[OMP_INNER_FOR_END_CRIT_EDGE:%.*]]
// ROT1:       omp.inner.for.end_crit_edge:
// ROT1-NEXT:    br label [[OMP_INNER_FOR_END]]
// ROT1:       omp.inner.for.end:
// ROT1-NEXT:    br label [[OMP_LOOP_EXIT:%.*]]
// ROT1:       omp.loop.exit:
// ROT1-NEXT:    call void @llvm.directive.region.exit(token [[TMP14]]) [ "DIR.OMP.END.LOOP"() ]
// ROT1-NEXT:    br label [[OMP_PRECOND_END]]
// ROT1:       omp.precond.end:
// ROT1-NEXT:    ret void
//
void func_unroll_for(int start, int end, int step) {
  #pragma omp for
  #pragma omp unroll partial
  #pragma omp unroll partial
  for (int i = start; i < end; i+=step)
    body(start, end, step, i);
}

//.
// ROT0: attributes #0 = { mustprogress noinline nounwind optnone "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+cx8,+mmx,+sse,+sse2,+x87" }
// ROT0: attributes #1 = { mustprogress noinline nounwind optnone "may-have-openmp-directive"="true" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+cx8,+mmx,+sse,+sse2,+x87" }
// ROT0: attributes #2 = { nounwind }
//.
// ROT1: attributes #0 = { mustprogress noinline nounwind optnone "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+cx8,+mmx,+sse,+sse2,+x87" }
// ROT1: attributes #1 = { mustprogress noinline nounwind optnone "may-have-openmp-directive"="true" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+cx8,+mmx,+sse,+sse2,+x87" }
// ROT1: attributes #2 = { nounwind }
//.
// ROT0: !0 = !{i32 1, !"wchar_size", i32 4}
// ROT0: !1 = !{i32 7, !"openmp", i32 51}
// ROT0: !3 = distinct !{!3, !4, !5, !6}
// ROT0: !4 = !{!"llvm.loop.mustprogress"}
// ROT0: !5 = !{!"llvm.loop.unroll.count", i32 4}
// ROT0: !6 = !{!"llvm.loop.unroll.enable"}
// ROT0: !7 = distinct !{!7, !4, !8}
// ROT0: !8 = !{!"llvm.loop.unroll.count", i32 2}
// ROT0: !9 = distinct !{!9, !4, !8}
// ROT0: !10 = distinct !{!10, !4, !11}
// ROT0: !11 = !{!"llvm.loop.unroll.full"}
// ROT0: !12 = distinct !{!12, !4, !6}
// ROT0: !13 = distinct !{!13, !4, !14}
// ROT0: !14 = !{!"llvm.loop.unroll.count", i32 7}
// ROT0: !15 = distinct !{!15, !4, !6}
// ROT0: !16 = distinct !{!16, !4, !8}
// ROT0: !17 = distinct !{!17, !4}
// ROT0: !18 = distinct !{!18, !4, !8}
// ROT0: !19 = distinct !{!19, !4, !8}
// ROT0: !20 = distinct !{!20, !4, !8}
// ROT0: !21 = distinct !{!21, !4, !8}
//.
// ROT1: !0 = !{i32 1, !"wchar_size", i32 4}
// ROT1: !1 = !{i32 7, !"openmp", i32 51}
// ROT1: !3 = distinct !{!3, !4, !5, !6}
// ROT1: !4 = !{!"llvm.loop.mustprogress"}
// ROT1: !5 = !{!"llvm.loop.unroll.count", i32 4}
// ROT1: !6 = !{!"llvm.loop.unroll.enable"}
// ROT1: !7 = distinct !{!7, !4, !8}
// ROT1: !8 = !{!"llvm.loop.unroll.count", i32 2}
// ROT1: !9 = distinct !{!9, !4, !8}
// ROT1: !10 = distinct !{!10, !4, !11}
// ROT1: !11 = !{!"llvm.loop.unroll.full"}
// ROT1: !12 = distinct !{!12, !4, !6}
// ROT1: !13 = distinct !{!13, !4, !14}
// ROT1: !14 = !{!"llvm.loop.unroll.count", i32 7}
// ROT1: !15 = distinct !{!15, !4, !6}
// ROT1: !16 = distinct !{!16, !4, !8}
// ROT1: !17 = distinct !{!17, !4}
// ROT1: !18 = distinct !{!18, !4, !8}
// ROT1: !19 = distinct !{!19, !4, !8}
// ROT1: !20 = distinct !{!20, !4, !8}
// ROT1: !21 = distinct !{!21, !4, !8}
//.
// end INTEL_COLLAB
