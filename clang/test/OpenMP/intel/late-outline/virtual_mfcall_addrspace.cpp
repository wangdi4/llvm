// INTEL_COLLAB
// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -opaque-pointers -triple x86_64-unknown-linux-gnu -fopenmp \
// RUN:  -fintel-compatibility -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN:  -fopenmp-targets=spir64 -emit-llvm-bc %s -o %t-host.bc

// RUN: %clang_cc1 -opaque-pointers -verify -triple spir64 -fopenmp \
// RUN:  -fintel-compatibility -fopenmp-late-outline -fopenmp-typed-clauses \
// RUN:  -fopenmp-targets=spir64 -fopenmp-is-device \
// RUN:  -fopenmp-host-ir-file-path %t-host.bc %s -emit-llvm -o - \
// RUN:  | FileCheck %s

// expected-no-diagnostics
struct S;

// CHECK-LABEL: @_Z1fP1SMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[S_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[S_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[S_ADDR_ASCAST:%.*]] = addrspacecast ptr [[S_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[S_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[S_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    [[P:%.*]] = load { i64, i64 }, ptr addrspace(4) [[TMP0:%.*]], align 8
// CHECK-NEXT:    store ptr addrspace(4) [[S:%.*]], ptr addrspace(4) [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[P]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 0), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[TMP1]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr addrspace(4) [[P_ADDR_ASCAST]], { i64, i64 } zeroinitializer, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[TMP3]] to ptr
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, ptr [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast ptr [[TMP6]] to ptr addrspace(4)
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[THIS_ADJUSTED]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr addrspace(4) [[VTABLE]], i64 [[TMP8]], !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load ptr, ptr addrspace(4) [[TMP9]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to ptr
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP10:%.*]] = phi ptr [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP10]](ptr addrspace(4) noundef align 1 dereferenceable_or_null(1) [[THIS_ADJUSTED]]) #[[ATTR4:[0-9]+]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void f(S *s, void (S::*p)()) {
   #pragma omp target
  (s->*p)();
}


struct __attribute__((visibility("default"))) A {
 #pragma omp declare target
  virtual void foo();
};

// CHECK-LABEL: @_Z6test_1P1A(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[P_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    store ptr addrspace(4) [[P:%.*]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 1), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[TMP0]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[TMP2]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds ptr, ptr addrspace(4) [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr addrspace(4) [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP3]](ptr addrspace(4) noundef align 8 dereferenceable_or_null(8) [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_1(A *p) {
 #pragma omp target
  p->foo();
}

struct __attribute__((visibility("hidden"))) [[clang::lto_visibility_public]] B {
 #pragma omp declare target
  virtual void foo();
};

// CHECK-LABEL: @_Z6test_2P1B(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[P_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    store ptr addrspace(4) [[P:%.*]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 2), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[TMP0]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[TMP2]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds ptr, ptr addrspace(4) [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr addrspace(4) [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP3]](ptr addrspace(4) noundef align 8 dereferenceable_or_null(8) [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_2(B *p) {
 #pragma omp target
  p->foo();
}

struct __attribute__((visibility("hidden"))) C {
 #pragma omp declare target
  virtual void foo();
  virtual void bar();
  #pragma omp end declare target
};

// CHECK-LABEL: @_Z6test_3P1C(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[P_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    store ptr addrspace(4) [[P:%.*]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 3), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[TMP0]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP0]], ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[TMP2]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds ptr, ptr addrspace(4) [[VTABLE]], i64 0
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr addrspace(4) [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP3]](ptr addrspace(4) noundef align 8 dereferenceable_or_null(8) [[TMP2]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP1]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_3(C *p) {
 #pragma omp target
  p->foo();
}


// CHECK-LABEL: @_Z6test_4P1C(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    store ptr addrspace(4) [[P:%.*]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[TMP0]], align 8
// CHECK-NEXT:    [[VFN:%.*]] = getelementptr inbounds ptr, ptr addrspace(4) [[VTABLE]], i64 1
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr addrspace(4) [[VFN]], align 8
// CHECK-NEXT:    call spir_func void [[TMP1]](ptr addrspace(4) noundef align 8 dereferenceable_or_null(8) [[TMP0]]) #[[ATTR5:[0-9]+]]
// CHECK-NEXT:    ret void
//
void test_4(C *p) {
  p->bar();
}

// CHECK-LABEL: @_Z6test_5P1CMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[Q_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[P_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[Q_ADDR_ASCAST:%.*]] = addrspacecast ptr [[Q_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[P_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    [[Q:%.*]] = load { i64, i64 }, ptr addrspace(4) [[TMP0:%.*]], align 8
// CHECK-NEXT:    store ptr addrspace(4) [[P:%.*]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[Q]], ptr addrspace(4) [[Q_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 4), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[TMP1]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr addrspace(4) [[Q_ADDR_ASCAST]], { i64, i64 } zeroinitializer, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[P_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, ptr addrspace(4) [[Q_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[TMP3]] to ptr
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, ptr [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast ptr [[TMP6]] to ptr addrspace(4)
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[THIS_ADJUSTED]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr addrspace(4) [[VTABLE]], i64 [[TMP8]], !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load ptr, ptr addrspace(4) [[TMP9]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to ptr
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP10:%.*]] = phi ptr [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP10]](ptr addrspace(4) noundef align 8 dereferenceable_or_null(8) [[THIS_ADJUSTED]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void test_5(C *p, void (C::*q)(void)) {
 #pragma omp target
  (p->*q)();
}


struct B1 {};
struct B2 {};
struct B3 : B2 {};
struct S : B1, B3 {};

// CHECK-LABEL: @_Z3fooP1SMS_FvvE(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[S_ADDR:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca { i64, i64 }, align 8
// CHECK-NEXT:    [[S_MAP_PTR_TMP:%.*]] = alloca ptr addrspace(4), align 8
// CHECK-NEXT:    [[S_ADDR_ASCAST:%.*]] = addrspacecast ptr [[S_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[P_ADDR_ASCAST:%.*]] = addrspacecast ptr [[P_ADDR]] to ptr addrspace(4)
// CHECK-NEXT:    [[S_MAP_PTR_TMP_ASCAST:%.*]] = addrspacecast ptr [[S_MAP_PTR_TMP]] to ptr addrspace(4)
// CHECK-NEXT:    [[P:%.*]] = load { i64, i64 }, ptr addrspace(4) [[TMP0:%.*]], align 8
// CHECK-NEXT:    store ptr addrspace(4) [[S:%.*]], ptr addrspace(4) [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store { i64, i64 } [[P]], ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[S_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call token @llvm.directive.region.entry() [ "DIR.OMP.TARGET"(), "QUAL.OMP.OFFLOAD.ENTRY.IDX"(i32 5), "QUAL.OMP.MAP.TOFROM"(ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[TMP1]], i64 0, i64 544, ptr null, ptr null), "QUAL.OMP.FIRSTPRIVATE:TYPED"(ptr addrspace(4) [[P_ADDR_ASCAST]], { i64, i64 } zeroinitializer, i32 1), "QUAL.OMP.PRIVATE:TYPED"(ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], ptr addrspace(4) null, i32 1) ]
// CHECK-NEXT:    store ptr addrspace(4) [[TMP1]], ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[S_MAP_PTR_TMP_ASCAST]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load { i64, i64 }, ptr addrspace(4) [[P_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[MEMPTR_ADJ:%.*]] = extractvalue { i64, i64 } [[TMP4]], 1
// CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast ptr addrspace(4) [[TMP3]] to ptr
// CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, ptr [[TMP5]], i64 [[MEMPTR_ADJ]]
// CHECK-NEXT:    [[THIS_ADJUSTED:%.*]] = addrspacecast ptr [[TMP6]] to ptr addrspace(4)
// CHECK-NEXT:    [[MEMPTR_PTR:%.*]] = extractvalue { i64, i64 } [[TMP4]], 0
// CHECK-NEXT:    [[TMP7:%.*]] = and i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[MEMPTR_ISVIRTUAL:%.*]] = icmp ne i64 [[TMP7]], 0
// CHECK-NEXT:    br i1 [[MEMPTR_ISVIRTUAL]], label [[MEMPTR_VIRTUAL:%.*]], label [[MEMPTR_NONVIRTUAL:%.*]]
// CHECK:       memptr.virtual:
// CHECK-NEXT:    [[VTABLE:%.*]] = load ptr addrspace(4), ptr addrspace(4) [[THIS_ADJUSTED]], align 8
// CHECK-NEXT:    [[TMP8:%.*]] = sub i64 [[MEMPTR_PTR]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr addrspace(4) [[VTABLE]], i64 [[TMP8]], !nosanitize !9
// CHECK-NEXT:    [[MEMPTR_VIRTUALFN:%.*]] = load ptr, ptr addrspace(4) [[TMP9]], align 8, !nosanitize !9
// CHECK-NEXT:    br label [[MEMPTR_END:%.*]]
// CHECK:       memptr.nonvirtual:
// CHECK-NEXT:    [[MEMPTR_NONVIRTUALFN:%.*]] = inttoptr i64 [[MEMPTR_PTR]] to ptr
// CHECK-NEXT:    br label [[MEMPTR_END]]
// CHECK:       memptr.end:
// CHECK-NEXT:    [[TMP10:%.*]] = phi ptr [ [[MEMPTR_VIRTUALFN]], [[MEMPTR_VIRTUAL]] ], [ [[MEMPTR_NONVIRTUALFN]], [[MEMPTR_NONVIRTUAL]] ]
// CHECK-NEXT:    call spir_func void [[TMP10]](ptr addrspace(4) noundef align 1 dereferenceable_or_null(1) [[THIS_ADJUSTED]]) #[[ATTR4]]
// CHECK-NEXT:    call void @llvm.directive.region.exit(token [[TMP2]]) [ "DIR.OMP.END.TARGET"() ]
// CHECK-NEXT:    ret void
//
void foo(S *s, void (S::*p)()) {
 #pragma omp target
  (s->*p)();
}

// end INTEL_COLLAB
